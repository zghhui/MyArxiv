<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-17T00:00:00Z">2024-09-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linguistic Bias in ChatGPT: Language Models Reinforce Dialect
  Discrimination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08818v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08818v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eve Fleisig, Genevieve Smith, Madeline Bossi, Ishita Rustagi, Xavier Yin, Dan Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a large-scale study of linguistic bias exhibited by ChatGPT
covering ten dialects of English (Standard American English, Standard British
English, and eight widely spoken non-"standard" varieties from around the
world). We prompted GPT-3.5 Turbo and GPT-4 with text by native speakers of
each variety and analyzed the responses via detailed linguistic feature
annotation and native speaker evaluation. We find that the models default to
"standard" varieties of English; based on evaluation by native speakers, we
also find that model responses to non-"standard" varieties consistently exhibit
a range of issues: stereotyping (19% worse than for "standard" varieties),
demeaning content (25% worse), lack of comprehension (9% worse), and
condescending responses (15% worse). We also find that if these models are
asked to imitate the writing style of prompts in non-"standard" varieties, they
produce text that exhibits lower comprehension of the input and is especially
prone to stereotyping. GPT-4 improves on GPT-3.5 in terms of comprehension,
warmth, and friendliness, but also exhibits a marked increase in stereotyping
(+18%). The results indicate that GPT-3.5 Turbo and GPT-4 can perpetuate
linguistic discrimination toward speakers of non-"standard" varieties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Schrodinger's Memory: Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wang, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Memory is the foundation of all human activities; without memory, it would be
nearly impossible for people to perform any task in daily life. With the
development of Large Language Models (LLMs), their language capabilities are
becoming increasingly comparable to those of humans. But do LLMs have memory?
Based on current performance, LLMs do appear to exhibit memory. So, what is the
underlying mechanism of this memory? Previous research has lacked a deep
exploration of LLMs' memory capabilities and the underlying theory. In this
paper, we use Universal Approximation Theorem (UAT) to explain the memory
mechanism in LLMs. We also conduct experiments to verify the memory
capabilities of various LLMs, proposing a new method to assess their abilities
based on these memory ability. We argue that LLM memory operates like
Schr\"odinger's memory, meaning that it only becomes observable when a specific
memory is queried. We can only determine if the model retains a memory based on
its output in response to the query; otherwise, it remains indeterminate.
Finally, we expand on this concept by comparing the memory capabilities of the
human brain and LLMs, highlighting the similarities and differences in their
operational mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmenting Automatic Speech Recognition Models with Disfluency Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10177v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10177v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Amann, Zhaolin Li, Barbara Bruno, Jan Niehues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech disfluency commonly occurs in conversational and spontaneous speech.
However, standard Automatic Speech Recognition (ASR) models struggle to
accurately recognize these disfluencies because they are typically trained on
fluent transcripts. Current research mainly focuses on detecting disfluencies
within transcripts, overlooking their exact location and duration in the
speech. Additionally, previous work often requires model fine-tuning and
addresses limited types of disfluencies.
  In this work, we present an inference-only approach to augment any ASR model
with the ability to detect open-set disfluencies. We first demonstrate that ASR
models have difficulty transcribing speech disfluencies. Next, this work
proposes a modified Connectionist Temporal Classification(CTC)-based forced
alignment algorithm from \cite{kurzinger2020ctc} to predict word-level
timestamps while effectively capturing disfluent speech. Additionally, we
develop a model to classify alignment gaps between timestamps as either
containing disfluent speech or silence. This model achieves an accuracy of
81.62% and an F1-score of 80.07%. We test the augmentation pipeline of
alignment gap detection and classification on a disfluent dataset. Our results
show that we captured 74.13% of the words that were initially missed by the
transcription, demonstrating the potential of this pipeline for downstream
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SLT2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ jina-embeddings-v3: Multilingual Embeddings With Task LoRA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael Günther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Andreas Koukounas, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce jina-embeddings-v3, a novel text embedding model with 570
million parameters, achieves state-of-the-art performance on multilingual data
and long-context retrieval tasks, supporting context lengths of up to 8192
tokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)
adapters to generate high-quality embeddings for query-document retrieval,
clustering, classification, and text matching. Additionally, Matryoshka
Representation Learning is integrated into the training process, allowing
flexible truncation of embedding dimensions without compromising performance.
Evaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the
latest proprietary embeddings from OpenAI and Cohere on English tasks, while
achieving superior performance compared to multilingual-e5-large-instruct
across all multilingual tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, pp11-13 references, pp14-20 appendix and experiment tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Concept Depth: How Large Language Models Acquire Knowledge at
  Different Layers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07066v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07066v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, Fan Yang, Mengnan Du, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performances across a wide
range of tasks. However, the mechanisms by which these models encode tasks of
varying complexities remain poorly understood. In this paper, we explore the
hypothesis that LLMs process concepts of varying complexities in different
layers, introducing the idea of ``Concept Depth'' to suggest that more complex
concepts are typically acquired in deeper layers. Specifically, we categorize
concepts based on their level of abstraction, defining them in the order of
increasing complexity within factual, emotional, and inferential tasks. We
conduct extensive probing experiments using layer-wise representations across
various LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the
three domains of tasks. Our findings reveal that models could efficiently
conduct probing for simpler tasks in shallow layers, and more complex tasks
typically necessitate deeper layers for accurate understanding. Additionally,
we examine how external factors, such as adding noise to the input and
quantizing the model weights, might affect layer-wise representations. Our
findings suggest that these factors can impede the development of a conceptual
understanding of LLMs until deeper layers are explored. We hope that our
proposed concept and experimental insights will enhance the understanding of
the mechanisms underlying LLMs. Our codes are available at
\url{https://github.com/Luckfort/CD}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Synthetic Free-text Medical Records with Low
  Re-identification Risk using Masked Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09831v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09831v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Belkadi, Libo Ren, Nicolo Micheletti, Lifeng Han, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a system that generates synthetic free-text medical
records, such as discharge summaries, admission notes and doctor
correspondences, using Masked Language Modeling (MLM). Our system is designed
to preserve the critical information of the records while introducing
significant diversity and minimizing re-identification risk. The system
incorporates a de-identification component that uses Philter to mask Protected
Health Information (PHI), followed by a Medical Entity Recognition (NER) model
to retain key medical information. We explore various masking ratios and
mask-filling techniques to balance the trade-off between diversity and fidelity
in the synthetic outputs without affecting overall readability. Our results
demonstrate that the system can produce high-quality synthetic data with
significant diversity while achieving a HIPAA-compliant PHI recall rate of 0.96
and a low re-identification risk of 0.035. Furthermore, downstream evaluations
using a NER task reveal that the synthetic data can be effectively used to
train models with performance comparable to those trained on real data. The
flexibility of the system allows it to be adapted for specific use cases,
making it a valuable tool for privacy-preserving data generation in medical
research and healthcare applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added references and rephrased some sentences</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model Based Generative Error Correction: A Challenge and
  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao-Han Huck Yang, Taejin Park, Yuan Gong, Yuanchao Li, Zhehuai Chen, Yen-Ting Lin, Chen Chen, Yuchen Hu, Kunal Dhawan, Piotr Żelasko, Chao Zhang, Yun-Nung Chen, Yu Tsao, Jagadeesh Balam, Boris Ginsburg, Sabato Marco Siniscalchi, Eng Siong Chng, Peter Bell, Catherine Lai, Shinji Watanabe, Andreas Stolcke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given recent advances in generative AI technology, a key question is how
large language models (LLMs) can enhance acoustic modeling tasks using text
decoding results from a frozen, pretrained automatic speech recognition (ASR)
model. To explore new capabilities in language modeling for speech processing,
we introduce the generative speech transcription error correction (GenSEC)
challenge. This challenge comprises three post-ASR language modeling tasks: (i)
post-ASR transcription correction, (ii) speaker tagging, and (iii) emotion
recognition. These tasks aim to emulate future LLM-based agents handling
voice-based interfaces while remaining accessible to a broad audience by
utilizing open pretrained language models or agent-based APIs. We also discuss
insights from baseline evaluations, as well as lessons learned for designing
future evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE SLT 2024. The initial draft version has been done in December
  2023. Post-ASR Text Processing and Understanding Community:
  https://huggingface.co/GenSEC-LLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExploreSelf: Fostering User-driven Exploration and Reflection on
  Personal Challenges with Adaptive Guidance by Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09662v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09662v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwa Song, SoHyun Park, Sachin R. Pendse, Jessica Lee Schleider, Munmun De Choudhury, Young-Ho Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expressing stressful experiences in words is proven to improve mental and
physical health, but individuals often disengage with writing interventions as
they struggle to organize their thoughts and emotions. Reflective prompts have
been used to provide direction, and large language models (LLMs) have
demonstrated the potential to provide tailored guidance. Current systems often
limit users' flexibility to direct their reflections. We thus present
ExploreSelf, an LLM-driven application designed to empower users to control
their reflective journey. ExploreSelf allows users to receive adaptive support
through dynamically generated questions. Through an exploratory study with 19
participants, we examine how participants explore and reflect on personal
challenges using ExploreSelf. Our findings demonstrate that participants valued
the balance between guided support and freedom to control their reflective
journey, leading to deeper engagement and insight. Building on our findings, we
discuss implications for designing LLM-driven tools that promote user
empowerment through effective reflective practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages excluding reference and appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>-Powered Ensemble Learning for Paper Source Tracing: A GPU-Free
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09383v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09383v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunlong Chen, Junjun Wang, Zhaoqun Chen, Kunjin Chen, Yitian Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We participated in the KDD CUP 2024 paper source tracing competition and
achieved the 3rd place. This competition tasked participants with identifying
the reference sources (i.e., ref-sources, as referred to by the organizers of
the competition) of given academic papers. Unlike most teams that addressed
this challenge by fine-tuning pre-trained neural language models such as BERT
or ChatGLM, our primary approach utilized closed-source large language models
(LLMs). With recent advancements in LLM technology, closed-source LLMs have
demonstrated the capability to tackle complex reasoning tasks in zero-shot or
few-shot scenarios. Consequently, in the absence of GPUs, we employed
closed-source LLMs to directly generate predicted reference sources from the
provided papers. We further refined these predictions through ensemble
learning. Notably, our method was the only one among the award-winning
approaches that did not require the use of GPUs for model training. Code
available at https://github.com/Cklwanfifa/KDDCUP2024-PST.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling <span class="highlight-title">Entity</span>-Level Unlearning for Large Language Models: A
  Comprehensive Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15796v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15796v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weitao Ma, Xiaocheng Feng, Weihong Zhong, Lei Huang, Yangfan Ye, Xiachong Feng, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model unlearning has garnered increasing attention due to its
potential to address security and privacy concerns, leading to extensive
research in the field. However, much of this research has concentrated on
instance-level unlearning, specifically targeting the removal of predefined
instances containing sensitive content. This focus has left a significant gap
in the exploration of full entity-level unlearning, which is critical in
real-world scenarios such as copyright protection. To this end, we propose a
novel task of Entity-level unlearning, which aims to erase entity-related
knowledge from the target model completely. To thoroughly investigate this
task, we systematically evaluate trending unlearning algorithms, revealing that
current methods struggle to achieve effective entity-level unlearning. Then, we
further explore the factors that influence the performance of the unlearning
algorithms, identifying that knowledge coverage and the size of the forget set
play pivotal roles. Notably, our analysis also uncovers that entities
introduced through fine-tuning are more vulnerable to unlearning than
pre-trained entities. These findings collectively offer valuable insights for
advancing entity-level unlearning for LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fuse4Seg: Image-Level Fusion Based Multi-Modality Medical Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Guo, Weifeng Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although multi-modality medical image segmentation holds significant
potential for enhancing the diagnosis and understanding of complex diseases by
integrating diverse imaging modalities, existing methods predominantly rely on
feature-level fusion strategies. We argue the current feature-level fusion
strategy is prone to semantic inconsistencies and misalignments across various
imaging modalities because it merges features at intermediate layers in a
neural network without evaluative control. To mitigate this, we introduce a
novel image-level fusion based multi-modality medical image segmentation
method, Fuse4Seg, which is a bi-level learning framework designed to model the
intertwined dependencies between medical image segmentation and medical image
fusion. The image-level fusion process is seamlessly employed to guide and
enhance the segmentation results through a layered optimization approach.
Besides, the knowledge gained from the segmentation module can effectively
enhance the fusion module. This ensures that the resultant fused image is a
coherent representation that accurately amalgamates information from all
modalities. Moreover, we construct a BraTS-Fuse benchmark based on BraTS
dataset, which includes 2040 paired original images, multi-modal fusion images,
and ground truth. This benchmark not only serves image-level medical
segmentation but is also the largest dataset for medical image fusion to date.
Extensive experiments on several public datasets and our benchmark demonstrate
the superiority of our approach over prior state-of-the-art (SOTA)
methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GlobalMapNet: An Online Framework for Vectorized Global HD Map
  Construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10063v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10063v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anqi Shi, Yuze Cai, Xiangyu Chen, Jian Pu, Zeyu Fu, Hong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-definition (HD) maps are essential for autonomous driving systems.
Traditionally, an expensive and labor-intensive pipeline is implemented to
construct HD maps, which is limited in scalability. In recent years,
crowdsourcing and online mapping have emerged as two alternative methods, but
they have limitations respectively. In this paper, we provide a novel
methodology, namely global map construction, to perform direct generation of
vectorized global maps, combining the benefits of crowdsourcing and online
mapping. We introduce GlobalMapNet, the first online framework for vectorized
global HD map construction, which updates and utilizes a global map on the ego
vehicle. To generate the global map from scratch, we propose GlobalMapBuilder
to match and merge local maps continuously. We design a new algorithm, Map NMS,
to remove duplicate map elements and produce a clean map. We also propose
GlobalMapFusion to aggregate historical map information, improving consistency
of prediction. We examine GlobalMapNet on two widely recognized datasets,
Argoverse2 and nuScenes, showing that our framework is capable of generating
globally consistent results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ REG: Refined Generalized Focal Loss for Road Asset Detection on Thai
  Highways Using Vision-Based Detection and Segmentation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09877v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09877v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teerapong Panboonyuen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel framework for detecting and segmenting critical
road assets on Thai highways using an advanced Refined Generalized Focal Loss
(REG) formulation. Integrated into state-of-the-art vision-based detection and
segmentation models, the proposed method effectively addresses class imbalance
and the challenges of localizing small, underrepresented road elements,
including pavilions, pedestrian bridges, information signs, single-arm poles,
bus stops, warning signs, and concrete guardrails. To improve both detection
and segmentation accuracy, a multi-task learning strategy is adopted,
optimizing REG across multiple tasks. REG is further enhanced by incorporating
a spatial-contextual adjustment term, which accounts for the spatial
distribution of road assets, and a probabilistic refinement that captures
prediction uncertainty in complex environments, such as varying lighting
conditions and cluttered backgrounds. Our rigorous mathematical formulation
demonstrates that REG minimizes localization and classification errors by
applying adaptive weighting to hard-to-detect instances while down-weighting
easier examples. Experimental results show a substantial performance
improvement, achieving a mAP50 of 80.34 and an F1-score of 77.87, significantly
outperforming conventional methods. This research underscores the capability of
advanced loss function refinements to enhance the robustness and accuracy of
road asset detection and segmentation, thereby contributing to improved road
safety and infrastructure management. For an in-depth discussion of the
mathematical background and related methods, please refer to previous work
available at \url{https://github.com/kaopanboonyuen/REG}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VGG-Tex: A Vivid Geometry-Guided Facial Texture Estimation Model for
  High Fidelity Monocular 3D Face Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09740v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09740v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Wu, Ziqiao Peng, Xukun Zhou, Yunfei Cheng, Jun He, Hongyan Liu, Zhaoxin Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D face reconstruction from monocular images has promoted the development of
various applications such as augmented reality. Though existing methods have
made remarkable progress, most of them emphasize geometric reconstruction,
while overlooking the importance of texture prediction. To address this issue,
we propose VGG-Tex, a novel Vivid Geometry-Guided Facial Texture Estimation
model designed for High Fidelity Monocular 3D Face Reconstruction. The core of
this approach is leveraging 3D parametric priors to enhance the outcomes of 2D
UV texture estimation. Specifically, VGG-Tex includes a Facial Attributes
Encoding Module, a Geometry-Guided Texture Generator, and a Visibility-Enhanced
Texture Completion Module. These components are responsible for extracting
parametric priors, generating initial textures, and refining texture details,
respectively. Based on the geometry-texture complementarity principle, VGG-Tex
also introduces a Texture-guided Geometry Refinement Module to further balance
the overall fidelity of the reconstructed 3D faces, along with corresponding
losses. Comprehensive experiments demonstrate that our method significantly
improves texture reconstruction performance compared to existing
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning Based 3D Segmentation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.05423v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.05423v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong He, Hongshan Yu, Xiaoyan Liu, Zhengeng Yang, Wei Sun, Saeed Anwar, Ajmal Mian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D segmentation is a fundamental and challenging problem in computer vision
with applications in autonomous driving and robotics. It has received
significant attention from the computer vision, graphics and machine learning
communities. Conventional methods for 3D segmentation, based on hand-crafted
features and machine learning classifiers, lack generalization ability. Driven
by their success in 2D computer vision, deep learning techniques have recently
become the tool of choice for 3D segmentation tasks. This has led to an influx
of many methods in the literature that have been evaluated on different
benchmark datasets. Whereas survey papers on RGB-D and point cloud segmentation
exist, there is a lack of a recent in-depth survey that covers all 3D data
modalities and application domains. This paper fills the gap and
comprehensively surveys the recent progress in deep learning-based 3D
segmentation techniques. We cover over 220 works from the last six years,
analyze their strengths and limitations, and discuss their competitive results
on benchmark datasets. The survey provides a summary of the most commonly used
pipelines and finally highlights promising research directions for the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 10 tables, 8 figures, update the segmentation method to
  2024, add the segmentation application in semantic map construction and
  cultural heritage preservation, change the paper format</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ jina-embeddings-v3: Multilingual Embeddings With Task LoRA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael Günther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Andreas Koukounas, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce jina-embeddings-v3, a novel text embedding model with 570
million parameters, achieves state-of-the-art performance on multilingual data
and long-context retrieval tasks, supporting context lengths of up to 8192
tokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)
adapters to generate high-quality embeddings for query-document retrieval,
clustering, classification, and text matching. Additionally, Matryoshka
Representation Learning is integrated into the training process, allowing
flexible truncation of embedding dimensions without compromising performance.
Evaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the
latest proprietary embeddings from OpenAI and Cohere on English tasks, while
achieving superior performance compared to multilingual-e5-large-instruct
across all multilingual tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, pp11-13 references, pp14-20 appendix and experiment tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Music auto-tagging in the long tail: A few-shot approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. Aleksandra Ma, Alexander Lerch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of digital music, using tags to efficiently organize and
retrieve music from extensive databases is crucial for music catalog owners.
Human tagging by experts is labor-intensive but mostly accurate, whereas
automatic tagging through supervised learning has approached satisfying
accuracy but is restricted to a predefined set of training tags. Few-shot
learning offers a viable solution to expand beyond this small set of predefined
tags by enabling models to learn from only a few human-provided examples to
understand tag meanings and subsequently apply these tags autonomously. We
propose to integrate few-shot learning methodology into multi-label music
auto-tagging by using features from pre-trained models as inputs to a
lightweight linear classifier, also known as a linear probe. We investigate
different popular pre-trained features, as well as different few-shot
parametrizations with varying numbers of classes and samples per class. Our
experiments demonstrate that a simple model with pre-trained features can
achieve performance close to state-of-the-art models while using significantly
less training data, such as 20 samples per tag. Additionally, our linear probe
performs competitively with leading models when trained on the entire training
dataset. The results show that this transfer learning-based few-shot approach
could effectively address the issue of automatically assigning long-tail tags
with only limited labeled data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Audio Engineering Society NY Show 2024 as a Peer
  Reviewed (Category 1) paper; typos corrected</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flash STU: Fast Spectral Transform Units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Y. Isabel Liu, Windsor Nguyen, Yagiz Devre, Evan Dogariu, Anirudha Majumdar, Elad Hazan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes an efficient, open source PyTorch implementation of the
Spectral Transform Unit. We investigate sequence prediction tasks over several
modalities including language, robotics, and simulated dynamical systems. We
find that for the same parameter count, the STU and its variants outperform the
Transformer as well as other leading state space models across various
modalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty Estimation of Transformers' Predictions via Topological
  Analysis of the Attention Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11295v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11295v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Kostenok, Daniil Cherniavskii, Alexey Zaytsev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based language models have set new benchmarks across a wide range
of NLP tasks, yet reliably estimating the uncertainty of their predictions
remains a significant challenge. Existing uncertainty estimation (UE)
techniques often fall short in classification tasks, either offering minimal
improvements over basic heuristics or relying on costly ensemble models.
Moreover, attempts to leverage common embeddings for UE in linear probing
scenarios have yielded only modest gains, indicating that alternative model
components should be explored.
  We tackle these limitations by harnessing the geometry of attention maps
across multiple heads and layers to assess model confidence. Our approach
extracts topological features from attention matrices, providing a
low-dimensional, interpretable representation of the model's internal dynamics.
Additionally, we introduce topological features to compare attention patterns
across heads and layers. Our method significantly outperforms existing UE
techniques on benchmarks for acceptability judgments and artificial text
detection, offering a more efficient and interpretable solution for uncertainty
estimation in large-scale language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mobility-GNN: a human mobility-based graph neural network for tracking
  and analyzing the spatial dynamics of the synthetic opioid crisis in the USA,
  2013-2020 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyue Xia, Kathleen Stewart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic opioids are the most common drugs involved in drug-involved
overdose mortalities in the U.S. The Center for Disease Control and Prevention
reported that in 2018, about 70% of all drug overdose deaths involved opioids
and 67% of all opioid-involved deaths were accounted for by synthetic opioids.
In this study, we investigated the spread of synthetic opioids between 2013 and
2020 in the U.S., and analyzed the relationship between the spatiotemporal
pattern of synthetic opioid-involved deaths and another key opioid, heroin, and
compared patterns of deaths involving these two types of drugs during this time
period. Spatial connections between counties were incorporated into a graph
convolutional neural network model to represent and analyze the spread of
synthetic opioid-involved deaths, and in the context of heroin-involved deaths.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Concept Depth: How Large Language Models Acquire Knowledge at
  Different Layers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07066v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07066v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, Fan Yang, Mengnan Du, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable performances across a wide
range of tasks. However, the mechanisms by which these models encode tasks of
varying complexities remain poorly understood. In this paper, we explore the
hypothesis that LLMs process concepts of varying complexities in different
layers, introducing the idea of ``Concept Depth'' to suggest that more complex
concepts are typically acquired in deeper layers. Specifically, we categorize
concepts based on their level of abstraction, defining them in the order of
increasing complexity within factual, emotional, and inferential tasks. We
conduct extensive probing experiments using layer-wise representations across
various LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the
three domains of tasks. Our findings reveal that models could efficiently
conduct probing for simpler tasks in shallow layers, and more complex tasks
typically necessitate deeper layers for accurate understanding. Additionally,
we examine how external factors, such as adding noise to the input and
quantizing the model weights, might affect layer-wise representations. Our
findings suggest that these factors can impede the development of a conceptual
understanding of LLMs until deeper layers are explored. We hope that our
proposed concept and experimental insights will enhance the understanding of
the mechanisms underlying LLMs. Our codes are available at
\url{https://github.com/Luckfort/CD}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variance-reduced first-order methods for deterministically constrained
  stochastic nonconvex optimization with strong convergence guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09906v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09906v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaosong Lu, Sanyou Mei, Yifeng Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study a class of deterministically constrained stochastic
optimization problems. Existing methods typically aim to find an
$\epsilon$-stochastic stationary point, where the expected violations of both
constraints and first-order stationarity are within a prescribed accuracy
$\epsilon$. However, in many practical applications, it is crucial that the
constraints be nearly satisfied with certainty, making such an
$\epsilon$-stochastic stationary point potentially undesirable due to the risk
of significant constraint violations. To address this issue, we propose
single-loop variance-reduced stochastic first-order methods, where the
stochastic gradient of the stochastic component is computed using either a
truncated recursive momentum scheme or a truncated Polyak momentum scheme for
variance reduction, while the gradient of the deterministic component is
computed exactly. Under the error bound condition with a parameter $\theta \geq
1$ and other suitable assumptions, we establish that the proposed methods
achieve a sample complexity and first-order operation complexity of $\widetilde
O(\epsilon^{-\max\{4, 2\theta\}})$ for finding a stronger $\epsilon$-stochastic
stationary point, where the constraint violation is within $\epsilon$ with
certainty, and the expected violation of first-order stationarity is within
$\epsilon$. To the best of our knowledge, this is the first work to develop
methods with provable complexity guarantees for finding an approximate
stochastic stationary point of such problems that nearly satisfies all
constraints with certainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fixed several typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Synthetic Free-text Medical Records with Low
  Re-identification Risk using Masked Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09831v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09831v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Belkadi, Libo Ren, Nicolo Micheletti, Lifeng Han, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a system that generates synthetic free-text medical
records, such as discharge summaries, admission notes and doctor
correspondences, using Masked Language Modeling (MLM). Our system is designed
to preserve the critical information of the records while introducing
significant diversity and minimizing re-identification risk. The system
incorporates a de-identification component that uses Philter to mask Protected
Health Information (PHI), followed by a Medical Entity Recognition (NER) model
to retain key medical information. We explore various masking ratios and
mask-filling techniques to balance the trade-off between diversity and fidelity
in the synthetic outputs without affecting overall readability. Our results
demonstrate that the system can produce high-quality synthetic data with
significant diversity while achieving a HIPAA-compliant PHI recall rate of 0.96
and a low re-identification risk of 0.035. Furthermore, downstream evaluations
using a NER task reveal that the synthetic data can be effectively used to
train models with performance comparable to those trained on real data. The
flexibility of the system allows it to be adapted for specific use cases,
making it a valuable tool for privacy-preserving data generation in medical
research and healthcare applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added references and rephrased some sentences</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model Based Generative Error Correction: A Challenge and
  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao-Han Huck Yang, Taejin Park, Yuan Gong, Yuanchao Li, Zhehuai Chen, Yen-Ting Lin, Chen Chen, Yuchen Hu, Kunal Dhawan, Piotr Żelasko, Chao Zhang, Yun-Nung Chen, Yu Tsao, Jagadeesh Balam, Boris Ginsburg, Sabato Marco Siniscalchi, Eng Siong Chng, Peter Bell, Catherine Lai, Shinji Watanabe, Andreas Stolcke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given recent advances in generative AI technology, a key question is how
large language models (LLMs) can enhance acoustic modeling tasks using text
decoding results from a frozen, pretrained automatic speech recognition (ASR)
model. To explore new capabilities in language modeling for speech processing,
we introduce the generative speech transcription error correction (GenSEC)
challenge. This challenge comprises three post-ASR language modeling tasks: (i)
post-ASR transcription correction, (ii) speaker tagging, and (iii) emotion
recognition. These tasks aim to emulate future LLM-based agents handling
voice-based interfaces while remaining accessible to a broad audience by
utilizing open pretrained language models or agent-based APIs. We also discuss
insights from baseline evaluations, as well as lessons learned for designing
future evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE SLT 2024. The initial draft version has been done in December
  2023. Post-ASR Text Processing and Understanding Community:
  https://huggingface.co/GenSEC-LLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TSELM: Target Speaker <span class="highlight-title">Extraction</span> using Discrete Tokens and Language
  Models <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07841v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07841v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beilong Tang, Bang Zeng, Ming Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose TSELM, a novel target speaker extraction network that leverages
discrete tokens and language models. TSELM utilizes multiple discretized layers
from WavLM as input tokens and incorporates cross-attention mechanisms to
integrate target speaker information. Language models are employed to capture
the sequence dependencies, while a scalable HiFi-GAN is used to reconstruct the
audio from the tokens. By applying a cross-entropy loss, TSELM models the
probability distribution of output tokens, thus converting the complex
regression problem of audio generation into a classification task. Experimental
results show that TSELM achieves excellent results in speech quality and
comparable results in speech intelligibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-16T00:00:00Z">2024-09-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">57</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RetrievalAttention: Accelerating Long-Context <span class="highlight-title">LLM</span> Inference via Vector
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based large Language Models (LLMs) become increasingly important
in various domains. However, the quadratic time complexity of attention
operation poses a significant challenge for scaling to longer contexts due to
the extremely high inference latency and GPU memory consumption for caching
key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free
approach to accelerate attention computation. To leverage the dynamic sparse
property of attention, RetrievalAttention builds approximate nearest neighbor
search (ANNS) indexes upon KV vectors in CPU memory and retrieves the most
relevant ones via vector search during generation. Due to the
out-of-distribution (OOD) between query vectors and key vectors, off-the-shelf
ANNS indexes still need to scan O(N) (usually 30% of all keys) data for
accurate retrieval, which fails to exploit the high sparsity.
RetrievalAttention first identifies the OOD challenge of ANNS-based attention,
and addresses it via an attention-aware vector search algorithm that can adapt
to queries and only access 1--3% of data, thus achieving a sub-linear time
complexity. RetrievalAttention greatly reduces the inference cost of
long-context LLM with much lower GPU memory requirements while maintaining the
model accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for
serving 128K tokens in LLMs with 8B parameters, which is capable of generating
one token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient Self-Learning Framework For Interactive Spoken Dialog
  Systems <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hitesh Tulsiani, David M. Chan, Shalini Ghosh, Garima Lalwani, Prabhat Pandey, Ankish Bansal, Sri Garimella, Ariya Rastrow, Björn Hoffmeister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialog systems, such as voice assistants, are expected to engage with users
in complex, evolving conversations. Unfortunately, traditional automatic speech
recognition (ASR) systems deployed in such applications are usually trained to
recognize each turn independently and lack the ability to adapt to the
conversational context or incorporate user feedback. In this work, we introduce
a general framework for ASR in dialog systems that can go beyond learning from
single-turn utterances and learn over time how to adapt to both explicit
supervision and implicit user feedback present in multi-turn conversations. We
accomplish that by leveraging advances in student-teacher learning and
context-aware dialog processing, and designing contrastive self-supervision
approaches with Ohm, a new online hard-negative mining approach. We show that
leveraging our new framework compared to traditional training leads to relative
WER reductions of close to 10% in real-world dialog systems, and up to 26% on
public synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ICML 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DILA: Dictionary Label Attention for Mechanistic Interpretability in
  High-dimensional Multi-label Medical Coding Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Wu, David Wu, Jimeng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting high-dimensional or extreme multilabels, such as in medical
coding, requires both accuracy and interpretability. Existing works often rely
on local interpretability methods, failing to provide comprehensive
explanations of the overall mechanism behind each label prediction within a
multilabel set. We propose a mechanistic interpretability module called
DIctionary Label Attention (\method) that disentangles uninterpretable dense
embeddings into a sparse embedding space, where each nonzero element (a
dictionary feature) represents a globally learned medical concept. Through
human evaluations, we show that our sparse embeddings are more human
understandable than its dense counterparts by at least 50 percent. Our
automated dictionary feature identification pipeline, leveraging large language
models (LLMs), uncovers thousands of learned medical concepts by examining and
summarizing the highest activating tokens for each dictionary feature. We
represent the relationships between dictionary features and medical codes
through a sparse interpretable matrix, enhancing the mechanistic and global
understanding of the model's predictions while maintaining competitive
performance and scalability without extensive human annotation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Language Modeling Can Elicit Search and Reasoning Capabilities on
  Logic Puzzles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kulin Shah, Nishanth Dikkala, Xin Wang, Rina Panigrahy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal language modeling using the Transformer architecture has yielded
remarkable capabilities in Large Language Models (LLMs) over the last few
years. However, the extent to which fundamental search and reasoning
capabilities emerged within LLMs remains a topic of ongoing debate. In this
work, we study if causal language modeling can learn a complex task such as
solving Sudoku puzzles. To solve a Sudoku, the model is first required to
search over all empty cells of the puzzle to decide on a cell to fill and then
apply an appropriate strategy to fill the decided cell. Sometimes, the
application of a strategy only results in thinning down the possible values in
a cell rather than concluding the exact value of the cell. In such cases,
multiple strategies are applied one after the other to fill a single cell. We
observe that Transformer models trained on this synthetic task can indeed learn
to solve Sudokus (our model solves $94.21\%$ of the puzzles fully correctly)
when trained on a logical sequence of steps taken by a solver. We find that
training Transformers with the logical sequence of steps is necessary and
without such training, they fail to learn Sudoku. We also extend our analysis
to Zebra puzzles (known as Einstein puzzles) and show that the model solves
$92.04 \%$ of the puzzles fully correctly. In addition, we study the internal
representations of the trained Transformer and find that through linear
probing, we can decode information about the set of possible values in any
given cell from them, pointing to the presence of a strong reasoning engine
implicit in the Transformer weights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating Classifier-Free Guidance in Diffusion Model-Based
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noah Buchanan, Susan Gauch, Quan Mai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a diffusion-based recommender system that incorporates
classifier-free guidance. Most current recommender systems provide
recommendations using conventional methods such as collaborative or
content-based filtering. Diffusion is a new approach to generative AI that
improves on previous generative AI approaches such as Variational Autoencoders
(VAEs) and Generative Adversarial Networks (GANs). We incorporate diffusion in
a recommender system that mirrors the sequence users take when browsing and
rating items. Although a few current recommender systems incorporate diffusion,
they do not incorporate classifier-free guidance, a new innovation in diffusion
models as a whole. In this paper, we present a diffusion recommender system
that augments the underlying recommender system model for improved performance
and also incorporates classifier-free guidance. Our findings show improvements
over state-of-the-art recommender systems for most metrics for several
recommendation tasks on a variety of datasets. In particular, our approach
demonstrates the potential to provide better recommendations when data is
sparse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Whisper: Speech-Based Meta-ICL for ASR on Low-Resource Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming-Hao Hsu, Kuan Po Huang, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Meta-Whisper, a novel approach to improve automatic
speech recognition (ASR) for low-resource languages using the Whisper model. By
leveraging Meta In-Context Learning (Meta-ICL) and a k-Nearest Neighbors (KNN)
algorithm for sample selection, Meta-Whisper enhances Whisper's ability to
recognize speech in unfamiliar languages without extensive fine-tuning.
Experiments on the ML-SUPERB dataset show that Meta-Whisper significantly
reduces the Character Error Rate (CER) for low-resource languages compared to
the original Whisper model. This method offers a promising solution for
developing more adaptable multilingual ASR systems, particularly for languages
with limited resources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Knowledge-Enhanced Disease Diagnosis Method Based on <span class="highlight-title">Prompt</span> Learning
  and BERT Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a knowledge-enhanced disease diagnosis method based on a
prompt learning framework. The method retrieves structured knowledge from
external knowledge graphs related to clinical cases, encodes it, and injects it
into the prompt templates to enhance the language model's understanding and
reasoning capabilities for the task.We conducted experiments on three public
datasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that the
proposed method significantly outperforms existing models across multiple
evaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC
dataset, 3.1% on the IMCS-V2-NER dataset,and 4.2% on the KUAKE-QTR dataset.
Additionally,ablation studies confirmed the critical role of the knowledge
injection module,as the removal of this module resulted in a significant drop
in F1 score. The experimental results demonstrate that the proposed method not
only effectively improves the accuracy of disease diagnosis but also enhances
the interpretability of the predictions, providing more reliable support and
evidence for clinical diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Knowledge Enhancement,Disease Diagnosis,Prompt
  Learning,BERT,Knowledge Graph</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instigating Cooperation among <span class="highlight-title">LLM</span> Agents Using Adaptive <span class="highlight-title">Information</span>
  Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiliang Chen,  Alireza,  Ilami, Nunzio Lore, Babak Heydari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel framework combining LLM agents as proxies for
human strategic behavior with reinforcement learning (RL) to engage these
agents in evolving strategic interactions within team environments. Our
approach extends traditional agent-based simulations by using strategic LLM
agents (SLA) and introducing dynamic and adaptive governance through a
pro-social promoting RL agent (PPA) that modulates information access across
agents in a network, optimizing social welfare and promoting pro-social
behavior. Through validation in iterative games, including the prisoner
dilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.
The PPA agent effectively learns to adjust information transparency, resulting
in enhanced cooperation rates. This framework offers significant insights into
AI-mediated social dynamics, contributing to the deployment of AI in real-world
team settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 2D or not 2D: How Does the Dimensionality of Gesture Representation
  Affect 3D Co-Speech Gesture Generation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Téo Guichoux, Laure Soulier, Nicolas Obin, Catherine Pelachaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Co-speech gestures are fundamental for communication. The advent of recent
deep learning techniques has facilitated the creation of lifelike, synchronous
co-speech gestures for Embodied Conversational Agents. "In-the-wild" datasets,
aggregating video content from platforms like YouTube via human pose detection
technologies, provide a feasible solution by offering 2D skeletal sequences
aligned with speech. Concurrent developments in lifting models enable the
conversion of these 2D sequences into 3D gesture databases. However, it is
important to note that the 3D poses estimated from the 2D extracted poses are,
in essence, approximations of the ground-truth, which remains in the 2D domain.
This distinction raises questions about the impact of gesture representation
dimensionality on the quality of generated motions - a topic that, to our
knowledge, remains largely unexplored. Our study examines the effect of using
either 2D or 3D joint coordinates as training data on the performance of
speech-to-gesture deep generative models. We employ a lifting model for
converting generated 2D pose sequences into 3D and assess how gestures created
directly in 3D stack up against those initially generated in 2D and then
converted to 3D. We perform an objective evaluation using widely used metrics
in the gesture generation field as well as a user study to qualitatively
evaluate the different approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2406.15111</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Sexism in German Online Newspaper Comments with Open-Source
  Text Embeddings (Team GDA, GermEval2024 Shared Task 1: GerMS-Detect, Subtasks
  1 and 2, Closed Track) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Bremm, Patrick Gustav Blaneck, Tobias Bornheim, Niklas Grieger, Stephan Bialonski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sexism in online media comments is a pervasive challenge that often manifests
subtly, complicating moderation efforts as interpretations of what constitutes
sexism can vary among individuals. We study monolingual and multilingual
open-source text embeddings to reliably detect sexism and misogyny in
German-language online comments from an Austrian newspaper. We observed
classifiers trained on text embeddings to mimic closely the individual
judgements of human annotators. Our method showed robust performance in the
GermEval 2024 GerMS-Detect Subtask 1 challenge, achieving an average macro F1
score of 0.597 (4th place, as reported on Codabench). It also accurately
predicted the distribution of human annotations in GerMS-Detect Subtask 2, with
an average Jensen-Shannon distance of 0.301 (2nd place). The computational
efficiency of our approach suggests potential for scalable applications across
various languages and linguistic contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The 20 questions game to distinguish large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gurvan Richardeau, Erwan Le Merrer, Camilla Penzo, Gilles Tredan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a parallel with the 20 questions game, we present a method to determine
whether two large language models (LLMs), placed in a black-box context, are
the same or not. The goal is to use a small set of (benign) binary questions,
typically under 20. We formalize the problem and first establish a baseline
using a random selection of questions from known benchmark datasets, achieving
an accuracy of nearly 100% within 20 questions. After showing optimal bounds
for this problem, we introduce two effective questioning heuristics able to
discriminate 22 LLMs by using half as many questions for the same task. These
methods offer significant advantages in terms of stealth and are thus of
interest to auditors or copyright owners facing suspicions of model leaks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MGSA: Multi-granularity Graph Structure Attention for Knowledge
  Graph-to-Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanshan Wang, Chun Zhang, Ning Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Knowledge Graph-to-Text Generation task aims to convert structured
knowledge graphs into coherent and human-readable natural language text. Recent
efforts in this field have focused on enhancing pre-trained language models
(PLMs) by incorporating graph structure information to capture the intricate
structure details of knowledge graphs. However, most of these approaches tend
to capture only single-granularity structure information, concentrating either
on the relationships between entities within the original graph or on the
relationships between words within the same entity or across different
entities. This narrow focus results in a significant limitation: models that
concentrate solely on entity-level structure fail to capture the nuanced
semantic relationships between words, while those that focus only on word-level
structure overlook the broader relationships between original entire entities.
To overcome these limitations, this paper introduces the Multi-granularity
Graph Structure Attention (MGSA), which is based on PLMs. The encoder of the
model architecture features an entity-level structure encoding module, a
word-level structure encoding module, and an aggregation module that
synthesizes information from both structure. This multi-granularity structure
encoding approach allows the model to simultaneously capture both entity-level
and word-level structure information, providing a more comprehensive
understanding of the knowledge graph's structure information, thereby
significantly improving the quality of the generated text. We conducted
extensive evaluations of the MGSA model using two widely recognized KG-to-Text
Generation benchmark datasets, WebNLG and EventNarrative, where it consistently
outperformed models that rely solely on single-granularity structure
information, demonstrating the effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReflectDiffu: Reflect between Emotion-intent Contagion and Mimicry for
  Empathetic Response Generation via a RL-Diffusion Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Yuan, Zixiang Di, Zhiqing Cui, Guisong Yang, Usman Naseem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empathetic response generation necessitates the integration of emotional and
intentional dynamics to foster meaningful interactions. Existing research
either neglects the intricate interplay between emotion and intent, leading to
suboptimal controllability of empathy, or resorts to large language models
(LLMs), which incur significant computational overhead. In this paper, we
introduce ReflectDiffu, a lightweight and comprehensive framework for
empathetic response generation. This framework incorporates emotion contagion
to augment emotional expressiveness and employs an emotion-reasoning mask to
pinpoint critical emotional elements. Additionally, it integrates intent
mimicry within reinforcement learning for refinement during diffusion. By
harnessing an intent twice reflect the mechanism of
Exploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional
decision-making into precise intent actions, thereby addressing empathetic
response misalignments stemming from emotional misrecognition. Through
reflection, the framework maps emotional states to intents, markedly enhancing
both response empathy and flexibility. Comprehensive experiments reveal that
ReflectDiffu outperforms existing models regarding relevance, controllability,
and informativeness, achieving state-of-the-art results in both automatic and
human evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes
  the Emoji Potential in <span class="highlight-title">LLM</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navya Jain, Zekun Wu, Cristian Munoz, Airlie Hilliard, Adriano Koshiyama, Emre Kazim, Philip Treleaven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the demand for human-like interactions with LLMs continues to grow, so
does the interest in manipulating their personality traits, which has emerged
as a key area of research. Methods like prompt-based In-Context Knowledge
Editing (IKE) and gradient-based Model Editor Networks (MEND) have been
explored but show irregularity and variability. IKE depends on the prompt,
leading to variability and sensitivity, while MEND yields inconsistent and
gibberish outputs. To address this, we employed Opinion QA Based
Parameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank
Adaptation (QLORA), to manipulate the Big Five personality traits: Openness,
Conscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,
models such as Mistral-7B-Instruct and Llama-2-7B-chat began generating emojis,
despite their absence in the PEFT data. For instance, Llama-2-7B-chat generated
emojis in 99.5% of extraversion-related test instances, while
Mistral-8B-Instruct did so in 92.5% of openness-related test instances.
Explainability analysis indicated that the LLMs used emojis intentionally to
express these traits. This paper provides a number of novel contributions.
First, introducing an Opinion QA dataset for PEFT-driven personality
manipulation; second, developing metric models to benchmark LLM personality
traits; third, demonstrating PEFT's superiority over IKE in personality
manipulation; and finally, analyzing and validating emoji usage through
explainability methods such as mechanistic interpretability and in-context
learning explainability methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to NeurIPS 2024 Workshop on Behavioral Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fit and Prune: Fast and Training-free Visual Token Pruning for
  Multi-modal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Ye, Qiong Wu, Wenhao Lin, Yiyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in Multimodal Large Language Models(MLLMs) often use large
image tokens to compensate the visual shortcoming of MLLMs, which not only
exhibits obvious redundancy but also greatly exacerbates the already high
computation. Token pruning is an effective solution for speeding up MLLMs, but
when and how to drop tokens still remains a challenge. In this paper, we
propose a novel and training-free approach for the effective visual token
pruning of MLLMs, termed FitPrune, which can quickly produce a complete pruning
recipe for MLLMs according to a pre-defined budget. Specifically, FitPrune
considers token pruning as a statistical problem of MLLM and its objective is
to find out an optimal pruning scheme that can minimize the divergence of the
attention distributions before and after pruning. In practice, FitPrune can be
quickly accomplished based on the attention statistics from a small batch of
inference data, avoiding the expensive trials of MLLMs. According to the
pruning recipe, an MLLM can directly remove the redundant visual tokens of
different examples during inference. To validate FitPrune, we apply it to a set
of recent MLLMs, including LLaVA-1.5, LLaVA-HR and LLaVA-NEXT, and conduct
extensive experiments on a set of benchmarks. The experimental results show
that our FitPrune can not only reduce the computational complexity to a large
extent, while retaining high performance, e.g., -54.9% FLOPs for LLaVA-NEXT
with only 0.5% accuracy drop. Notably, the pruning recipe can be obtained in
about 5 minutes. Our code is available at https://github.com/ywh187/FitPrune.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>s for clinical risk prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Rezk, Patricia Cabanillas Silva, Fried-Michael Dahlweid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study compares the efficacy of GPT-4 and clinalytix Medical AI in
predicting the clinical risk of delirium development. Findings indicate that
GPT-4 exhibited significant deficiencies in identifying positive cases and
struggled to provide reliable probability estimates for delirium risk, while
clinalytix Medical AI demonstrated superior accuracy. A thorough analysis of
the large language model's (LLM) outputs elucidated potential causes for these
discrepancies, consistent with limitations reported in extant literature. These
results underscore the challenges LLMs face in accurately diagnosing conditions
and interpreting complex clinical data. While LLMs hold substantial potential
in healthcare, they are currently unsuitable for independent clinical
decision-making. Instead, they should be employed in assistive roles,
complementing clinical expertise. Continued human oversight remains essential
to ensure optimal outcomes for both patients and healthcare providers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantile Regression for Distributional Reward Models in RLHF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolai Dorka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning from human feedback (RLHF) has become a key method for
aligning large language models (LLMs) with human preferences through the use of
reward models. However, traditional reward models typically generate point
estimates, which oversimplify the diversity and complexity of human values and
preferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel
approach to reward modeling that learns a distribution over rewards instead of
a single scalar value. Our method uses quantile regression to estimate a full,
potentially multimodal distribution over preferences, providing a more powerful
and nuanced representation of preferences. This distributional approach can
better capture the diversity of human values, addresses label noise, and
accommodates conflicting preferences by modeling them as distinct modes in the
distribution. Our experimental results show that QRM outperforms comparable
traditional point-estimate models on RewardBench. Furthermore, we demonstrate
that the additional information provided by the distributional estimates can be
utilized in downstream applications, such as risk-aware reinforcement learning,
resulting in LLM policies that generate fewer extremely negative responses. Our
code and model are released at https://github.com/Nicolinho/QRM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>s4OL 2024 <span class="highlight-title">Overview</span>: The 1st Large Language Models for Ontology
  Learning Challenge <span class="chip">ISWC
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Babaei Giglou, Jennifer D'Souza, Sören Auer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper outlines the LLMs4OL 2024, the first edition of the Large Language
Models for Ontology Learning Challenge. LLMs4OL is a community development
initiative collocated with the 23rd International Semantic Web Conference
(ISWC) to explore the potential of Large Language Models (LLMs) in Ontology
Learning (OL), a vital process for enhancing the web with structured knowledge
to improve interoperability. By leveraging LLMs, the challenge aims to advance
understanding and innovation in OL, aligning with the goals of the Semantic Web
to create a more intelligent and user-friendly web. In this paper, we give an
overview of the 2024 edition of the LLMs4OL challenge and summarize the
contributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 1 figure, Will appear in "The 1st LLMs4OL Challenge @ ISWC
  2024" proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StruEdit: Structured Outputs Enable the Fast and Accurate Knowledge
  Editing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Junfeng Fang, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the modern tool of choice for question answering, large language models
(LLMs) are expected to deliver answers with up-to-date knowledge. To achieve
such ideal question-answering systems, locating and then editing outdated
knowledge in the natural language outputs is a general target of popular
knowledge editing methods. However, this target is challenging, as both
identifying which tokens to edit in the reasoning steps and ensuring the
coherence of the revised reasoning chain are difficult tasks. We argue that
these challenges stem from the unstructured nature of natural language outputs.
To address the above challenges, we propose $\textbf{Stru}$ctural
$\textbf{Edit}$ing ($\textbf{StruEdit}$), an improved baseline for knowledge
editing. We first prompt LLMs to produce structured outputs consisting of
reasoning triplets. Then, StruEdit removes any potentially outdated knowledge
and efficiently refills the structured outputs with up-to-date information in a
single step. Experimental results show that StruEdit consistently delivers the
highest accuracy with lowest latency compared with other knowledge editing
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-Supervised</span> Syllable Discovery Based on Speaker-Disentangled HuBERT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryota Komatsu, Takahiro Shinozaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised speech representation learning has become essential for
extracting meaningful features from untranscribed audio. Recent advances
highlight the potential of deriving discrete symbols from the features
correlated with linguistic units, which enables text-less training across
diverse tasks. In particular, sentence-level Self-Distillation of the
pretrained HuBERT (SD-HuBERT) induces syllabic structures within latent speech
frame representations extracted from an intermediate Transformer layer. In
SD-HuBERT, sentence-level representation is accumulated from speech frame
features through self-attention layers using a special CLS token. However, we
observe that the information aggregated in the CLS token correlates more with
speaker identity than with linguistic content. To address this, we propose a
speech-only self-supervised fine-tuning approach that separates syllabic units
from speaker information. Our method introduces speaker perturbation as data
augmentation and adopts a frame-level training objective to prevent the CLS
token from aggregating paralinguistic information. Experimental results show
that our approach surpasses the current state-of-the-art method in most
syllable segmentation and syllabic unit quality metrics on Librispeech,
underscoring its effectiveness in promoting syllabic organization within
speech-only models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE SLT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trustworthiness in Retrieval-Augmented Generation Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng Dou, Tsung-Yi Ho, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal
paradigm in the development of Large Language Models (LLMs). While much of the
current research in this field focuses on performance optimization,
particularly in terms of accuracy and efficiency, the trustworthiness of RAG
systems remains an area still under exploration. From a positive perspective,
RAG systems are promising to enhance LLMs by providing them with useful and
up-to-date knowledge from vast external databases, thereby mitigating the
long-standing problem of hallucination. While from a negative perspective, RAG
systems are at the risk of generating undesirable contents if the retrieved
information is either inappropriate or poorly utilized. To address these
concerns, we propose a unified framework that assesses the trustworthiness of
RAG systems across six key dimensions: factuality, robustness, fairness,
transparency, accountability, and privacy. Within this framework, we thoroughly
review the existing literature on each dimension. Additionally, we create the
evaluation benchmark regarding the six dimensions and conduct comprehensive
evaluations for a variety of proprietary and open-source models. Finally, we
identify the potential challenges for future research based on our
investigation results. Through this work, we aim to lay a structured foundation
for future investigations and provide practical insights for enhancing the
trustworthiness of RAG systems in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>-DER:A Named <span class="highlight-title">Entity</span> Recognition Method Based on Large Language Models
  for Chinese Coal Chemical Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Xiao, Yunfei Xu, Jing Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain-specific Named Entity Recognition (NER), whose goal is to recognize
domain-specific entities and their categories, provides an important support
for constructing domain knowledge graphs. Currently, deep learning-based
methods are widely used and effective in NER tasks, but due to the reliance on
large-scale labeled data. As a result, the scarcity of labeled data in a
specific domain will limit its application.Therefore, many researches started
to introduce few-shot methods and achieved some results. However, the entity
structures in specific domains are often complex, and the current few-shot
methods are difficult to adapt to NER tasks with complex features.Taking the
Chinese coal chemical industry domain as an example,there exists a complex
structure of multiple entities sharing a single entity, as well as multiple
relationships for the same pair of entities, which affects the NER task under
the sample less condition.In this paper, we propose a Large Language Models
(LLMs)-based entity recognition framework LLM-DER for the domain-specific
entity recognition problem in Chinese, which enriches the entity information by
generating a list of relationships containing entity types through LLMs, and
designing a plausibility and consistency evaluation method to remove
misrecognized entities, which can effectively solve the complex structural
entity recognition problem in a specific domain.The experimental results of
this paper on the Resume dataset and the self-constructed coal chemical dataset
Coal show that LLM-DER performs outstandingly in domain-specific entity
recognition, not only outperforming the existing GPT-3.5-turbo baseline, but
also exceeding the fully-supervised baseline, verifying its effectiveness in
entity recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Increasing faithfulness in human-human dialog summarization with Spoken
  Language Understanding tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eunice Akani, Benoit Favre, Frederic Bechet, Romain Gemignani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue summarization aims to provide a concise and coherent summary of
conversations between multiple speakers. While recent advancements in language
models have enhanced this process, summarizing dialogues accurately and
faithfully remains challenging due to the need to understand speaker
interactions and capture relevant information. Indeed, abstractive models used
for dialog summarization may generate summaries that contain inconsistencies.
We suggest using the semantic information proposed for performing Spoken
Language Understanding (SLU) in human-machine dialogue systems for
goal-oriented human-human dialogues to obtain a more semantically faithful
summary regarding the task. This study introduces three key contributions:
First, we propose an exploration of how incorporating task-related information
can enhance the summarization process, leading to more semantically accurate
summaries. Then, we introduce a new evaluation criterion based on task
semantics. Finally, we propose a new dataset version with increased annotated
data standardized for research on task-oriented dialogue summarization. The
study evaluates these methods using the DECODA corpus, a collection of French
spoken dialogues from a call center. Results show that integrating models with
task-related information improves summary accuracy, even with varying word
error rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid
  via Edge <span class="highlight-title">LLM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijie Ji, Xinzhe Zheng, Jiawei Sun, Renqi Chen, Wei Gao, Mani Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mental health disorders are among the most prevalent diseases worldwide,
affecting nearly one in four people. Despite their widespread impact, the
intervention rate remains below 25%, largely due to the significant cooperation
required from patients for both diagnosis and intervention. The core issue
behind this low treatment rate is stigma, which discourages over half of those
affected from seeking help. This paper presents MindGuard, an accessible,
stigma-free, and professional mobile mental healthcare system designed to
provide mental health first aid. The heart of MindGuard is an innovative edge
LLM, equipped with professional mental health knowledge, that seamlessly
integrates objective mobile sensor data with subjective Ecological Momentary
Assessment records to deliver personalized screening and intervention
conversations. We conduct a broad evaluation of MindGuard using open datasets
spanning four years and real-world deployment across various mobile devices
involving 20 subjects for two weeks. Remarkably, MindGuard achieves results
comparable to GPT-4 and outperforms its counterpart with more than 10 times the
model size. We believe that MindGuard paves the way for mobile LLM
applications, potentially revolutionizing mental healthcare practices by
substituting self-reporting and intervention conversations with passive,
integrated monitoring within daily life, thus ensuring accessible and
stigma-free mental health support.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Householder Pseudo-Rotation: A Novel Approach to Activation Editing in
  <span class="highlight-title">LLM</span>s with Direction-Magnitude Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van-Cuong Pham, <span class="highlight-author">Thien Huu Nguyen</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activation Editing, which involves directly editting the internal
representations of large language models (LLMs) to alter their behaviors and
achieve desired properties, has emerged as a promising area of research.
Existing works primarily treat LLMs' activations as points in space and modify
them by adding steering vectors. However, this approach is limited in its
ability to achieve greater performance improvement while maintaining the
necessary consistency of activation magnitudes. To overcome these issues, we
propose a novel editing method that views activations in terms of their
directions and magnitudes. Our method, named Householder Pseudo-Rotation (HPR),
mimics the rotation transformation, thus preserving activation norms and
resulting in an improved performance on various safety benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Large Language Model Uncertainty for <span class="highlight-title">Prompt</span> Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei-Fu Guo, Yun-Da Tsai, Shou-De Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt optimization algorithms for Large Language Models (LLMs) excel in
multi-step reasoning but still lack effective uncertainty estimation. This
paper introduces a benchmark dataset to evaluate uncertainty metrics, focusing
on Answer, Correctness, Aleatoric, and Epistemic Uncertainty. Through analysis
of models like GPT-3.5-Turbo and Meta-Llama-3.1-8B-Instruct, we show that
current metrics align more with Answer Uncertainty, which reflects output
confidence and diversity, rather than Correctness Uncertainty, highlighting the
need for improved metrics that are optimization-objective-aware to better guide
prompt optimization. Our code and dataset are available at
https://github.com/0Frett/PO-Uncertainty-Benchmarking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Diagram of Thought 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhang, Yang Yuan, Andrew Chi-Chih Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Diagram of Thought (DoT), a framework that models iterative
reasoning in large language models (LLMs) as the construction of a directed
acyclic graph (DAG) within a single model. Unlike traditional approaches that
represent reasoning as linear chains or trees, DoT organizes propositions,
critiques, refinements, and verifications into a cohesive DAG structure,
allowing the model to explore complex reasoning pathways while maintaining
logical consistency. Each node in the diagram corresponds to a proposition that
has been proposed, critiqued, refined, or verified, enabling the LLM to
iteratively improve its reasoning through natural language feedback. By
leveraging auto-regressive next-token prediction with role-specific tokens, DoT
facilitates seamless transitions between proposing ideas and critically
evaluating them, providing richer feedback than binary signals. Furthermore, we
formalize the DoT framework using Topos Theory, providing a mathematical
foundation that ensures logical consistency and soundness in the reasoning
process. This approach enhances both the training and inference processes
within a single LLM, eliminating the need for multiple models or external
control mechanisms. DoT offers a conceptual framework for designing
next-generation reasoning-specialized models, emphasizing training efficiency,
robust reasoning capabilities, and theoretical grounding. The code is available
at https://github.com/diagram-of-thought/diagram-of-thought.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AceParse: A Comprehensive <span class="highlight-title">Dataset</span> with Diverse Structured Texts for
  Academic Literature Parsing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huawei Ji, Cheng Deng, Bo Xue, Zhouyang Jin, Jiaxin Ding, Xiaoying Gan, Luoyi Fu, Xinbing Wang, Chenghu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of data-centric AI, the focus has shifted from
model-driven approaches to improving data quality. Academic literature, as one
of the crucial types, is predominantly stored in PDF formats and needs to be
parsed into texts before further processing. However, parsing diverse
structured texts in academic literature remains challenging due to the lack of
datasets that cover various text structures. In this paper, we introduce
AceParse, the first comprehensive dataset designed to support the parsing of a
wide range of structured texts, including formulas, tables, lists, algorithms,
and sentences with embedded mathematical expressions. Based on AceParse, we
fine-tuned a multimodal model, named AceParser, which accurately parses various
structured texts within academic literature. This model outperforms the
previous state-of-the-art by 4.1% in terms of F1 score and by 5% in Jaccard
Similarity, demonstrating the potential of multimodal models in academic
literature parsing. Our dataset is available at
https://github.com/JHW5981/AceParse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HALO: Hallucination Analysis and Learning Optimization to Empower <span class="highlight-title">LLM</span>s
  with Retrieval-Augmented Context for Guided Clinical Decision Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumera Anjum, Hanzhi Zhang, Wenjun Zhou, Eun Jin Paek, Xiaopeng Zhao, Yunhe Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have significantly advanced natural language
processing tasks, yet they are susceptible to generating inaccurate or
unreliable responses, a phenomenon known as hallucination. In critical domains
such as health and medicine, these hallucinations can pose serious risks. This
paper introduces HALO, a novel framework designed to enhance the accuracy and
reliability of medical question-answering (QA) systems by focusing on the
detection and mitigation of hallucinations. Our approach generates multiple
variations of a given query using LLMs and retrieves relevant information from
external open knowledge bases to enrich the context. We utilize maximum
marginal relevance scoring to prioritize the retrieved context, which is then
provided to LLMs for answer generation, thereby reducing the risk of
hallucinations. The integration of LangChain further streamlines this process,
resulting in a notable and robust increase in the accuracy of both open-source
and commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56%
to 70%). This framework underscores the critical importance of addressing
hallucinations in medical QA systems, ultimately improving clinical
decision-making and patient care. The open-source HALO is available at:
https://github.com/ResponsibleAILab/HALO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SelECT-SQL: Self-correcting ensemble Chain-of-Thought for Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Shen, Mayank Kejriwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years,Text-to-SQL, the problem of automatically converting
questions posed in natural language to formal SQL queries, has emerged as an
important problem at the intersection of natural language processing and data
management research. Large language models (LLMs) have delivered impressive
performance when used in an off-the-shelf performance, but still fall
significantly short of expected expert-level performance. Errors are especially
probable when a nuanced understanding is needed of database schemas, questions,
and SQL clauses to do proper Text-to-SQL conversion. We introduce SelECT-SQL, a
novel in-context learning solution that uses an algorithmic combination of
chain-of-thought (CoT) prompting, self-correction, and ensemble methods to
yield a new state-of-the-art result on challenging Text-to-SQL benchmarks.
Specifically, when configured using GPT-3.5-Turbo as the base LLM, SelECT-SQL
achieves 84.2% execution accuracy on the Spider leaderboard's development set,
exceeding both the best results of other baseline GPT-3.5-Turbo-based solutions
(81.1%), and the peak performance (83.5%) of the GPT-4 result reported on the
leaderboard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive Study on Sentiment Analysis: From Rule-based to modern <span class="highlight-title">LLM</span>
  based system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shailja Gupta, Rajesh Ranjan, Surya Narayan Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper provides a comprehensive survey of sentiment analysis within the
context of artificial intelligence (AI) and large language models (LLMs).
Sentiment analysis, a critical aspect of natural language processing (NLP), has
evolved significantly from traditional rule-based methods to advanced deep
learning techniques. This study examines the historical development of
sentiment analysis, highlighting the transition from lexicon-based and
pattern-based approaches to more sophisticated machine learning and deep
learning models. Key challenges are discussed, including handling bilingual
texts, detecting sarcasm, and addressing biases. The paper reviews
state-of-the-art approaches, identifies emerging trends, and outlines future
research directions to advance the field. By synthesizing current methodologies
and exploring future opportunities, this survey aims to understand sentiment
analysis in the AI and LLM context thoroughly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 Images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gaps or Hallucinations? Gazing into Machine-Generated Legal Analysis for
  Fine-grained Text Evaluations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abe Bohan Hou, William Jurayj, Nils Holzenberger, Andrew Blair-Stanek, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) show promise as a writing aid for professionals
performing legal analyses. However, LLMs can often hallucinate in this setting,
in ways difficult to recognize by non-professionals and existing text
evaluation metrics. In this work, we pose the question: when can
machine-generated legal analysis be evaluated as acceptable? We introduce the
neutral notion of gaps, as opposed to hallucinations in a strict erroneous
sense, to refer to the difference between human-written and machine-generated
legal analysis. Gaps do not always equate to invalid generation. Working with
legal experts, we consider the CLERC generation task proposed in Hou et al.
(2024b), leading to a taxonomy, a fine-grained detector for predicting gap
categories, and an annotated dataset for automatic evaluation. Our best
detector achieves 67% F1 score and 80% precision on the test set. Employing
this detector as an automated metric on legal analysis generated by SOTA LLMs,
we find around 80% contain hallucinations of different kinds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Data Contamination Detection for Modern Large Language Models:
  Limitations, Inconsistencies, and Oracle Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09927v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09927v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinay Samuel, Yue Zhou, Henry Peng Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models achieve increasingly impressive results, questions
arise about whether such performance is from generalizability or mere data
memorization. Thus, numerous data contamination detection methods have been
proposed. However, these approaches are often validated with traditional
benchmarks and early-stage LLMs, leaving uncertainty about their effectiveness
when evaluating state-of-the-art LLMs on the contamination of more challenging
benchmarks. To address this gap and provide a dual investigation of SOTA LLM
contamination status and detection method robustness, we evaluate five
contamination detection approaches with four state-of-the-art LLMs across eight
challenging datasets often used in modern LLM evaluation. Our analysis reveals
that (1) Current methods have non-trivial limitations in their assumptions and
practical applications; (2) Notable difficulties exist in detecting
contamination introduced during instruction fine-tuning with answer
augmentation; and (3) Limited consistencies between SOTA contamination
detection techniques. These findings highlight the complexity of contamination
detection in advanced LLMs and the urgent need for further research on robust
and generalizable contamination evaluation. Our code is available at
https://github.com/vsamuel2003/data-contamination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SFR-RAG: Towards Contextually Faithful <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan-Phi Nguyen, Shrey Pandit, Senthil Purushwalkam, Austin Xu, Hailin Chen, Yifei Ming, Zixuan Ke, Silvio Savarese, Caiming Xong, Shafiq Joty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG), a paradigm that integrates external
contextual information with large language models (LLMs) to enhance factual
accuracy and relevance, has emerged as a pivotal area in generative AI. The
LLMs used in RAG applications are required to faithfully and completely
comprehend the provided context and users' questions, avoid hallucination,
handle unanswerable, counterfactual or otherwise low-quality and irrelevant
contexts, perform complex multi-hop reasoning and produce reliable citations.
In this paper, we introduce SFR-RAG, a small LLM that is instruction-tuned with
an emphasis on context-grounded generation and hallucination minimization. We
also present ContextualBench, a new evaluation framework compiling multiple
popular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with
consistent RAG settings to ensure reproducibility and consistency in model
assessments. Experimental results demonstrate that our SFR-RAG-9B model
outperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving
state-of-the-art results in 3 out of 7 benchmarks in ContextualBench with
significantly fewer parameters. The model is also shown to be resilient to
alteration in the contextual information and behave appropriately when relevant
context is removed. Additionally, the SFR-RAG model maintains competitive
performance in general instruction-following tasks and function-calling
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rediscovering the Latent Dimensions of Personality with Large Language
  Models as Trait Descriptors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Suh, Suhong Moon, Minwoo Kang, David M. Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing personality traits using large language models (LLMs) has emerged
as an interesting and challenging area of research. While previous methods
employ explicit questionnaires, often derived from the Big Five model of
personality, we hypothesize that LLMs implicitly encode notions of personality
when modeling next-token responses. To demonstrate this, we introduce a novel
approach that uncovers latent personality dimensions in LLMs by applying
singular value de-composition (SVD) to the log-probabilities of
trait-descriptive adjectives. Our experiments show that LLMs "rediscover" core
personality traits such as extraversion, agreeableness, conscientiousness,
neuroticism, and openness without relying on direct questionnaire inputs, with
the top-5 factors corresponding to Big Five traits explaining 74.3% of the
variance in the latent space. Moreover, we can use the derived principal
components to assess personality along the Big Five dimensions, and achieve
improvements in average personality prediction accuracy of up to 5% over
fine-tuned models, and up to 21% over direct LLM-based scoring techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing biomedical knowledge robustness in large language models by
  query-efficient sampling attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10527v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10527v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. Patrick Xian, Alex J. Lee, Satvik Lolla, Vincent Wang, Qiming Cui, Russell Ro, Reza Abbasi-Asl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing depth of parametric domain knowledge in large language models
(LLMs) is fueling their rapid deployment in real-world applications.
Understanding model vulnerabilities in high-stakes and knowledge-intensive
tasks is essential for quantifying the trustworthiness of model predictions and
regulating their use. The recent discovery of named entities as adversarial
examples (i.e. adversarial entities) in natural language processing tasks
raises questions about their potential impact on the knowledge robustness of
pre-trained and finetuned LLMs in high-stakes and specialized domains. We
examined the use of type-consistent entity substitution as a template for
collecting adversarial entities for billion-parameter LLMs with biomedical
knowledge. To this end, we developed an embedding-space attack based on
powerscaled distance-weighted sampling to assess the robustness of their
biomedical knowledge with a low query budget and controllable coverage. Our
method has favorable query efficiency and scaling over alternative approaches
based on random sampling and blackbox gradient-guided search, which we
demonstrated for adversarial distractor generation in biomedical question
answering. Subsequent failure mode analysis uncovered two regimes of
adversarial entities on the attack surface with distinct characteristics and we
showed that entity substitution attacks can manipulate token-wise Shapley value
explanations, which become deceptive in this setting. Our approach complements
standard evaluations for high-capacity models and the results highlight the
brittleness of domain knowledge in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages incl. appendix, updated version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Security Attacks on <span class="highlight-title">LLM</span>-based Code Completion Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11006v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11006v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Cheng, Ke Sun, Xinyu Zhang, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of large language models (LLMs) has significantly
advanced code completion capabilities, giving rise to a new generation of
LLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these
tools possess unique workflows, integrating multiple information sources as
input and prioritizing code suggestions over natural language interaction,
which introduces distinct security challenges. Additionally, LCCTs often rely
on proprietary code datasets for training, raising concerns about the potential
exposure of sensitive data. This paper exploits these distinct characteristics
of LCCTs to develop targeted attack methodologies on two critical security
risks: jailbreaking and training data extraction attacks. Our experimental
results expose significant vulnerabilities within LCCTs, including a 99.4%
success rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate
on Amazon Q. Furthermore, We successfully extracted sensitive user data from
GitHub Copilot, including 54 real email addresses and 314 physical addresses
associated with GitHub usernames. Our study also demonstrates that these
code-based attack methods are effective against general-purpose LLMs, such as
the GPT series, highlighting a broader security misalignment in the handling of
code by modern LLMs. These findings underscore critical security challenges
associated with LCCTs and suggest essential directions for strengthening their
security frameworks. The example code and attack samples from our research are
provided at https://github.com/Sensente/Security-Attacks-on-LCCTs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop
  Question Answering <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengliang Shi, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, Zhaochun Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Hop Question Answering (MHQA) tasks present a significant challenge for
large language models (LLMs) due to the intensive knowledge required. Current
solutions, like Retrieval-Augmented Generation, typically retrieve potential
documents from an external corpus to read an answer. However, the performance
of this retrieve-then-read paradigm is constrained by the retriever and the
inevitable noise in the retrieved documents. To mitigate these challenges, we
introduce a novel generate-then-ground (GenGround) framework, synergizing the
parametric knowledge of LLMs and external documents to solve a multi-hop
question. GenGround empowers LLMs to alternate two phases until the final
answer is derived: (1) formulate a simpler, single-hop question and directly
generate the answer; (2) ground the question-answer pair in retrieved
documents, amending any wrong predictions in the answer. We also propose an
instructional grounding distillation method to generalize our method into
smaller models. Extensive experiments conducted on four datasets illustrate the
superiority of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can GPT-3.5 Generate and Code Discharge Summaries? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matúš Falis, Aryo Pradipta Gema, Hang Dong, Luke Daines, Siddharth Basetti, Michael Holder, Rose S Penfold, Alexandra Birch, Beatrice Alex
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: To investigate GPT-3.5 in generating and coding medical documents
with ICD-10 codes for data augmentation on low-resources labels.
  Materials and Methods: Employing GPT-3.5 we generated and coded 9,606
discharge summaries based on lists of ICD-10 code descriptions of patients with
infrequent (generation) codes within the MIMIC-IV dataset. Combined with the
baseline training set, this formed an augmented training set. Neural coding
models were trained on baseline and augmented data and evaluated on a MIMIC-IV
test set. We report micro- and macro-F1 scores on the full codeset, generation
codes, and their families. Weak Hierarchical Confusion Matrices were employed
to determine within-family and outside-of-family coding errors in the latter
codesets. The coding performance of GPT-3.5 was evaluated both on prompt-guided
self-generated data and real MIMIC-IV data. Clinical professionals evaluated
the clinical acceptability of the generated documents.
  Results: Augmentation slightly hinders the overall performance of the models
but improves performance for the generation candidate codes and their families,
including one unseen in the baseline training data. Augmented models display
lower out-of-family error rates. GPT-3.5 can identify ICD-10 codes by the
prompted descriptions, but performs poorly on real data. Evaluators note the
correctness of generated concepts while suffering in variety, supporting
information, and narrative.
  Discussion and Conclusion: GPT-3.5 alone is unsuitable for ICD-10 coding.
Augmentation positively affects generation code families but mainly benefits
codes with existing examples. Augmentation reduces out-of-family errors.
Discharge summaries generated by GPT-3.5 state prompted concepts correctly but
lack variety, and authenticity in narratives. They are unsuitable for clinical
practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages; 250 words in abstract; 4,152 words in main body; 4 figures
  (1 black and white, 3 colour); 4 tables; 34 references; Accepted and
  published by the Journal of the American Medical Informatics Association</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do <span class="highlight-title">Prompt</span>s Really <span class="highlight-title">Prompt</span>? Exploring the <span class="highlight-title">Prompt</span> Understanding Capability
  of Whisper 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05806v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05806v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Kai Yang, Kuan-Po Huang, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research explores how the information of prompts interacts with the
high-performing speech recognition model, Whisper. We compare its performances
when prompted by prompts with correct information and those corrupted with
incorrect information. Our results unexpectedly show that Whisper may not
understand the textual prompts in a human-expected way. Additionally, we find
that performance improvement is not guaranteed even with stronger adherence to
the topic information in textual prompts. It is also noted that English prompts
generally outperform Mandarin ones on datasets of both languages, likely due to
differences in training data distributions for these languages despite the
mismatch with pre-training scenarios. Conversely, we discover that Whisper
exhibits awareness of misleading information in language tokens by ignoring
incorrect language tokens and focusing on the correct ones. In sum, We raise
insightful questions about Whisper's prompt understanding and reveal its
counter-intuitive behaviors. We encourage further studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2024 IEEE Spoken Language Technology Workshop (SLT 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discrete Neural Algorithmic Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11628v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11628v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gleb Rodionov, Liudmila Prokhorenkova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural algorithmic reasoning aims to capture computations with neural
networks via learning the models to imitate the execution of classic
algorithms. While common architectures are expressive enough to contain the
correct model in the weights space, current neural reasoners are struggling to
generalize well on out-of-distribution data. On the other hand, classic
computations are not affected by distributional shifts as they can be described
as transitions between discrete computational states. In this work, we propose
to force neural reasoners to maintain the execution trajectory as a combination
of finite predefined states. To achieve that, we separate discrete and
continuous data flows and describe the interaction between them. Trained with
supervision on the algorithm's state transitions, such models are able to
perfectly align with the original algorithm. To show this, we evaluate our
approach on multiple algorithmic problems and get perfect test scores both in
single-task and multitask setups. Moreover, the proposed architectural choice
allows us to prove the correctness of the learned algorithms for any test~data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VideoStudio: Generating Consistent-Content and Multi-Scene Videos <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent innovations and breakthroughs in diffusion models have
significantly expanded the possibilities of generating high-quality videos for
the given prompts. Most existing works tackle the single-scene scenario with
only one video event occurring in a single background. Extending to generate
multi-scene videos nevertheless is not trivial and necessitates to nicely
manage the logic in between while preserving the consistent visual appearance
of key content across video scenes. In this paper, we propose a novel
framework, namely VideoStudio, for consistent-content and multi-scene video
generation. Technically, VideoStudio leverages Large Language Models (LLM) to
convert the input prompt into comprehensive multi-scene script that benefits
from the logical knowledge learnt by LLM. The script for each scene includes a
prompt describing the event, the foreground/background entities, as well as
camera movement. VideoStudio identifies the common entities throughout the
script and asks LLM to detail each entity. The resultant entity description is
then fed into a text-to-image model to generate a reference image for each
entity. Finally, VideoStudio outputs a multi-scene video by generating each
scene video via a diffusion process that takes the reference images, the
descriptive prompt of the event and camera movement into account. The diffusion
model incorporates the reference images as the condition and alignment to
strengthen the content consistency of multi-scene videos. Extensive experiments
demonstrate that VideoStudio outperforms the SOTA video generation models in
terms of visual quality, content consistency, and user preference. Source code
is available at \url{https://github.com/FuchenUSTC/VideoStudio}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024. Source code is available at
  https://github.com/FuchenUSTC/VideoStudio</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigate the Gap: Investigating Approaches for Improving Cross-Modal
  Alignment in CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17639v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17639v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sedigheh Eslami, Gerard de Melo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language--Image Pre-training (CLIP) has manifested remarkable
improvements in zero-shot classification and cross-modal vision-language tasks.
Yet, from a geometrical point of view, the CLIP embedding space has been found
to have a pronounced modality gap. This gap renders the embedding space overly
sparse and disconnected, with different modalities being densely distributed in
distinct subregions of the hypersphere. In this work, we aim at answering three
main questions: 1. Does sharing the parameter space between the multi-modal
encoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart
the uni-modal embeddings via intra-modality separation? 3. How do these gap
reduction approaches affect the downstream performance? We design AlignCLIP, in
order to answer these questions and through extensive experiments, we show that
AlignCLIP achieves noticeable enhancements in the cross-modal alignment of the
embeddings, and thereby, reduces the modality gap, while improving the
performance across several zero-shot and fine-tuning downstream evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WinoPron: Revisiting English Winogender Schemas for Consistency,
  Coverage, and Grammatical Case 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05653v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05653v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vagrant Gautam, Julius Steuer, Eileen Bingert, Ray Johns, Anne Lauscher, Dietrich Klakow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While measuring bias and robustness in coreference resolution are important
goals, such measurements are only as good as the tools we use to measure them
with. Winogender schemas (Rudinger et al., 2018) are an influential dataset
proposed to evaluate gender bias in coreference resolution, but a closer look
reveals issues with the data that compromise its use for reliable evaluation,
including treating different pronominal forms as equivalent, violations of
template constraints, and typographical errors. We identify these issues and
fix them, contributing a new dataset: WinoPron. Our changes affect performance
with state-of-the-art supervised coreference resolution systems as well as all
model sizes of the language model FLAN-T5, with F1 dropping on average 10
percentage points. We also propose a new method to evaluate pronominal bias in
coreference resolution that goes beyond the binary. With this method and our
new dataset which is balanced for grammatical case, we empirically demonstrate
that bias characteristics vary not just across pronoun sets, but also across
surface forms of those sets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Against the RAG: Jamming Retrieval-Augmented Generation with
  Blocker Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avital Shafran, Roei Schuster, Vitaly Shmatikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) systems respond to queries by retrieving
relevant documents from a knowledge database, then generating an answer by
applying an LLM to the retrieved documents. We demonstrate that RAG systems
that operate on databases with untrusted content are vulnerable to a new class
of denial-of-service attacks we call jamming. An adversary can add a single
``blocker'' document to the database that will be retrieved in response to a
specific query and result in the RAG system not answering this query -
ostensibly because it lacks the information or because the answer is unsafe.
  We describe and measure the efficacy of several methods for generating
blocker documents, including a new method based on black-box optimization. This
method (1) does not rely on instruction injection, (2) does not require the
adversary to know the embedding or LLM used by the target RAG system, and (3)
does not use an auxiliary LLM to generate blocker documents.
  We evaluate jamming attacks on several LLMs and embeddings and demonstrate
that the existing safety metrics for LLMs do not capture their vulnerability to
jamming. We then discuss defenses against blocker documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Can We Effectively Expand the Vocabulary of <span class="highlight-title">LLM</span>s with 0.01GB of
  Target Language Text? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsuki Yamaguchi, Aline Villavicencio, Nikolaos Aletras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable capabilities in many
languages beyond English. Yet, LLMs require more inference steps when
generating non-English text due to their reliance on English-centric tokenizers
and vocabulary, resulting in higher usage costs to non-English speakers.
Vocabulary expansion with target language tokens is a widely used cross-lingual
vocabulary adaptation approach to remedy this issue. Despite its effectiveness
in inference speedup, previous work on vocabulary expansion has focused on
high-resource settings assuming access to a substantial amount of target
language data to effectively initialize the embeddings of the new tokens and
adapt the LLM to the target language. However, vocabulary expansion in
low-resource settings has yet to be explored. In this paper, we investigate
vocabulary expansion in low-resource settings by considering embedding
initialization methods and continual pre-training strategies. Through extensive
experiments across typologically diverse languages, tasks and models, we
establish a set of strategies to perform vocabulary expansion for faster
inference, maintaining competitive downstream performance to baselines with
only 30K sentences ($\sim$0.01GB text data) from the target language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Membership Inference Attacks Work on Large Language Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, Hannaneh Hajishirzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Membership inference attacks (MIAs) attempt to predict whether a particular
datapoint is a member of a target model's training data. Despite extensive
research on traditional machine learning models, there has been limited work
studying MIA on the pre-training data of large language models (LLMs). We
perform a large-scale evaluation of MIAs over a suite of language models (LMs)
trained on the Pile, ranging from 160M to 12B parameters. We find that MIAs
barely outperform random guessing for most settings across varying LLM sizes
and domains. Our further analyses reveal that this poor performance can be
attributed to (1) the combination of a large dataset and few training
iterations, and (2) an inherently fuzzy boundary between members and
non-members. We identify specific settings where LLMs have been shown to be
vulnerable to membership inference and show that the apparent success in such
settings can be attributed to a distribution shift, such as when members and
non-members are drawn from the seemingly identical domain but with different
temporal ranges. We release our code and data as a unified benchmark package
that includes all existing MIAs, supporting future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Conference on Language Modeling (COLM), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Fully Autonomous Research Powered by <span class="highlight-title">LLM</span>s: Case Study on
  Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihan Liu, Yubo Chai, Jianfeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) has created new opportunities for
the automation of scientific research, spanning both experimental processes and
computational simulations. This study explores the feasibility of constructing
an autonomous simulation agent (ASA) powered by LLM, through sophisticated API
integration, to automate the entire research process, from experimental design,
remote upload and simulation execution, data analysis, to report compilation.
Using a simulation problem of polymer chain conformations as a case study, we
assessed the performance of ASAs powered by different LLMs including
GPT-4-Turbo. Our findings revealed that ASA-GPT-4o achieved near-flawless
execution on designated research missions, underscoring the potential of LLMs
to manage complete scientific investigations autonomously. The outlined
automation can be iteratively performed up to twenty cycles without human
intervention, illustrating the potential of LLMs for large-scale autonomous
research endeavors. Additionally, we discussed the intrinsic traits of ASAs in
managing extensive tasks, focusing on self-validation mechanisms and the
balance between local attention and global oversight.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For additional code and data, please visit our GitHub repository:
  https://github.com/zokaraa/autonomous_simulation_agent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eir: Thai Medical Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutthakorn Thiprak, Rungtam Ngodngamthaweesuk, Songtam Ngodngamtaweesuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Eir-8B, a large language model with 8 billion parameters,
specifically designed to enhance the accuracy of handling medical tasks in the
Thai language. This model focuses on providing clear and easy-to-understand
answers for both healthcare professionals and patients, thereby improving the
efficiency of diagnosis and treatment processes. Human evaluation was conducted
to ensure that the model adheres to care standards and provides unbiased
answers.
  To prioritize data security, the model is deployed within the hospital's
internal network, ensuring both high security and faster processing speeds. The
internal API connection is secured with encryption and strict authentication
measures to prevent data leaks and unauthorized access.
  We evaluated several open-source large language models with 8 billion
parameters on four medical benchmarks: MedQA, MedMCQA, PubMedQA, and the
medical subset of MMLU. The best-performing baselines were used to develop
Eir-8B. Our evaluation employed multiple questioning strategies, including
zero-shot, few-shot, chain-of-thought reasoning, and ensemble/self-consistency
voting methods. Our model outperformed commercially available Thai-language
large language models by more than 10%. In addition, we developed enhanced
model testing tailored for clinical use in Thai across 18 clinical tasks, where
our model exceeded GPT-4o performance by more than 11%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>typos corrected, and references added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of Google Translate for Mandarin Chinese translation using
  sentiment and semantic analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuechun Wang, Rodney Beard, Rohitash Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine translation using large language models (LLMs) is having a
significant global impact, making communication easier. Mandarin Chinese is the
official language used for communication by the government and media in China.
In this study, we provide an automated assessment of translation quality of
Google Translate with human experts using sentiment and semantic analysis. In
order to demonstrate our framework, we select the classic early
twentieth-century novel 'The True Story of Ah Q' with selected Mandarin Chinese
to English translations. We use Google Translate to translate the given text
into English and then conduct a chapter-wise sentiment analysis and semantic
analysis to compare the extracted sentiments across the different translations.
Our results indicate that the precision of Google Translate differs both in
terms of semantic and sentiment analysis when compared to human expert
translations. We find that Google Translate is unable to translate some of the
specific words or phrases in Chinese, such as Chinese traditional allusions.
The mistranslations may be due to lack of contextual significance and
historical knowledge of China.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PMB5: Gaining More Insight into Neural Semantic Parsing with Challenging
  Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.08354v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.08354v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Zhang, Chunliu Wang, Rik van Noord, Johan Bos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Parallel Meaning Bank (PMB) serves as a corpus for semantic processing
with a focus on semantic parsing and text generation. Currently, we witness an
excellent performance of neural parsers and generators on the PMB. This might
suggest that such semantic processing tasks have by and large been solved. We
argue that this is not the case and that performance scores from the past on
the PMB are inflated by non-optimal data splits and test sets that are too
easy. In response, we introduce several changes. First, instead of the prior
random split, we propose a more systematic splitting approach to improve the
reliability of the standard test data. Second, except for the standard test
set, we also propose two challenge sets: one with longer texts including
discourse structure, and one that addresses compositional generalization. We
evaluate five neural models for semantic parsing and meaning-to-text
generation. Our results show that model performance declines (in some cases
dramatically) on the challenge sets, revealing the limitations of neural models
when confronting such challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large language models and linguistic intentionality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09576v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09576v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jumbly Grindrod
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Do large language models like Chat-GPT or LLaMa meaningfully use the words
they produce? Or are they merely clever prediction machines, simulating
language use by producing statistically plausible text? There have already been
some initial attempts to answer this question by showing that these models meet
the criteria for entering meaningful states according to metasemantic theories
of mental content. In this paper, I will argue for a different approach - that
we should instead consider whether language models meet the criteria given by
our best metasemantic theories of linguistic content. In that vein, I will
illustrate how this can be done by applying two such theories to the case of
language models: Gareth Evans' (1982) account of naming practices and Ruth
Millikan's (1984, 2004, 2005) teleosemantics. In doing so, I will argue that it
is a mistake to think that the failure of LLMs to meet plausible conditions for
mental intentionality thereby renders their outputs meaningless, and that a
distinguishing feature of linguistic intentionality - dependency on a
pre-existing linguistic system - allows for the plausible result LLM outputs
are meaningful.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-Resourced Speech Recognition for Iu Mien Language via
  Weakly-Supervised Phoneme-based Multilingual <span class="highlight-title">Pre-train</span>ing <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13292v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13292v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukuan Dong, Donghong Qin, Fengbo Bai, Fanhua Song, Yan Liu, Chen Xu, Zhijian Ou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The mainstream automatic speech recognition (ASR) technology usually requires
hundreds to thousands of hours of annotated speech data. Three approaches to
low-resourced ASR are phoneme or subword based supervised pre-training, and
self-supervised pre-training over multilingual data. The Iu Mien language is
the main ethnic language of the Yao ethnic group in China and is low-resourced
in the sense that the annotated speech is very limited. With less than 10 hours
of transcribed Iu Mien language, this paper investigates and compares the three
approaches for Iu Mien speech recognition. Our experiments are based on the
recently released, three backbone models pretrained over the 10 languages from
the CommonVoice dataset (CV-Lang10), which correspond to the three approaches
for low-resourced ASR. It is found that phoneme supervision can achieve better
results compared to subword supervision and self-supervision, thereby providing
higher data-efficiency. Particularly, the Whistle models, i.e., obtained by the
weakly-supervised phoneme-based multilingual pre-training, obtain the most
competitive results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted into ISCSLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Refuse: Towards Mitigating Privacy Risks in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10058v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10058v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhua Liu, Tong Zhu, Chuanyuan Tan, Wenliang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit remarkable capabilities in understanding
and generating natural language. However, these models can inadvertently
memorize private information, posing significant privacy risks. This study
addresses the challenge of enabling LLMs to protect specific individuals'
private data without the need for complete retraining. We propose \return, a
Real-world pErsonal daTa UnleaRNing dataset, comprising 2,492 individuals from
Wikipedia with associated QA pairs, to evaluate machine unlearning (MU) methods
for protecting personal data in a realistic scenario. Additionally, we
introduce the Name-Aware Unlearning Framework (NAUF) for Privacy Protection,
which enables the model to learn which individuals' information should be
protected without affecting its ability to answer questions related to other
unrelated individuals. Our extensive experiments demonstrate that NAUF achieves
a state-of-the-art average unlearning score, surpassing the best baseline
method by 5.65 points, effectively protecting target individuals' personal data
while maintaining the model's general capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Central Answer Modeling for an Embodied Multi-<span class="highlight-title">LLM</span> System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10918v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10918v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhrij Patel, Vishnu Sashank Dorbala, Amrit Singh Bedi, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied Question Answering (EQA) is an important problem, which involves an
agent exploring the environment to answer user queries. In the existing
literature, EQA has exclusively been studied in single-agent scenarios, where
exploration can be time-consuming and costly. In this work, we consider EQA in
a multi-agent framework involving multiple large language models (LLM) based
agents independently answering queries about a household environment. To
generate one answer for each query, we use the individual responses to train a
Central Answer Model (CAM) that aggregates responses for a robust answer. While
prior Question Answering (QA) work has used a central module based on answers
from multiple LLM-based experts, we specifically look at applying this
framework to embodied LLM-based agents that must physically explore the
environment first to become experts on their given environment to answer
questions. Our work is the first to utilize a central answer model framework
with embodied agents that must rely on exploring an unknown environment. We set
up a variation of EQA where instead of the agents exploring the environment
after the question is asked, the agents first explore the environment for a set
amount of time and then answer a set of queries. Using CAM, we observe a $46\%$
higher EQA accuracy when compared against aggregation methods for ensemble LLM,
such as voting schemes and debates. CAM does not require any form of agent
communication, alleviating it from the associated costs. We ablate CAM with
various nonlinear (neural network, random forest, decision tree, XGBoost) and
linear (logistic regression classifier, SVM) algorithms. We experiment in
various topological graph environments and examine the case where one of the
agents is malicious and purposes contribute responses it believes to be wrong.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 Figures, 5 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Privacy Amidst Innovation with Large Language Models Through a
  Critical Assessment of the Risks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao-Shun Chuang, Atiquer Rahman Sarkar, Yu-Chun Hsu, Noman Mohammed, Xiaoqian Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines integrating EHRs and NLP with large language models
(LLMs) to improve healthcare data management and patient care. It focuses on
using advanced models to create secure, HIPAA-compliant synthetic patient notes
for biomedical research. The study used de-identified and re-identified MIMIC
III datasets with GPT-3.5, GPT-4, and Mistral 7B to generate synthetic notes.
Text generation employed templates and keyword extraction for contextually
relevant notes, with one-shot generation for comparison. Privacy assessment
checked PHI occurrence, while text utility was tested using an ICD-9 coding
task. Text quality was evaluated with ROUGE and cosine similarity metrics to
measure semantic similarity with source notes. Analysis of PHI occurrence and
text utility via the ICD-9 coding task showed that the keyword-based method had
low risk and good performance. One-shot generation showed the highest PHI
exposure and PHI co-occurrence, especially in geographic location and date
categories. The Normalized One-shot method achieved the highest classification
accuracy. Privacy analysis revealed a critical balance between data utility and
privacy protection, influencing future data use and sharing. Re-identified data
consistently outperformed de-identified data. This study demonstrates the
effectiveness of keyword-based methods in generating privacy-protecting
synthetic clinical notes that retain data usability, potentially transforming
clinical data-sharing practices. The superior performance of re-identified over
de-identified data suggests a shift towards methods that enhance utility and
privacy by using dummy PHIs to perplex privacy attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, 1 table, 1 supplementary, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatGPT Based Data Augmentation for Improved Parameter-Efficient
  Debiasing of <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengrui Han, Rafal Kocielnik, Adhithya Saravanan, Roy Jiang, Or Sharir, Anima Anandkumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language models (LLMs), while powerful, exhibit harmful social biases.
Debiasing is often challenging due to computational costs, data constraints,
and potential degradation of multi-task language capabilities. This work
introduces a novel approach utilizing ChatGPT to generate synthetic training
data, aiming to enhance the debiasing of LLMs. We propose two strategies:
Targeted Prompting, which provides effective debiasing for known biases but
necessitates prior specification of bias in question; and General Prompting,
which, while slightly less effective, offers debiasing across various
categories. We leverage resource-efficient LLM debiasing using adapter tuning
and compare the effectiveness of our synthetic data to existing debiasing
datasets. Our results reveal that: (1) ChatGPT can efficiently produce
high-quality training data for debiasing other LLMs; (2) data produced via our
approach surpasses existing datasets in debiasing performance while also
preserving internal knowledge of a pre-trained LLM; and (3) synthetic data
exhibits generalizability across categories, effectively mitigating various
biases, including intersectional ones. These findings underscore the potential
of synthetic data in advancing the fairness of LLMs with minimal retraining
cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To Appear in the Proceedings of the 1st Conference on Language
  Modeling (COLM) 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">100</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do <span class="highlight-title">Pre-train</span>ed Vision-Language Models Encode Object States? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaleb Newman, Shijie Wang, Yuan Zang, David Heffren, Chen Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For a vision-language model (VLM) to understand the physical world, such as
cause and effect, a first step is to capture the temporal dynamics of the
visual world, for example how the physical states of objects evolve over time
(e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs
pre-trained on web-scale data learn to encode object states, which can be
extracted with zero-shot text prompts. We curate an object state recognition
dataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models
trained with contrastive and generative objectives. We observe that while these
state-of-the-art vision-language models can reliably perform object
recognition, they consistently fail to accurately distinguish the objects'
physical states. Through extensive experiments, we identify three areas for
improvements for VLMs to better encode object states, namely the quality of
object localization, the architecture to bind concepts to objects, and the
objective to learn discriminative visual and language encoders on object
states. Data and code are released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring 3D Face Reconstruction and Fusion Methods for Face
  Verification: A Case-Study in Video Surveillance <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Maurizio La Cava, Sara Concas, Ruben Tolosana, Roberto Casula, Giulia Orrù, Martin Drahansky, Julian Fierrez, Gian Luca Marcialis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D face reconstruction (3DFR) algorithms are based on specific assumptions
tailored to distinct application scenarios. These assumptions limit their use
when acquisition conditions, such as the subject's distance from the camera or
the camera's characteristics, are different than expected, as typically happens
in video surveillance. Additionally, 3DFR algorithms follow various strategies
to address the reconstruction of a 3D shape from 2D data, such as statistical
model fitting, photometric stereo, or deep learning. In the present study, we
explore the application of three 3DFR algorithms representative of the SOTA,
employing each one as the template set generator for a face verification
system. The scores provided by each system are combined by score-level fusion.
We show that the complementarity induced by different 3DFR algorithms improves
performance when tests are conducted at never-seen-before distances from the
camera and camera characteristics (cross-distance and cross-camera settings),
thus encouraging further investigations on multiple 3DFR-based approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at T-CAP - Towards a Complete Analysis of People:
  Fine-grained Understanding for Real-World Applications, workshop in
  conjunction with the 18th European Conference on Computer Vision ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimInversion: A Simple Framework for Inversion-Based Text-to-Image
  Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Qian, Haiyang Xu, Ming Yan, Juhua Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models demonstrate impressive image generation performance with
text guidance. Inspired by the learning process of diffusion, existing images
can be edited according to text by DDIM inversion. However, the vanilla DDIM
inversion is not optimized for classifier-free guidance and the accumulated
error will result in the undesired performance. While many algorithms are
developed to improve the framework of DDIM inversion for editing, in this work,
we investigate the approximation error in DDIM inversion and propose to
disentangle the guidance scale for the source and target branches to reduce the
error while keeping the original framework. Moreover, a better guidance scale
(i.e., 0.5) than default settings can be derived theoretically. Experiments on
PIE-Bench show that our proposal can improve the performance of DDIM inversion
dramatically without sacrificing efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lehong Wu, Lilang Lin, Jiahang Zhang, Yiyang Ma, Jiaying Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning has proved effective for skeleton-based human action
understanding. However, previous works either rely on contrastive learning that
suffers false negative problems or are based on reconstruction that learns too
much unessential low-level clues, leading to limited representations for
downstream tasks. Recently, great advances have been made in generative
learning, which is naturally a challenging yet meaningful pretext task to model
the general underlying data distributions. However, the representation learning
capacity of generative models is under-explored, especially for the skeletons
with spacial sparsity and temporal redundancy. To this end, we propose Masked
Conditional Diffusion (MacDiff) as a unified framework for human skeleton
modeling. For the first time, we leverage diffusion models as effective
skeleton representation learners. Specifically, we train a diffusion decoder
conditioned on the representations extracted by a semantic encoder. Random
masking is applied to encoder inputs to introduce a information bottleneck and
remove redundancy of skeletons. Furthermore, we theoretically demonstrate that
our generative objective involves the contrastive learning objective which
aligns the masked and noisy views. Meanwhile, it also enforces the
representation to complement for the noisy view, leading to better
generalization performance. MacDiff achieves state-of-the-art performance on
representation learning benchmarks while maintaining the competence for
generative tasks. Moreover, we leverage the diffusion model for data
augmentation, significantly enhancing the fine-tuning performance in scenarios
with scarce labeled data. Our project is available at
https://lehongwu.github.io/ECCV24MacDiff/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep-Wide Learning Assistance for Insect Pest Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toan Nguyen, Huy Nguyen, Huy Ung, Hieu Ung, Binh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate insect pest recognition plays a critical role in agriculture. It is
a challenging problem due to the intricate characteristics of insects. In this
paper, we present DeWi, novel learning assistance for insect pest
classification. With a one-stage and alternating training strategy, DeWi
simultaneously improves several Convolutional Neural Networks in two
perspectives: discrimination (by optimizing a triplet margin loss in a
supervised training manner) and generalization (via data augmentation). From
that, DeWi can learn discriminative and in-depth features of insect pests
(deep) yet still generalize well to a large number of insect categories (wide).
Experimental results show that DeWi achieves the highest performances on two
insect pest classification benchmarks (76.44\% accuracy on the IP102 dataset
and 99.79\% accuracy on the D0 dataset, respectively). In addition, extensive
evaluations and ablation studies are conducted to thoroughly investigate our
DeWi and demonstrate its superiority. Our source code is available at
https://github.com/toannguyen1904/DeWi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using
  a Single Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingpei Lu, Zekai Liang, Tristin Xie, Florian Ritcher, Shan Lin, Sainan Liu, Michael C. Yip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera-to-robot calibration is crucial for vision-based robot control and
requires effort to make it accurate. Recent advancements in markerless pose
estimation methods have eliminated the need for time-consuming physical setups
for camera-to-robot calibration. While the existing markerless pose estimation
methods have demonstrated impressive accuracy without the need for cumbersome
setups, they rely on the assumption that all the robot joints are visible
within the camera's field of view. However, in practice, robots usually move in
and out of view, and some portion of the robot may stay out-of-frame during the
whole manipulation task due to real-world constraints, leading to a lack of
sufficient visual features and subsequent failure of these approaches. To
address this challenge and enhance the applicability to vision-based robot
control, we propose a novel framework capable of estimating the robot pose with
partially visible robot manipulators. Our approach leverages the
Vision-Language Models for fine-grained robot components detection, and
integrates it into a keypoint-based pose estimation network, which enables more
robust performance in varied operational conditions. The framework is evaluated
on both public robot datasets and self-collected partial-view datasets to
demonstrate our robustness and generalizability. As a result, this method is
effective for robot pose estimation in a wider range of real-world manipulation
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, project website:
  https://sites.google.com/ucsd.edu/ctrnet-x</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Semi-Supervised Medical Image Segmentation from Spatial
  Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianying Liu, Paul Henderson, Xiao Gu, Hang Dai, Fani Deligianni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised medical image segmentation has shown promise in training
models with limited labeled data and abundant unlabeled data. However,
state-of-the-art methods ignore a potentially valuable source of unsupervised
semantic information -- spatial registration transforms between image volumes.
To address this, we propose CCT-R, a contrastive cross-teaching framework
incorporating registration information. To leverage the semantic information
available in registrations between volume pairs, CCT-R incorporates two
proposed modules: Registration Supervision Loss (RSL) and Registration-Enhanced
Positive Sampling (REPS). The RSL leverages segmentation knowledge derived from
transforms between labeled and unlabeled volume pairs, providing an additional
source of pseudo-labels. REPS enhances contrastive learning by identifying
anatomically-corresponding positives across volumes using registration
transforms. Experimental results on two challenging medical segmentation
benchmarks demonstrate the effectiveness and superiority of CCT-R across
various semi-supervised settings, with as few as one labeled case. Our code is
available at
https://github.com/kathyliu579/ContrastiveCross-teachingWithRegistration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>-and-Transfer: Dynamic Class-aware Enhancement for Few-shot
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10389v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10389v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanbo Bi, Yingchao Feng, Wenhui Diao, Peijin Wang, Yongqiang Mao, Kun Fu, Hongqi Wang, Xian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For more efficient generalization to unseen domains (classes), most Few-shot
Segmentation (FSS) would directly exploit pre-trained encoders and only
fine-tune the decoder, especially in the current era of large models. However,
such fixed feature encoders tend to be class-agnostic, inevitably activating
objects that are irrelevant to the target class. In contrast, humans can
effortlessly focus on specific objects in the line of sight. This paper mimics
the visual perception pattern of human beings and proposes a novel and powerful
prompt-driven scheme, called ``Prompt and Transfer" (PAT), which constructs a
dynamic class-aware prompting paradigm to tune the encoder for focusing on the
interested object (target class) in the current task. Three key points are
elaborated to enhance the prompting: 1) Cross-modal linguistic information is
introduced to initialize prompts for each task. 2) Semantic Prompt Transfer
(SPT) that precisely transfers the class-specific semantics within the images
to prompts. 3) Part Mask Generator (PMG) that works in conjunction with SPT to
adaptively generate different but complementary part prompts for different
individuals. Surprisingly, PAT achieves competitive performance on 4 different
tasks including standard FSS, Cross-domain FSS (e.g., CV, medical, and remote
sensing domains), Weak-label FSS, and Zero-shot Segmentation, setting new
state-of-the-arts on 11 benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mamba-ST: State Space Model for Efficient Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filippo Botti, Alex Ergasti, Leonardo Rossi, Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, Andrea Prati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of style transfer is, given a content image and a style source,
generating a new image preserving the content but with the artistic
representation of the style source. Most of the state-of-the-art architectures
use transformers or diffusion-based models to perform this task, despite the
heavy computational burden that they require. In particular, transformers use
self- and cross-attention layers which have large memory footprint, while
diffusion models require high inference time. To overcome the above, this paper
explores a novel design of Mamba, an emergent State-Space Model (SSM), called
Mamba-ST, to perform style transfer. To do so, we adapt Mamba linear equation
to simulate the behavior of cross-attention layers, which are able to combine
two separate embeddings into a single output, but drastically reducing memory
usage and time complexity. We modified the Mamba's inner equations so to accept
inputs from, and combine, two separate data streams. To the best of our
knowledge, this is the first attempt to adapt the equations of SSMs to a vision
task like style transfer without requiring any other module like
cross-attention or custom normalization layers. An extensive set of experiments
demonstrates the superiority and efficiency of our method in performing style
transfer compared to transformers and diffusion models. Results show improved
quality in terms of both ArtFID and FID metrics. Code is available at
https://github.com/FilippoBotti/MambaST.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust image representations with counterfactual contrastive learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mélanie Roschewitz, Fabio De Sousa Ribeiro, Tian Xia, Galvin Khara, Ben Glocker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive pretraining can substantially increase model generalisation and
downstream performance. However, the quality of the learned representations is
highly dependent on the data augmentation strategy applied to generate positive
pairs. Positive contrastive pairs should preserve semantic meaning while
discarding unwanted variations related to the data acquisition domain.
Traditional contrastive pipelines attempt to simulate domain shifts through
pre-defined generic image transformations. However, these do not always mimic
realistic and relevant domain variations for medical imaging such as scanner
differences. To tackle this issue, we herein introduce counterfactual
contrastive learning, a novel framework leveraging recent advances in causal
image synthesis to create contrastive positive pairs that faithfully capture
relevant domain variations. Our method, evaluated across five datasets
encompassing both chest radiography and mammography data, for two established
contrastive objectives (SimCLR and DINO-v2), outperforms standard contrastive
learning in terms of robustness to acquisition shift. Notably, counterfactual
contrastive learning achieves superior downstream performance on both
in-distribution and on external datasets, especially for images acquired with
scanners under-represented in the training set. Further experiments show that
the proposed framework extends beyond acquisition shifts, with models trained
with counterfactual contrastive learning substantially improving subgroup
performance across biological sex.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at
  https://github.com/biomedia-mira/counterfactual-contrastive/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frequency-Guided Masking for Enhanced Vision <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Karimi Monsefi, Mengxi Zhou, Nastaran Karimi Monsefi, Ser-Nam Lim, Wei-Lun Chao, Rajiv Ramnath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel frequency-based Self-Supervised Learning (SSL) approach
that significantly enhances its efficacy for pre-training. Prior work in this
direction masks out pre-defined frequencies in the input image and employs a
reconstruction loss to pre-train the model. While achieving promising results,
such an implementation has two fundamental limitations as identified in our
paper. First, using pre-defined frequencies overlooks the variability of image
frequency responses. Second, pre-trained with frequency-filtered images, the
resulting model needs relatively more data to adapt to naturally looking images
during fine-tuning. To address these drawbacks, we propose FOurier transform
compression with seLf-Knowledge distillation (FOLK), integrating two dedicated
ideas. First, inspired by image compression, we adaptively select the
masked-out frequencies based on image frequency responses, creating more
suitable SSL tasks for pre-training. Second, we employ a two-branch framework
empowered by knowledge distillation, enabling the model to take both the
filtered and original images as input, largely reducing the burden of
downstream tasks. Our experimental results demonstrate the effectiveness of
FOLK in achieving competitive performance to many state-of-the-art SSL methods
across various downstream tasks, including image classification, few-shot
learning, and semantic segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 2D or not 2D: How Does the Dimensionality of Gesture Representation
  Affect 3D Co-Speech Gesture Generation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Téo Guichoux, Laure Soulier, Nicolas Obin, Catherine Pelachaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Co-speech gestures are fundamental for communication. The advent of recent
deep learning techniques has facilitated the creation of lifelike, synchronous
co-speech gestures for Embodied Conversational Agents. "In-the-wild" datasets,
aggregating video content from platforms like YouTube via human pose detection
technologies, provide a feasible solution by offering 2D skeletal sequences
aligned with speech. Concurrent developments in lifting models enable the
conversion of these 2D sequences into 3D gesture databases. However, it is
important to note that the 3D poses estimated from the 2D extracted poses are,
in essence, approximations of the ground-truth, which remains in the 2D domain.
This distinction raises questions about the impact of gesture representation
dimensionality on the quality of generated motions - a topic that, to our
knowledge, remains largely unexplored. Our study examines the effect of using
either 2D or 3D joint coordinates as training data on the performance of
speech-to-gesture deep generative models. We employ a lifting model for
converting generated 2D pose sequences into 3D and assess how gestures created
directly in 3D stack up against those initially generated in 2D and then
converted to 3D. We perform an objective evaluation using widely used metrics
in the gesture generation field as well as a user study to qualitatively
evaluate the different approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2406.15111</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming Diffusion Models for Image Restoration: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sjölund, Thomas B. Schön
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved remarkable progress in generative modelling,
particularly in enhancing image quality to conform to human preferences.
Recently, these models have also been applied to low-level computer vision for
photo-realistic image restoration (IR) in tasks such as image denoising,
deblurring, dehazing, etc. In this review paper, we introduce key constructions
in diffusion models and survey contemporary techniques that make use of
diffusion models in solving general IR tasks. Furthermore, we point out the
main challenges and limitations of existing diffusion-based IR frameworks and
provide potential directions for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Review paper; any comments and suggestions are most welcome!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point2Graph: An End-to-end Point Cloud-based 3D Open-Vocabulary Scene
  Graph for Robot Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Xu, Ziming Luo, Qianwei Wang, Vineet Kamat, Carol Menassa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current open-vocabulary scene graph generation algorithms highly rely on both
3D scene point cloud data and posed RGB-D images and thus have limited
applications in scenarios where RGB-D images or camera poses are not readily
available. To solve this problem, we propose Point2Graph, a novel end-to-end
point cloud-based 3D open-vocabulary scene graph generation framework in which
the requirement of posed RGB-D image series is eliminated. This hierarchical
framework contains room and object detection/segmentation and open-vocabulary
classification. For the room layer, we leverage the advantage of merging the
geometry-based border detection algorithm with the learning-based region
detection to segment rooms and create a "Snap-Lookup" framework for
open-vocabulary room classification. In addition, we create an end-to-end
pipeline for the object layer to detect and classify 3D objects based solely on
3D point cloud data. Our evaluation results show that our framework can
outperform the current state-of-the-art (SOTA) open-vocabulary object and room
segmentation and classification algorithm on widely used real-scene datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VAE-QWGAN: Improving Quantum GANs for High Resolution Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Mark Thomas, Sharu Theresa Jose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel hybrid quantum generative model, the VAE-QWGAN,
which combines the strengths of a classical Variational AutoEncoder (VAE) with
a hybrid Quantum Wasserstein Generative Adversarial Network (QWGAN). The
VAE-QWGAN integrates the VAE decoder and QGAN generator into a single quantum
model with shared parameters, utilizing the VAE's encoder for latent vector
sampling during training. To generate new data from the trained model at
inference, input latent vectors are sampled from a Gaussian Mixture Model
(GMM), learnt on the training latent vectors. This, in turn, enhances the
diversity and quality of generated images. We evaluate the model's performance
on MNIST/Fashion-MNIST datasets, and demonstrate improved quality and diversity
of generated images compared to existing approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Phys3DGS: Physically-based 3D Gaussian Splatting for Inverse Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Euntae Choi, Sungjoo Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose two novel ideas (adoption of deferred rendering and mesh-based
representation) to improve the quality of 3D Gaussian splatting (3DGS) based
inverse rendering. We first report a problem incurred by hidden Gaussians,
where Gaussians beneath the surface adversely affect the pixel color in the
volume rendering adopted by the existing methods. In order to resolve the
problem, we propose applying deferred rendering and report new problems
incurred in a naive application of deferred rendering to the existing
3DGS-based inverse rendering. In an effort to improve the quality of 3DGS-based
inverse rendering under deferred rendering, we propose a novel two-step
training approach which (1) exploits mesh extraction and utilizes a hybrid
mesh-3DGS representation and (2) applies novel regularization methods to better
exploit the mesh. Our experiments show that, under relighting, the proposed
method offers significantly better rendering quality than the existing
3DGS-based inverse rendering methods. Compared with the SOTA voxel grid-based
inverse rendering method, it gives better rendering quality while offering
real-time rendering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRIVE: Dependable Robust Interpretable Visionary Ensemble Framework in
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songning Lai, Tianlang Xue, Hongru Xiao, Lijie Hu, Jiemin Wu, Ninghui Feng, Runwei Guan, Haicheng Liao, Zhenning Li, Yutao Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in autonomous driving have seen a paradigm shift towards
end-to-end learning paradigms, which map sensory inputs directly to driving
actions, thereby enhancing the robustness and adaptability of autonomous
vehicles. However, these models often sacrifice interpretability, posing
significant challenges to trust, safety, and regulatory compliance. To address
these issues, we introduce DRIVE -- Dependable Robust Interpretable Visionary
Ensemble Framework in Autonomous Driving, a comprehensive framework designed to
improve the dependability and stability of explanations in end-to-end
unsupervised autonomous driving models. Our work specifically targets the
inherent instability problems observed in the Driving through the Concept
Gridlock (DCG) model, which undermine the trustworthiness of its explanations
and decision-making processes. We define four key attributes of DRIVE:
consistent interpretability, stable interpretability, consistent output, and
stable output. These attributes collectively ensure that explanations remain
reliable and robust across different scenarios and perturbations. Through
extensive empirical evaluations, we demonstrate the effectiveness of our
framework in enhancing the stability and dependability of explanations, thereby
addressing the limitations of current models. Our contributions include an
in-depth analysis of the dependability issues within the DCG model, a rigorous
definition of DRIVE with its fundamental properties, a framework to implement
DRIVE, and novel metrics for evaluating the dependability of concept-based
explainable autonomous driving models. These advancements lay the groundwork
for the development of more reliable and trusted autonomous driving systems,
paving the way for their broader acceptance and deployment in real-world
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InfoDisent: Explainability of Image Classification Models by <span class="highlight-title">Information</span>
  Disentanglement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Łukasz Struski, Jacek Tabor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the decisions made by image classification networks is a
critical area of research in deep learning. This task is traditionally divided
into two distinct approaches: post-hoc methods and intrinsic methods. Post-hoc
methods, such as GradCam, aim to interpret the decisions of pre-trained models
by identifying regions of the image where the network focuses its attention.
However, these methods provide only a high-level overview, making it difficult
to fully understand the network's decision-making process. Conversely,
intrinsic methods, like prototypical parts models, offer a more detailed
understanding of network predictions but are constrained by specific
architectures, training methods, and datasets.
  In this paper, we introduce InfoDisent, a hybrid model that combines the
advantages of both approaches. By utilizing an information bottleneck,
InfoDisent disentangles the information in the final layer of a pre-trained
deep network, enabling the breakdown of classification decisions into basic,
understandable atomic components. Unlike standard prototypical parts
approaches, InfoDisent can interpret the decisions of pre-trained
classification networks and be used for making classification decisions,
similar to intrinsic models. We validate the effectiveness of InfoDisent on
benchmark datasets such as ImageNet, CUB-200-2011, Stanford Cars, and Stanford
Dogs for both convolutional and transformer backbones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Baking Relightable NeRF for Real-time Direct/Indirect Illumination
  Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Euntae Choi, Vincent Carpentier, Seunghun Shin, Sungjoo Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relighting, which synthesizes a novel view under a given lighting condition
(unseen in training time), is a must feature for immersive photo-realistic
experience. However, real-time relighting is challenging due to high
computation cost of the rendering equation which requires shape and material
decomposition and visibility test to model shadow. Additionally, for indirect
illumination, additional computation of rendering equation on each secondary
surface point (where reflection occurs) is required rendering real-time
relighting challenging. We propose a novel method that executes a CNN renderer
to compute primary surface points and rendering parameters, required for direct
illumination. We also present a lightweight hash grid-based renderer, for
indirect illumination, which is recursively executed to perform the secondary
ray tracing process. Both renderers are trained in a distillation from a
pre-trained teacher model and provide real-time physically-based rendering
under unseen lighting condition at a negligible loss of rendering quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Synthetic Texture <span class="highlight-title">Dataset</span>s: Challenges, Creation, and Curation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blaine Hoak, Patrick McDaniel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The influence of textures on machine learning models has been an ongoing
investigation, specifically in texture bias/learning, interpretability, and
robustness. However, due to the lack of large and diverse texture data
available, the findings in these works have been limited, as more comprehensive
evaluations have not been feasible. Image generative models are able to provide
data creation at scale, but utilizing these models for texture synthesis has
been unexplored and poses additional challenges both in creating accurate
texture images and validating those images. In this work, we introduce an
extensible methodology and corresponding new dataset for generating
high-quality, diverse texture images capable of supporting a broad set of
texture-based tasks. Our pipeline consists of: (1) developing prompts from a
range of descriptors to serve as input to text-to-image models, (2) adopting
and adapting Stable Diffusion pipelines to generate and filter the
corresponding images, and (3) further filtering down to the highest quality
images. Through this, we create the Prompted Textures Dataset (PTD), a dataset
of 362,880 texture images that span 56 textures. During the process of
generating images, we find that NSFW safety filters in image generation
pipelines are highly sensitive to texture (and flag up to 60\% of our texture
images), uncovering a potential bias in these models and presenting unique
challenges when working with texture data. Through both standard metrics and a
human evaluation, we find that our dataset is high quality and diverse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPAC: Sampling-based Progressive Attribute Compression for Dense Point
  Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolong Mao, Hui Yuan, Tian Guo, Shiqi Jiang, Raouf Hamzaoui, Sam Kwong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an end-to-end attribute compression method for dense point clouds.
The proposed method combines a frequency sampling module, an adaptive scale
feature extraction module with geometry assistance, and a global hyperprior
entropy model. The frequency sampling module uses a Hamming window and the Fast
Fourier Transform to extract high-frequency components of the point cloud. The
difference between the original point cloud and the sampled point cloud is
divided into multiple sub-point clouds. These sub-point clouds are then
partitioned using an octree, providing a structured input for feature
extraction. The feature extraction module integrates adaptive convolutional
layers and uses offset-attention to capture both local and global features.
Then, a geometry-assisted attribute feature refinement module is used to refine
the extracted attribute features. Finally, a global hyperprior model is
introduced for entropy encoding. This model propagates hyperprior parameters
from the deepest (base) layer to the other layers, further enhancing the
encoding efficiency. At the decoder, a mirrored network is used to
progressively restore features and reconstruct the color attribute through
transposed convolutional layers. The proposed method encodes base layer
information at a low bitrate and progressively adds enhancement layer
information to improve reconstruction accuracy. Compared to the latest G-PCC
test model (TMC13v23) under the MPEG common test conditions (CTCs), the
proposed method achieved an average Bjontegaard delta bitrate reduction of
24.58% for the Y component (21.23% for YUV combined) on the MPEG Category Solid
dataset and 22.48% for the Y component (17.19% for YUV combined) on the MPEG
Category Dense dataset. This is the first instance of a learning-based codec
outperforming the G-PCC standard on these datasets under the MPEG CTCs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>136pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anatomical Positional Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Goncharov, Valentin Samokhin, Eugenia Soboleva, Roman Sokolov, Boris Shirokikh, Mikhail Belyaev, Anvar Kurmukov, Ivan Oseledets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a self-supervised model producing 3D anatomical positional
embeddings (APE) of individual medical image voxels. APE encodes voxels'
anatomical closeness, i.e., voxels of the same organ or nearby organs always
have closer positional embeddings than the voxels of more distant body parts.
In contrast to the existing models of anatomical positional embeddings, our
method is able to efficiently produce a map of voxel-wise embeddings for a
whole volumetric input image, which makes it an optimal choice for different
downstream applications. We train our APE model on 8400 publicly available CT
images of abdomen and chest regions. We demonstrate its superior performance
compared with the existing models on anatomical landmark retrieval and
weakly-supervised few-shot localization of 13 abdominal organs. As a practical
application, we show how to cheaply train APE to crop raw CT images to
different anatomical regions of interest with 0.99 recall, while reducing the
image volume by 10-100 times. The code and the pre-trained APE model are
available at https://github.com/mishgon/ape .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Image Classification in Small and Unbalanced <span class="highlight-title">Dataset</span>s through
  Synthetic Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neil De La Fuente, Mireia Majó, Irina Luzko, Henry Córdova, Gloria Fernández-Esparrach, Jorge Bernal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and robust medical image classification is a challenging task,
especially in application domains where available annotated datasets are small
and present high imbalance between target classes. Considering that data
acquisition is not always feasible, especially for underrepresented classes,
our approach introduces a novel synthetic augmentation strategy using
class-specific Variational Autoencoders (VAEs) and latent space interpolation
to improve discrimination capabilities.
  By generating realistic, varied synthetic data that fills feature space gaps,
we address issues of data scarcity and class imbalance. The method presented in
this paper relies on the interpolation of latent representations within each
class, thus enriching the training set and improving the model's
generalizability and diagnostic accuracy. The proposed strategy was tested in a
small dataset of 321 images created to train and validate an automatic method
for assessing the quality of cleanliness of esophagogastroduodenoscopy images.
By combining real and synthetic data, an increase of over 18\% in the accuracy
of the most challenging underrepresented class was observed. The proposed
strategy not only benefited the underrepresented class but also led to a
general improvement in other metrics, including a 6\% increase in global
accuracy and precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance of Human Annotators in Object Detection and Segmentation of
  Remotely Sensed Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roni Blushtein-Livnon, Tal Svoray, Michael Dorman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a laboratory experiment designed to assess the
influence of annotation strategies, levels of imbalanced data, and prior
experience, on the performance of human annotators. The experiment focuses on
labeling aerial imagery, using ArcGIS Pro tools, to detect and segment
small-scale photovoltaic solar panels, selected as a case study for rectangular
objects. The experiment is conducted using images with a pixel size of
0.15\textbf{$m$}, involving both expert and non-expert participants, across
different setup strategies and target-background ratio datasets. Our findings
indicate that human annotators generally perform more effectively in object
detection than in segmentation tasks. A marked tendency to commit more Type II
errors (False Negatives, i.e., undetected objects) than Type I errors (False
Positives, i.e. falsely detecting objects that do not exist) was observed
across all experimental setups and conditions, suggesting a consistent bias in
detection and segmentation processes. Performance was better in tasks with
higher target-background ratios (i.e., more objects per unit area). Prior
experience did not significantly impact performance and may, in some cases,
even lead to overestimation in segmentation. These results provide evidence
that human annotators are relatively cautious and tend to identify objects only
when they are confident about them, prioritizing underestimation over
overestimation. Annotators' performance is also influenced by object scarcity,
showing a decline in areas with extremely imbalanced datasets and a low ratio
of target-to-background. These findings may enhance annotation strategies for
remote sensing research while efficient human annotators are crucial in an era
characterized by growing demands for high-quality training data to improve
segmentation and detection models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BAFNet: Bilateral Attention Fusion Network for Lightweight Semantic
  Segmentation of Urban Remote Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Wang, Xili Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale semantic segmentation networks often achieve high performance,
while their application can be challenging when faced with limited sample sizes
and computational resources. In scenarios with restricted network size and
computational complexity, models encounter significant challenges in capturing
long-range dependencies and recovering detailed information in images. We
propose a lightweight bilateral semantic segmentation network called bilateral
attention fusion network (BAFNet) to efficiently segment high-resolution urban
remote sensing images. The model consists of two paths, namely dependency path
and remote-local path. The dependency path utilizes large kernel attention to
acquire long-range dependencies in the image. Besides, multi-scale local
attention and efficient remote attention are designed to construct remote-local
path. Finally, a feature aggregation module is designed to effectively utilize
the different features of the two paths. Our proposed method was tested on
public high-resolution urban remote sensing datasets Vaihingen and Potsdam,
with mIoU reaching 83.20% and 86.53%, respectively. As a lightweight semantic
segmentation model, BAFNet not only outperforms advanced lightweight models in
accuracy but also demonstrates comparable performance to non-lightweight
state-of-the-art methods on two datasets, despite a tenfold variance in
floating-point operations and a fifteenfold difference in network parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hydra-SGG: Hybrid Relation Assignment for One-stage Scene Graph
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghan Chen, Guikun Chen, Wenguan Wang, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DETR introduces a simplified one-stage framework for scene graph generation
(SGG). However, DETR-based SGG models face two challenges: i) Sparse
supervision, as each image typically contains fewer than 10 relation
annotations, while the models employ over 100 relation queries. This sparsity
arises because each ground truth relation is assigned to only one single query
during training. ii) False negative samples, since one ground truth relation
may have multiple queries with similar matching scores. These suboptimally
matched queries are simply treated as negative samples, causing the loss of
valuable supervisory signals. As a response, we devise Hydra-SGG, a one-stage
SGG method that adopts a new Hybrid Relation Assignment. This assignment
combines a One-to-One Relation Assignment with a newly introduced IoU-based
One-to-Many Relation Assignment. Specifically, each ground truth is assigned to
multiple relation queries with high IoU subject-object boxes. This Hybrid
Relation Assignment increases the number of positive training samples,
alleviating sparse supervision. Moreover, we, for the first time, empirically
show that self-attention over relation queries helps reduce duplicated relation
predictions. We, therefore, propose Hydra Branch, a parameter-sharing auxiliary
decoder without a self-attention layer. This design promotes One-to-Many
Relation Assignment by enabling different queries to predict the same relation.
Hydra-SGG achieves state-of-the-art performance with 10.6 mR@20 and 16.0 mR@50
on VG150, while only requiring 12 training epochs. It also sets a new
state-of-the-art on Open Images V6 and and GQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Updating Vehicle Monitoring Framework Employing Distributed
  Acoustic Sensing towards Real-World Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Wang, Xin Liu, Songming Zhu, Zhanwen Li, Lina Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent emergence of Distributed Acoustic Sensing (DAS) technology has
facilitated the effective capture of traffic-induced seismic data. The
traffic-induced seismic wave is a prominent contributor to urban vibrations and
contain crucial information to advance urban exploration and governance.
However, identifying vehicular movements within massive noisy data poses a
significant challenge. In this study, we introduce a real-time semi-supervised
vehicle monitoring framework tailored to urban settings. It requires only a
small fraction of manual labels for initial training and exploits unlabeled
data for model improvement. Additionally, the framework can autonomously adapt
to newly collected unlabeled data. Before DAS data undergo object detection as
two-dimensional images to preserve spatial information, we leveraged
comprehensive one-dimensional signal preprocessing to mitigate noise.
Furthermore, we propose a novel prior loss that incorporates the shapes of
vehicular traces to track a single vehicle with varying speeds. To evaluate our
model, we conducted experiments with seismic data from the Stanford 2 DAS
Array. The results showed that our model outperformed the baseline model
Efficient Teacher and its supervised counterpart, YOLO (You Only Look Once), in
both accuracy and robustness. With only 35 labeled images, our model surpassed
YOLO's mAP 0.5:0.95 criterion by 18% and showed a 7% increase over Efficient
Teacher. We conducted comparative experiments with multiple update strategies
for self-updating and identified an optimal approach. This approach surpasses
the performance of non-overfitting training conducted with all data in a single
pass.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SOLVR: Submap Oriented LiDAR-Visual Re-Localisation <span class="chip">ICRA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Knights, Sebastián Barbas Laina, Peyman Moghadam, Stefan Leutenegger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes SOLVR, a unified pipeline for learning based LiDAR-Visual
re-localisation which performs place recognition and 6-DoF registration across
sensor modalities. We propose a strategy to align the input sensor modalities
by leveraging stereo image streams to produce metric depth predictions with
pose information, followed by fusing multiple scene views from a local window
using a probabilistic occupancy framework to expand the limited field-of-view
of the camera. Additionally, SOLVR adopts a flexible definition of what
constitutes positive examples for different training losses, allowing us to
simultaneously optimise place recognition and registration performance.
Furthermore, we replace RANSAC with a registration function that weights a
simple least-squares fitting with the estimated inlier likelihood of sparse
keypoint correspondences, improving performance in scenarios with a low inlier
ratio between the query and retrieved place. Our experiments on the KITTI and
KITTI360 datasets show that SOLVR achieves state-of-the-art performance for
LiDAR-Visual place recognition and registration, particularly improving
registration accuracy over larger distances between the query and retrieved
place.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICRA2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FGR-Net:Interpretable fundus imagegradeability classification based on
  deepreconstruction learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saif Khalid, Hatem A. Rashwan, Saddam Abdulwahab, Mohamed Abdel-Nasser, Facundo Manuel Quiroga, Domenec Puig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of diagnostic Computer-Aided Design (CAD) systems for retinal
diseases depends on the quality of the retinal images being screened. Thus,
many studies have been developed to evaluate and assess the quality of such
retinal images. However, most of them did not investigate the relationship
between the accuracy of the developed models and the quality of the
visualization of interpretability methods for distinguishing between gradable
and non-gradable retinal images. Consequently, this paper presents a novel
framework called FGR-Net to automatically assess and interpret underlying
fundus image quality by merging an autoencoder network with a classifier
network. The FGR-Net model also provides an interpretable quality assessment
through visualizations. In particular, FGR-Net uses a deep autoencoder to
reconstruct the input image in order to extract the visual characteristics of
the input fundus images based on self-supervised learning. The extracted
features by the autoencoder are then fed into a deep classifier network to
distinguish between gradable and ungradable fundus images. FGR-Net is evaluated
with different interpretability methods, which indicates that the autoencoder
is a key factor in forcing the classifier to focus on the relevant structures
of the fundus images, such as the fovea, optic disk, and prominent blood
vessels. Additionally, the interpretability methods can provide visual feedback
for ophthalmologists to understand how our model evaluates the quality of
fundus images. The experimental results showed the superiority of FGR-Net over
the state-of-the-art quality assessment methods, with an accuracy of 89% and an
F1-score of 87%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Bird's Eye View Segmentation by Adapting DINOv2 <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Merve Rabia Barın, Görkay Aydemir, Fatma Güney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting a Bird's Eye View (BEV) representation from multiple camera images
offers a cost-effective, scalable alternative to LIDAR-based solutions in
autonomous driving. However, the performance of the existing BEV methods drops
significantly under various corruptions such as brightness and weather changes
or camera failures. To improve the robustness of BEV perception, we propose to
adapt a large vision foundational model, DINOv2, to BEV estimation using Low
Rank Adaptation (LoRA). Our approach builds on the strong representation space
of DINOv2 by adapting it to the BEV task in a state-of-the-art framework,
SimpleBEV. Our experiments show increased robustness of BEV perception under
various corruptions, with increasing gains from scaling up the model and the
input resolution. We also showcase the effectiveness of the adapted
representations in terms of fewer learnable parameters and faster convergence
during training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024 - 2nd Workshop on Vision-Centric Autonomous Driving (VCAD)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuromorphic Facial Analysis with Cross-Modal Supervision <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Becattini, Luca Cultrera, Lorenzo Berlincioni, Claudio Ferrari, Andrea Leonardo, Alberto Del Bimbo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches for analyzing RGB frames are capable of providing a
fine-grained understanding of a face from different angles by inferring
emotions, poses, shapes, landmarks. However, when it comes to subtle movements
standard RGB cameras might fall behind due to their latency, making it hard to
detect micro-movements that carry highly informative cues to infer the true
emotions of a subject. To address this issue, the usage of event cameras to
analyze faces is gaining increasing interest. Nonetheless, all the expertise
matured for RGB processing is not directly transferrable to neuromorphic data
due to a strong domain shift and intrinsic differences in how data is
represented. The lack of labeled data can be considered one of the main causes
of this gap, yet gathering data is harder in the event domain since it cannot
be crawled from the web and labeling frames should take into account event
aggregation rates and the fact that static parts might not be visible in
certain frames. In this paper, we first present FACEMORPHIC, a multimodal
temporally synchronized face dataset comprising both RGB videos and event
streams. The data is labeled at a video level with facial Action Units and also
contains streams collected with a variety of applications in mind, ranging from
3D shape estimation to lip-reading. We then show how temporal synchronization
can allow effective neuromorphic face analysis without the need to manually
annotate videos: we instead leverage cross-modal supervision bridging the
domain gap by representing face shapes in a 3D space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the ECCV 2024 workshop on Neuromorphic
  Vision: Advantages and Applications of Event Cameras (NEVI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Garment Attribute Manipulation with Multi-level Attention <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vittorio Casula, Lorenzo Berlincioni, Luca Cultrera, Federico Becattini, Chiara Pero, Carmen Bisogni, Marco Bertini, Alberto Del Bimbo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving field of online fashion shopping, the need for more
personalized and interactive image retrieval systems has become paramount.
Existing methods often struggle with precisely manipulating specific garment
attributes without inadvertently affecting others. To address this challenge,
we propose GAMMA (Garment Attribute Manipulation with Multi-level Attention), a
novel framework that integrates attribute-disentangled representations with a
multi-stage attention-based architecture. GAMMA enables targeted manipulation
of fashion image attributes, allowing users to refine their searches with high
accuracy. By leveraging a dual-encoder Transformer and memory block, our model
achieves state-of-the-art performance on popular datasets like Shopping100k and
DeepFashion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the ECCV 2024 workshop FashionAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SteeredMarigold: Steering Diffusion Towards Depth Completion of Largely
  Incomplete Depth Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Gregorek, Lazaros Nalpantidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Even if the depth maps captured by RGB-D sensors deployed in real
environments are often characterized by large areas missing valid depth
measurements, the vast majority of depth completion methods still assumes depth
values covering all areas of the scene. To address this limitation, we
introduce SteeredMarigold, a training-free, zero-shot depth completion method
capable of producing metric dense depth, even for largely incomplete depth
maps. SteeredMarigold achieves this by using the available sparse depth points
as conditions to steer a denoising diffusion probabilistic model. Our method
outperforms relevant top-performing methods on the NYUv2 dataset, in tests
where no depth was provided for a large area, achieving state-of-art
performance and exhibiting remarkable robustness against depth map
incompleteness. Our code will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fit and Prune: Fast and Training-free Visual Token Pruning for
  Multi-modal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Ye, Qiong Wu, Wenhao Lin, Yiyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in Multimodal Large Language Models(MLLMs) often use large
image tokens to compensate the visual shortcoming of MLLMs, which not only
exhibits obvious redundancy but also greatly exacerbates the already high
computation. Token pruning is an effective solution for speeding up MLLMs, but
when and how to drop tokens still remains a challenge. In this paper, we
propose a novel and training-free approach for the effective visual token
pruning of MLLMs, termed FitPrune, which can quickly produce a complete pruning
recipe for MLLMs according to a pre-defined budget. Specifically, FitPrune
considers token pruning as a statistical problem of MLLM and its objective is
to find out an optimal pruning scheme that can minimize the divergence of the
attention distributions before and after pruning. In practice, FitPrune can be
quickly accomplished based on the attention statistics from a small batch of
inference data, avoiding the expensive trials of MLLMs. According to the
pruning recipe, an MLLM can directly remove the redundant visual tokens of
different examples during inference. To validate FitPrune, we apply it to a set
of recent MLLMs, including LLaVA-1.5, LLaVA-HR and LLaVA-NEXT, and conduct
extensive experiments on a set of benchmarks. The experimental results show
that our FitPrune can not only reduce the computational complexity to a large
extent, while retaining high performance, e.g., -54.9% FLOPs for LLaVA-NEXT
with only 0.5% accuracy drop. Notably, the pruning recipe can be obtained in
about 5 minutes. Our code is available at https://github.com/ywh187/FitPrune.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NEUSIS: A Compositional Neuro-Symbolic Framework for Autonomous
  Perception, Reasoning, and Planning in Complex UAV Search Missions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixi Cai, Cristian Rojas Cardenas, Kevin Leo, Chenyuan Zhang, Kal Backman, Hanbing Li, Boying Li, Mahsa Ghorbanali, Stavya Datta, Lizhen Qu, Julian Gutierrez Santiago, Alexey Ignatiev, Yuan-Fang Li, Mor Vered, Peter J Stuckey, Maria Garcia de la Banda, Hamid Rezatofighi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of autonomous UAV search missions, where a
UAV must locate specific Entities of Interest (EOIs) within a time limit, based
on brief descriptions in large, hazard-prone environments with keep-out zones.
The UAV must perceive, reason, and make decisions with limited and uncertain
information. We propose NEUSIS, a compositional neuro-symbolic system designed
for interpretable UAV search and navigation in realistic scenarios. NEUSIS
integrates neuro-symbolic visual perception, reasoning, and grounding (GRiD) to
process raw sensory inputs, maintains a probabilistic world model for
environment representation, and uses a hierarchical planning component (SNaC)
for efficient path planning. Experimental results from simulated urban search
missions using AirSim and Unreal Engine show that NEUSIS outperforms a
state-of-the-art (SOTA) vision-language model and a SOTA search planning model
in success rate, search efficiency, and 3D localization. These results
demonstrate the effectiveness of our compositional neuro-symbolic approach in
handling complex, real-world scenarios, making it a promising solution for
autonomous UAV systems in search missions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RealDiff: Real-world 3D Shape Completion using <span class="highlight-title">Self-Supervised</span> Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Başak Melis Öcal, Maxim Tatarchenko, Sezer Karaoglu, Theo Gevers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud completion aims to recover the complete 3D shape of an object
from partial observations. While approaches relying on synthetic shape priors
achieved promising results in this domain, their applicability and
generalizability to real-world data are still limited. To tackle this problem,
we propose a self-supervised framework, namely RealDiff, that formulates point
cloud completion as a conditional generation problem directly on real-world
measurements. To better deal with noisy observations without resorting to
training on synthetic data, we leverage additional geometric cues.
Specifically, RealDiff simulates a diffusion process at the missing object
parts while conditioning the generation on the partial input to address the
multimodal nature of the task. We further regularize the training by matching
object silhouettes and depth maps, predicted by our method, with the externally
estimated ones. Experimental results show that our method consistently
outperforms state-of-the-art methods in real-world point cloud completion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExelMap: Explainable Element-based HD-Map Change Detection and Update 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lena Wild, Ludvig Ericson, Rafael Valencia, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acquisition and maintenance are central problems in deploying high-definition
(HD) maps for autonomous driving, with two lines of research prevalent in
current literature: Online HD map generation and HD map change detection.
However, the generated map's quality is currently insufficient for safe
deployment, and many change detection approaches fail to precisely localize and
extract the changed map elements, hence lacking explainability and hindering a
potential fleet-based cooperative HD map update. In this paper, we propose the
novel task of explainable element-based HD map change detection and update. In
extending recent approaches that use online mapping techniques informed with an
outdated map prior for HD map updating, we present ExelMap, an explainable
element-based map updating strategy that specifically identifies changed map
elements. In this context, we discuss how currently used metrics fail to
capture change detection performance, while allowing for unfair comparison
between prior-less and prior-informed map generation methods. Finally, we
present an experimental study on real-world changes related to pedestrian
crossings of the Argoverse 2 Map Change Dataset. To the best of our knowledge,
this is the first comprehensive problem investigation of real-world end-to-end
element-based HD map change detection and update, and ExelMap the first
proposed solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoRun2D: Cost-Effective Markerless Motion Capture for Sprint
  Biomechanics <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gonzalo Garrido-Lopez, Luis F. Gomez, Julian Fierrez, Aythami Morales, Ruben Tolosana, Javier Rueda, Enrique Navarro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sprinting is a determinant ability, especially in team sports. The kinematics
of the sprint have been studied in the past using different methods specially
developed considering human biomechanics and, among those methods, markerless
systems stand out as very cost-effective. On the other hand, we have now
multiple general methods for pixel and body tracking based on recent machine
learning breakthroughs with excellent performance in body tracking, but these
excellent trackers do not generally consider realistic human biomechanics. This
investigation first adapts two of these general trackers (MoveNet and
CoTracker) for realistic biomechanical analysis and then evaluate them in
comparison to manual tracking (with key points manually marked using the
software Kinovea).
  Our best resulting markerless body tracker particularly adapted for sprint
biomechanics is termed VideoRun2D. The experimental development and assessment
of VideoRun2D is reported on forty sprints recorded with a video camera from 5
different subjects, focusing our analysis in 3 key angles in sprint
biomechanics: inclination of the trunk, flex extension of the hip and the knee.
The CoTracker method showed huge differences compared to the manual labeling
approach. However, the angle curves were correctly estimated by the MoveNet
method, finding errors between 3.2{\deg} and 5.5{\deg}.
  In conclusion, our proposed VideoRun2D based on MoveNet core seems to be a
helpful tool for evaluating sprint kinematics in some scenarios. On the other
hand, the observed precision of this first version of VideoRun2D as a
markerless sprint analysis system may not be yet enough for highly demanding
applications. Future research lines towards that purpose are also discussed at
the end: better tracking post-processing and user- and time-dependent
adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of the paper presented to the Workshop on IAPR International
  Conference on Pattern Recognition (ICPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using
  Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Nomaan Qureshi, Sparsh Garg, Francisco Yandun, David Held, George Kantor, Abhishesh Silwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sim2Real transfer, particularly for manipulation policies relying on RGB
images, remains a critical challenge in robotics due to the significant domain
shift between synthetic and real-world visual data. In this paper, we propose
SplatSim, a novel framework that leverages Gaussian Splatting as the primary
rendering primitive to reduce the Sim2Real gap for RGB-based manipulation
policies. By replacing traditional mesh representations with Gaussian Splats in
simulators, SplatSim produces highly photorealistic synthetic data while
maintaining the scalability and cost-efficiency of simulation. We demonstrate
the effectiveness of our framework by training manipulation policies within
SplatSim}and deploying them in the real world in a zero-shot manner, achieving
an average success rate of 86.25%, compared to 97.5% for policies trained on
real-world data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning for Character Detection in Ancient Greek Papyri 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vedasri Nakka, Andreas Fischer, Rolf Ingold, Lars Vogtlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis investigates the effectiveness of SimCLR, a contrastive learning
technique, in Greek letter recognition, focusing on the impact of various
augmentation techniques. We pretrain the SimCLR backbone using the Alpub
dataset (pretraining dataset) and fine-tune it on a smaller ICDAR dataset
(finetuning dataset) to compare SimCLR's performance against traditional
baseline models, which use cross-entropy and triplet loss functions.
Additionally, we explore the role of different data augmentation strategies,
essential for the SimCLR training process. Methodologically, we examine three
primary approaches: (1) a baseline model using cross-entropy loss, (2) a
triplet embedding model with a classification layer, and (3) a SimCLR
pretrained model with a classification layer. Initially, we train the baseline,
triplet, and SimCLR models using 93 augmentations on ResNet-18 and ResNet-50
networks with the ICDAR dataset. From these, the top four augmentations are
selected using a statistical t-test. Pretraining of SimCLR is conducted on the
Alpub dataset, followed by fine-tuning on the ICDAR dataset. The triplet loss
model undergoes a similar process, being pretrained on the top four
augmentations before fine-tuning on ICDAR. Our experiments show that SimCLR
does not outperform the baselines in letter recognition tasks. The baseline
model with cross-entropy loss demonstrates better performance than both SimCLR
and the triplet loss model. This study provides a detailed evaluation of
contrastive learning for letter recognition, highlighting SimCLR's limitations
while emphasizing the strengths of traditional supervised learning models in
this task. We believe SimCLR's cropping strategies may cause a semantic shift
in the input image, reducing training effectiveness despite the large
pretraining dataset. Our code is available at
https://github.com/DIVA-DIA/MT_augmentation_and_contrastive_learning/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoPET Challenge III: Testing the Robustness of Generalized Dice Focal
  Loss trained 3D Residual UNet for FDG and PSMA Lesion Segmentation from
  Whole-Body PET/CT Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shadab Ahamed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated segmentation of cancerous lesions in PET/CT scans is a crucial
first step in quantitative image analysis. However, training deep learning
models for segmentation with high accuracy is particularly challenging due to
the variations in lesion size, shape, and radiotracer uptake. These lesions can
appear in different parts of the body, often near healthy organs that also
exhibit considerable uptake, making the task even more complex. As a result,
creating an effective segmentation model for routine PET/CT image analysis is
challenging. In this study, we utilized a 3D Residual UNet model and employed
the Generalized Dice Focal Loss function to train the model on the AutoPET
Challenge 2024 dataset. We conducted a 5-fold cross-validation and used an
average ensembling technique using the models from the five folds. In the
preliminary test phase for Task-1, the average ensemble achieved a mean Dice
Similarity Coefficient (DSC) of 0.6687, mean false negative volume (FNV) of
10.9522 ml and mean false positive volume (FPV) 2.9684 ml. More details about
the algorithm can be found on our GitHub repository:
https://github.com/ahxmeds/autosegnet2024.git. The training code has been
shared via the repository: https://github.com/ahxmeds/autopet2024.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, 2 tables. arXiv admin note: substantial text
  overlap with arXiv:2309.13553</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ P2U-SLAM: A Monocular Wide-FoV SLAM System Based on Point Uncertainty
  and Pose Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufan Zhang, Kailun Yang, Ze Wang, Kaiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents P2U-SLAM, a visual Simultaneous Localization And Mapping
(SLAM) system with a wide Field of View (FoV) camera, which utilizes pose
uncertainty and point uncertainty. While the wide FoV enables considerable
repetitive observations of historical map points for matching cross-view
features, the data properties of the historical map points and the poses of
historical keyframes have changed during the optimization process. The neglect
of data property changes triggers the absence of a partial information matrix
in optimization and leads to the risk of long-term positioning performance
degradation. The purpose of our research is to reduce the risk of the wide
field of view visual input to the SLAM system. Based on the conditional
probability model, this work reveals the definite impact of the above data
properties changes on the optimization process, concretizes it as point
uncertainty and pose uncertainty, and gives a specific mathematical form.
P2U-SLAM respectively embeds point uncertainty and pose uncertainty into the
tracking module and local mapping, and updates these uncertainties after each
optimization operation including local mapping, map merging, and loop closing.
We present an exhaustive evaluation in 27 sequences from two popular public
datasets with wide-FoV visual input. P2U-SLAM shows excellent performance
compared with other state-of-the-art methods. The source code will be made
publicly available at https://github.com/BambValley/P2U-SLAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code will be made publicly available at
  https://github.com/BambValley/P2U-SLAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PSHuman: Photorealistic Single-view Human Reconstruction using
  Cross-Scale Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Li, Wangguandong Zheng, Yuan Liu, Tao Yu, Yangguang Li, Xingqun Qi, Mengfei Li, Xiaowei Chi, Siyu Xia, Wei Xue, Wenhan Luo, Qifeng Liu, Yike Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detailed and photorealistic 3D human modeling is essential for various
applications and has seen tremendous progress. However, full-body
reconstruction from a monocular RGB image remains challenging due to the
ill-posed nature of the problem and sophisticated clothing topology with
self-occlusions. In this paper, we propose PSHuman, a novel framework that
explicitly reconstructs human meshes utilizing priors from the multiview
diffusion model. It is found that directly applying multiview diffusion on
single-view human images leads to severe geometric distortions, especially on
generated faces. To address it, we propose a cross-scale diffusion that models
the joint probability distribution of global full-body shape and local facial
characteristics, enabling detailed and identity-preserved novel-view generation
without any geometric distortion. Moreover, to enhance cross-view body shape
consistency of varied human poses, we condition the generative model on
parametric models like SMPL-X, which provide body priors and prevent unnatural
views inconsistent with human anatomy. Leveraging the generated multi-view
normal and color images, we present SMPLX-initialized explicit human carving to
recover realistic textured human meshes efficiently. Extensive experimental
results and quantitative evaluations on CAPE and THuman2.1 datasets demonstrate
PSHumans superiority in geometry details, texture fidelity, and generalization
capability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Centric Strategies for Overcoming PET/CT Heterogeneity: Insights
  from the AutoPET III Lesion Segmentation Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Balint Kovacs, Shuhan Xiao, Maximilian Rokuss, Constantin Ulrich, Fabian Isensee, Klaus H. Maier-Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The third autoPET challenge introduced a new data-centric task this year,
shifting the focus from model development to improving metastatic lesion
segmentation on PET/CT images through data quality and handling strategies. In
response, we developed targeted methods to enhance segmentation performance
tailored to the characteristics of PET/CT imaging. Our approach encompasses two
key elements. First, to address potential alignment errors between CT and PET
modalities as well as the prevalence of punctate lesions, we modified the
baseline data augmentation scheme and extended it with misalignment
augmentation. This adaptation aims to improve segmentation accuracy,
particularly for tiny metastatic lesions. Second, to tackle the variability in
image dimensions significantly affecting the prediction time, we implemented a
dynamic ensembling and test-time augmentation (TTA) strategy. This method
optimizes the use of ensembling and TTA within a 5-minute prediction time
limit, effectively leveraging the generalization potential for both small and
large images. Both of our solutions are designed to be robust across different
tracers and institutional settings, offering a general, yet imaging-specific
approach to the multi-tracer and multi-institutional challenges of the
competition. We made the challenge repository with our modifications publicly
available at \url{https://github.com/MIC-DKFZ/miccai2024_autopet3_datacentric}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Contribution to the data-centric task of the autoPET III Challenge
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study of Open Source Computer Vision Models for
  Application on Small Data: The Case of CFRP Tape Laying 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Fraunholz, Dennis Rall, Tim Köhler, Alfons Schuster, Monika Mayer, Lars Larsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of industrial manufacturing, Artificial Intelligence (AI) is
playing an increasing role, from automating existing processes to aiding in the
development of new materials and techniques. However, a significant challenge
arises in smaller, experimental processes characterized by limited training
data availability, questioning the possibility to train AI models in such small
data contexts. In this work, we explore the potential of Transfer Learning to
address this challenge, specifically investigating the minimum amount of data
required to develop a functional AI model. For this purpose, we consider the
use case of quality control of Carbon Fiber Reinforced Polymer (CFRP) tape
laying in aerospace manufacturing using optical sensors. We investigate the
behavior of different open-source computer vision models with a continuous
reduction of the training data. Our results show that the amount of data
required to successfully train an AI model can be drastically reduced, and the
use of smaller models does not necessarily lead to a loss of performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Segmentation-Based Initialization for Steered Mixture of
  Experts Image Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Hsin Li, Sebastian Knorr, Mårten Sjöström, Thomas Sikora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kernel image regression methods have shown to provide excellent efficiency in
many image processing task, such as image and light-field compression, Gaussian
Splatting, denoising and super-resolution. The estimation of parameters for
these methods frequently employ gradient descent iterative optimization, which
poses significant computational burden for many applications. In this paper, we
introduce a novel adaptive segmentation-based initialization method targeted
for optimizing Steered-Mixture-of Experts (SMoE) gating networks and
Radial-Basis-Function (RBF) networks with steering kernels. The novel
initialization method allocates kernels into pre-calculated image segments. The
optimal number of kernels, kernel positions, and steering parameters are
derived per segment in an iterative optimization and kernel sparsification
procedure. The kernel information from "local" segments is then transferred
into a "global" initialization, ready for use in iterative optimization of
SMoE, RBF, and related kernel image regression methods. Results show that
drastic objective and subjective quality improvements are achievable compared
to widely used regular grid initialization, "state-of-the-art" K-Means
initialization and previously introduced segmentation-based initialization
methods, while also drastically improving the sparsity of the regression
models. For same quality, the novel initialization results in models with
around 50% reduction of kernels. In addition, a significant reduction of
convergence time is achieved, with overall run-time savings of up to 50%. The
segmentation-based initialization strategy itself admits heavy parallel
computation; in theory, it may be divided into as many tasks as there are
segments in the images. By accessing only four parallel GPUs, run-time savings
of already 50% for initialization are achievable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Insights Driven Latent Space for Different Driving Perspectives: A
  Unified Encoder for Efficient Multi-Task Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy-Dung Nguyen, Anass Bairouk, Mirjana Maras, Wei Xiao, Tsun-Hsuan Wang, Patrick Chareyre, Ramin Hasani, Marc Blanchon, Daniela Rus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving holds great potential to transform road safety and traffic
efficiency by minimizing human error and reducing congestion. A key challenge
in realizing this potential is the accurate estimation of steering angles,
which is essential for effective vehicle navigation and control. Recent
breakthroughs in deep learning have made it possible to estimate steering
angles directly from raw camera inputs. However, the limited available
navigation data can hinder optimal feature learning, impacting the system's
performance in complex driving scenarios. In this paper, we propose a shared
encoder trained on multiple computer vision tasks critical for urban
navigation, such as depth, pose, and 3D scene flow estimation, as well as
semantic, instance, panoptic, and motion segmentation. By incorporating diverse
visual information used by humans during navigation, this unified encoder might
enhance steering angle estimation. To achieve effective multi-task learning
within a single encoder, we introduce a multi-scale feature network for pose
estimation to improve depth learning. Additionally, we employ knowledge
distillation from a multi-backbone model pretrained on these navigation tasks
to stabilize training and boost performance. Our findings demonstrate that a
shared backbone trained on diverse visual tasks is capable of providing overall
perception capabilities. While our performance in steering angle estimation is
comparable to existing methods, the integration of human-like perception
through multi-task learning holds significant potential for advancing
autonomous driving systems. More details and the pretrained model are available
at https://hi-computervision.github.io/uni-encoder/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDoS: Diffusion Distribution Similarity for Out-of-Distribution
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Fang, Qinghua Tao, Zuopeng Yang, Xiaolin Huang, Jie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-Distribution (OoD) detection determines whether the given samples are
from the training distribution of the classifier-under-protection, i.e., the
In-Distribution (InD), or from a different OoD. Latest researches introduce
diffusion models pre-trained on InD data to advocate OoD detection by
transferring an OoD image into a generated one that is close to InD, so that
one could capture the distribution disparities between original and generated
images to detect OoD data. Existing diffusion-based detectors adopt perceptual
metrics on the two images to measure such disparities, but ignore a fundamental
fact: Perceptual metrics are devised essentially for human-perceived
similarities of low-level image patterns, e.g., textures and colors, and are
not advisable in evaluating distribution disparities, since images with
different low-level patterns could possibly come from the same distribution. To
address this issue, we formulate a diffusion-based detection framework that
considers the distribution similarity between a tested image and its generated
counterpart via a novel proper similarity metric in the informative feature
space and probability space learned by the classifier-under-protection. An
anomaly-removal strategy is further presented to enlarge such distribution
disparities by removing abnormal OoD information in the feature space to
facilitate the detection. Extensive empirical results unveil the insufficiency
of perceptual metrics and the effectiveness of our distribution similarity
framework with new state-of-the-art detection performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MotionCom: Automatic and Motion-Aware Image Composition with <span class="highlight-title">LLM</span> and
  Video Diffusion Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijing Tao, Xiaofeng Yang, Miaomiao Cui, Guosheng Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents MotionCom, a training-free motion-aware diffusion based
image composition, enabling automatic and seamless integration of target
objects into new scenes with dynamically coherent results without finetuning or
optimization. Traditional approaches in this area suffer from two significant
limitations: they require manual planning for object placement and often
generate static compositions lacking motion realism. MotionCom addresses these
issues by utilizing a Large Vision Language Model (LVLM) for intelligent
planning, and a Video Diffusion prior for motion-infused image synthesis,
streamlining the composition process. Our multi-modal Chain-of-Thought (CoT)
prompting with LVLM automates the strategic placement planning of foreground
objects, considering their potential motion and interaction within the scenes.
Complementing this, we propose a novel method MotionPaint to distill
motion-aware information from pretrained video diffusion models in the
generation phase, ensuring that these objects are not only seamlessly
integrated but also endowed with realistic motion. Extensive quantitative and
qualitative results highlight MotionCom's superiority, showcasing its
efficiency in streamlining the planning process and its capability to produce
compositions that authentically depict motion and interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-modality image synthesis from TOF-MRA to CTA using diffusion-based
  models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Koch, Orhun Utku Aydin, Adam Hilbert, Jana Rieger, Satoru Tanioka, Fujimaro Ishida, Dietmar Frey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cerebrovascular disease often requires multiple imaging modalities for
accurate diagnosis, treatment, and monitoring. Computed Tomography Angiography
(CTA) and Time-of-Flight Magnetic Resonance Angiography (TOF-MRA) are two
common non-invasive angiography techniques, each with distinct strengths in
accessibility, safety, and diagnostic accuracy. While CTA is more widely used
in acute stroke due to its faster acquisition times and higher diagnostic
accuracy, TOF-MRA is preferred for its safety, as it avoids radiation exposure
and contrast agent-related health risks. Despite the predominant role of CTA in
clinical workflows, there is a scarcity of open-source CTA data, limiting the
research and development of AI models for tasks such as large vessel occlusion
detection and aneurysm segmentation. This study explores diffusion-based
image-to-image translation models to generate synthetic CTA images from TOF-MRA
input. We demonstrate the modality conversion from TOF-MRA to CTA and show that
diffusion models outperform a traditional U-Net-based approach. Our work
compares different state-of-the-art diffusion architectures and samplers,
offering recommendations for optimal model performance in this cross-modality
translation task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAE-Fuse: An Adaptive Discriminative Autoencoder for Multi-Modality
  Image Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Guo, Ruoxiang Xu, Rongcheng Li, Zhenghao Wu, Weifeng Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modality image fusion aims to integrate complementary data information
from different imaging modalities into a single image. Existing methods often
generate either blurry fused images that lose fine-grained semantic information
or unnatural fused images that appear perceptually cropped from the inputs. In
this work, we propose a novel two-phase discriminative autoencoder framework,
termed DAE-Fuse, that generates sharp and natural fused images. In the
adversarial feature extraction phase, we introduce two discriminative blocks
into the encoder-decoder architecture, providing an additional adversarial loss
to better guide feature extraction by reconstructing the source images. While
the two discriminative blocks are adapted in the attention-guided
cross-modality fusion phase to distinguish the structural differences between
the fused output and the source inputs, injecting more naturalness into the
results. Extensive experiments on public infrared-visible, medical image
fusion, and downstream object detection datasets demonstrate our method's
superiority and generalizability in both quantitative and qualitative
evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Physically-Realizable Adversarial Attacks in Embodied Vision
  Navigation <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Chen, Jiawei Tu, Chao Qi, Yonghao Dang, Feng Zhou, Wei Wei, Jianqin Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of embodied navigation agents in safety-critical environments
raises concerns about their vulnerability to adversarial attacks on deep neural
networks. However, current attack methods often lack practicality due to
challenges in transitioning from the digital to the physical world, while
existing physical attacks for object detection fail to achieve both multi-view
effectiveness and naturalness. To address this, we propose a practical attack
method for embodied navigation by attaching adversarial patches with learnable
textures and opacity to objects. Specifically, to ensure effectiveness across
varying viewpoints, we employ a multi-view optimization strategy based on
object-aware sampling, which uses feedback from the navigation model to
optimize the patch's texture. To make the patch inconspicuous to human
observers, we introduce a two-stage opacity optimization mechanism, where
opacity is refined after texture optimization. Experimental results show our
adversarial patches reduce navigation success rates by about 40%, outperforming
previous methods in practicality, effectiveness, and naturalness. Code is
available at:
[https://github.com/chen37058/Physical-Attacks-in-Embodied-Navigation].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, submitted to the 2025 IEEE International
  Conference on Robotics & Automation (ICRA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DENSER: 3D Gaussians Splatting for Scene Reconstruction of Dynamic Urban
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmud A. Mohamad, Gamal Elghazaly, Arthur Hubert, Raphael Frank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents DENSER, an efficient and effective approach leveraging 3D
Gaussian splatting (3DGS) for the reconstruction of dynamic urban environments.
While several methods for photorealistic scene representations, both implicitly
using neural radiance fields (NeRF) and explicitly using 3DGS have shown
promising results in scene reconstruction of relatively complex dynamic scenes,
modeling the dynamic appearance of foreground objects tend to be challenging,
limiting the applicability of these methods to capture subtleties and details
of the scenes, especially far dynamic objects. To this end, we propose DENSER,
a framework that significantly enhances the representation of dynamic objects
and accurately models the appearance of dynamic objects in the driving scene.
Instead of directly using Spherical Harmonics (SH) to model the appearance of
dynamic objects, we introduce and integrate a new method aiming at dynamically
estimating SH bases using wavelets, resulting in better representation of
dynamic objects appearance in both space and time. Besides object appearance,
DENSER enhances object shape representation through densification of its point
cloud across multiple scene frames, resulting in faster convergence of model
training. Extensive evaluations on KITTI dataset show that the proposed
approach significantly outperforms state-of-the-art methods by a wide margin.
Source codes and models will be uploaded to this repository
https://github.com/sntubix/denser
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AttnMod: Attention-Based New Art Styles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10028v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10028v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shih-Chieh Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imagine a human artist looking at the generated photo of a diffusion model,
and hoping to create a painting out of it. There could be some feature of the
object in the photo that the artist wants to emphasize, some color to disperse,
some silhouette to twist, or some part of the scene to be materialized. These
intentions can be viewed as the modification of the cross attention from the
text prompt onto UNet, during the desoising diffusion. This work presents
AttnMod, to modify attention for creating new unpromptable art styles out of
existing diffusion models. The style-creating behavior is studied across
different setups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LithoHoD: A Litho Simulator-Powered Framework for IC Layout Hotspot
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Chiang Shao, Guan-Yu Chen, Yu-Hsien Lin, Chia-Wen Lin, Shao-Yun Fang, Pin-Yian Tsai, Yan-Hsiu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in VLSI fabrication technology have led to die shrinkage and
increased layout density, creating an urgent demand for advanced hotspot
detection techniques. However, by taking an object detection network as the
backbone, recent learning-based hotspot detectors learn to recognize only the
problematic layout patterns in the training data. This fact makes these hotspot
detectors difficult to generalize to real-world scenarios. We propose a novel
lithography simulator-powered hotspot detection framework to overcome this
difficulty. Our framework integrates a lithography simulator with an object
detection backbone, merging the extracted latent features from both the
simulator and the object detector via well-designed cross-attention blocks.
Consequently, the proposed framework can be used to detect potential hotspot
regions based on I) the variation of possible circuit shape deformation
estimated by the lithography simulator, and ii) the problematic layout patterns
already known. To this end, we utilize RetinaNet with a feature pyramid network
as the object detection backbone and leverage LithoNet as the lithography
simulator. Extensive experiments demonstrate that our proposed simulator-guided
hotspot detection framework outperforms previous state-of-the-art methods on
real-world data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages to appear in IEEE Transactions on Computer-Aided Design of
  Integrated Circuits and Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 2S-ODIS: Two-Stage Omni-Directional Image Synthesis by Geometric
  Distortion Correction <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsuya Nakata, Takao Yamanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Omni-directional images have been increasingly used in various applications,
including virtual reality and SNS (Social Networking Services). However, their
availability is comparatively limited in contrast to normal field of view
(NFoV) images, since specialized cameras are required to take omni-directional
images. Consequently, several methods have been proposed based on generative
adversarial networks (GAN) to synthesize omni-directional images, but these
approaches have shown difficulties in training of the models, due to
instability and/or significant time consumption in the training. To address
these problems, this paper proposes a novel omni-directional image synthesis
method, 2S-ODIS (Two-Stage Omni-Directional Image Synthesis), which generated
high-quality omni-directional images but drastically reduced the training time.
This was realized by utilizing the VQGAN (Vector Quantized GAN) model
pre-trained on a large-scale NFoV image database such as ImageNet without
fine-tuning. Since this pre-trained model does not represent distortions of
omni-directional images in the equi-rectangular projection (ERP), it cannot be
applied directly to the omni-directional image synthesis in ERP. Therefore,
two-stage structure was adopted to first create a global coarse image in ERP
and then refine the image by integrating multiple local NFoV images in the
higher resolution to compensate the distortions in ERP, both of which are based
on the pre-trained VQGAN model. As a result, the proposed method, 2S-ODIS,
achieved the reduction of the training time from 14 days in OmniDreamer to four
days in higher image quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2024 https://github.com/islab-sophia/2S-ODIS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Intelligence-Based Opportunistic Coronary Calcium Screening
  in the Veterans Affairs National Healthcare System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raffi Hagopian, Timothy Strebel, Simon Bernatz, Gregory A Myers, Erik Offerman, Eric Zuniga, Cy Y Kim, Angie T Ng, James A Iwaz, Sunny P Singh, Evan P Carey, Michael J Kim, R Spencer Schaefer, Jeannie Yu, Amilcare Gentili, Hugo JWL Aerts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coronary artery calcium (CAC) is highly predictive of cardiovascular events.
While millions of chest CT scans are performed annually in the United States,
CAC is not routinely quantified from scans done for non-cardiac purposes. A
deep learning algorithm was developed using 446 expert segmentations to
automatically quantify CAC on non-contrast, non-gated CT scans (AI-CAC). Our
study differs from prior works as we leverage imaging data across the Veterans
Affairs national healthcare system, from 98 medical centers, capturing
extensive heterogeneity in imaging protocols, scanners, and patients. AI-CAC
performance on non-gated scans was compared against clinical standard ECG-gated
CAC scoring. Non-gated AI-CAC differentiated zero vs. non-zero and less than
100 vs. 100 or greater Agatston scores with accuracies of 89.4% (F1 0.93) and
87.3% (F1 0.89), respectively, in 795 patients with paired gated scans within a
year of a non-gated CT scan. Non-gated AI-CAC was predictive of 10-year
all-cause mortality (CAC 0 vs. >400 group: 25.4% vs. 60.2%, Cox HR 3.49, p <
0.005), and composite first-time stroke, MI, or death (CAC 0 vs. >400 group:
33.5% vs. 63.8%, Cox HR 3.00, p < 0.005). In a screening dataset of 8,052
patients with low-dose lung cancer-screening CTs (LDCT), 3,091/8,052 (38.4%)
individuals had AI-CAC >400. Four cardiologists qualitatively reviewed LDCT
images from a random sample of >400 AI-CAC patients and verified that 527/531
(99.2%) would benefit from lipid-lowering therapy. To the best of our
knowledge, this is the first non-gated CT CAC algorithm developed across a
national healthcare system, on multiple imaging protocols, without filtering
intra-cardiac hardware, and compared against a strong gated CT reference. We
report superior performance relative to previous CAC algorithms evaluated
against paired gated scans that included patients with intra-cardiac hardware.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Guided Appearance-Motion Association Network for
  Out-of-Distribution Action Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Fang, Arvind Easwaran, Blaise Genest
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection targets to detect and reject test samples
with semantic shifts, to prevent models trained on in-distribution (ID) dataset
from producing unreliable predictions. Existing works only extract the
appearance features on image datasets, and cannot handle dynamic multimedia
scenarios with much motion information. Therefore, we target a more realistic
and challenging OOD detection task: OOD action detection (ODAD). Given an
untrimmed video, ODAD first classifies the ID actions and recognizes the OOD
actions, and then localizes ID and OOD actions. To this end, in this paper, we
propose a novel Uncertainty-Guided Appearance-Motion Association Network
(UAAN), which explores both appearance features and motion contexts to reason
spatial-temporal inter-object interaction for ODAD.Firstly, we design separate
appearance and motion branches to extract corresponding appearance-oriented and
motion-aspect object representations. In each branch, we construct a
spatial-temporal graph to reason appearance-guided and motion-driven
inter-object interaction. Then, we design an appearance-motion attention module
to fuse the appearance and motion features for final action detection.
Experimental results on two challenging datasets show that UAAN beats
state-of-the-art methods by a significant margin, illustrating its
effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MIPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Real-Time Generation of Delay-Compensated Video Feeds for
  Outdoor Mobile Robot Teleoperation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neeloy Chakraborty, Yixiao Fang, Andre Schreiber, Tianchen Ji, Zhe Huang, Aganze Mihigo, Cassidy Wall, Abdulrahman Almana, Katherine Driggs-Campbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teleoperation is an important technology to enable supervisors to control
agricultural robots remotely. However, environmental factors in dense crop rows
and limitations in network infrastructure hinder the reliability of data
streamed to teleoperators. These issues result in delayed and variable frame
rate video feeds that often deviate significantly from the robot's actual
viewpoint. We propose a modular learning-based vision pipeline to generate
delay-compensated images in real-time for supervisors. Our extensive offline
evaluations demonstrate that our method generates more accurate images compared
to state-of-the-art approaches in our setting. Additionally, we are one of the
few works to evaluate a delay-compensation method in outdoor field environments
with complex terrain on data from a real robot in real-time. Additional videos
are provided at https://sites.google.com/illinois.edu/comp-teleop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forearm Ultrasound based Gesture Recognition on Edge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keshav Bimbraw, Haichong K. Zhang, Bashima Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound imaging of the forearm has demonstrated significant potential for
accurate hand gesture classification. Despite this progress, there has been
limited focus on developing a stand-alone end- to-end gesture recognition
system which makes it mobile, real-time and more user friendly. To bridge this
gap, this paper explores the deployment of deep neural networks for forearm
ultrasound-based hand gesture recognition on edge devices. Utilizing
quantization techniques, we achieve substantial reductions in model size while
maintaining high accuracy and low latency. Our best model, with Float16
quantization, achieves a test accuracy of 92% and an inference time of 0.31
seconds on a Raspberry Pi. These results demonstrate the feasibility of
efficient, real-time gesture recognition on resource-limited edge devices,
paving the way for wearable ultrasound-based systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Please contact the authors for code and any additional questions
  pertaining to the project. You can reach Keshav Bimbraw at bimbrawkeshav at
  gmail dot com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rapid Adaptation of Earth Observation Foundation Models for Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karthick Panner Selvam, Raul Ramos-Pollan, Freddie Kalaitzis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the efficacy of Low-Rank Adaptation (LoRA) in
fine-tuning Earth Observation (EO) foundation models for flood segmentation. We
hypothesize that LoRA, a parameter-efficient technique, can significantly
accelerate the adaptation of large-scale EO models to this critical task while
maintaining high performance. We apply LoRA to fine-tune a state-of-the-art EO
foundation model pre-trained on diverse satellite imagery, using a curated
dataset of flood events. Our results demonstrate that LoRA-based fine-tuning
(r-256) improves F1 score by 6.66 points and IoU by 0.11 compared to a frozen
encoder baseline, while significantly reducing computational costs. Notably,
LoRA outperforms full fine-tuning, which proves computationally infeasible on
our hardware. We further assess generalization through out-of-distribution
(OOD) testing on a geographically distinct flood event. While LoRA
configurations show improved OOD performance over the baseline. This work
contributes to research on efficient adaptation of foundation models for
specialized EO tasks, with implications for rapid response systems in disaster
management. Our findings demonstrate LoRA's potential for enabling faster
deployment of accurate flood segmentation models in resource-constrained,
time-critical scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Visual Inertial SLAM with Magnetic Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bharat Joshi, Ioannis Rekleitis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an extension to visual inertial odometry (VIO) by
introducing tightly-coupled fusion of magnetometer measurements. A sliding
window of keyframes is optimized by minimizing re-projection errors, relative
inertial errors, and relative magnetometer orientation errors. The results of
IMU orientation propagation are used to efficiently transform magnetometer
measurements between frames producing relative orientation constraints between
consecutive frames. The soft and hard iron effects are calibrated using an
ellipsoid fitting algorithm. The introduction of magnetometer data results in
significant reductions in the orientation error and also in recovery of the
true yaw orientation with respect to the magnetic north. The proposed framework
operates in all environments with slow-varying magnetic fields, mainly outdoors
and underwater. We have focused our work on the underwater domain, especially
in underwater caves, as the narrow passage and turbulent flow make it difficult
to perform loop closures and reset the localization drift. The underwater caves
present challenges to VIO due to the absence of ambient light and the confined
nature of the environment, while also being a crucial source of fresh water and
providing valuable historical records. Experimental results from underwater
caves demonstrate the improvements in accuracy and robustness introduced by the
proposed VIO extension.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Manydepth2: Motion-Aware <span class="highlight-title">Self-Supervised</span> Monocular Depth Estimation in
  Dynamic Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15268v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15268v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaichen Zhou, Jia-Wang Bian, Qian Xie, Jian-Qing Zheng, Niki Trigoni, Andrew Markham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite advancements in self-supervised monocular depth estimation,
challenges persist in dynamic scenarios due to the dependence on assumptions
about a static world. In this paper, we present Manydepth2, a Motion-Guided
Cost Volume Depth Net, to achieve precise depth estimation for both dynamic
objects and static backgrounds, all while maintaining computational efficiency.
To tackle the challenges posed by dynamic content, we incorporate optical flow
and coarse monocular depth to create a novel static reference frame. This frame
is then utilized to build a motion-guided cost volume in collaboration with the
target frame. Additionally, to enhance the accuracy and resilience of the
network structure, we introduce an attention-based depth net architecture to
effectively integrate information from feature maps with varying resolutions.
Compared to methods with similar computational costs, Manydepth2 achieves a
significant reduction of approximately five percent in root-mean-square error
for self-supervised monocular depth estimation on the KITTI-2015 dataset. The
code could be found: https://github.com/kaichen-z/Manydepth2
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Monocular Depth Estimation, Self-Supervised, Optical Flow</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boundary Attention: Learning curves, corners, junctions and grouping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00935v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00935v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mia Gaia Polansky, Charles Herrmann, Junhwa Hur, Deqing Sun, Dor Verbin, Todd Zickler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a lightweight network that infers grouping and boundaries,
including curves, corners and junctions. It operates in a bottom-up fashion,
analogous to classical methods for sub-pixel edge localization and
edge-linking, but with a higher-dimensional representation of local boundary
structure, and notions of local scale and spatial consistency that are learned
instead of designed. Our network uses a mechanism that we call boundary
attention: a geometry-aware local attention operation that, when applied
densely and repeatedly, progressively refines a pixel-resolution field of
variables that specify the boundary structure in every overlapping patch within
an image. Unlike many edge detectors that produce rasterized binary edge maps,
our model provides a rich, unrasterized representation of the geometric
structure in every local region. We find that its intentional geometric bias
allows it to be trained on simple synthetic shapes and then generalize to
extracting boundaries from noisy low-light photographs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website at boundaryattention.github.io:
  http://boundaryattention.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GMISeg: General Medical Image Segmentation without Re-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12539v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12539v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models have become the dominant method for medical image
segmentation. However, they often struggle to be generalisable to unknown tasks
involving new anatomical structures, labels, or shapes. In these cases, the
model needs to be re-trained for the new tasks, posing a significant challenge
for non-machine learning experts and requiring a considerable time investment.
Here I developed a general model that can solve unknown medical image
segmentation tasks without requiring additional training. Given an example set
of images and visual prompts for defining new segmentation tasks, GMISeg
(General Medical Image Segmentation) leverages a pre-trained image encoder
based on ViT and applies a low-rank fine-tuning strategy to the prompt encoder
and mask decoder to fine-tune the model without in an efficient manner. I
evaluated the performance of the proposed method on medical image datasets with
different imaging modalities and anatomical structures. The proposed method
facilitated the deployment of pre-trained AI models to new segmentation works
in a user-friendly way.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MS-Twins: Multi-Scale Deep Self-Attention Networks for Medical Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07128v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07128v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although transformer is preferred in natural language processing, some
studies has only been applied to the field of medical imaging in recent years.
For its long-term dependency, the transformer is expected to contribute to
unconventional convolution neural net conquer their inherent spatial induction
bias. The lately suggested transformer-based segmentation method only uses the
transformer as an auxiliary module to help encode the global context into a
convolutional representation. How to optimally integrate self-attention with
convolution has not been investigated in depth. To solve the problem, this
paper proposes MS-Twins (Multi-Scale Twins), which is a powerful segmentation
model on account of the bond of self-attention and convolution. MS-Twins can
better capture semantic and fine-grained information by combining different
scales and cascading features. Compared with the existing network structure,
MS-Twins has made progress on the previous method based on the transformer of
two in common use data sets, Synapse and ACDC. In particular, the performance
of MS-Twins on Synapse is 8% higher than SwinUNet. Even compared with nnUNet,
the best entirely convoluted medical image segmentation network, the
performance of MS-Twins on Synapse and ACDC still has a bit advantage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PMT-MAE: Dual-Branch <span class="highlight-title">Self-Supervised</span> Learning with Distillation for
  Efficient Point Cloud Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02007v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02007v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Zheng, Chao Zhang, Jian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in self-supervised learning are essential for enhancing feature
extraction and understanding in point cloud processing. This paper introduces
PMT-MAE (Point MLP-Transformer Masked Autoencoder), a novel self-supervised
learning framework for point cloud classification. PMT-MAE features a
dual-branch architecture that integrates Transformer and MLP components to
capture rich features. The Transformer branch leverages global self-attention
for intricate feature interactions, while the parallel MLP branch processes
tokens through shared fully connected layers, offering a complementary feature
transformation pathway. A fusion mechanism then combines these features,
enhancing the model's capacity to learn comprehensive 3D representations.
Guided by the sophisticated teacher model Point-M2AE, PMT-MAE employs a
distillation strategy that includes feature distillation during pre-training
and logit distillation during fine-tuning, ensuring effective knowledge
transfer. On the ModelNet40 classification task, achieving an accuracy of
93.6\% without employing voting strategy, PMT-MAE surpasses the baseline
Point-MAE (93.2\%) and the teacher Point-M2AE (93.4\%), underscoring its
ability to learn discriminative 3D point cloud representations. Additionally,
this framework demonstrates high efficiency, requiring only 40 epochs for both
pre-training and fine-tuning. PMT-MAE's effectiveness and efficiency render it
well-suited for scenarios with limited computational resources, positioning it
as a promising solution for practical point cloud analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PointMT: Efficient Point Cloud Analysis with Hybrid MLP-Transformer
  Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Zheng, Chao Zhang, Jian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, point cloud analysis methods based on the Transformer
architecture have made significant progress, particularly in the context of
multimedia applications such as 3D modeling, virtual reality, and autonomous
systems. However, the high computational resource demands of the Transformer
architecture hinder its scalability, real-time processing capabilities, and
deployment on mobile devices and other platforms with limited computational
resources. This limitation remains a significant obstacle to its practical
application in scenarios requiring on-device intelligence and multimedia
processing. To address this challenge, we propose an efficient point cloud
analysis architecture, \textbf{Point} \textbf{M}LP-\textbf{T}ransformer
(PointMT). This study tackles the quadratic complexity of the self-attention
mechanism by introducing a linear complexity local attention mechanism for
effective feature aggregation. Additionally, to counter the Transformer's focus
on token differences while neglecting channel differences, we introduce a
parameter-free channel temperature adaptation mechanism that adaptively adjusts
the attention weight distribution in each channel, enhancing the precision of
feature aggregation. To improve the Transformer's slow convergence speed due to
the limited scale of point cloud datasets, we propose an MLP-Transformer hybrid
module, which significantly enhances the model's convergence speed.
Furthermore, to boost the feature representation capability of point tokens, we
refine the classification head, enabling point tokens to directly participate
in prediction. Experimental results on multiple evaluation benchmarks
demonstrate that PointMT achieves performance comparable to state-of-the-art
methods while maintaining an optimal balance between performance and accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VideoStudio: Generating Consistent-Content and Multi-Scene Videos <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent innovations and breakthroughs in diffusion models have
significantly expanded the possibilities of generating high-quality videos for
the given prompts. Most existing works tackle the single-scene scenario with
only one video event occurring in a single background. Extending to generate
multi-scene videos nevertheless is not trivial and necessitates to nicely
manage the logic in between while preserving the consistent visual appearance
of key content across video scenes. In this paper, we propose a novel
framework, namely VideoStudio, for consistent-content and multi-scene video
generation. Technically, VideoStudio leverages Large Language Models (LLM) to
convert the input prompt into comprehensive multi-scene script that benefits
from the logical knowledge learnt by LLM. The script for each scene includes a
prompt describing the event, the foreground/background entities, as well as
camera movement. VideoStudio identifies the common entities throughout the
script and asks LLM to detail each entity. The resultant entity description is
then fed into a text-to-image model to generate a reference image for each
entity. Finally, VideoStudio outputs a multi-scene video by generating each
scene video via a diffusion process that takes the reference images, the
descriptive prompt of the event and camera movement into account. The diffusion
model incorporates the reference images as the condition and alignment to
strengthen the content consistency of multi-scene videos. Extensive experiments
demonstrate that VideoStudio outperforms the SOTA video generation models in
terms of visual quality, content consistency, and user preference. Source code
is available at \url{https://github.com/FuchenUSTC/VideoStudio}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024. Source code is available at
  https://github.com/FuchenUSTC/VideoStudio</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention
  and Text Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03018v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03018v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Wang, Jiaxi Gu, Panwen Hu, Songcen Xu, Hang Xu, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-to-video generation, which aims to generate a video starting from a
given reference image, has drawn great attention. Existing methods try to
extend pre-trained text-guided image diffusion models to image-guided video
generation models. Nevertheless, these methods often result in either low
fidelity or flickering over time due to their limitation to shallow image
guidance and poor temporal consistency. To tackle these problems, we propose a
high-fidelity image-to-video generation method by devising a frame retention
branch based on a pre-trained video diffusion model, named DreamVideo. Instead
of integrating the reference image into the diffusion process at a semantic
level, our DreamVideo perceives the reference image via convolution layers and
concatenates the features with the noisy latents as model input. By this means,
the details of the reference image can be preserved to the greatest extent. In
addition, by incorporating double-condition classifier-free guidance, a single
image can be directed to videos of different actions by providing varying
prompt texts. This has significant implications for controllable video
generation and holds broad application prospects. We conduct comprehensive
experiments on the public dataset, and both quantitative and qualitative
results indicate that our method outperforms the state-of-the-art method.
Especially for fidelity, our model has a powerful image retention ability and
delivers the best results in UCF101 compared to other image-to-video models to
our best knowledge. Also, precise control can be achieved by giving different
text prompts. Further details and comprehensive results of our model will be
presented in https://anonymous0769.github.io/DreamVideo/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EasyControl: Transfer ControlNet to Video Diffusion for Controllable
  Generation and Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13005v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13005v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Wang, Jiaxi Gu, Panwen Hu, Haoyu Zhao, Yuanfan Guo, Jianhua Han, Hang Xu, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following the advancements in text-guided image generation technology
exemplified by Stable Diffusion, video generation is gaining increased
attention in the academic community. However, relying solely on text guidance
for video generation has serious limitations, as videos contain much richer
content than images, especially in terms of motion. This information can hardly
be adequately described with plain text. Fortunately, in computer vision,
various visual representations can serve as additional control signals to guide
generation. With the help of these signals, video generation can be controlled
in finer detail, allowing for greater flexibility for different applications.
Integrating various controls, however, is nontrivial. In this paper, we propose
a universal framework called EasyControl. By propagating and injecting
condition features through condition adapters, our method enables users to
control video generation with a single condition map. With our framework,
various conditions including raw pixels, depth, HED, etc., can be integrated
into different Unet-based pre-trained video diffusion models at a low practical
cost. We conduct comprehensive experiments on public datasets, and both
quantitative and qualitative results indicate that our method outperforms
state-of-the-art methods. EasyControl significantly improves various evaluation
metrics across multiple validation datasets compared to previous works.
Specifically, for the sketch-to-video generation task, EasyControl achieves an
improvement of 152.0 on FVD and 19.9 on IS, respectively, in UCF101 compared
with VideoComposer. For fidelity, our model demonstrates powerful image
retention ability, resulting in high FVD and IS in UCF101 and MSR-VTT compared
to other image-to-video models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Point Cloud Classification via Offline Distillation Framework
  and Negative-Weight Self-Distillation Technique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Zheng, Chao Zhang, Jian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement in point cloud processing technologies has
significantly increased the demand for efficient and compact models that
achieve high-accuracy classification. Knowledge distillation has emerged as a
potent model compression technique. However, traditional KD often requires
extensive computational resources for forward inference of large teacher
models, thereby reducing training efficiency for student models and increasing
resource demands. To address these challenges, we introduce an innovative
offline recording strategy that avoids the simultaneous loading of both teacher
and student models, thereby reducing hardware demands. This approach feeds a
multitude of augmented samples into the teacher model, recording both the data
augmentation parameters and the corresponding logit outputs. By applying
shape-level augmentation operations such as random scaling and translation,
while excluding point-level operations like random jittering, the size of the
records is significantly reduced. Additionally, to mitigate the issue of small
student model over-imitating the teacher model's outputs and converging to
suboptimal solutions, we incorporate a negative-weight self-distillation
strategy. Experimental results demonstrate that the proposed distillation
strategy enables the student model to achieve performance comparable to
state-of-the-art models while maintaining lower parameter count. This approach
strikes an optimal balance between performance and complexity. This study
highlights the potential of our method to optimize knowledge distillation for
point cloud classification tasks, particularly in resource-constrained
environments, providing a novel solution for efficient point cloud analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Bare Queries: Open-Vocabulary Object Grounding with 3D Scene
  Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.07113v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.07113v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergey Linok, Tatiana Zemskova, Svetlana Ladanova, Roman Titkov, Dmitry Yudin, Maxim Monastyrny, Aleksei Valenkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locating objects described in natural language presents a significant
challenge for autonomous agents. Existing CLIP-based open-vocabulary methods
successfully perform 3D object grounding with simple (bare) queries, but cannot
cope with ambiguous descriptions that demand an understanding of object
relations. To tackle this problem, we propose a modular approach called BBQ
(Beyond Bare Queries), which constructs 3D scene graph representation with
metric and semantic edges and utilizes a large language model as a
human-to-agent interface through our deductive scene reasoning algorithm. BBQ
employs robust DINO-powered associations to construct 3D object-centric map and
an advanced raycasting algorithm with a 2D vision-language model to describe
them as graph nodes. On the Replica and ScanNet datasets, we have demonstrated
that BBQ takes a leading place in open-vocabulary 3D semantic segmentation
compared to other zero-shot methods. Also, we show that leveraging spatial
relations is especially effective for scenes containing multiple entities of
the same semantic class. On challenging Sr3D+, Nr3D and ScanRefer benchmarks,
our deductive approach demonstrates a significant improvement, enabling objects
grounding by complex queries compared to other state-of-the-art methods. The
combination of our design choices and software implementation has resulted in
significant data processing speed in experiments on the robot on-board
computer. This promising performance enables the application of our approach in
intelligent robotics projects. We made the code publicly available at
https://linukc.github.io/BeyondBareQueries/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigate the Gap: Investigating Approaches for Improving Cross-Modal
  Alignment in CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17639v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17639v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sedigheh Eslami, Gerard de Melo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language--Image Pre-training (CLIP) has manifested remarkable
improvements in zero-shot classification and cross-modal vision-language tasks.
Yet, from a geometrical point of view, the CLIP embedding space has been found
to have a pronounced modality gap. This gap renders the embedding space overly
sparse and disconnected, with different modalities being densely distributed in
distinct subregions of the hypersphere. In this work, we aim at answering three
main questions: 1. Does sharing the parameter space between the multi-modal
encoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart
the uni-modal embeddings via intra-modality separation? 3. How do these gap
reduction approaches affect the downstream performance? We design AlignCLIP, in
order to answer these questions and through extensive experiments, we show that
AlignCLIP achieves noticeable enhancements in the cross-modal alignment of the
embeddings, and thereby, reduces the modality gap, while improving the
performance across several zero-shot and fine-tuning downstream evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PointViG: A Lightweight GNN-based Model for Efficient Point Cloud
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00921v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00921v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Zheng, Yafei Qi, Chen Wang, Chao Zhang, Jian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of point cloud analysis, despite the significant capabilities
of Graph Neural Networks (GNNs) in managing complex 3D datasets, existing
approaches encounter challenges like high computational costs and scalability
issues with extensive scenarios. These limitations restrict the practical
deployment of GNNs, notably in resource-constrained environments. To address
these issues, this study introduce <b>Point<\b> <b>Vi<\b>sion <b>G<\b>NN
(PointViG), an efficient framework for point cloud analysis. PointViG
incorporates a lightweight graph convolutional module to efficiently aggregate
local features and mitigate over-smoothing. For large-scale point cloud scenes,
we propose an adaptive dilated graph convolution technique that searches for
sparse neighboring nodes within a dilated neighborhood based on semantic
correlation, thereby expanding the receptive field and ensuring computational
efficiency. Experiments demonstrate that PointViG achieves performance
comparable to state-of-the-art models while balancing performance and
complexity. On the ModelNet40 classification task, PointViG achieved 94.3%
accuracy with 1.5M parameters. For the S3DIS segmentation task, it achieved an
mIoU of 71.7% with 5.3M parameters. These results underscore the potential and
efficiency of PointViG in point cloud analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying and Learning Static vs. Dynamic <span class="highlight-title">Information</span> in Deep
  Spatiotemporal Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Kowal, Mennatullah Siam, Md Amirul Islam, Neil D. B. Bruce, Richard P. Wildes, Konstantinos G. Derpanis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is limited understanding of the information captured by deep
spatiotemporal models in their intermediate representations. For example, while
evidence suggests that action recognition algorithms are heavily influenced by
visual appearance in single frames, no quantitative methodology exists for
evaluating such static bias in the latent representation compared to bias
toward dynamics. We tackle this challenge by proposing an approach for
quantifying the static and dynamic biases of any spatiotemporal model, and
apply our approach to three tasks, action recognition, automatic video object
segmentation (AVOS) and video instance segmentation (VIS). Our key findings
are: (i) Most examined models are biased toward static information. (ii) Some
datasets that are assumed to be biased toward dynamics are actually biased
toward static information. (iii) Individual channels in an architecture can be
biased toward static, dynamic or a combination of the two. (iv) Most models
converge to their culminating biases in the first half of training. We then
explore how these biases affect performance on dynamically biased datasets. For
action recognition, we propose StaticDropout, a semantically guided dropout
that debiases a model from static information toward dynamics. For AVOS, we
design a better combination of fusion and cross connection layers compared with
previous architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TPAMI 2024. arXiv admin note: substantial text overlap with
  arXiv:2206.02846</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Selection of Anomaly Detectors in the Absence of Labeled
  Validation Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10461v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10461v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clement Fung, Chen Qiu, Aodong Li, Maja Rudolph
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection is the task of identifying abnormal samples in large
unlabeled datasets. While the advent of foundation models has produced powerful
zero-shot anomaly detection methods, their deployment in practice is often
hindered by the absence of labeled validation data -- without it, their
detection performance cannot be evaluated reliably. In this work, we propose
SWSA (Selection With Synthetic Anomalies): a general-purpose framework to
select image-based anomaly detectors without labeled validation data. Instead
of collecting labeled validation data, we generate synthetic anomalies without
any training or fine-tuning, using only a small support set of normal images.
Our synthetic anomalies are used to create detection tasks that compose a
validation framework for model selection. In an empirical study, we evaluate
SWSA with three types of synthetic anomalies and on two selection tasks: model
selection of image-based anomaly detectors and prompt selection for CLIP-based
anomaly detection. SWSA often selects models and prompts that match selections
made with a ground-truth validation set, outperforming baseline selection
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SMAFormer: Synergistic Multi-Attention Transformer for Medical Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuchen Zheng, Xuhang Chen, Weihuang Liu, Haolun Li, Yingtie Lei, Jiahui He, Chi-Man Pun, Shounjun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In medical image segmentation, specialized computer vision techniques,
notably transformers grounded in attention mechanisms and residual networks
employing skip connections, have been instrumental in advancing performance.
Nonetheless, previous models often falter when segmenting small, irregularly
shaped tumors. To this end, we introduce SMAFormer, an efficient,
Transformer-based architecture that fuses multiple attention mechanisms for
enhanced segmentation of small tumors and organs. SMAFormer can capture both
local and global features for medical image segmentation. The architecture
comprises two pivotal components. First, a Synergistic Multi-Attention (SMA)
Transformer block is proposed, which has the benefits of Pixel Attention,
Channel Attention, and Spatial Attention for feature enrichment. Second,
addressing the challenge of information loss incurred during attention
mechanism transitions and feature fusion, we design a Feature Fusion Modulator.
This module bolsters the integration between the channel and spatial attention
by mitigating reshaping-induced information attrition. To evaluate our method,
we conduct extensive experiments on various medical image segmentation tasks,
including multi-organ, liver tumor, and bladder tumor segmentation, achieving
state-of-the-art results. Code and models are available at:
\url{https://github.com/CXH-Research/SMAFormer}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE BIBM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning Machine and Human Visual Representations across Abstraction
  Levels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06509v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06509v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Muttenthaler, Klaus Greff, Frieda Born, Bernhard Spitzer, Simon Kornblith, Michael C. Mozer, Klaus-Robert Müller, Thomas Unterthiner, Andrew K. Lampinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have achieved success across a wide range of
applications, including as models of human behavior in vision tasks. However,
neural network training and human learning differ in fundamental ways, and
neural networks often fail to generalize as robustly as humans do, raising
questions regarding the similarity of their underlying representations. What is
missing for modern learning systems to exhibit more human-like behavior? We
highlight a key misalignment between vision models and humans: whereas human
conceptual knowledge is hierarchically organized from fine- to coarse-scale
distinctions, model representations do not accurately capture all these levels
of abstraction. To address this misalignment, we first train a teacher model to
imitate human judgments, then transfer human-like structure from its
representations into pretrained state-of-the-art vision foundation models.
These human-aligned models more accurately approximate human behavior and
uncertainty across a wide range of similarity tasks, including a new dataset of
human judgments spanning multiple levels of semantic abstractions. They also
perform better on a diverse set of machine learning tasks, increasing
generalization and out-of-distribution robustness. Thus, infusing neural
networks with additional human knowledge yields a best-of-both-worlds
representation that is both more consistent with human cognition and more
practically useful, thus paving the way toward more robust, interpretable, and
human-like artificial intelligence systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoReEcho: Continuous Representation Learning for 2D+time
  Echocardiography Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fadillah Adamsyah Maani, Numan Saeed, Aleksandr Matsun, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) models have been advancing automatic medical image
analysis on various modalities, including echocardiography, by offering a
comprehensive end-to-end training pipeline. This approach enables DL models to
regress ejection fraction (EF) directly from 2D+time echocardiograms, resulting
in superior performance. However, the end-to-end training pipeline makes the
learned representations less explainable. The representations may also fail to
capture the continuous relation among echocardiogram clips, indicating the
existence of spurious correlations, which can negatively affect the
generalization. To mitigate this issue, we propose CoReEcho, a novel training
framework emphasizing continuous representations tailored for direct EF
regression. Our extensive experiments demonstrate that CoReEcho: 1) outperforms
the current state-of-the-art (SOTA) on the largest echocardiography dataset
(EchoNet-Dynamic) with MAE of 3.90 & R2 of 82.44, and 2) provides robust and
generalizable features that transfer more effectively in related downstream
tasks. The code is publicly available at https://github.com/fadamsyah/CoReEcho.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prior Knowledge Integration via <span class="highlight-title">LLM</span> Encoding and Pseudo <span class="highlight-title">Event</span> Regulation
  for Video Moment Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15051v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15051v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyang Jiang, Wengyu Zhang, Xulu Zhang, Xiaoyong Wei, Chang Wen Chen, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the feasibility of leveraging large language
models (LLMs) for integrating general knowledge and incorporating pseudo-events
as priors for temporal content distribution in video moment retrieval (VMR)
models. The motivation behind this study arises from the limitations of using
LLMs as decoders for generating discrete textual descriptions, which hinders
their direct application to continuous outputs like salience scores and
inter-frame embeddings that capture inter-frame relations. To overcome these
limitations, we propose utilizing LLM encoders instead of decoders. Through a
feasibility study, we demonstrate that LLM encoders effectively refine
inter-concept relations in multimodal embeddings, even without being trained on
textual embeddings. We also show that the refinement capability of LLM encoders
can be transferred to other embeddings, such as BLIP and T5, as long as these
embeddings exhibit similar inter-concept similarity patterns to CLIP
embeddings. We present a general framework for integrating LLM encoders into
existing VMR architectures, specifically within the fusion module. Through
experimental validation, we demonstrate the effectiveness of our proposed
methods by achieving state-of-the-art performance in VMR. The source code can
be accessed at https://github.com/fletcherjiang/LLMEPET.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Multimedia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIPCleaner: Cleaning Noisy Labels with CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Feng, Georgios Tzimiropoulos, Ioannis Patras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning with Noisy labels (LNL) poses a significant challenge for the
Machine Learning community. Some of the most widely used approaches that select
as clean samples for which the model itself (the in-training model) has high
confidence, e.g., `small loss', can suffer from the so called
`self-confirmation' bias. This bias arises because the in-training model, is at
least partially trained on the noisy labels. Furthermore, in the classification
case, an additional challenge arises because some of the label noise is between
classes that are visually very similar (`hard noise'). This paper addresses
these challenges by proposing a method (\textit{CLIPCleaner}) that leverages
CLIP, a powerful Vision-Language (VL) model for constructing a zero-shot
classifier for efficient, offline, clean sample selection. This has the
advantage that the sample selection is decoupled from the in-training model and
that the sample selection is aware of the semantic and visual similarities
between the classes due to the way that CLIP is trained. We provide theoretical
justifications and empirical evidence to demonstrate the advantages of CLIP for
LNL compared to conventional pre-trained models. Compared to current methods
that combine iterative sample selection with various techniques,
\textit{CLIPCleaner} offers a simple, single-step approach that achieves
competitive or superior performance on benchmark datasets. To the best of our
knowledge, this is the first time a VL model has been used for sample selection
to address the problem of Learning with Noisy Labels (LNL), highlighting their
potential in the domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACMMM2024. Codes are available at
  https://github.com/MrChenFeng/CLIPCleaner_ACMMM2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Discovery in Optical Music Recognition: Enhancing <span class="highlight-title">Information</span>
  Retrieval with Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15002v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15002v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elona Shatri, George Fazekas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical Music Recognition (OMR) automates the transcription of musical
notation from images into machine-readable formats like MusicXML, MEI, or MIDI,
significantly reducing the costs and time of manual transcription. This study
explores knowledge discovery in OMR by applying instance segmentation using
Mask R-CNN to enhance the detection and delineation of musical symbols in sheet
music. Unlike Optical Character Recognition (OCR), OMR must handle the
intricate semantics of Common Western Music Notation (CWMN), where symbol
meanings depend on shape, position, and context. Our approach leverages
instance segmentation to manage the density and overlap of musical symbols,
facilitating more precise information retrieval from music scores. Evaluations
on the DoReMi and MUSCIMA++ datasets demonstrate substantial improvements, with
our method achieving a mean Average Precision (mAP) of up to 59.70\% in dense
symbol environments, achieving comparable results to object detection.
Furthermore, using traditional computer vision techniques, we add a parallel
step for staff detection to infer the pitch for the recognised symbols. This
study emphasises the role of pixel-wise segmentation in advancing accurate
music symbol recognition, contributing to knowledge discovery in OMR. Our
findings indicate that instance segmentation provides more precise
representations of musical symbols, particularly in densely populated scores,
advancing OMR technology. We make our implementation, pre-processing scripts,
trained models, and evaluation results publicly available to support further
research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages content and one references, accepted version at the
  International Conference on Knowledge Discovery and Information Retrieval
  2024, Porto, Portugal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Evaluating the Robustness of Visual State Space Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09407v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09407v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hashmat Shadab Malik, Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar, Fahad Shahbaz Khan, Salman Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision State Space Models (VSSMs), a novel architecture that combines the
strengths of recurrent neural networks and latent variable models, have
demonstrated remarkable performance in visual perception tasks by efficiently
capturing long-range dependencies and modeling complex visual dynamics.
However, their robustness under natural and adversarial perturbations remains a
critical concern. In this work, we present a comprehensive evaluation of VSSMs'
robustness under various perturbation scenarios, including occlusions, image
structure, common corruptions, and adversarial attacks, and compare their
performance to well-established architectures such as transformers and
Convolutional Neural Networks. Furthermore, we investigate the resilience of
VSSMs to object-background compositional changes on sophisticated benchmarks
designed to test model performance in complex visual scenes. We also assess
their robustness on object detection and segmentation tasks using corrupted
datasets that mimic real-world scenarios. To gain a deeper understanding of
VSSMs' adversarial robustness, we conduct a frequency-based analysis of
adversarial attacks, evaluating their performance against low-frequency and
high-frequency perturbations. Our findings highlight the strengths and
limitations of VSSMs in handling complex visual corruptions, offering valuable
insights for future research. Our code and models will be available at
https://github.com/HashmatShadab/MambaRobustness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AD-CLIP: Adapting Domains in <span class="highlight-title">Prompt</span> Space Using CLIP <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.05659v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.05659v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mainak Singha, Harsh Pal, Ankit Jha, Biplab Banerjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although deep learning models have shown impressive performance on supervised
learning tasks, they often struggle to generalize well when the training
(source) and test (target) domains differ. Unsupervised domain adaptation (DA)
has emerged as a popular solution to this problem. However, current DA
techniques rely on visual backbones, which may lack semantic richness. Despite
the potential of large-scale vision-language foundation models like CLIP, their
effectiveness for DA has yet to be fully explored. To address this gap, we
introduce \textsc{AD-CLIP}, a domain-agnostic prompt learning strategy for CLIP
that aims to solve the DA problem in the prompt space. We leverage the frozen
vision backbone of CLIP to extract both image style (domain) and content
information, which we apply to learn prompt tokens. Our prompts are designed to
be domain-invariant and class-generalizable, by conditioning prompt learning on
image style and content features simultaneously. We use standard supervised
contrastive learning in the source domain, while proposing an entropy
minimization strategy to align domains in the embedding space given the target
domain data. We also consider a scenario where only target domain samples are
available during testing, without any source domain data, and propose a
cross-domain style mapping network to hallucinate domain-agnostic tokens. Our
extensive experiments on three benchmark DA datasets demonstrate the
effectiveness of \textsc{AD-CLIP} compared to existing literature. Code is
available at \url{https://github.com/mainaksingha01/AD-CLIP}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 4 tables. Accepted at OOD-CV, ICCV Workshop,
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NGD-SLAM: Towards Real-Time Dynamic SLAM without GPU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.07392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.07392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Zhang, Mihai Bujanca, Mikel Luján
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing SLAM (Simultaneous Localization and Mapping) algorithms have
achieved remarkable localization accuracy in dynamic environments by using deep
learning techniques to identify dynamic objects. However, they usually require
GPUs to operate in real-time. Therefore, this paper proposes an open-source
real-time dynamic SLAM system that runs solely on CPU by incorporating a mask
prediction mechanism, which allows the deep learning method and the camera
tracking to run entirely in parallel at different frequencies. Our SLAM system
further introduces a dual-stage optical flow tracking approach and employs a
hybrid usage of optical flow and ORB features, enhancing efficiency and
robustness by selectively allocating computational resources to input frames.
Compared with previous methods, our system maintains high localization accuracy
in dynamic environments while achieving a tracking frame rate of 56 FPS on a
laptop CPU, proving that deep learning methods are feasible for dynamic SLAM
without GPU support. To the best of our knowledge, this is the first SLAM
system to achieve this.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Closer Look at the Explainability of Contrastive Language-Image
  <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.05653v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.05653v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Li, Hualiang Wang, Yiqun Duan, Jiheng Zhang, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive language-image pre-training (CLIP) is a powerful vision-language
model that has shown great benefits for various tasks. However, we have
identified some issues with its explainability, which undermine its credibility
and limit the capacity for related tasks. Specifically, we find that CLIP tends
to focus on background regions rather than foregrounds, with noisy activations
at irrelevant positions on the visualization results. These phenomena conflict
with conventional explainability methods based on the class attention map
(CAM), where the raw model can highlight the local foreground regions using
global supervision without alignment. To address these problems, we take a
closer look at its architecture and features. Based on thorough analyses, we
find the raw self-attentions link to inconsistent semantic regions, resulting
in the opposite visualization. Besides, the noisy activations are owing to
redundant features among categories. Building on these insights, we propose the
CLIP Surgery for reliable CAM, a method that allows surgery-like modifications
to the inference architecture and features, without further fine-tuning as
classical CAM methods. This approach significantly improves the explainability
of CLIP, surpassing existing methods by large margins. Besides, it enables
multimodal visualization and extends the capacity of raw CLIP on
open-vocabulary tasks without extra alignment. The code is available at
https://github.com/xmed-lab/CLIP_Surgery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 11 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating analytical variability in fMRI results with style transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elodie Germani, Camille Maumet, Elisa Fromont
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach to improve the reproducibility of neuroimaging
results by converting statistic maps across different functional MRI pipelines.
We make the assumption that pipelines used to compute fMRI statistic maps can
be considered as a style component and we propose to use different generative
models, among which, Generative Adversarial Networks (GAN) and Diffusion Models
(DM) to convert statistic maps across different pipelines. We explore the
performance of multiple GAN frameworks, and design a new DM framework for
unsupervised multi-domain styletransfer. We constrain the generation of 3D fMRI
statistic maps using the latent space of an auxiliary classifier that
distinguishes statistic maps from different pipelines and extend traditional
sampling techniques used in DM to improve the transition performance. Our
experiments demonstrate that our proposed methods aresuccessful: pipelines can
indeed be transferred as a style component, providing animportant source of
data augmentation for future medical studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eye in the Sky: Detection and Compliance Monitoring of Brick Kilns using
  Satellite Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10723v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10723v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishabh Mondal, Shataxi Dubey, Vannsh Jani, Shrimay Shah, Suraj Jaiswal, Zeel B Patel, Nipun Batra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Air pollution kills 7 million people annually. The brick manufacturing
industry accounts for 8%-14% of air pollution in the densely populated
Indo-Gangetic plain. Due to the unorganized nature of brick kilns, policy
violation detection, such as proximity to human habitats, remains challenging.
While previous studies have utilized computer vision-based machine learning
methods for brick kiln detection from satellite imagery, they utilize
proprietary satellite data and rarely focus on compliance with government
policies. In this research, we introduce a scalable framework for brick kiln
detection and automatic compliance monitoring. We use Google Maps Static API to
download the satellite imagery followed by the YOLOv8x model for detection. We
identified and hand-verified 19579 new brick kilns across 9 states within the
Indo-Gangetic plain. Furthermore, we automate and test the compliance to the
policies affecting human habitats, rivers and hospitals. Our results show that
a substantial number of brick kilns do not meet the compliance requirements.
Our framework offers a valuable tool for governments worldwide to automate and
enforce policy regulations for brick kilns, addressing critical environmental
and public health concerns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The PI was not in favor of making the work public on arXiv as the
  content is not yet ready to be released</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Depth Restoration of Hand-Held Transparent Objects for Human-to-Robot
  Handover 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Yu, Haixin Yu, Shoujie Li, Huang Yan, Ziwu Song, Wenbo Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transparent objects are common in daily life, while their optical properties
pose challenges for RGB-D cameras to capture accurate depth information. This
issue is further amplified when these objects are hand-held, as hand occlusions
further complicate depth estimation. For assistant robots, however, accurately
perceiving hand-held transparent objects is critical to effective human-robot
interaction. This paper presents a Hand-Aware Depth Restoration (HADR) method
based on creating an implicit neural representation function from a single
RGB-D image. The proposed method utilizes hand posture as an important guidance
to leverage semantic and geometric information of hand-object interaction. To
train and evaluate the proposed method, we create a high-fidelity synthetic
dataset named TransHand-14K with a real-to-sim data generation scheme.
Experiments show that our method has better performance and generalization
ability compared with existing methods. We further develop a real-world
human-to-robot handover system based on HADR, demonstrating its potential in
human-robot interaction applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InfNeRF: Towards Infinite Scale NeRF Rendering with O(log n) Space
  Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiabin Liang, Lanqing Zhang, Zhuoran Zhao, Xiangyu Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional mesh-based Level of Detail (LoD) technique, exemplified by
applications such as Google Earth and many game engines, exhibits the
capability to holistically represent a large scene even the Earth, and achieves
rendering with a space complexity of O(log n). This constrained data
requirement not only enhances rendering efficiency but also facilitates dynamic
data fetching, thereby enabling a seamless 3D navigation experience for users.
In this work, we extend this proven LoD technique to Neural Radiance Fields
(NeRF) by introducing an octree structure to represent the scenes in different
scales. This innovative approach provides a mathematically simple and elegant
representation with a rendering space complexity of O(log n), aligned with the
efficiency of mesh-based LoD techniques. We also present a novel training
strategy that maintains a complexity of O(n). This strategy allows for parallel
training with minimal overhead, ensuring the scalability and efficiency of our
proposed method. Our contribution is not only in extending the capabilities of
existing techniques but also in establishing a foundation for scalable and
efficient large-scale scene representation using NeRF and octree structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, version accepted by Siggraph Asia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Two-Stage Progressive <span class="highlight-title">Pre-train</span>ing using Multi-Modal Contrastive
  Masked Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02245v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02245v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Abdullah Jamal, Omid Mohareri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new progressive pre-training method for image
understanding tasks which leverages RGB-D datasets. The method utilizes
Multi-Modal Contrastive Masked Autoencoder and Denoising techniques. Our
proposed approach consists of two stages. In the first stage, we pre-train the
model using contrastive learning to learn cross-modal representations. In the
second stage, we further pre-train the model using masked autoencoding and
denoising/noise prediction used in diffusion models. Masked autoencoding
focuses on reconstructing the missing patches in the input modality using local
spatial correlations, while denoising learns high frequency components of the
input data. Moreover, it incorporates global distillation in the second stage
by leveraging the knowledge acquired in stage one. Our approach is scalable,
robust and suitable for pre-training RGB-D datasets. Extensive experiments on
multiple datasets such as ScanNet, NYUv2 and SUN RGB-D show the efficacy and
superior performance of our approach. Specifically, we show an improvement of
+1.3% mIoU against Mask3D on ScanNet semantic segmentation. We further
demonstrate the effectiveness of our approach in low-data regime by evaluating
it for semantic segmentation task against the state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mamba-YOLO-World: Marrying YOLO-World with Mamba for Open-Vocabulary
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxuan Wang, Qingdong He, Jinlong Peng, Hao Yang, Mingmin Chi, Yabiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary detection (OVD) aims to detect objects beyond a predefined
set of categories. As a pioneering model incorporating the YOLO series into
OVD, YOLO-World is well-suited for scenarios prioritizing speed and efficiency.
However, its performance is hindered by its neck feature fusion mechanism,
which causes the quadratic complexity and the limited guided receptive fields.
To address these limitations, we present Mamba-YOLO-World, a novel YOLO-based
OVD model employing the proposed MambaFusion Path Aggregation Network
(MambaFusion-PAN) as its neck architecture. Specifically, we introduce an
innovative State Space Model-based feature fusion mechanism consisting of a
Parallel-Guided Selective Scan algorithm and a Serial-Guided Selective Scan
algorithm with linear complexity and globally guided receptive fields. It
leverages multi-modal input sequences and mamba hidden states to guide the
selective scanning process. Experiments demonstrate that our model outperforms
the original YOLO-World on the COCO and LVIS benchmarks in both zero-shot and
fine-tuning settings while maintaining comparable parameters and FLOPs.
Additionally, it surpasses existing state-of-the-art OVD methods with fewer
parameters and FLOPs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Adversarial Robustness And Backdoor Mitigation in SSL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15918v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15918v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryan Satpathy, Nilaksh Singh, Dhruva Rajwade, Somesh Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-Supervised Learning (SSL) has shown great promise in learning
representations from unlabeled data. The power of learning representations
without the need for human annotations has made SSL a widely used technique in
real-world problems. However, SSL methods have recently been shown to be
vulnerable to backdoor attacks, where the learned model can be exploited by
adversaries to manipulate the learned representations, either through tampering
the training data distribution, or via modifying the model itself. This work
aims to address defending against backdoor attacks in SSL, where the adversary
has access to a realistic fraction of the SSL training data, and no access to
the model. We use novel methods that are computationally efficient as well as
generalizable across different problem settings. We also investigate the
adversarial robustness of SSL models when trained with our method, and show
insights into increased robustness in SSL via frequency domain augmentations.
We demonstrate the effectiveness of our method on a variety of SSL benchmarks,
and show that our method is able to mitigate backdoor attacks while maintaining
high performance on downstream tasks. Code for our work is available at
github.com/Aryan-Satpathy/Backdoor
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Npix2Cpix: A GAN-Based Image-to-Image Translation Network With
  Retrieval- Classification Integration for Watermark Retrieval From Historical
  Document Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03556v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03556v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Utsab Saha, Sawradip Saha, Shaikh Anowarul Fattah, Mohammad Saquib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The identification and restoration of ancient watermarks have long been a
major topic in codicology and history. Classifying historical documents based
on watermarks is challenging due to their diversity, noisy samples, multiple
representation modes, and minor distinctions between classes and intra-class
variations. This paper proposes a modified U-net-based conditional generative
adversarial network (GAN) named Npix2Cpix to translate noisy raw historical
watermarked images into clean, handwriting-free watermarked images by
performing image translation from degraded (noisy) pixels to clean pixels.
Using image-to-image translation and adversarial learning, the network creates
clutter-free images for watermark restoration and categorization. The generator
and discriminator of the proposed GAN are trained using two separate loss
functions, each based on the distance between images, to learn the mapping from
the input noisy image to the output clean image. After using the proposed GAN
to pre-process noisy watermarked images, Siamese-based one-shot learning is
employed for watermark classification. Experimental results on a large-scale
historical watermark dataset demonstrate that cleaning the noisy watermarked
images can help to achieve high one-shot classification accuracy. The
qualitative and quantitative evaluation of the retrieved watermarked image
highlights the effectiveness of the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SSTD: Stripe-Like Space Target Detection Using Single-Point Weak
  Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Zhu, Ali Zia, Xuesong Li, Bingbing Dan, Yuebo Ma, Enhai Liu, Rujin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stripe-like space target detection (SSTD) plays a key role in enhancing space
situational awareness and assessing spacecraft behaviour. This domain faces
three challenges: the lack of publicly available datasets, interference from
stray light and stars, and the variability of stripe-like targets, which makes
manual labeling both inaccurate and labor-intensive. In response, we introduces
`AstroStripeSet', a pioneering dataset designed for SSTD, aiming to bridge the
gap in academic resources and advance research in SSTD. Furthermore, we propose
a novel teacher-student label evolution framework with single-point weak
supervision, providing a new solution to the challenges of manual labeling.
This framework starts with generating initial pseudo-labels using the zero-shot
capabilities of the Segment Anything Model (SAM) in a single-point setting.
After that, the fine-tuned StripeSAM serves as the teacher and the newly
developed StripeNet as the student, consistently improving segmentation
performance through label evolution, which iteratively refines these labels. We
also introduce `GeoDice', a new loss function customized for the linear
characteristics of stripe-like targets. Extensive experiments show that our
method matches fully supervised approaches, exhibits strong zero-shot
generalization for diverse space-based and ground-based real-world images, and
sets a new state-of-the-art (SOTA) benchmark. Our AstroStripeSet dataset and
code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LightningDrag: Lightning Fast and Accurate Drag-based Image Editing
  Emerging from Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujun Shi, Jun Hao Liew, Hanshu Yan, Vincent Y. F. Tan, Jiashi Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accuracy and speed are critical in image editing tasks. Pan et al. introduced
a drag-based image editing framework that achieves pixel-level control using
Generative Adversarial Networks (GANs). A flurry of subsequent studies enhanced
this framework's generality by leveraging large-scale diffusion models.
However, these methods often suffer from inordinately long processing times
(exceeding 1 minute per edit) and low success rates. Addressing these issues
head on, we present LightningDrag, a rapid approach enabling high quality
drag-based image editing in ~1 second. Unlike most previous methods, we
redefine drag-based editing as a conditional generation task, eliminating the
need for time-consuming latent optimization or gradient-based guidance during
inference. In addition, the design of our pipeline allows us to train our model
on large-scale paired video frames, which contain rich motion information such
as object translations, changing poses and orientations, zooming in and out,
etc. By learning from videos, our approach can significantly outperform
previous methods in terms of accuracy and consistency. Despite being trained
solely on videos, our model generalizes well to perform local shape
deformations not presented in the training data (e.g., lengthening of hair,
twisting rainbows, etc.). Extensive qualitative and quantitative evaluations on
benchmark datasets corroborate the superiority of our approach. The code and
model will be released at https://github.com/magic-research/LightningDrag.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://lightning-drag.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InterACT: Inter-dependency Aware Action Chunking with Hierarchical
  Attention Transformers for Bimanual Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Lee, Ian Chuang, Ling-Yuan Chen, Iman Soltani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present InterACT: Inter-dependency aware Action Chunking with Hierarchical
Attention Transformers, a novel imitation learning framework for bimanual
manipulation that integrates hierarchical attention to capture
inter-dependencies between dual-arm joint states and visual inputs. InterACT
consists of a Hierarchical Attention Encoder and a Multi-arm Decoder, both
designed to enhance information aggregation and coordination. The encoder
processes multi-modal inputs through segment-wise and cross-segment attention
mechanisms, while the decoder leverages synchronization blocks to refine
individual action predictions, providing the counterpart's prediction as
context. Our experiments on a variety of simulated and real-world bimanual
manipulation tasks demonstrate that InterACT significantly outperforms existing
methods. Detailed ablation studies validate the contributions of key components
of our work, including the impact of CLS tokens, cross-segment encoders, and
synchronization blocks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Conference on Robot Learning (CoRL) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Self-training for Semi-supervised Landmark Detection: A
  Selection-free Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04556v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04556v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibo Jin, Haoxuan Che, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-training is a simple yet effective method for semi-supervised learning,
during which pseudo-label selection plays an important role for handling
confirmation bias. Despite its popularity, applying self-training to landmark
detection faces three problems: 1) The selected confident pseudo-labels often
contain data bias, which may hurt model performance; 2) It is not easy to
decide a proper threshold for sample selection as the localization task can be
sensitive to noisy pseudo-labels; 3) coordinate regression does not output
confidence, making selection-based self-training infeasible. To address the
above issues, we propose Self-Training for Landmark Detection (STLD), a method
that does not require explicit pseudo-label selection. Instead, STLD constructs
a task curriculum to deal with confirmation bias, which progressively
transitions from more confident to less confident tasks over the rounds of
self-training. Pseudo pretraining and shrink regression are two essential
components for such a curriculum, where the former is the first task of the
curriculum for providing a better model initialization and the latter is
further added in the later rounds to directly leverage the pseudo-labels in a
coarse-to-fine manner. Experiments on three facial and one medical landmark
detection benchmark show that STLD outperforms the existing methods
consistently in both semi- and omni-supervised settings. The code is available
at https://github.com/jhb86253817/STLD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Image Processing (TIP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NeRFmentation: NeRF-based Augmentation for Monocular Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03771v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03771v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Casimir Feldmann, Niall Siegenheim, Nikolas Hars, Lovro Rabuzin, Mert Ertugrul, Luca Wolfart, Marc Pollefeys, Zuria Bauer, Martin R. Oswald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capabilities of monocular depth estimation (MDE) models are limited by
the availability of sufficient and diverse datasets. In the case of MDE models
for autonomous driving, this issue is exacerbated by the linearity of the
captured data trajectories. We propose a NeRF-based data augmentation pipeline
to introduce synthetic data with more diverse viewing directions into training
datasets and demonstrate the benefits of our approach to model performance and
robustness. Our data augmentation pipeline, which we call
\textit{NeRFmentation}, trains NeRFs on each scene in a dataset, filters out
subpar NeRFs based on relevant metrics, and uses them to generate synthetic
RGB-D images captured from new viewing directions. In this work, we apply our
technique in conjunction with three state-of-the-art MDE architectures on the
popular autonomous driving dataset, KITTI, augmenting its training set of the
Eigen split. We evaluate the resulting performance gain on the original test
set, a separate popular driving dataset, and our own synthetic test set.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating Classifier-Free Guidance in Diffusion Model-Based
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noah Buchanan, Susan Gauch, Quan Mai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a diffusion-based recommender system that incorporates
classifier-free guidance. Most current recommender systems provide
recommendations using conventional methods such as collaborative or
content-based filtering. Diffusion is a new approach to generative AI that
improves on previous generative AI approaches such as Variational Autoencoders
(VAEs) and Generative Adversarial Networks (GANs). We incorporate diffusion in
a recommender system that mirrors the sequence users take when browsing and
rating items. Although a few current recommender systems incorporate diffusion,
they do not incorporate classifier-free guidance, a new innovation in diffusion
models as a whole. In this paper, we present a diffusion recommender system
that augments the underlying recommender system model for improved performance
and also incorporates classifier-free guidance. Our findings show improvements
over state-of-the-art recommender systems for most metrics for several
recommendation tasks on a variety of datasets. In particular, our approach
demonstrates the potential to provide better recommendations when data is
sparse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Model Enhanced Hard Sample Identification for Denoising
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianrui Song, Wenshuo Chao, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit feedback, often used to build recommender systems, unavoidably
confronts noise due to factors such as misclicks and position bias. Previous
studies have attempted to alleviate this by identifying noisy samples based on
their diverged patterns, such as higher loss values, and mitigating the noise
through sample dropping or reweighting. Despite the progress, we observe
existing approaches struggle to distinguish hard samples and noise samples, as
they often exhibit similar patterns, thereby limiting their effectiveness in
denoising recommendations. To address this challenge, we propose a Large
Language Model Enhanced Hard Sample Denoising (LLMHD) framework. Specifically,
we construct an LLM-based scorer to evaluate the semantic consistency of items
with the user preference, which is quantified based on summarized historical
user interactions. The resulting scores are used to assess the hardness of
samples for the pointwise or pairwise training objectives. To ensure
efficiency, we introduce a variance-based sample pruning strategy to filter
potential hard samples before scoring. Besides, we propose an iterative
preference update module designed to continuously refine summarized user
preference, which may be biased due to false-positive user-item interactions.
Extensive experiments on three real-world datasets and four backbone
recommenders demonstrate the effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ beeFormer: Bridging the Gap Between Semantic and Interaction Similarity
  in Recommender Systems <span class="chip">RecSys 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vojtěch Vančura, Pavel Kordík, Milan Straka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems often use text-side information to improve their
predictions, especially in cold-start or zero-shot recommendation scenarios,
where traditional collaborative filtering approaches cannot be used. Many
approaches to text-mining side information for recommender systems have been
proposed over recent years, with sentence Transformers being the most prominent
one. However, these models are trained to predict semantic similarity without
utilizing interaction data with hidden patterns specific to recommender
systems. In this paper, we propose beeFormer, a framework for training sentence
Transformer models with interaction data. We demonstrate that our models
trained with beeFormer can transfer knowledge between datasets while
outperforming not only semantic similarity sentence Transformers but also
traditional collaborative filtering methods. We also show that training on
multiple datasets from different domains accumulates knowledge in a single
model, unlocking the possibility of training universal, domain-agnostic
sentence Transformer models to mine text representations for recommender
systems. We release the source code, trained models, and additional details
allowing replication of our experiments at
https://github.com/recombee/beeformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to RecSys 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Discovery in Recommender Systems: Example and Discussion <span class="chip">RecSys
  '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Cavenaghi, Fabio Stella, Markus Zanker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causality is receiving increasing attention by the artificial intelligence
and machine learning communities. This paper gives an example of modelling a
recommender system problem using causal graphs. Specifically, we approached the
causal discovery task to learn a causal graph by combining observational data
from an open-source dataset with prior knowledge. The resulting causal graph
shows that only a few variables effectively influence the analysed feedback
signals. This contrasts with the recent trend in the machine learning community
to include more and more variables in massive models, such as neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the CONSEQUENCES '24 workshop, co-located with ACM RecSys
  '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Personalized Recipe Recommendation Through Multi-Class
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harish Neelam, Koushik Sai Veerella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper intends to address the challenge of personalized recipe
recommendation in the realm of diverse culinary preferences. The problem domain
involves recipe recommendations, utilizing techniques such as association
analysis and classification. Association analysis explores the relationships
and connections between different ingredients to enhance the user experience.
Meanwhile, the classification aspect involves categorizing recipes based on
user-defined ingredients and preferences. A unique aspect of the paper is the
consideration of recipes and ingredients belonging to multiple classes,
recognizing the complexity of culinary combinations. This necessitates a
sophisticated approach to classification and recommendation, ensuring the
system accommodates the nature of recipe categorization. The paper seeks not
only to recommend recipes but also to explore the process involved in achieving
accurate and personalized recommendations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trustworthiness in Retrieval-Augmented Generation Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng Dou, Tsung-Yi Ho, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal
paradigm in the development of Large Language Models (LLMs). While much of the
current research in this field focuses on performance optimization,
particularly in terms of accuracy and efficiency, the trustworthiness of RAG
systems remains an area still under exploration. From a positive perspective,
RAG systems are promising to enhance LLMs by providing them with useful and
up-to-date knowledge from vast external databases, thereby mitigating the
long-standing problem of hallucination. While from a negative perspective, RAG
systems are at the risk of generating undesirable contents if the retrieved
information is either inappropriate or poorly utilized. To address these
concerns, we propose a unified framework that assesses the trustworthiness of
RAG systems across six key dimensions: factuality, robustness, fairness,
transparency, accountability, and privacy. Within this framework, we thoroughly
review the existing literature on each dimension. Additionally, we create the
evaluation benchmark regarding the six dimensions and conduct comprehensive
evaluations for a variety of proprietary and open-source models. Finally, we
identify the potential challenges for future research based on our
investigation results. Through this work, we aim to lay a structured foundation
for future investigations and provide practical insights for enhancing the
trustworthiness of RAG systems in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Lightning-Ignited Wildfires Prediction and Climate Change
  Projections based on Explainable Machine Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Assaf Shmuel, Teddy Lazebnik, Oren Glickman, Eyal Heifetz, Colin Price
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wildfires pose a significant natural disaster risk to populations and
contribute to accelerated climate change. As wildfires are also affected by
climate change, extreme wildfires are becoming increasingly frequent. Although
they occur less frequently globally than those sparked by human activities,
lightning-ignited wildfires play a substantial role in carbon emissions and
account for the majority of burned areas in certain regions. While existing
computational models, especially those based on machine learning, aim to
predict lightning-ignited wildfires, they are typically tailored to specific
regions with unique characteristics, limiting their global applicability. In
this study, we present machine learning models designed to characterize and
predict lightning-ignited wildfires on a global scale. Our approach involves
classifying lightning-ignited versus anthropogenic wildfires, and estimating
with high accuracy the probability of lightning to ignite a fire based on a
wide spectrum of factors such as meteorological conditions and vegetation.
Utilizing these models, we analyze seasonal and spatial trends in
lightning-ignited wildfires shedding light on the impact of climate change on
this phenomenon. We analyze the influence of various features on the models
using eXplainable Artificial Intelligence (XAI) frameworks. Our findings
highlight significant global differences between anthropogenic and
lightning-ignited wildfires. Moreover, we demonstrate that, even over a short
time span of less than a decade, climate changes have steadily increased the
global risk of lightning-ignited wildfires. This distinction underscores the
imperative need for dedicated predictive models and fire weather indices
tailored specifically to each type of wildfire.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffATR: Diffusion-based Generative Modeling for Audio-Text Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Xin, Xuxin Cheng, Zhihong Zhu, Xusheng Yang, Yuexian Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing audio-text retrieval (ATR) methods are essentially discriminative
models that aim to maximize the conditional likelihood, represented as
p(candidates|query). Nevertheless, this methodology fails to consider the
intrinsic data distribution p(query), leading to difficulties in discerning
out-of-distribution data. In this work, we attempt to tackle this constraint
through a generative perspective and model the relationship between audio and
text as their joint probability p(candidates,query). To this end, we present a
diffusion-based ATR framework (DiffATR), which models ATR as an iterative
procedure that progressively generates joint distribution from noise.
Throughout its training phase, DiffATR is optimized from both generative and
discriminative viewpoints: the generator is refined through a generation loss,
while the feature extractor benefits from a contrastive loss, thus combining
the merits of both methodologies. Experiments on the AudioCaps and Clotho
datasets with superior performances, verify the effectiveness of our approach.
Notably, without any alterations, our DiffATR consistently exhibits strong
performance in out-of-domain retrieval settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Interspeech2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Practical and Asymptotically Optimal Quantization of High-Dimensional
  Vectors in Euclidean Space for Approximate Nearest Neighbor Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyang Gao, Yutong Gou, Yuexuan Xu, Yongyi Yang, Cheng Long, Raymond Chi-Wing Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate nearest neighbor (ANN) query in high-dimensional Euclidean space
is a key operator in database systems. For this query, quantization is a
popular family of methods developed for compressing vectors and reducing memory
consumption. Recently, a method called RaBitQ achieves the state-of-the-art
performance among these methods. It produces better empirical performance in
both accuracy and efficiency when using the same compression rate and provides
rigorous theoretical guarantees. However, the method is only designed for
compressing vectors at high compression rates (32x) and lacks support for
achieving higher accuracy by using more space. In this paper, we introduce a
new quantization method to address this limitation by extending RaBitQ. The new
method inherits the theoretical guarantees of RaBitQ and achieves the
asymptotic optimality in terms of the trade-off between space and error bounds
as to be proven in this study. Additionally, we present efficient
implementations of the method, enabling its application to ANN queries to
reduce both space and time consumption. Extensive experiments on real-world
datasets confirm that our method consistently outperforms the state-of-the-art
baselines in both accuracy and efficiency when using the same amount of memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop
  Question Answering <span class="chip">ACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengliang Shi, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, Zhaochun Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Hop Question Answering (MHQA) tasks present a significant challenge for
large language models (LLMs) due to the intensive knowledge required. Current
solutions, like Retrieval-Augmented Generation, typically retrieve potential
documents from an external corpus to read an answer. However, the performance
of this retrieve-then-read paradigm is constrained by the retriever and the
inevitable noise in the retrieved documents. To mitigate these challenges, we
introduce a novel generate-then-ground (GenGround) framework, synergizing the
parametric knowledge of LLMs and external documents to solve a multi-hop
question. GenGround empowers LLMs to alternate two phases until the final
answer is derived: (1) formulate a simpler, single-hop question and directly
generate the answer; (2) ground the question-answer pair in retrieved
documents, amending any wrong predictions in the answer. We also propose an
instructional grounding distillation method to generalize our method into
smaller models. Extensive experiments conducted on four datasets illustrate the
superiority of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Discovery in Optical Music Recognition: Enhancing <span class="highlight-title">Information</span>
  Retrieval with Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15002v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15002v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elona Shatri, George Fazekas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical Music Recognition (OMR) automates the transcription of musical
notation from images into machine-readable formats like MusicXML, MEI, or MIDI,
significantly reducing the costs and time of manual transcription. This study
explores knowledge discovery in OMR by applying instance segmentation using
Mask R-CNN to enhance the detection and delineation of musical symbols in sheet
music. Unlike Optical Character Recognition (OCR), OMR must handle the
intricate semantics of Common Western Music Notation (CWMN), where symbol
meanings depend on shape, position, and context. Our approach leverages
instance segmentation to manage the density and overlap of musical symbols,
facilitating more precise information retrieval from music scores. Evaluations
on the DoReMi and MUSCIMA++ datasets demonstrate substantial improvements, with
our method achieving a mean Average Precision (mAP) of up to 59.70\% in dense
symbol environments, achieving comparable results to object detection.
Furthermore, using traditional computer vision techniques, we add a parallel
step for staff detection to infer the pitch for the recognised symbols. This
study emphasises the role of pixel-wise segmentation in advancing accurate
music symbol recognition, contributing to knowledge discovery in OMR. Our
findings indicate that instance segmentation provides more precise
representations of musical symbols, particularly in densely populated scores,
advancing OMR technology. We make our implementation, pre-processing scripts,
trained models, and evaluation results publicly available to support further
research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages content and one references, accepted version at the
  International Conference on Knowledge Discovery and Information Retrieval
  2024, Porto, Portugal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $Δ\text{-}{\rm OPE}$: Off-Policy Estimation with Pairs of Policies <span class="chip">RecSys '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Jeunen, Aleksei Ustimenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The off-policy paradigm casts recommendation as a counterfactual
decision-making task, allowing practitioners to unbiasedly estimate online
metrics using offline data. This leads to effective evaluation metrics, as well
as learning procedures that directly optimise online success. Nevertheless, the
high variance that comes with unbiasedness is typically the crux that
complicates practical applications. An important insight is that the difference
between policy values can often be estimated with significantly reduced
variance, if said policies have positive covariance. This allows us to
formulate a pairwise off-policy estimation task: $\Delta\text{-}{\rm OPE}$.
  $\Delta\text{-}{\rm OPE}$ subsumes the common use-case of estimating
improvements of a learnt policy over a production policy, using data collected
by a stochastic logging policy. We introduce $\Delta\text{-}{\rm OPE}$ methods
based on the widely used Inverse Propensity Scoring estimator and its
extensions. Moreover, we characterise a variance-optimal additive control
variate that further enhances efficiency. Simulated, offline, and online
experiments show that our methods significantly improve performance for both
evaluation and learning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a short paper in the 2024 ACM Conference on Recommender
  Systems (RecSys '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Objective Recommendation via Multivariate Policy Learning <span class="chip">RecSys '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02141v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02141v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Jeunen, Jatin Mandav, Ivan Potapov, Nakul Agarwal, Sourabh Vaid, Wenzhe Shi, Aleksei Ustimenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world recommender systems often need to balance multiple objectives when
deciding which recommendations to present to users. These include behavioural
signals (e.g. clicks, shares, dwell time), as well as broader objectives (e.g.
diversity, fairness). Scalarisation methods are commonly used to handle this
balancing task, where a weighted average of per-objective reward signals
determines the final score used for ranking. Naturally, how these weights are
computed exactly, is key to success for any online platform. We frame this as a
decision-making task, where the scalarisation weights are actions taken to
maximise an overall North Star reward (e.g. long-term user retention or
growth). We extend existing policy learning methods to the continuous
multivariate action domain, proposing to maximise a pessimistic lower bound on
the North Star reward that the learnt policy will yield. Typical lower bounds
based on normal approximations suffer from insufficient coverage, and we
propose an efficient and effective policy-dependent correction for this. We
provide guidance to design stochastic data collection policies, as well as
highly sensitive reward signals. Empirical observations from simulations,
offline and online experiments highlight the efficacy of our deployed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper in the 2024 ACM Conference on Recommender
  Systems (RecSys '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DrugAgent: Explainable Drug Repurposing Agent with Large Language
  Model-based Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13378v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13378v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoshitaka Inoue, Tianci Song, Tianfan Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drug repurposing offers a promising avenue for accelerating drug development
by identifying new therapeutic potentials of existing drugs. In this paper, we
propose a multi-agent framework to enhance the drug repurposing process using
state-of-the-art machine learning techniques and knowledge integration. Our
framework comprises several specialized agents: an AI Agent trains robust
drug-target interaction (DTI) models; a Knowledge Graph Agent utilizes the
drug-gene interaction database (DGIdb), DrugBank, Comparative Toxicogenomics
Database (CTD), and Search Tool for Interactions of Chemicals (STITCH) to
systematically extract DTIs; and a Search Agent interacts with biomedical
literature to annotate and verify computational predictions. By integrating
outputs from these agents, our system effectively harnesses diverse data
sources, including external databases, to propose viable repurposing
candidates. Preliminary results demonstrate the potential of our approach in
not only predicting drug-disease interactions but also in reducing the time and
cost associated with traditional drug discovery methods. This paper highlights
the scalability of multi-agent systems in biomedical research and their role in
driving innovation in drug repurposing. Our approach not only outperforms
existing methods in predicting drug repurposing potential but also provides
interpretable results, paving the way for more efficient and cost-effective
drug discovery processes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">119</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RetrievalAttention: Accelerating Long-Context <span class="highlight-title">LLM</span> Inference via Vector
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based large Language Models (LLMs) become increasingly important
in various domains. However, the quadratic time complexity of attention
operation poses a significant challenge for scaling to longer contexts due to
the extremely high inference latency and GPU memory consumption for caching
key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free
approach to accelerate attention computation. To leverage the dynamic sparse
property of attention, RetrievalAttention builds approximate nearest neighbor
search (ANNS) indexes upon KV vectors in CPU memory and retrieves the most
relevant ones via vector search during generation. Due to the
out-of-distribution (OOD) between query vectors and key vectors, off-the-shelf
ANNS indexes still need to scan O(N) (usually 30% of all keys) data for
accurate retrieval, which fails to exploit the high sparsity.
RetrievalAttention first identifies the OOD challenge of ANNS-based attention,
and addresses it via an attention-aware vector search algorithm that can adapt
to queries and only access 1--3% of data, thus achieving a sub-linear time
complexity. RetrievalAttention greatly reduces the inference cost of
long-context LLM with much lower GPU memory requirements while maintaining the
model accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for
serving 128K tokens in LLMs with 8B parameters, which is capable of generating
one token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Language Modeling Can Elicit Search and Reasoning Capabilities on
  Logic Puzzles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kulin Shah, Nishanth Dikkala, Xin Wang, Rina Panigrahy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal language modeling using the Transformer architecture has yielded
remarkable capabilities in Large Language Models (LLMs) over the last few
years. However, the extent to which fundamental search and reasoning
capabilities emerged within LLMs remains a topic of ongoing debate. In this
work, we study if causal language modeling can learn a complex task such as
solving Sudoku puzzles. To solve a Sudoku, the model is first required to
search over all empty cells of the puzzle to decide on a cell to fill and then
apply an appropriate strategy to fill the decided cell. Sometimes, the
application of a strategy only results in thinning down the possible values in
a cell rather than concluding the exact value of the cell. In such cases,
multiple strategies are applied one after the other to fill a single cell. We
observe that Transformer models trained on this synthetic task can indeed learn
to solve Sudokus (our model solves $94.21\%$ of the puzzles fully correctly)
when trained on a logical sequence of steps taken by a solver. We find that
training Transformers with the logical sequence of steps is necessary and
without such training, they fail to learn Sudoku. We also extend our analysis
to Zebra puzzles (known as Einstein puzzles) and show that the model solves
$92.04 \%$ of the puzzles fully correctly. In addition, we study the internal
representations of the trained Transformer and find that through linear
probing, we can decode information about the set of possible values in any
given cell from them, pointing to the presence of a strong reasoning engine
implicit in the Transformer weights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partial Distribution Matching via Partial Wasserstein Adversarial
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi-Ming Wang, Nan Xue, Ling Lei, Rebecka Jörnsten, Gui-Song Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the problem of distribution matching (DM), which is a
fundamental machine learning problem seeking to robustly align two probability
distributions. Our approach is established on a relaxed formulation, called
partial distribution matching (PDM), which seeks to match a fraction of the
distributions instead of matching them completely. We theoretically derive the
Kantorovich-Rubinstein duality for the partial Wasserstain-1 (PW) discrepancy,
and develop a partial Wasserstein adversarial network (PWAN) that efficiently
approximates the PW discrepancy based on this dual form. Partial matching can
then be achieved by optimizing the network using gradient descent. Two
practical tasks, point set registration and partial domain adaptation are
investigated, where the goals are to partially match distributions in 3D space
and high-dimensional feature space respectively. The experiment results confirm
that the proposed PWAN effectively produces highly robust matching results,
performing better or on par with the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2203.02227</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MusicLIME: Explainable Multimodal Music Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodoros Sotirou, Vassilis Lyberatos, Orfeas Menis Mastromichalakis, Giorgos Stamou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal models are critical for music understanding tasks, as they capture
the complex interplay between audio and lyrics. However, as these models become
more prevalent, the need for explainability grows-understanding how these
systems make decisions is vital for ensuring fairness, reducing bias, and
fostering trust. In this paper, we introduce MusicLIME, a model-agnostic
feature importance explanation method designed for multimodal music models.
Unlike traditional unimodal methods, which analyze each modality separately
without considering the interaction between them, often leading to incomplete
or misleading explanations, MusicLIME reveals how audio and lyrical features
interact and contribute to predictions, providing a holistic view of the
model's decision-making. Additionally, we enhance local explanations by
aggregating them into global explanations, giving users a broader perspective
of model behavior. Through this work, we contribute to improving the
interpretability of multimodal music models, empowering users to make informed
choices, and fostering more equitable, fair, and transparent music
understanding systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GitHub repository: https://github.com/IamTheo2000/MusicLIME</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Nonconvex Bilevel Optimization with Bregman Divergences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Bohne, David Rosenberg, Gary Kazantsev, Pawel Polak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilevel optimization methods are increasingly relevant within machine
learning, especially for tasks such as hyperparameter optimization and
meta-learning. Compared to the offline setting, online bilevel optimization
(OBO) offers a more dynamic framework by accommodating time-varying functions
and sequentially arriving data. This study addresses the online
nonconvex-strongly convex bilevel optimization problem. In deterministic
settings, we introduce a novel online Bregman bilevel optimizer (OBBO) that
utilizes adaptive Bregman divergences. We demonstrate that OBBO enhances the
known sublinear rates for bilevel local regret through a novel hypergradient
error decomposition that adapts to the underlying geometry of the problem. In
stochastic contexts, we introduce the first stochastic online bilevel optimizer
(SOBBO), which employs a window averaging method for updating outer-level
variables using a weighted average of recent stochastic approximations of
hypergradients. This approach not only achieves sublinear rates of bilevel
local regret but also serves as an effective variance reduction strategy,
obviating the need for additional stochastic gradient samples at each timestep.
Experiments on online hyperparameter optimization and online meta-learning
highlight the superior performance, efficiency, and adaptability of our
Bregman-based algorithms compared to established online and offline bilevel
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kolmogorov-Arnold Networks in Low-Data Regimes: A Comparative Study with
  Multilayer Perceptrons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farhad Pourkamali-Anaraki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilayer Perceptrons (MLPs) have long been a cornerstone in deep learning,
known for their capacity to model complex relationships. Recently,
Kolmogorov-Arnold Networks (KANs) have emerged as a compelling alternative,
utilizing highly flexible learnable activation functions directly on network
edges, a departure from the neuron-centric approach of MLPs. However, KANs
significantly increase the number of learnable parameters, raising concerns
about their effectiveness in data-scarce environments. This paper presents a
comprehensive comparative study of MLPs and KANs from both algorithmic and
experimental perspectives, with a focus on low-data regimes. We introduce an
effective technique for designing MLPs with unique, parameterized activation
functions for each neuron, enabling a more balanced comparison with KANs. Using
empirical evaluations on simulated data and two real-world data sets from
medicine and engineering, we explore the trade-offs between model complexity
and accuracy, with particular attention to the role of network depth. Our
findings show that MLPs with individualized activation functions achieve
significantly higher predictive accuracy with only a modest increase in
parameters, especially when the sample size is limited to around one hundred.
For example, in a three-class classification problem within additive
manufacturing, MLPs achieve a median accuracy of 0.91, significantly
outperforming KANs, which only reach a median accuracy of 0.53 with default
hyperparameters. These results offer valuable insights into the impact of
activation function selection in neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Signed Graph Autoencoder for Explainable and Polarization-Aware Network
  Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Nakis, Chrysoula Kosma, Giannis Nikolentzos, Michalis Chatzianastasis, Iakovos Evdaimon, Michalis Vazirgiannis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoencoders based on Graph Neural Networks (GNNs) have garnered significant
attention in recent years for their ability to extract informative latent
representations, characterizing the structure of complex topologies, such as
graphs. Despite the prevalence of Graph Autoencoders, there has been limited
focus on developing and evaluating explainable neural-based graph generative
models specifically designed for signed networks. To address this gap, we
propose the Signed Graph Archetypal Autoencoder (SGAAE) framework. SGAAE
extracts node-level representations that express node memberships over distinct
extreme profiles, referred to as archetypes, within the network. This is
achieved by projecting the graph onto a learned polytope, which governs its
polarization. The framework employs a recently proposed likelihood for
analyzing signed networks based on the Skellam distribution, combined with
relational archetypal analysis and GNNs. Our experimental evaluation
demonstrates the SGAAEs' capability to successfully infer node memberships over
the different underlying latent structures while extracting competing
communities formed through the participation of the opposing views in the
network. Additionally, we introduce the 2-level network polarization problem
and show how SGAAE is able to characterize such a setting. The proposed model
achieves high performance in different tasks of signed link prediction across
four real-world datasets, outperforming several baseline models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure-preserving learning for multi-symplectic PDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Süleyman Yıldız, Pawan Goyal, Peter Benner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an energy-preserving machine learning method for
inferring reduced-order models (ROMs) by exploiting the multi-symplectic form
of partial differential equations (PDEs). The vast majority of
energy-preserving reduced-order methods use symplectic Galerkin projection to
construct reduced-order Hamiltonian models by projecting the full models onto a
symplectic subspace. However, symplectic projection requires the existence of
fully discrete operators, and in many cases, such as black-box PDE solvers,
these operators are inaccessible. In this work, we propose an energy-preserving
machine learning method that can infer the dynamics of the given PDE using data
only, so that the proposed framework does not depend on the fully discrete
operators. In this context, the proposed method is non-intrusive. The proposed
method is grey box in the sense that it requires only some basic knowledge of
the multi-symplectic model at the partial differential equation level. We prove
that the proposed method satisfies spatially discrete local energy conservation
and preserves the multi-symplectic conservation laws. We test our method on the
linear wave equation, the Korteweg-de Vries equation, and the
Zakharov-Kuznetsov equation. We test the generalization of our learned models
by testing them far outside the training time interval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TPFL: Tsetlin-Personalized Federated Learning with Confidence-Based
  Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rasoul Jafari Gohari, Laya Aliahmadipour, Ezat Valipour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The world of Machine Learning (ML) has witnessed rapid changes in terms of
new models and ways to process users data. The majority of work that has been
done is focused on Deep Learning (DL) based approaches. However, with the
emergence of new algorithms such as the Tsetlin Machine (TM) algorithm, there
is growing interest in exploring alternative approaches that may offer unique
advantages in certain domains or applications. One of these domains is
Federated Learning (FL), in which users privacy is of utmost importance. Due to
its novelty, FL has seen a surge in the incorporation of personalization
techniques to enhance model accuracy while maintaining user privacy under
personalized conditions. In this work, we propose a novel approach dubbed TPFL:
Tsetlin-Personalized Federated Learning, in which models are grouped into
clusters based on their confidence towards a specific class. In this way,
clustering can benefit from two key advantages. Firstly, clients share only
what they are confident about, resulting in the elimination of wrongful weight
aggregation among clients whose data for a specific class may have not been
enough during the training. This phenomenon is prevalent when the data are
non-Independent and Identically Distributed (non-IID). Secondly, by sharing
only weights towards a specific class, communication cost is substantially
reduced, making TPLF efficient in terms of both accuracy and communication
cost. The results of TPFL demonstrated the highest accuracy on three different
datasets; namely MNIST, FashionMNIST and FEMNIST.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revising the Structure of Recurrent Neural Networks to Eliminate
  Numerical Derivatives in Forming Physics Informed Loss Terms with Respect to
  Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahyar Jahani-nasab, Mohamad Ali Bijarchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving unsteady partial differential equations (PDEs) using recurrent neural
networks (RNNs) typically requires numerical derivatives between each block of
the RNN to form the physics informed loss function. However, this introduces
the complexities of numerical derivatives into the training process of these
models. In this study, we propose modifying the structure of the traditional
RNN to enable the prediction of each block over a time interval, making it
possible to calculate the derivative of the output with respect to time using
the backpropagation algorithm. To achieve this, the time intervals of these
blocks are overlapped, defining a mutual loss function between them.
Additionally, the employment of conditional hidden states enables us to achieve
a unique solution for each block. The forget factor is utilized to control the
influence of the conditional hidden state on the prediction of the subsequent
block. This new model, termed the Mutual Interval RNN (MI-RNN), is applied to
solve three different benchmarks: the Burgers equation, unsteady heat
conduction in an irregular domain, and the Green vortex problem. Our results
demonstrate that MI-RNN can find the exact solution more accurately compared to
existing RNN models. For instance, in the second problem, MI-RNN achieved one
order of magnitude less relative error compared to the RNN model with numerical
derivatives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Gentle Grasping from Human-Free Force Control Demonstration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxuan Li, Lunwei Zhang, Tiemin Li, Yao Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can steadily and gently grasp unfamiliar objects based on tactile
perception. Robots still face challenges in achieving similar performance due
to the difficulty of learning accurate grasp-force predictions and force
control strategies that can be generalized from limited data. In this article,
we propose an approach for learning grasping from ideal force control
demonstrations, to achieve similar performance of human hands with limited data
size. Our approach utilizes objects with known contact characteristics to
automatically generate reference force curves without human demonstrations. In
addition, we design the dual convolutional neural networks (Dual-CNN)
architecture which incorporating a physics-based mechanics module for learning
target grasping force predictions from demonstrations. The described method can
be effectively applied in vision-based tactile sensors and enables gentle and
stable grasping of objects from the ground. The described prediction model and
grasping strategy were validated in offline evaluations and online experiments,
and the accuracy and generalizability were demonstrated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering the Mechanism of Hepatotoxiciy of PFAS Targeting L-FABP Using
  GCN and Computational Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Jividen, Tibo Duran, Xi-Zhi Niu, Jun Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Per- and polyfluoroalkyl substances (PFAS) are persistent environmental
pollutants with known toxicity and bioaccumulation issues. Their widespread
industrial use and resistance to degradation have led to global environmental
contamination and significant health concerns. While a minority of PFAS have
been extensively studied, the toxicity of many PFAS remains poorly understood
due to limited direct toxicological data. This study advances the predictive
modeling of PFAS toxicity by combining semi-supervised graph convolutional
networks (GCNs) with molecular descriptors and fingerprints. We propose a novel
approach to enhance the prediction of PFAS binding affinities by isolating
molecular fingerprints to construct graphs where then descriptors are set as
the node features. This approach specifically captures the structural,
physicochemical, and topological features of PFAS without overfitting due to an
abundance of features. Unsupervised clustering then identifies representative
compounds for detailed binding studies. Our results provide a more accurate
ability to estimate PFAS hepatotoxicity to provide guidance in chemical
discovery of new PFAS and the development of new safety regulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures, submitted to IEEE BIBM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 2D or not 2D: How Does the Dimensionality of Gesture Representation
  Affect 3D Co-Speech Gesture Generation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Téo Guichoux, Laure Soulier, Nicolas Obin, Catherine Pelachaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Co-speech gestures are fundamental for communication. The advent of recent
deep learning techniques has facilitated the creation of lifelike, synchronous
co-speech gestures for Embodied Conversational Agents. "In-the-wild" datasets,
aggregating video content from platforms like YouTube via human pose detection
technologies, provide a feasible solution by offering 2D skeletal sequences
aligned with speech. Concurrent developments in lifting models enable the
conversion of these 2D sequences into 3D gesture databases. However, it is
important to note that the 3D poses estimated from the 2D extracted poses are,
in essence, approximations of the ground-truth, which remains in the 2D domain.
This distinction raises questions about the impact of gesture representation
dimensionality on the quality of generated motions - a topic that, to our
knowledge, remains largely unexplored. Our study examines the effect of using
either 2D or 3D joint coordinates as training data on the performance of
speech-to-gesture deep generative models. We employ a lifting model for
converting generated 2D pose sequences into 3D and assess how gestures created
directly in 3D stack up against those initially generated in 2D and then
converted to 3D. We perform an objective evaluation using widely used metrics
in the gesture generation field as well as a user study to qualitatively
evaluate the different approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2406.15111</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperedge Modeling in Hypergraph Neural Networks by using Densest
  Overlapping Subgraphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehrad Soltani, Luis Rueda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypergraphs tackle the limitations of traditional graphs by introducing {\em
hyperedges}. While graph edges connect only two nodes, hyperedges connect an
arbitrary number of nodes along their edges. Also, the underlying
message-passing mechanisms in Hypergraph Neural Networks (HGNNs) are in the
form of vertex-hyperedge-vertex, which let HGNNs capture and utilize richer and
more complex structural information than traditional Graph Neural Networks
(GNNs). More recently, the idea of overlapping subgraphs has emerged. These
subgraphs can capture more information about subgroups of vertices without
limiting one vertex belonging to just one group, allowing vertices to belong to
multiple groups or subgraphs. In addition, one of the most important problems
in graph clustering is to find densest overlapping subgraphs (DOS). In this
paper, we propose a solution to the DOS problem via Agglomerative Greedy
Enumeration (DOSAGE) algorithm as a novel approach to enhance the process of
generating the densest overlapping subgraphs and, hence, a robust construction
of the hypergraphs. Experiments on standard benchmarks show that the DOSAGE
algorithm significantly outperforms the HGNNs and six other methods on the node
classification task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VAE-QWGAN: Improving Quantum GANs for High Resolution Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Mark Thomas, Sharu Theresa Jose
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel hybrid quantum generative model, the VAE-QWGAN,
which combines the strengths of a classical Variational AutoEncoder (VAE) with
a hybrid Quantum Wasserstein Generative Adversarial Network (QWGAN). The
VAE-QWGAN integrates the VAE decoder and QGAN generator into a single quantum
model with shared parameters, utilizing the VAE's encoder for latent vector
sampling during training. To generate new data from the trained model at
inference, input latent vectors are sampled from a Gaussian Mixture Model
(GMM), learnt on the training latent vectors. This, in turn, enhances the
diversity and quality of generated images. We evaluate the model's performance
on MNIST/Fashion-MNIST datasets, and demonstrate improved quality and diversity
of generated images compared to existing approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research and Design of a Financial Intelligent Risk Control Platform
  Based on Big Data Analysis and Deep Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10331v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10331v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuochen Bi, Yufan Lian, Ziyue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the financial field of the United States, the application of big data
technology has become one of the important means for financial institutions to
enhance competitiveness and reduce risks. The core objective of this article is
to explore how to fully utilize big data technology to achieve complete
integration of internal and external data of financial institutions, and create
an efficient and reliable platform for big data collection, storage, and
analysis. With the continuous expansion and innovation of financial business,
traditional risk management models are no longer able to meet the increasingly
complex market demands. This article adopts big data mining and real-time
streaming data processing technology to monitor, analyze, and alert various
business data. Through statistical analysis of historical data and precise
mining of customer transaction behavior and relationships, potential risks can
be more accurately identified and timely responses can be made. This article
designs and implements a financial big data intelligent risk control platform.
This platform not only achieves effective integration, storage, and analysis of
internal and external data of financial institutions, but also intelligently
displays customer characteristics and their related relationships, as well as
intelligent supervision of various risk information
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Hardness of Meaningful Local Guarantees in Nonsmooth Nonconvex
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Kornowski, Swati Padmanabhan, Ohad Shamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the oracle complexity of nonsmooth nonconvex optimization, with the
algorithm assumed to have access only to local function information. It has
been shown by Davis, Drusvyatskiy, and Jiang (2023) that for nonsmooth
Lipschitz functions satisfying certain regularity and strictness conditions,
perturbed gradient descent converges to local minimizers asymptotically.
Motivated by this result and by other recent algorithmic advances in nonconvex
nonsmooth optimization concerning Goldstein stationarity, we consider the
question of obtaining a non-asymptotic rate of convergence to local minima for
this problem class.
  We provide the following negative answer to this question: Local algorithms
acting on regular Lipschitz functions cannot, in the worst case, provide
meaningful local guarantees in terms of function value in sub-exponential time,
even when all near-stationary points are global minima. This sharply contrasts
with the smooth setting, for which it is well-known that standard gradient
methods can do so in a dimension-independent rate. Our result complements the
rich body of work in the theoretical computer science literature that provide
hardness results conditional on conjectures such as $\mathsf{P}\neq\mathsf{NP}$
or cryptographic assumptions, in that ours holds unconditional of any such
assumptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages; comments welcome!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEAL: Towards Safe Autonomous Driving via Skill-Enabled Adversary
  Learning for Closed-Loop Scenario Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Stoler, Ingrid Navarro, Jonathan Francis, Jean Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verification and validation of autonomous driving (AD) systems and components
is of increasing importance, as such technology increases in real-world
prevalence. Safety-critical scenario generation is a key approach to robustify
AD policies through closed-loop training. However, existing approaches for
scenario generation rely on simplistic objectives, resulting in
overly-aggressive or non-reactive adversarial behaviors. To generate diverse
adversarial yet realistic scenarios, we propose SEAL, a scenario perturbation
approach which leverages learned scoring functions and adversarial, human-like
skills. SEAL-perturbed scenarios are more realistic than SOTA baselines,
leading to improved ego task success across real-world, in-distribution, and
out-of-distribution scenarios, of more than 20%. To facilitate future research,
we release our code and tools: https://github.com/cmubig/SEAL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to do impactful research in artificial intelligence for chemistry
  and materials science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Austin Cheng, Cher Tian Ser, Marta Skreta, Andrés Guzmán-Cordero, Luca Thiede, Andreas Burger, Abdulrahman Aldossary, Shi Xuan Leong, Sergio Pablo-García, Felix Strieth-Kalthoff, Alán Aspuru-Guzik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning has been pervasively touching many fields of science.
Chemistry and materials science are no exception. While machine learning has
been making a great impact, it is still not reaching its full potential or
maturity. In this perspective, we first outline current applications across a
diversity of problems in chemistry. Then, we discuss how machine learning
researchers view and approach problems in the field. Finally, we provide our
considerations for maximizing impact when researching machine learning for
chemistry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReflectDiffu: Reflect between Emotion-intent Contagion and Mimicry for
  Empathetic Response Generation via a RL-Diffusion Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Yuan, Zixiang Di, Zhiqing Cui, Guisong Yang, Usman Naseem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empathetic response generation necessitates the integration of emotional and
intentional dynamics to foster meaningful interactions. Existing research
either neglects the intricate interplay between emotion and intent, leading to
suboptimal controllability of empathy, or resorts to large language models
(LLMs), which incur significant computational overhead. In this paper, we
introduce ReflectDiffu, a lightweight and comprehensive framework for
empathetic response generation. This framework incorporates emotion contagion
to augment emotional expressiveness and employs an emotion-reasoning mask to
pinpoint critical emotional elements. Additionally, it integrates intent
mimicry within reinforcement learning for refinement during diffusion. By
harnessing an intent twice reflect the mechanism of
Exploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional
decision-making into precise intent actions, thereby addressing empathetic
response misalignments stemming from emotional misrecognition. Through
reflection, the framework maps emotional states to intents, markedly enhancing
both response empathy and flexibility. Comprehensive experiments reveal that
ReflectDiffu outperforms existing models regarding relevance, controllability,
and informativeness, achieving state-of-the-art results in both automatic and
human evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Image Classification in Small and Unbalanced <span class="highlight-title">Dataset</span>s through
  Synthetic Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neil De La Fuente, Mireia Majó, Irina Luzko, Henry Córdova, Gloria Fernández-Esparrach, Jorge Bernal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and robust medical image classification is a challenging task,
especially in application domains where available annotated datasets are small
and present high imbalance between target classes. Considering that data
acquisition is not always feasible, especially for underrepresented classes,
our approach introduces a novel synthetic augmentation strategy using
class-specific Variational Autoencoders (VAEs) and latent space interpolation
to improve discrimination capabilities.
  By generating realistic, varied synthetic data that fills feature space gaps,
we address issues of data scarcity and class imbalance. The method presented in
this paper relies on the interpolation of latent representations within each
class, thus enriching the training set and improving the model's
generalizability and diagnostic accuracy. The proposed strategy was tested in a
small dataset of 321 images created to train and validate an automatic method
for assessing the quality of cleanliness of esophagogastroduodenoscopy images.
By combining real and synthetic data, an increase of over 18\% in the accuracy
of the most challenging underrepresented class was observed. The proposed
strategy not only benefited the underrepresented class but also led to a
general improvement in other metrics, including a 6\% increase in global
accuracy and precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BAFNet: Bilateral Attention Fusion Network for Lightweight Semantic
  Segmentation of Urban Remote Sensing Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Wang, Xili Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale semantic segmentation networks often achieve high performance,
while their application can be challenging when faced with limited sample sizes
and computational resources. In scenarios with restricted network size and
computational complexity, models encounter significant challenges in capturing
long-range dependencies and recovering detailed information in images. We
propose a lightweight bilateral semantic segmentation network called bilateral
attention fusion network (BAFNet) to efficiently segment high-resolution urban
remote sensing images. The model consists of two paths, namely dependency path
and remote-local path. The dependency path utilizes large kernel attention to
acquire long-range dependencies in the image. Besides, multi-scale local
attention and efficient remote attention are designed to construct remote-local
path. Finally, a feature aggregation module is designed to effectively utilize
the different features of the two paths. Our proposed method was tested on
public high-resolution urban remote sensing datasets Vaihingen and Potsdam,
with mIoU reaching 83.20% and 86.53%, respectively. As a lightweight semantic
segmentation model, BAFNet not only outperforms advanced lightweight models in
accuracy but also demonstrates comparable performance to non-lightweight
state-of-the-art methods on two datasets, despite a tenfold variance in
floating-point operations and a fifteenfold difference in network parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Personalized Recipe Recommendation Through Multi-Class
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harish Neelam, Koushik Sai Veerella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper intends to address the challenge of personalized recipe
recommendation in the realm of diverse culinary preferences. The problem domain
involves recipe recommendations, utilizing techniques such as association
analysis and classification. Association analysis explores the relationships
and connections between different ingredients to enhance the user experience.
Meanwhile, the classification aspect involves categorizing recipes based on
user-defined ingredients and preferences. A unique aspect of the paper is the
consideration of recipes and ingredients belonging to multiple classes,
recognizing the complexity of culinary combinations. This necessitates a
sophisticated approach to classification and recommendation, ensuring the
system accommodates the nature of recipe categorization. The paper seeks not
only to recommend recipes but also to explore the process involved in achieving
accurate and personalized recommendations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Graph Pooling Based on Minimum Description Length 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan von Pichowski, Christopher Blöcker, Ingo Scholtes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph pooling is an essential part of deep graph representation learning. We
introduce MapEqPool, a principled pooling operator that takes the inherent
hierarchical structure of real-world graphs into account. MapEqPool builds on
the map equation, an information-theoretic objective function for community
detection based on the minimum description length principle which naturally
implements Occam's razor and balances between model complexity and fit. We
demonstrate MapEqPool's competitive performance with an empirical comparison
against various baselines across standard graph classification datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Updating Vehicle Monitoring Framework Employing Distributed
  Acoustic Sensing towards Real-World Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Wang, Xin Liu, Songming Zhu, Zhanwen Li, Lina Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent emergence of Distributed Acoustic Sensing (DAS) technology has
facilitated the effective capture of traffic-induced seismic data. The
traffic-induced seismic wave is a prominent contributor to urban vibrations and
contain crucial information to advance urban exploration and governance.
However, identifying vehicular movements within massive noisy data poses a
significant challenge. In this study, we introduce a real-time semi-supervised
vehicle monitoring framework tailored to urban settings. It requires only a
small fraction of manual labels for initial training and exploits unlabeled
data for model improvement. Additionally, the framework can autonomously adapt
to newly collected unlabeled data. Before DAS data undergo object detection as
two-dimensional images to preserve spatial information, we leveraged
comprehensive one-dimensional signal preprocessing to mitigate noise.
Furthermore, we propose a novel prior loss that incorporates the shapes of
vehicular traces to track a single vehicle with varying speeds. To evaluate our
model, we conducted experiments with seismic data from the Stanford 2 DAS
Array. The results showed that our model outperformed the baseline model
Efficient Teacher and its supervised counterpart, YOLO (You Only Look Once), in
both accuracy and robustness. With only 35 labeled images, our model surpassed
YOLO's mAP 0.5:0.95 criterion by 18% and showed a 7% increase over Efficient
Teacher. We conducted comparative experiments with multiple update strategies
for self-updating and identified an optimal approach. This approach surpasses
the performance of non-overfitting training conducted with all data in a single
pass.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hedging Is Not All You Need: A Simple Baseline for Online Learning Under
  Haphazard Inputs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Himanshu Buckchash, Momojit Biswas, Rohit Agarwal, Dilip K. Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Handling haphazard streaming data, such as data from edge devices, presents a
challenging problem. Over time, the incoming data becomes inconsistent, with
missing, faulty, or new inputs reappearing. Therefore, it requires models that
are reliable. Recent methods to solve this problem depend on a hedging-based
solution and require specialized elements like auxiliary dropouts, forked
architectures, and intricate network design. We observed that hedging can be
reduced to a special case of weighted residual connection; this motivated us to
approximate it with plain self-attention. In this work, we propose HapNet, a
simple baseline that is scalable, does not require online backpropagation, and
is adaptable to varying input types. All present methods are restricted to
scaling with a fixed window; however, we introduce a more complex problem of
scaling with a variable window where the data becomes positionally
uncorrelated, and cannot be addressed by present methods. We demonstrate that a
variant of the proposed approach can work even for this complex scenario. We
extensively evaluated the proposed approach on five benchmarks and found
competitive performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safety-Oriented Pruning and Interpretation of Reinforcement Learning
  Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pruning neural networks (NNs) can streamline them but risks removing vital
parameters from safe reinforcement learning (RL) policies. We introduce an
interpretable RL method called VERINTER, which combines NN pruning with model
checking to ensure interpretable RL safety. VERINTER exactly quantifies the
effects of pruning and the impact of neural connections on complex safety
properties by analyzing changes in safety measurements. This method maintains
safety in pruned RL policies and enhances understanding of their safety
dynamics, which has proven effective in multiple RL settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embedded Image-to-Image Translation for Efficient Sim-to-Real Transfer
  in Learning-based Robot-Assisted Soft Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacinto Colan, Keisuke Sugita, Ana Davila, Yutaro Yamada, Yasuhisa Hasegawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in robotic learning in simulation have shown impressive
results in accelerating learning complex manipulation skills. However, the
sim-to-real gap, caused by discrepancies between simulation and reality, poses
significant challenges for the effective deployment of autonomous surgical
systems. We propose a novel approach utilizing image translation models to
mitigate domain mismatches and facilitate efficient robot skill learning in a
simulated environment. Our method involves the use of contrastive unpaired
Image-to-image translation, allowing for the acquisition of embedded
representations from these transformed images. Subsequently, these embeddings
are used to improve the efficiency of training surgical manipulation models. We
conducted experiments to evaluate the performance of our approach,
demonstrating that it significantly enhances task success rates and reduces the
steps required for task completion compared to traditional methods. The results
indicate that our proposed system effectively bridges the sim-to-real gap,
providing a robust framework for advancing the autonomy of surgical robots in
minimally invasive procedures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 IEEE International Symposium on
  Micro-NanoMechatronics and Human Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Milling Quality Prediction with Explainable Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker, Arnaud Gotlieb, Ricardo Knoblauch, Mohamed Elmansori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an explainable machine learning (ML) approach for
predicting surface roughness in milling. Utilizing a dataset from milling
aluminum alloy 2017A, the study employs random forest regression models and
feature importance techniques. The key contributions include developing ML
models that accurately predict various roughness values and identifying
redundant sensors, particularly those for measuring normal cutting force. Our
experiments show that removing certain sensors can reduce costs without
sacrificing predictive accuracy, highlighting the potential of explainable
machine learning to improve cost-effectiveness in machining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2403.18731</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing RL Safety with Counterfactual <span class="highlight-title">LLM</span> Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) policies may exhibit unsafe behavior and are hard
to explain. We use counterfactual large language model reasoning to enhance RL
policy safety post-training. We show that our approach improves and helps to
explain the RL policy safety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TCDformer-based Momentum Transfer Model for Long-term Sports Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Liu, Jiacheng Gu, Xiyuan Huang, Junjie Shi, Tongtong Feng, Ning He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate sports prediction is a crucial skill for professional coaches, which
can assist in developing effective training strategies and scientific
competition tactics. Traditional methods often use complex mathematical
statistical techniques to boost predictability, but this often is limited by
dataset scale and has difficulty handling long-term predictions with variable
distributions, notably underperforming when predicting point-set-game
multi-level matches. To deal with this challenge, this paper proposes TM2, a
TCDformer-based Momentum Transfer Model for long-term sports prediction, which
encompasses a momentum encoding module and a prediction module based on
momentum transfer. TM2 initially encodes momentum in large-scale unstructured
time series using the local linear scaling approximation (LLSA) module. Then it
decomposes the reconstructed time series with momentum transfer into trend and
seasonal components. The final prediction results are derived from the additive
combination of a multilayer perceptron (MLP) for predicting trend components
and wavelet attention mechanisms for seasonal components. Comprehensive
experimental results show that on the 2023 Wimbledon men's tournament datasets,
TM2 significantly surpasses existing sports prediction models in terms of
performance, reducing MSE by 61.64% and MAE by 63.64%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under reviewing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe and Stable Closed-Loop Learning for Neural-Network-Supported Model
  Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Hirt, Maik Pfefferkorn, Rolf Findeisen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe learning of control policies remains challenging, both in optimal
control and reinforcement learning. In this article, we consider safe learning
of parametrized predictive controllers that operate with incomplete information
about the underlying process. To this end, we employ Bayesian optimization for
learning the best parameters from closed-loop data. Our method focuses on the
system's overall long-term performance in closed-loop while keeping it safe and
stable. Specifically, we parametrize the stage cost function of an MPC using a
feedforward neural network. This allows for a high degree of flexibility,
enabling the system to achieve a better closed-loop performance with respect to
a superordinate measure. However, this flexibility also necessitates safety
measures, especially with respect to closed-loop stability. To this end, we
explicitly incorporated stability information in the
Bayesian-optimization-based learning procedure, thereby achieving rigorous
probabilistic safety guarantees. The proposed approach is illustrated using a
numeric example.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, accepted for CDC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantile Regression for Distributional Reward Models in RLHF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolai Dorka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning from human feedback (RLHF) has become a key method for
aligning large language models (LLMs) with human preferences through the use of
reward models. However, traditional reward models typically generate point
estimates, which oversimplify the diversity and complexity of human values and
preferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel
approach to reward modeling that learns a distribution over rewards instead of
a single scalar value. Our method uses quantile regression to estimate a full,
potentially multimodal distribution over preferences, providing a more powerful
and nuanced representation of preferences. This distributional approach can
better capture the diversity of human values, addresses label noise, and
accommodates conflicting preferences by modeling them as distinct modes in the
distribution. Our experimental results show that QRM outperforms comparable
traditional point-estimate models on RewardBench. Furthermore, we demonstrate
that the additional information provided by the distributional estimates can be
utilized in downstream applications, such as risk-aware reinforcement learning,
resulting in LLM policies that generate fewer extremely negative responses. Our
code and model are released at https://github.com/Nicolinho/QRM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using
  Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Nomaan Qureshi, Sparsh Garg, Francisco Yandun, David Held, George Kantor, Abhishesh Silwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sim2Real transfer, particularly for manipulation policies relying on RGB
images, remains a critical challenge in robotics due to the significant domain
shift between synthetic and real-world visual data. In this paper, we propose
SplatSim, a novel framework that leverages Gaussian Splatting as the primary
rendering primitive to reduce the Sim2Real gap for RGB-based manipulation
policies. By replacing traditional mesh representations with Gaussian Splats in
simulators, SplatSim produces highly photorealistic synthetic data while
maintaining the scalability and cost-efficiency of simulation. We demonstrate
the effectiveness of our framework by training manipulation policies within
SplatSim}and deploying them in the real world in a zero-shot manner, achieving
an average success rate of 86.25%, compared to 97.5% for policies trained on
real-world data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Network Embedding by Approximate Equitable Partitions <span class="chip">ICDM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Squillace, Mirco Tribastone, Max Tschaikowski, Andrea Vandin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structural network embedding is a crucial step in enabling effective
downstream tasks for complex systems that aims to project a network into a
lower-dimensional space while preserving similarities among nodes. We introduce
a simple and efficient embedding technique based on approximate variants of
equitable partitions. The approximation consists in introducing a user-tunable
tolerance parameter relaxing the otherwise strict condition for exact equitable
partitions that can be hardly found in real-world networks. We exploit a
relationship between equitable partitions and equivalence relations for Markov
chains and ordinary differential equations to develop a partition refinement
algorithm for computing an approximate equitable partition in polynomial time.
We compare our method against state-of-the-art embedding techniques on
benchmark networks. We report comparable -- when not superior -- performance
for visualization, classification, and regression tasks at a cost between one
and three orders of magnitude smaller using a prototype implementation,
enabling the embedding of large-scale networks which could not be efficiently
handled by most of the competing techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICDM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning for Character Detection in Ancient Greek Papyri 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vedasri Nakka, Andreas Fischer, Rolf Ingold, Lars Vogtlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis investigates the effectiveness of SimCLR, a contrastive learning
technique, in Greek letter recognition, focusing on the impact of various
augmentation techniques. We pretrain the SimCLR backbone using the Alpub
dataset (pretraining dataset) and fine-tune it on a smaller ICDAR dataset
(finetuning dataset) to compare SimCLR's performance against traditional
baseline models, which use cross-entropy and triplet loss functions.
Additionally, we explore the role of different data augmentation strategies,
essential for the SimCLR training process. Methodologically, we examine three
primary approaches: (1) a baseline model using cross-entropy loss, (2) a
triplet embedding model with a classification layer, and (3) a SimCLR
pretrained model with a classification layer. Initially, we train the baseline,
triplet, and SimCLR models using 93 augmentations on ResNet-18 and ResNet-50
networks with the ICDAR dataset. From these, the top four augmentations are
selected using a statistical t-test. Pretraining of SimCLR is conducted on the
Alpub dataset, followed by fine-tuning on the ICDAR dataset. The triplet loss
model undergoes a similar process, being pretrained on the top four
augmentations before fine-tuning on ICDAR. Our experiments show that SimCLR
does not outperform the baselines in letter recognition tasks. The baseline
model with cross-entropy loss demonstrates better performance than both SimCLR
and the triplet loss model. This study provides a detailed evaluation of
contrastive learning for letter recognition, highlighting SimCLR's limitations
while emphasizing the strengths of traditional supervised learning models in
this task. We believe SimCLR's cropping strategies may cause a semantic shift
in the input image, reducing training effectiveness despite the large
pretraining dataset. Our code is available at
https://github.com/DIVA-DIA/MT_augmentation_and_contrastive_learning/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AALF: Almost Always Linear Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Jakobs, Thomas Liebig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works for time-series forecasting more and more leverage the high
predictive power of Deep Learning models. With this increase in model
complexity, however, comes a lack in understanding of the underlying model
decision process, which is problematic for high-stakes decision making. At the
same time, simple, interpretable forecasting methods such as Linear Models can
still perform very well, sometimes on-par, with Deep Learning approaches. We
argue that simple models are good enough most of the time, and forecasting
performance can be improved by choosing a Deep Learning method only for certain
predictions, increasing the overall interpretability of the forecasting
process. In this context, we propose a novel online model selection framework
which uses meta-learning to identify these predictions and only rarely uses a
non-interpretable, large model. An extensive empirical study on various
real-world datasets shows that our selection methodology outperforms
state-of-the-art online model selections methods in most cases. We find that
almost always choosing a simple Linear Model for forecasting results in
competitive performance, suggesting that the need for opaque black-box models
in time-series forecasting is smaller than recent works would suggest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Efficacy of Instance Incremental vs. Batch Learning in
  Delayed Label Environments: An Empirical Study on Tabular Data Streaming for
  Fraud Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kodjo Mawuena Amekoe, Mustapha Lebbah, Gregoire Jaffre, Hanene Azzag, Zaineb Chelly Dagdia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world tabular learning production scenarios typically involve evolving
data streams, where data arrives continuously and its distribution may change
over time. In such a setting, most studies in the literature regarding
supervised learning favor the use of instance incremental algorithms due to
their ability to adapt to changes in the data distribution. Another significant
reason for choosing these algorithms is \textit{avoid storing observations in
memory} as commonly done in batch incremental settings. However, the design of
instance incremental algorithms often assumes immediate availability of labels,
which is an optimistic assumption. In many real-world scenarios, such as fraud
detection or credit scoring, labels may be delayed. Consequently, batch
incremental algorithms are widely used in many real-world tasks. This raises an
important question: "In delayed settings, is instance incremental learning the
best option regarding predictive performance and computational efficiency?"
Unfortunately, this question has not been studied in depth, probably due to the
scarcity of real datasets containing delayed information. In this study, we
conduct a comprehensive empirical evaluation and analysis of this question
using a real-world fraud detection problem and commonly used generated
datasets. Our findings indicate that instance incremental learning is not the
superior option, considering on one side state-of-the-art models such as
Adaptive Random Forest (ARF) and other side batch learning models such as
XGBoost. Additionally, when considering the interpretability of the learning
systems, batch incremental solutions tend to be favored. Code:
\url{https://github.com/anselmeamekoe/DelayedLabelStream}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study of Open Source Computer Vision Models for
  Application on Small Data: The Case of CFRP Tape Laying 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Fraunholz, Dennis Rall, Tim Köhler, Alfons Schuster, Monika Mayer, Lars Larsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of industrial manufacturing, Artificial Intelligence (AI) is
playing an increasing role, from automating existing processes to aiding in the
development of new materials and techniques. However, a significant challenge
arises in smaller, experimental processes characterized by limited training
data availability, questioning the possibility to train AI models in such small
data contexts. In this work, we explore the potential of Transfer Learning to
address this challenge, specifically investigating the minimum amount of data
required to develop a functional AI model. For this purpose, we consider the
use case of quality control of Carbon Fiber Reinforced Polymer (CFRP) tape
laying in aerospace manufacturing using optical sensors. We investigate the
behavior of different open-source computer vision models with a continuous
reduction of the training data. Our results show that the amount of data
required to successfully train an AI model can be drastically reduced, and the
use of smaller models does not necessarily lead to a loss of performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Reinforcement Learning with Dynamic Distortion Risk Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Coache, Sebastian Jaimungal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a reinforcement learning (RL) setting, the agent's optimal strategy
heavily depends on her risk preferences and the underlying model dynamics of
the training environment. These two aspects influence the agent's ability to
make well-informed and time-consistent decisions when facing testing
environments. In this work, we devise a framework to solve robust risk-aware RL
problems where we simultaneously account for environmental uncertainty and risk
with a class of dynamic robust distortion risk measures. Robustness is
introduced by considering all models within a Wasserstein ball around a
reference model. We estimate such dynamic robust risk measures using neural
networks by making use of strictly consistent scoring functions, derive policy
gradient formulae using the quantile representation of distortion risk
measures, and construct an actor-critic algorithm to solve this class of robust
risk-aware RL problems. We demonstrate the performance of our algorithm on a
portfolio allocation example.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDoS: Diffusion Distribution Similarity for Out-of-Distribution
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Fang, Qinghua Tao, Zuopeng Yang, Xiaolin Huang, Jie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-Distribution (OoD) detection determines whether the given samples are
from the training distribution of the classifier-under-protection, i.e., the
In-Distribution (InD), or from a different OoD. Latest researches introduce
diffusion models pre-trained on InD data to advocate OoD detection by
transferring an OoD image into a generated one that is close to InD, so that
one could capture the distribution disparities between original and generated
images to detect OoD data. Existing diffusion-based detectors adopt perceptual
metrics on the two images to measure such disparities, but ignore a fundamental
fact: Perceptual metrics are devised essentially for human-perceived
similarities of low-level image patterns, e.g., textures and colors, and are
not advisable in evaluating distribution disparities, since images with
different low-level patterns could possibly come from the same distribution. To
address this issue, we formulate a diffusion-based detection framework that
considers the distribution similarity between a tested image and its generated
counterpart via a novel proper similarity metric in the informative feature
space and probability space learned by the classifier-under-protection. An
anomaly-removal strategy is further presented to enlarge such distribution
disparities by removing abnormal OoD information in the feature space to
facilitate the detection. Extensive empirical results unveil the insufficiency
of perceptual metrics and the effectiveness of our distribution similarity
framework with new state-of-the-art detection performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Riemannian Approach to Ground Metric Learning for Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratik Jawanpuria, Dai Shi, Bamdev Mishra, Junbin Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal transport (OT) theory has attracted much attention in machine
learning and signal processing applications. OT defines a notion of distance
between probability distributions of source and target data points. A crucial
factor that influences OT-based distances is the ground metric of the embedding
space in which the source and target data points lie. In this work, we propose
to learn a suitable latent ground metric parameterized by a symmetric positive
definite matrix. We use the rich Riemannian geometry of symmetric positive
definite matrices to jointly learn the OT distance along with the ground
metric. Empirical results illustrate the efficacy of the learned metric in
OT-based domain adaptation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steinmetz Neural Networks for Complex-Valued Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shyam Venkatasubramanian, Ali Pezeshki, Vahid Tarokh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce a new approach to processing complex-valued data
using DNNs consisting of parallel real-valued subnetworks with coupled outputs.
Our proposed class of architectures, referred to as Steinmetz Neural Networks,
leverages multi-view learning to construct more interpretable representations
within the latent space. Subsequently, we present the Analytic Neural Network,
which implements a consistency penalty that encourages analytic signal
representations in the Steinmetz neural network's latent space. This penalty
enforces a deterministic and orthogonal relationship between the real and
imaginary components. Utilizing an information-theoretic construction, we
demonstrate that the upper bound on the generalization error posited by the
analytic neural network is lower than that of the general class of Steinmetz
neural networks. Our numerical experiments demonstrate the improved performance
and robustness to additive noise, afforded by our proposed networks on
benchmark datasets and synthetic examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Anomaly Detection via Generating Diversified and
  Hard-to-distinguish Synthetic Anomalies <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyuntae Kim, Changhee Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised anomaly detection is a daunting task, as it relies solely on
normality patterns from the training data to identify unseen anomalies during
testing. Recent approaches have focused on leveraging domain-specific
transformations or perturbations to generate synthetic anomalies from normal
samples. The objective here is to acquire insights into normality patterns by
learning to differentiate between normal samples and these crafted anomalies.
However, these approaches often encounter limitations when domain-specific
transformations are not well-specified such as in tabular data, or when it
becomes trivial to distinguish between them. To address these issues, we
introduce a novel domain-agnostic method that employs a set of conditional
perturbators and a discriminator. The perturbators are trained to generate
input-dependent perturbations, which are subsequently utilized to construct
synthetic anomalies, and the discriminator is trained to distinguish normal
samples from them. We ensure that the generated anomalies are both diverse and
hard to distinguish through two key strategies: i) directing perturbations to
be orthogonal to each other and ii) constraining perturbations to remain in
proximity to normal samples. Throughout experiments on real-world datasets, we
demonstrate the superiority of our method over state-of-the-art benchmarks,
which is evident not only in image data but also in tabular data, where
domain-specific transformation is not readily accessible. Additionally, we
empirically confirm the adaptability of our method to semi-supervised settings,
demonstrating its capacity to incorporate supervised signals to enhance anomaly
detection performance even further.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatiotemporal Covariance Neural Networks <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Cavallo, Mohammad Sabbaqi, Elvin Isufi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling spatiotemporal interactions in multivariate time series is key to
their effective processing, but challenging because of their irregular and
often unknown structure. Statistical properties of the data provide useful
biases to model interdependencies and are leveraged by correlation and
covariance-based networks as well as by processing pipelines relying on
principal component analysis (PCA). However, PCA and its temporal extensions
suffer instabilities in the covariance eigenvectors when the corresponding
eigenvalues are close to each other, making their application to dynamic and
streaming data settings challenging. To address these issues, we exploit the
analogy between PCA and graph convolutional filters to introduce the
SpatioTemporal coVariance Neural Network (STVNN), a relational learning model
that operates on the sample covariance matrix of the time series and leverages
joint spatiotemporal convolutions to model the data. To account for the
streaming and non-stationary setting, we consider an online update of the
parameters and sample covariance matrix. We prove the STVNN is stable to the
uncertainties introduced by these online estimations, thus improving over
temporal PCA-based methods. Experimental results corroborate our theoretical
findings and show that STVNN is competitive for multivariate time series
processing, it adapts to changes in the data distribution, and it is orders of
magnitude more stable than online temporal PCA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Joint European Conference on Machine Learning and Knowledge Discovery
  in Databases (ECML PKDD) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Lightning-Ignited Wildfires Prediction and Climate Change
  Projections based on Explainable Machine Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Assaf Shmuel, Teddy Lazebnik, Oren Glickman, Eyal Heifetz, Colin Price
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wildfires pose a significant natural disaster risk to populations and
contribute to accelerated climate change. As wildfires are also affected by
climate change, extreme wildfires are becoming increasingly frequent. Although
they occur less frequently globally than those sparked by human activities,
lightning-ignited wildfires play a substantial role in carbon emissions and
account for the majority of burned areas in certain regions. While existing
computational models, especially those based on machine learning, aim to
predict lightning-ignited wildfires, they are typically tailored to specific
regions with unique characteristics, limiting their global applicability. In
this study, we present machine learning models designed to characterize and
predict lightning-ignited wildfires on a global scale. Our approach involves
classifying lightning-ignited versus anthropogenic wildfires, and estimating
with high accuracy the probability of lightning to ignite a fire based on a
wide spectrum of factors such as meteorological conditions and vegetation.
Utilizing these models, we analyze seasonal and spatial trends in
lightning-ignited wildfires shedding light on the impact of climate change on
this phenomenon. We analyze the influence of various features on the models
using eXplainable Artificial Intelligence (XAI) frameworks. Our findings
highlight significant global differences between anthropogenic and
lightning-ignited wildfires. Moreover, we demonstrate that, even over a short
time span of less than a decade, climate changes have steadily increased the
global risk of lightning-ignited wildfires. This distinction underscores the
imperative need for dedicated predictive models and fire weather indices
tailored specifically to each type of wildfire.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Latent Wireless Dynamics from Channel State <span class="highlight-title">Information</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charbel Bou Chaaya, Abanoub M. Girgis, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel data-driven machine learning (ML) technique
to model and predict the dynamics of the wireless propagation environment in
latent space. Leveraging the idea of channel charting, which learns compressed
representations of high-dimensional channel state information (CSI), we
incorporate a predictive component to capture the dynamics of the wireless
system. Hence, we jointly learn a channel encoder that maps the estimated CSI
to an appropriate latent space, and a predictor that models the relationships
between such representations. Accordingly, our problem boils down to training a
joint-embedding predictive architecture (JEPA) that simulates the latent
dynamics of a wireless network from CSI. We present numerical evaluations on
measured data and show that the proposed JEPA displays a two-fold increase in
accuracy over benchmarks, for longer look-ahead prediction tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Large Language Model Uncertainty for <span class="highlight-title">Prompt</span> Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei-Fu Guo, Yun-Da Tsai, Shou-De Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt optimization algorithms for Large Language Models (LLMs) excel in
multi-step reasoning but still lack effective uncertainty estimation. This
paper introduces a benchmark dataset to evaluate uncertainty metrics, focusing
on Answer, Correctness, Aleatoric, and Epistemic Uncertainty. Through analysis
of models like GPT-3.5-Turbo and Meta-Llama-3.1-8B-Instruct, we show that
current metrics align more with Answer Uncertainty, which reflects output
confidence and diversity, rather than Correctness Uncertainty, highlighting the
need for improved metrics that are optimization-objective-aware to better guide
prompt optimization. Our code and dataset are available at
https://github.com/0Frett/PO-Uncertainty-Benchmarking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Diagram of Thought 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhang, Yang Yuan, Andrew Chi-Chih Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Diagram of Thought (DoT), a framework that models iterative
reasoning in large language models (LLMs) as the construction of a directed
acyclic graph (DAG) within a single model. Unlike traditional approaches that
represent reasoning as linear chains or trees, DoT organizes propositions,
critiques, refinements, and verifications into a cohesive DAG structure,
allowing the model to explore complex reasoning pathways while maintaining
logical consistency. Each node in the diagram corresponds to a proposition that
has been proposed, critiqued, refined, or verified, enabling the LLM to
iteratively improve its reasoning through natural language feedback. By
leveraging auto-regressive next-token prediction with role-specific tokens, DoT
facilitates seamless transitions between proposing ideas and critically
evaluating them, providing richer feedback than binary signals. Furthermore, we
formalize the DoT framework using Topos Theory, providing a mathematical
foundation that ensures logical consistency and soundness in the reasoning
process. This approach enhances both the training and inference processes
within a single LLM, eliminating the need for multiple models or external
control mechanisms. DoT offers a conceptual framework for designing
next-generation reasoning-specialized models, emphasizing training efficiency,
robust reasoning capabilities, and theoretical grounding. The code is available
at https://github.com/diagram-of-thought/diagram-of-thought.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement learning-based statistical search strategy for an axion
  model from flavor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satsuki Nishimura, Coh Miyao, Hajime Otsuka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a reinforcement learning-based search strategy to explore new
physics beyond the Standard Model. The reinforcement learning, which is one of
machine learning methods, is a powerful approach to find model parameters with
phenomenological constraints. As a concrete example, we focus on a minimal
axion model with a global $U(1)$ flavor symmetry. Agents of the learning
succeed in finding $U(1)$ charge assignments of quarks and leptons solving the
flavor and cosmological puzzles in the Standard Model, and find more than 150
realistic solutions for the quark sector taking renormalization effects into
account. For the solutions found by the reinforcement learning-based analysis,
we discuss the sensitivity of future experiments for the detection of an axion
which is a Nambu-Goldstone boson of the spontaneously broken $U(1)$. We also
examine how fast the reinforcement learning-based searching method finds the
best discrete parameters in comparison with conventional optimization methods.
In conclusion, the efficient parameter search based on the reinforcement
learning-based strategy enables us to perform a statistical analysis of the
vast parameter space associated with the axion model from flavor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreeMark: A Non-Invasive White-Box Watermarking for Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhang Chen, Jiangnan Zhu, Yujie Gu, Minoru Kuribayashi, Kouichi Sakurai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) have achieved significant success in real-world
applications. However, safeguarding their intellectual property (IP) remains
extremely challenging. Existing DNN watermarking for IP protection often
require modifying DNN models, which reduces model performance and limits their
practicality.
  This paper introduces FreeMark, a novel DNN watermarking framework that
leverages cryptographic principles without altering the original host DNN
model, thereby avoiding any reduction in model performance. Unlike traditional
DNN watermarking methods, FreeMark innovatively generates secret keys from a
pre-generated watermark vector and the host model using gradient descent. These
secret keys, used to extract watermark from the model's activation values, are
securely stored with a trusted third party, enabling reliable watermark
extraction from suspect models. Extensive experiments demonstrate that FreeMark
effectively resists various watermark removal attacks while maintaining high
watermark capacity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SHIRE: Enhancing Sample Efficiency using Human Intuition in
  REinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amogh Joshi, Adarsh Kumar Kosta, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of neural networks to perform robotic perception and control
tasks such as depth and optical flow estimation, simultaneous localization and
mapping (SLAM), and automatic control has led to their widespread adoption in
recent years. Deep Reinforcement Learning has been used extensively in these
settings, as it does not have the unsustainable training costs associated with
supervised learning. However, DeepRL suffers from poor sample efficiency, i.e.,
it requires a large number of environmental interactions to converge to an
acceptable solution. Modern RL algorithms such as Deep Q Learning and Soft
Actor-Critic attempt to remedy this shortcoming but can not provide the
explainability required in applications such as autonomous robotics. Humans
intuitively understand the long-time-horizon sequential tasks common in
robotics. Properly using such intuition can make RL policies more explainable
while enhancing their sample efficiency. In this work, we propose SHIRE, a
novel framework for encoding human intuition using Probabilistic Graphical
Models (PGMs) and using it in the Deep RL training pipeline to enhance sample
efficiency. Our framework achieves 25-78% sample efficiency gains across the
environments we evaluate at negligible overhead cost. Additionally, by teaching
RL agents the encoded elementary behavior, SHIRE enhances policy
explainability. A real-world demonstration further highlights the efficacy of
policies trained using our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence of Sharpness-Aware Minimization Algorithms using Increasing
  Batch Size and Decaying Learning Rate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hinata Harada, Hideaki Iiduka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sharpness-aware minimization (SAM) algorithm and its variants, including
gap guided SAM (GSAM), have been successful at improving the generalization
capability of deep neural network models by finding flat local minima of the
empirical loss in training. Meanwhile, it has been shown theoretically and
practically that increasing the batch size or decaying the learning rate avoids
sharp local minima of the empirical loss. In this paper, we consider the GSAM
algorithm with increasing batch sizes or decaying learning rates, such as
cosine annealing or linear learning rate, and theoretically show its
convergence. Moreover, we numerically compare SAM (GSAM) with and without an
increasing batch size and conclude that using an increasing batch size or
decaying learning rate finds flatter local minima than using a constant batch
size and learning rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Bytes to Bites: Using Country Specific Machine Learning Models to
  Predict Famine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salloni Kapoor, Simeon Sayer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hunger crises are critical global issues affecting millions, particularly in
low-income and developing countries. This research investigates how machine
learning can be utilized to predict and inform decisions regarding famine and
hunger crises. By leveraging a diverse set of variables (natural, economic, and
conflict-related), three machine learning models (Linear Regression, XGBoost,
and RandomForestRegressor) were employed to predict food consumption scores, a
key indicator of household nutrition. The RandomForestRegressor emerged as the
most accurate model, with an average prediction error of 10.6%, though accuracy
varied significantly across countries, ranging from 2% to over 30%. Notably,
economic indicators were consistently the most significant predictors of
average household nutrition, while no single feature dominated across all
regions, underscoring the necessity for comprehensive data collection and
tailored, country-specific models. These findings highlight the potential of
machine learning, particularly Random Forests, to enhance famine prediction,
suggesting that continued research and improved data gathering are essential
for more effective global hunger forecasting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Conditioned Spatio-Temporal Predictive Learning for Reliable V2V
  Channel Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Chu, Daoud Burghal, Michael Neuman, Andreas F. Molisch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving reliable multidimensional Vehicle-to-Vehicle (V2V) channel state
information (CSI) prediction is both challenging and crucial for optimizing
downstream tasks that depend on instantaneous CSI. This work extends
traditional prediction approaches by focusing on four-dimensional (4D) CSI,
which includes predictions over time, bandwidth, and antenna (TX and RX) space.
Such a comprehensive framework is essential for addressing the dynamic nature
of mobility environments within intelligent transportation systems,
necessitating the capture of both temporal and spatial dependencies across
diverse domains. To address this complexity, we propose a novel
context-conditioned spatiotemporal predictive learning method. This method
leverages causal convolutional long short-term memory (CA-ConvLSTM) to
effectively capture dependencies within 4D CSI data, and incorporates
context-conditioned attention mechanisms to enhance the efficiency of
spatiotemporal memory updates. Additionally, we introduce an adaptive
meta-learning scheme tailored for recurrent networks to mitigate the issue of
accumulative prediction errors. We validate the proposed method through
empirical studies conducted across three different geometric configurations and
mobility scenarios. Our results demonstrate that the proposed approach
outperforms existing state-of-the-art predictive models, achieving superior
performance across various geometries. Moreover, we show that the meta-learning
framework significantly enhances the performance of recurrent-based predictive
models in highly challenging cross-geometry settings, thus highlighting its
robustness and adaptability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Offline Adaptation Framework for Constrained Multi-Objective
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09958v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09958v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Lin, Zongkai Liu, Danying Mo, Chao Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, significant progress has been made in multi-objective
reinforcement learning (RL) research, which aims to balance multiple objectives
by incorporating preferences for each objective. In most existing studies,
specific preferences must be provided during deployment to indicate the desired
policies explicitly. However, designing these preferences depends heavily on
human prior knowledge, which is typically obtained through extensive
observation of high-performing demonstrations with expected behaviors. In this
work, we propose a simple yet effective offline adaptation framework for
multi-objective RL problems without assuming handcrafted target preferences,
but only given several demonstrations to implicitly indicate the preferences of
expected policies. Additionally, we demonstrate that our framework can
naturally be extended to meet constraints on safety-critical objectives by
utilizing safe demonstrations, even when the safety thresholds are unknown.
Empirical results on offline multi-objective and safe tasks demonstrate the
capability of our framework to infer policies that align with real preferences
while meeting the constraints implied by the provided demonstrations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Graph Anomaly Detection: A <span class="highlight-title">Survey</span> and New Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hezhe Qiao, Hanghang Tong, Bo An, Irwin King, Charu Aggarwal, Guansong Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph anomaly detection (GAD), which aims to identify unusual graph instances
(nodes, edges, subgraphs, or graphs), has attracted increasing attention in
recent years due to its significance in a wide range of applications. Deep
learning approaches, graph neural networks (GNNs) in particular, have been
emerging as a promising paradigm for GAD, owing to its strong capability in
capturing complex structure and/or node attributes in graph data. Considering
the large number of methods proposed for GNN-based GAD, it is of paramount
importance to summarize the methodologies and findings in the existing GAD
studies, so that we can pinpoint effective model designs for tackling open GAD
problems. To this end, in this work we aim to present a comprehensive review of
deep learning approaches for GAD. Existing GAD surveys are focused on
task-specific discussions, making it difficult to understand the technical
insights of existing methods and their limitations in addressing some unique
challenges in GAD. To fill this gap, we first discuss the problem complexities
and their resulting challenges in GAD, and then provide a systematic review of
current deep GAD methods from three novel perspectives of methodology,
including GNN backbone design, proxy task design for GAD, and graph anomaly
measures. To deepen the discussions, we further propose a taxonomy of 13
fine-grained method categories under these three perspectives to provide more
in-depth insights into the model designs and their capabilities. To facilitate
the experiments and validation, we also summarize a collection of widely-used
GAD datasets and empirical comparison. We further discuss multiple open
problems to inspire more future high-quality research. A continuously updated
repository for datasets, links to the codes of algorithms, and empirical
comparison is available at
https://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 6 figures, and 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal ablation for interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Li, Lucas Janson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretability studies often involve tracing the flow of information
through machine learning models to identify specific model components that
perform relevant computations for tasks of interest. Prior work quantifies the
importance of a model component on a particular task by measuring the impact of
performing ablation on that component, or simulating model inference with the
component disabled. We propose a new method, optimal ablation (OA), and show
that OA-based component importance has theoretical and empirical advantages
over measuring importance via other ablation methods. We also show that
OA-based component importance can benefit several downstream interpretability
tasks, including circuit discovery, localization of factual recall, and latent
prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fault Analysis And Predictive Maintenance Of Induction Motor Using
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kavana Venkatesh, Neethi M
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Induction motors are one of the most crucial electrical equipment and are
extensively used in industries in a wide range of applications. This paper
presents a machine learning model for the fault detection and classification of
induction motor faults by using three phase voltages and currents as inputs.
The aim of this work is to protect vital electrical components and to prevent
abnormal event progression through early detection and diagnosis. This work
presents a fast forward artificial neural network model to detect some of the
commonly occurring electrical faults like overvoltage, under voltage, single
phasing, unbalanced voltage, overload, ground fault. A separate model free
monitoring system wherein the motor itself acts like a sensor is presented and
the only monitored signals are the input given to the motor. Limits for current
and voltage values are set for the faulty and healthy conditions, which is done
by a classifier. Real time data from a 0.33 HP induction motor is used to train
and test the neural network. The model so developed analyses the voltage and
current values given at a particular instant and classifies the data into no
fault or the specific fault. The model is then interfaced with a real motor to
accurately detect and classify the faults so that further necessary action can
be taken.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ICEECCOT-2018, Published in IEEE Xplore, 6 pages, 3
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizability of Graph Neural Network Force Fields for Predicting
  Solid-State Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaswat Mohanty, Yifan Wang, Wei Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine-learned force fields (MLFFs) promise to offer a computationally
efficient alternative to ab initio simulations for complex molecular systems.
However, ensuring their generalizability beyond training data is crucial for
their wide application in studying solid materials. This work investigates the
ability of a graph neural network (GNN)-based MLFF, trained on Lennard-Jones
Argon, to describe solid-state phenomena not explicitly included during
training. We assess the MLFF's performance in predicting phonon density of
states (PDOS) for a perfect face-centered cubic (FCC) crystal structure at both
zero and finite temperatures. Additionally, we evaluate vacancy migration rates
and energy barriers in an imperfect crystal using direct molecular dynamics
(MD) simulations and the string method. Notably, vacancy configurations were
absent from the training data. Our results demonstrate the MLFF's capability to
capture essential solid-state properties with good agreement to reference data,
even for unseen configurations. We further discuss data engineering strategies
to enhance the generalizability of MLFFs. The proposed set of benchmark tests
and workflow for evaluating MLFF performance in describing perfect and
imperfect crystals pave the way for reliable application of MLFFs in studying
complex solid-state materials.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mining of Switching Sparse Networks for Missing Value Imputation in
  Multivariate Time Series <span class="chip">KDD 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kohei Obata, Koki Kawabata, Yasuko Matsubara, Yasushi Sakurai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate time series data suffer from the problem of missing values,
which hinders the application of many analytical methods. To achieve the
accurate imputation of these missing values, exploiting inter-correlation by
employing the relationships between sequences (i.e., a network) is as important
as the use of temporal dependency, since a sequence normally correlates with
other sequences. Moreover, exploiting an adequate network depending on time is
also necessary since the network varies over time. However, in real-world
scenarios, we normally know neither the network structure nor when the network
changes beforehand. Here, we propose a missing value imputation method for
multivariate time series, namely MissNet, that is designed to exploit temporal
dependency with a state-space model and inter-correlation by switching sparse
networks. The network encodes conditional independence between features, which
helps us understand the important relationships for imputation visually. Our
algorithm, which scales linearly with reference to the length of the data,
alternatively infers networks and fills in missing values using the networks
while discovering the switching of the networks. Extensive experiments
demonstrate that MissNet outperforms the state-of-the-art algorithms for
multivariate time series imputation and provides interpretable results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Step Embed to Control: A Novel Deep Learning-based Approach for
  Surrogate Modelling in Reservoir Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungang Chen, Eduardo Gildin, John Killough
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reduced-order models, also known as proxy model or surrogate model, are
approximate models that are less computational expensive as opposed to fully
descriptive models. With the integration of machine learning, these models have
garnered increasing research interests recently. However, many existing
reduced-order modeling methods, such as embed to control (E2C) and embed to
control and observe (E2CO), fall short in long-term predictions due to the
accumulation of prediction errors over time. This issue arises partly from the
one-step prediction framework inherent in E2C and E2CO architectures. This
paper introduces a deep learning-based surrogate model, referred as multi-step
embed-to-control model, for the construction of proxy models with improved
long-term prediction performance. Unlike E2C and E2CO, the proposed network
considers multiple forward transitions in the latent space at a time using
Koopman operator, allowing the model to incorporate a sequence of state
snapshots during training phrases. Additionally, the loss function of this
novel approach has been redesigned to accommodate these multiple transitions
and to respect the underlying physical principles. To validate the efficacy of
the proposed method, the developed framework was implemented within two-phase
(oil and water) reservoir model under a waterflooding scheme. Comparative
analysis demonstrate that the proposed model significantly outperforms the
conventional E2C model in long-term simulation scenarios. Notably, there was a
substantial reduction in temporal errors in the prediction of saturation
profiles and a decent improvement in pressure forecasting accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning large softmax mixtures with warm start EM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Bing, Florentina Bunea, Jonathan Niles-Weed, Marten Wegkamp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed multinomial logits are discrete mixtures introduced several decades ago
to model the probability of choosing an attribute from $p$ possible candidates,
in heterogeneous populations. The model has recently attracted attention in the
AI literature, under the name softmax mixtures, where it is routinely used in
the final layer of a neural network to map a large number $p$ of vectors in
$\mathbb{R}^L$ to a probability vector. Despite its wide applicability and
empirical success, statistically optimal estimators of the mixture parameters,
obtained via algorithms whose running time scales polynomially in $L$, are not
known. This paper provides a solution to this problem for contemporary
applications, such as large language models, in which the mixture has a large
number $p$ of support points, and the size $N$ of the sample observed from the
mixture is also large. Our proposed estimator combines two classical
estimators, obtained respectively via a method of moments (MoM) and the
expectation-minimization (EM) algorithm. Although both estimator types have
been studied, from a theoretical perspective, for Gaussian mixtures, no similar
results exist for softmax mixtures for either procedure. We develop a new MoM
parameter estimator based on latent moment estimation that is tailored to our
model, and provide the first theoretical analysis for a MoM-based procedure in
softmax mixtures. Although consistent, MoM for softmax mixtures can exhibit
poor numerical performance, as observed other mixture models. Nevertheless, as
MoM is provably in a neighborhood of the target, it can be used as warm start
for any iterative algorithm. We study in detail the EM algorithm, and provide
its first theoretical analysis for softmax mixtures. Our final proposal for
parameter estimation is the EM algorithm with a MoM warm start.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Monkeys: Scaling Inference Compute with Repeated Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, Azalia Mirhoseini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling the amount of compute used to train language models has dramatically
improved their capabilities. However, when it comes to inference, we often
limit the amount of compute to only one attempt per problem. Here, we explore
inference compute as another axis for scaling by increasing the number of
generated samples. Across multiple tasks and models, we observe that coverage -
the fraction of problems solved by any attempt - scales with the number of
samples over four orders of magnitude. In domains like coding and formal
proofs, where all answers can be automatically verified, these increases in
coverage directly translate into improved performance. When we apply repeated
sampling to SWE-bench Lite, the fraction of issues solved with
DeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250
samples, outperforming the single-attempt state-of-the-art of 43% which uses
more capable frontier models. Moreover, using current API pricing, amplifying
the cheaper DeepSeek model with five samples is more cost-effective and solves
more issues than paying a premium for one sample from GPT-4o or Claude 3.5
Sonnet. Interestingly, the relationship between coverage and the number of
samples is often log-linear and can be modelled with an exponentiated power
law, suggesting the existence of inference-time scaling laws. Finally, we find
that identifying correct samples out of many generations remains an important
direction for future research in domains without automatic verifiers. When
solving math word problems from GSM8K and MATH, coverage with Llama-3 models
grows to over 95% with 10,000 samples. However, common methods to pick correct
solutions from a sample collection, such as majority voting or reward models,
plateau beyond several hundred samples and fail to fully scale with the sample
budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assumption-Lean and Data-Adaptive Post-Prediction Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14220v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14220v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Miao, Xinran Miao, Yixuan Wu, Jiwei Zhao, Qiongshi Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A primary challenge facing modern scientific research is the limited
availability of gold-standard data which can be costly, labor-intensive, or
invasive to obtain. With the rapid development of machine learning (ML),
scientists can now employ ML algorithms to predict gold-standard outcomes with
variables that are easier to obtain. However, these predicted outcomes are
often used directly in subsequent statistical analyses, ignoring imprecision
and heterogeneity introduced by the prediction procedure. This will likely
result in false positive findings and invalid scientific conclusions. In this
work, we introduce PoSt-Prediction Adaptive inference (PSPA) that allows valid
and powerful inference based on ML-predicted data. Its "assumption-lean"
property guarantees reliable statistical inference without assumptions on the
ML prediction. Its "data-adaptive" feature guarantees an efficiency gain over
existing methods, regardless of the accuracy of ML prediction. We demonstrate
the statistical superiority and broad applicability of our method through
simulations and real-data applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model-independent variable selection via the rule-based variable
  priority 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09003v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09003v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Lu, Hemant Ishwaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While achieving high prediction accuracy is a fundamental goal in machine
learning, an equally important task is finding a small number of features with
high explanatory power. One popular selection technique is permutation
importance, which assesses a variable's impact by measuring the change in
prediction error after permuting the variable. However, this can be problematic
due to the need to create artificial data, a problem shared by other methods as
well. Another problem is that variable selection methods can be limited by
being model-specific. We introduce a new model-independent approach, Variable
Priority (VarPro), which works by utilizing rules without the need to generate
artificial data or evaluate prediction error. The method is relatively easy to
use, requiring only the calculation of sample averages of simple statistics,
and can be applied to many data settings, including regression, classification,
and survival. We investigate the asymptotic properties of VarPro and show,
among other things, that VarPro has a consistent filtering property for noise
variables. Empirical studies using synthetic and real-world data show the
method achieves a balanced performance and compares favorably to many
state-of-the-art procedures currently used for variable selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Ensemble Averages: Leveraging Climate Model Ensembles for
  Subseasonal Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15856v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15856v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena Orlova, Haokun Liu, Raphael Rossellini, Benjamin A. Cash, Rebecca Willett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Producing high-quality forecasts of key climate variables, such as
temperature and precipitation, on subseasonal time scales has long been a gap
in operational forecasting. This study explores an application of machine
learning (ML) models as post-processing tools for subseasonal forecasting.
Lagged numerical ensemble forecasts (i.e., an ensemble where the members have
different initialization dates) and observational data, including relative
humidity, pressure at sea level, and geopotential height, are incorporated into
various ML methods to predict monthly average precipitation and two-meter
temperature two weeks in advance for the continental United States. For
regression, quantile regression, and tercile classification tasks, we consider
using linear models, random forests, convolutional neural networks, and stacked
models (a multi-model approach based on the prediction of the individual ML
models). Unlike previous ML approaches that often use ensemble mean alone, we
leverage information embedded in the ensemble forecasts to enhance prediction
accuracy. Additionally, we investigate extreme event predictions that are
crucial for planning and mitigation efforts. Considering ensemble members as a
collection of spatial forecasts, we explore different approaches to using
spatial information. Trade-offs between different approaches may be mitigated
with model stacking. Our proposed models outperform standard baselines such as
climatological forecasts and ensemble means. In addition, we investigate
feature importance, trade-offs between using the full ensemble or only the
ensemble mean, and different modes of accounting for spatial variability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This Work has been accepted to Artificial Intelligence for the Earth
  Systems journal. The AMS does not guarantee that the copy provided here is an
  accurate copy of the Version of Record (VoR).
  https://doi.org/10.1175/AIES-D-23-0103.1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Distributed Algorithms for Size-Constrained Submodular
  Maximization in the MapReduce and Adaptive Complexity Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.09563v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.09563v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Chen, Tonmoy Dey, Alan Kuhnle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed maximization of a submodular function in the MapReduce (MR) model
has received much attention, culminating in two frameworks that allow a
centralized algorithm to be run in the MR setting without loss of
approximation, as long as the centralized algorithm satisfies a certain
consistency property -- which had previously only been known to be satisfied by
the standard greedy and continous greedy algorithms. A separate line of work
has studied parallelizability of submodular maximization in the adaptive
complexity model, where each thread may have access to the entire ground set.
For the size-constrained maximization of a monotone and submodular function, we
show that several sublinearly adaptive (highly parallelizable) algorithms
satisfy the consistency property required to work in the MR setting, which
yields practical, parallelizable and distributed algorithms. Separately, we
develop the first distributed algorithm with linear query complexity for this
problem. Finally, we provide a method to increase the maximum cardinality
constraint for MR algorithms at the cost of additional MR rounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Local Methods with Adaptivity via Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00846v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00846v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Savelii Chezhegov, Sergey Skorik, Nikolas Khachaturov, Danil Shalagin, Aram Avetisyan, Martin Takáč, Yaroslav Kholodov, Aleksandr Beznosikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of machine learning and deep learning has introduced
increasingly complex optimization challenges that must be addressed. Indeed,
training modern, advanced models has become difficult to implement without
leveraging multiple computing nodes in a distributed environment. Distributed
optimization is also fundamental to emerging fields such as federated learning.
Specifically, there is a need to organize the training process to minimize the
time lost due to communication. A widely used and extensively researched
technique to mitigate the communication bottleneck involves performing local
training before communication. This approach is the focus of our paper.
Concurrently, adaptive methods that incorporate scaling, notably led by Adam,
have gained significant popularity in recent years. Therefore, this paper aims
to merge the local training technique with the adaptive approach to develop
efficient distributed learning methods. We consider the classical Local SGD
method and enhance it with a scaling feature. A crucial aspect is that the
scaling is described generically, allowing us to analyze various approaches,
including Adam, RMSProp, and OASIS, in a unified manner. In addition to
theoretical analysis, we validate the performance of our methods in practice by
training a neural network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 2 algorithms, 6 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discrete Neural Algorithmic Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11628v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11628v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gleb Rodionov, Liudmila Prokhorenkova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural algorithmic reasoning aims to capture computations with neural
networks via learning the models to imitate the execution of classic
algorithms. While common architectures are expressive enough to contain the
correct model in the weights space, current neural reasoners are struggling to
generalize well on out-of-distribution data. On the other hand, classic
computations are not affected by distributional shifts as they can be described
as transitions between discrete computational states. In this work, we propose
to force neural reasoners to maintain the execution trajectory as a combination
of finite predefined states. To achieve that, we separate discrete and
continuous data flows and describe the interaction between them. Trained with
supervision on the algorithm's state transitions, such models are able to
perfectly align with the original algorithm. To show this, we evaluate our
approach on multiple algorithmic problems and get perfect test scores both in
single-task and multitask setups. Moreover, the proposed architectural choice
allows us to prove the correctness of the learned algorithms for any test~data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Human-Like Driving: Active Inference in Autonomous Vehicle
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elahe Delavari, John Moore, Junho Hong, Jaerock Kwon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to Autonomous Vehicle (AV) control
through the application of active inference, a theory derived from neuroscience
that conceptualizes the brain as a predictive machine. Traditional autonomous
driving systems rely heavily on Modular Pipelines, Imitation Learning, or
Reinforcement Learning, each with inherent limitations in adaptability,
generalization, and computational efficiency. Active inference addresses these
challenges by minimizing prediction error (termed "surprise") through a dynamic
model that balances perception and action. Our method integrates active
inference with deep learning to manage lateral control in AVs, enabling them to
perform lane following maneuvers within a simulated urban environment. We
demonstrate that our model, despite its simplicity, effectively learns and
generalizes from limited data without extensive retraining, significantly
reducing computational demands. The proposed approach not only enhances the
adaptability and performance of AVs in dynamic scenarios but also aligns
closely with human-like driving behavior, leveraging a generative model to
predict and adapt to environmental changes. Results from extensive experiments
in the CARLA simulator show promising outcomes, outperforming traditional
methods in terms of adaptability and efficiency, thereby advancing the
potential of active inference in real-world autonomous driving applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The work is partly supported by a sponsor. Authors need to complete
  the final report submission before any type of publication according to the
  sponsor. The final report will be submitted in few weeks. Then, authors will
  reinstate this paper after that</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Reinforcement Learning for Robotics: A <span class="highlight-title">Survey</span> of Real-World
  Successes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03539v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03539v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Tang, Ben Abbatematteo, Jiaheng Hu, Rohan Chandra, Roberto Martín-Martín, Peter Stone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL), particularly its combination with deep neural
networks referred to as deep RL (DRL), has shown tremendous promise across a
wide range of applications, suggesting its potential for enabling the
development of sophisticated robotic behaviors. Robotics problems, however,
pose fundamental difficulties for the application of RL, stemming from the
complexity and cost of interacting with the physical world. This article
provides a modern survey of DRL for robotics, with a particular focus on
evaluating the real-world successes achieved with DRL in realizing several key
robotic competencies. Our analysis aims to identify the key factors underlying
those exciting successes, reveal underexplored areas, and provide an overall
characterization of the status of DRL in robotics. We highlight several
important avenues for future work, emphasizing the need for stable and
sample-efficient real-world RL paradigms, holistic approaches for discovering
and integrating various competencies to tackle complex long-horizon, open-world
tasks, and principled development and evaluation procedures. This survey is
designed to offer insights for both RL practitioners and roboticists toward
harnessing RL's power to create generally capable real-world robotic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first three authors contributed equally. Accepted to Annual
  Review of Control, Robotics, and Autonomous Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigate the Gap: Investigating Approaches for Improving Cross-Modal
  Alignment in CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17639v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17639v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sedigheh Eslami, Gerard de Melo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language--Image Pre-training (CLIP) has manifested remarkable
improvements in zero-shot classification and cross-modal vision-language tasks.
Yet, from a geometrical point of view, the CLIP embedding space has been found
to have a pronounced modality gap. This gap renders the embedding space overly
sparse and disconnected, with different modalities being densely distributed in
distinct subregions of the hypersphere. In this work, we aim at answering three
main questions: 1. Does sharing the parameter space between the multi-modal
encoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart
the uni-modal embeddings via intra-modality separation? 3. How do these gap
reduction approaches affect the downstream performance? We design AlignCLIP, in
order to answer these questions and through extensive experiments, we show that
AlignCLIP achieves noticeable enhancements in the cross-modal alignment of the
embeddings, and thereby, reduces the modality gap, while improving the
performance across several zero-shot and fine-tuning downstream evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Reinforcement Learning for Autonomous Cyber Operations: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07745v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07745v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregory Palmer, Chris Parry, Daniel J. B. Harrold, Chris Willis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid increase in the number of cyber-attacks in recent years raises the
need for principled methods for defending networks against malicious actors.
Deep reinforcement learning (DRL) has emerged as a promising approach for
mitigating these attacks. However, while DRL has shown much potential for cyber
defence, numerous challenges must be overcome before DRL can be applied to
autonomous cyber operations (ACO) at scale. Principled methods are required for
environments that confront learners with very high-dimensional state spaces,
large multi-discrete action spaces, and adversarial learning. Recent works have
reported success in solving these problems individually. There have also been
impressive engineering efforts towards solving all three for real-time strategy
games. However, applying DRL to the full ACO problem remains an open challenge.
Here, we survey the relevant DRL literature and conceptualize an idealised
ACO-DRL agent. We provide: i.) A summary of the domain properties that define
the ACO problem; ii.) A comprehensive comparison of current ACO environments
used for benchmarking DRL approaches; iii.) An overview of state-of-the-art
approaches for scaling DRL to domains that confront learners with the curse of
dimensionality, and; iv.) A survey and critique of current methods for limiting
the exploitability of agents within adversarial settings from the perspective
of ACO. We conclude with open research questions that we hope will motivate
future directions for researchers and practitioners working on ACO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>89 pages, 14 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Supervised Performance on Speaker Verification with
  <span class="highlight-title">Self-Supervised</span> Learning by Leveraging Large-Scale ASR Models <span class="chip">INTERSPEECH 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02285v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02285v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Miara, Theo Lepage, Reda Dehak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Self-Supervised Learning (SSL) have shown promising
results in Speaker Verification (SV). However, narrowing the performance gap
with supervised systems remains an ongoing challenge. Several studies have
observed that speech representations from large-scale ASR models contain
valuable speaker information. This work explores the limitations of fine-tuning
these models for SV using an SSL contrastive objective in an end-to-end
approach. Then, we propose a framework to learn speaker representations in an
SSL context by fine-tuning a pre-trained WavLM with a supervised loss using
pseudo-labels. Initial pseudo-labels are derived from an SSL DINO-based model
and are iteratively refined by clustering the model embeddings. Our method
achieves 0.99% EER on VoxCeleb1-O, establishing the new state-of-the-art on
self-supervised SV. As this performance is close to our supervised baseline of
0.94% EER, this contribution is a step towards supervised performance on SV
with SSL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at INTERSPEECH 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Scalable and Parallelizable Digital Twin Framework for Sustainable
  Sim2Real Transition of Multi-Agent Reinforcement Learning Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Vilas Samak, Tanmay Vilas Samak, Venkat Krovi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent reinforcement learning (MARL) systems usually require
significantly long training times due to their inherent complexity.
Furthermore, deploying them in the real world demands a feature-rich
environment along with multiple embodied agents, which may not be feasible due
to budget or space limitations, not to mention energy consumption and safety
issues. This work tries to address these pain points by presenting a
sustainable digital twin framework capable of accelerating MARL training by
selectively scaling parallelized workloads on-demand, and transferring the
trained policies from simulation to reality using minimal hardware resources.
The applicability of the proposed digital twin framework is highlighted through
two representative use cases, which cover cooperative as well as competitive
classes of MARL problems. We study the effect of agent and environment
parallelization on training time and that of systematic domain randomization on
zero-shot sim2real transfer across both the case studies. Results indicate up
to 76.3% reduction in training time with the proposed parallelization scheme
and as low as 2.9% sim2real gap using the suggested deployment method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Against the RAG: Jamming Retrieval-Augmented Generation with
  Blocker Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avital Shafran, Roei Schuster, Vitaly Shmatikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) systems respond to queries by retrieving
relevant documents from a knowledge database, then generating an answer by
applying an LLM to the retrieved documents. We demonstrate that RAG systems
that operate on databases with untrusted content are vulnerable to a new class
of denial-of-service attacks we call jamming. An adversary can add a single
``blocker'' document to the database that will be retrieved in response to a
specific query and result in the RAG system not answering this query -
ostensibly because it lacks the information or because the answer is unsafe.
  We describe and measure the efficacy of several methods for generating
blocker documents, including a new method based on black-box optimization. This
method (1) does not rely on instruction injection, (2) does not require the
adversary to know the embedding or LLM used by the target RAG system, and (3)
does not use an auxiliary LLM to generate blocker documents.
  We evaluate jamming attacks on several LLMs and embeddings and demonstrate
that the existing safety metrics for LLMs do not capture their vulnerability to
jamming. We then discuss defenses against blocker documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Spatio-temporal Quantile Forecasting with Curriculum Learning:
  Lessons Learned 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Du Yin, Jinliang Deng, Shuang Ao, Zechen Li, Hao Xue, Arian Prabowo, Renhe Jiang, Xuan Song, Flora Salim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training models on spatio-temporal (ST) data poses an open problem due to the
complicated and diverse nature of the data itself, and it is challenging to
ensure the model's performance directly trained on the original ST data. While
limiting the variety of training data can make training easier, it can also
lead to a lack of knowledge and information for the model, resulting in a
decrease in performance. To address this challenge, we presented an innovative
paradigm that incorporates three separate forms of curriculum learning
specifically targeting from spatial, temporal, and quantile perspectives.
Furthermore, our framework incorporates a stacking fusion module to combine
diverse information from three types of curriculum learning, resulting in a
strong and thorough learning process. We demonstrated the effectiveness of this
framework with extensive empirical evaluations, highlighting its better
performance in addressing complex ST challenges. We provided thorough ablation
studies to investigate the effectiveness of our curriculum and to explain how
it contributes to the improvement of learning efficiency on ST data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accept by sigspatial 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Next Destination Prediction: A Novel Long Short-Term Memory
  Neural Network Approach Using Real-World Airline Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12830v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12830v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salih Salihoglu, Gulser Koksal, Orhan Abar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the modern transportation industry, accurate prediction of travelers' next
destinations brings multiple benefits to companies, such as customer
satisfaction and targeted marketing. This study focuses on developing a precise
model that captures the sequential patterns and dependencies in travel data,
enabling accurate predictions of individual travelers' future destinations. To
achieve this, a novel model architecture with a sliding window approach based
on Long Short-Term Memory (LSTM) is proposed for destination prediction in the
transportation industry. The experimental results highlight satisfactory
performance and high scores achieved by the proposed model across different
data sizes and performance metrics. This research contributes to advancing
destination prediction methods, empowering companies to deliver personalized
recommendations and optimize customer experiences in the dynamic travel
landscape.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic energy forecasting through quantile regression in
  reproducing kernel Hilbert spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04405v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04405v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Pernigo, Rohan Sen, Davide Baroli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate energy demand forecasting is crucial for sustainable and resilient
energy development. To meet the Net Zero Representative Concentration Pathways
(RCP) $4.5$ scenario in the DACH countries, increased renewable energy
production, energy storage, and reduced commercial building consumption are
needed. This scenario's success depends on hydroelectric capacity and climatic
factors. Informed decisions require quantifying uncertainty in forecasts. This
study explores a non-parametric method based on \emph{reproducing kernel
Hilbert spaces (RKHS)}, known as kernel quantile regression, for energy
prediction. Our experiments demonstrate its reliability and sharpness, and we
benchmark it against state-of-the-art methods in load and price forecasting for
the DACH region. We offer our implementation in conjunction with additional
scripts to ensure the reproducibility of our research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, {Owner/Author | ACM} {2024}. This is the author's version
  of the work. It is posted here for your personal use. Not for redistribution.
  The definitive Version of Record will published in https://energy.acm.org/eir</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Selection of Anomaly Detectors in the Absence of Labeled
  Validation Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10461v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10461v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clement Fung, Chen Qiu, Aodong Li, Maja Rudolph
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection is the task of identifying abnormal samples in large
unlabeled datasets. While the advent of foundation models has produced powerful
zero-shot anomaly detection methods, their deployment in practice is often
hindered by the absence of labeled validation data -- without it, their
detection performance cannot be evaluated reliably. In this work, we propose
SWSA (Selection With Synthetic Anomalies): a general-purpose framework to
select image-based anomaly detectors without labeled validation data. Instead
of collecting labeled validation data, we generate synthetic anomalies without
any training or fine-tuning, using only a small support set of normal images.
Our synthetic anomalies are used to create detection tasks that compose a
validation framework for model selection. In an empirical study, we evaluate
SWSA with three types of synthetic anomalies and on two selection tasks: model
selection of image-based anomaly detectors and prompt selection for CLIP-based
anomaly detection. SWSA often selects models and prompts that match selections
made with a ground-truth validation set, outperforming baseline selection
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Review</span>ing AI's Role in Non-Muscle-Invasive Bladder Cancer Recurrence
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saram Abbas, Rishad Shafik, Naeem Soomro, Rakesh Heer, Kabita Adhikari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Notorious for its 70-80% recurrence rate, Non-muscle-invasive Bladder Cancer
(NMIBC) imposes a significant human burden and is one of the costliest cancers
to manage. Current tools for predicting NMIBC recurrence rely on scoring
systems that often overestimate risk and have poor accuracy. This is where
Machine learning (ML)-based techniques have emerged as a promising approach for
predicting NMIBC recurrence by leveraging molecular and clinical data. This
comprehensive review paper critically analyses ML-based frameworks for
predicting NMIBC recurrence, focusing on their statistical robustness and
algorithmic efficacy. We meticulously examine the strengths and weaknesses of
each study, by focusing on various prediction tasks, data modalities, and ML
models, highlighting their remarkable performance alongside inherent
limitations. A diverse array of ML algorithms that leverage multimodal data
spanning radiomics, clinical, histopathological, and genomic data, exhibit
significant promise in accurately predicting NMIBC recurrence. However, the
path to widespread adoption faces challenges concerning the generalisability
and interpretability of models, emphasising the need for collaborative efforts,
robust datasets, and the incorporation of cost-effectiveness. Our detailed
categorisation and in-depth analysis illuminate the nuances, complexities, and
contexts that influence real-world advancement and adoption of these AI-based
techniques. This rigorous analysis equips researchers with a deeper
understanding of the intricacies of the ML algorithms employed. Researchers can
use these insights to refine approaches, address limitations, and boost
generalisability of their ML models, ultimately leading to reduced healthcare
costs and improved patient outcomes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parameterized Approximation for Robust Clustering in Discrete Geometric
  Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fateme Abbasi, Sandip Banerjee, Jarosław Byrka, Parinya Chalermsook, Ameet Gadekar, Kamyar Khodamoradi, Dániel Marx, Roohani Sharma, Joachim Spoerhase
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the well-studied Robust $(k, z)$-Clustering problem, which
generalizes the classic $k$-Median, $k$-Means, and $k$-Center problems. Given a
constant $z\ge 1$, the input to Robust $(k, z)$-Clustering is a set $P$ of $n$
weighted points in a metric space $(M,\delta)$ and a positive integer $k$.
Further, each point belongs to one (or more) of the $m$ many different groups
$S_1,S_2,\ldots,S_m$. Our goal is to find a set $X$ of $k$ centers such that
$\max_{i \in [m]} \sum_{p \in S_i} w(p) \delta(p,X)^z$ is minimized.
  This problem arises in the domains of robust optimization [Anthony, Goyal,
Gupta, Nagarajan, Math. Oper. Res. 2010] and in algorithmic fairness. For
polynomial time computation, an approximation factor of $O(\log m/\log\log m)$
is known [Makarychev, Vakilian, COLT $2021$], which is tight under a plausible
complexity assumption even in the line metrics. For FPT time, there is a
$(3^z+\epsilon)$-approximation algorithm, which is tight under GAP-ETH [Goyal,
Jaiswal, Inf. Proc. Letters, 2023].
  Motivated by the tight lower bounds for general discrete metrics, we focus on
\emph{geometric} spaces such as the (discrete) high-dimensional Euclidean
setting and metrics of low doubling dimension, which play an important role in
data analysis applications. First, for a universal constant $\eta_0 >0.0006$,
we devise a $3^z(1-\eta_{0})$-factor FPT approximation algorithm for discrete
high-dimensional Euclidean spaces thereby bypassing the lower bound for general
metrics. We complement this result by showing that even the special case of
$k$-Center in dimension $\Theta(\log n)$ is $(\sqrt{3/2}- o(1))$-hard to
approximate for FPT algorithms. Finally, we complete the FPT approximation
landscape by designing an FPT $(1+\epsilon)$-approximation scheme (EPAS) for
the metric of sub-logarithmic doubling dimension.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing learning in spiking neural networks through neuronal
  heterogeneity and neuromodulatory signaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Rodriguez-Garcia, Jie Mei, Srikanth Ramaswamy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in artificial intelligence (AI) has been driven by insights
from neuroscience, particularly with the development of artificial neural
networks (ANNs). This has significantly enhanced the replication of complex
cognitive tasks such as vision and natural language processing. Despite these
advances, ANNs struggle with continual learning, adaptable knowledge transfer,
robustness, and resource efficiency - capabilities that biological systems
handle seamlessly. Specifically, ANNs often overlook the functional and
morphological diversity of the brain, hindering their computational
capabilities. Furthermore, incorporating cell-type specific neuromodulatory
effects into ANNs with neuronal heterogeneity could enable learning at two
spatial scales: spiking behavior at the neuronal level, and synaptic plasticity
at the circuit level, thereby potentially enhancing their learning abilities.
In this article, we summarize recent bio-inspired models, learning rules and
architectures and propose a biologically-informed framework for enhancing ANNs.
Our proposed dual-framework approach highlights the potential of spiking neural
networks (SNNs) for emulating diverse spiking behaviors and dendritic
compartments to simulate morphological and functional diversity of neuronal
computations. Finally, we outline how the proposed approach integrates
brain-inspired compartmental models and task-driven SNNs, balances
bioinspiration and complexity, and provides scalable solutions for pressing AI
challenges, such as continual learning, adaptability, robustness, and
resource-efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 4 figures, 3 boxes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Causality in Domain Adaptation and Semi-Supervised Learning: an
  <span class="highlight-title">Information</span>-Theoretic Analysis for Parametric Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.04641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.04641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuetong Wu, Mingming Gong, Jonathan H. Manton, Uwe Aickelin, Jingge Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in unsupervised domain adaptation (UDA) and
semi-supervised learning (SSL), particularly incorporating causality, have led
to significant methodological improvements in these learning problems. However,
a formal theory that explains the role of causality in the generalization
performance of UDA/SSL is still lacking. In this paper, we consider the UDA/SSL
scenarios where we access $m$ labelled source data and $n$ unlabelled target
data as training instances under different causal settings with a parametric
probabilistic model. We study the learning performance (e.g., excess risk) of
prediction in the target domain from an information-theoretic perspective.
Specifically, we distinguish two scenarios: the learning problem is called
causal learning if the feature is the cause and the label is the effect, and is
called anti-causal learning otherwise. We show that in causal learning, the
excess risk depends on the size of the source sample at a rate of
$O(\frac{1}{m})$ only if the labelling distribution between the source and
target domains remains unchanged. In anti-causal learning, we show that the
unlabelled data dominate the performance at a rate of typically
$O(\frac{1}{n})$. These results bring out the relationship between the data
sample size and the hardness of the learning problem with different causal
mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>56 pages Including Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Maximum Mean Discrepancy on Exponential Windows for Online Change
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.12706v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.12706v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Kalinke, Marco Heyden, Georg Gntuni, Edouard Fouché, Klemens Böhm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting changes is of fundamental importance when analyzing data streams
and has many applications, e.g., in predictive maintenance, fraud detection, or
medicine. A principled approach to detect changes is to compare the
distributions of observations within the stream to each other via hypothesis
testing. Maximum mean discrepancy (MMD), a (semi-)metric on the space of
probability distributions, provides powerful non-parametric two-sample tests on
kernel-enriched domains. In particular, MMD is able to detect any disparity
between distributions under mild conditions. However, classical MMD estimators
suffer from a quadratic runtime complexity, which renders their direct use for
change detection in data streams impractical. In this article, we propose a new
change detection algorithm, called Maximum Mean Discrepancy on Exponential
Windows (MMDEW), that combines the benefits of MMD with an efficient
computation based on exponential windows. We prove that MMDEW enjoys
polylogarithmic runtime and logarithmic memory complexity and show empirically
that it outperforms the state of the art on benchmark data streams.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Add experiments regarding impact of test level</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning Machine and Human Visual Representations across Abstraction
  Levels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06509v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06509v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Muttenthaler, Klaus Greff, Frieda Born, Bernhard Spitzer, Simon Kornblith, Michael C. Mozer, Klaus-Robert Müller, Thomas Unterthiner, Andrew K. Lampinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have achieved success across a wide range of
applications, including as models of human behavior in vision tasks. However,
neural network training and human learning differ in fundamental ways, and
neural networks often fail to generalize as robustly as humans do, raising
questions regarding the similarity of their underlying representations. What is
missing for modern learning systems to exhibit more human-like behavior? We
highlight a key misalignment between vision models and humans: whereas human
conceptual knowledge is hierarchically organized from fine- to coarse-scale
distinctions, model representations do not accurately capture all these levels
of abstraction. To address this misalignment, we first train a teacher model to
imitate human judgments, then transfer human-like structure from its
representations into pretrained state-of-the-art vision foundation models.
These human-aligned models more accurately approximate human behavior and
uncertainty across a wide range of similarity tasks, including a new dataset of
human judgments spanning multiple levels of semantic abstractions. They also
perform better on a diverse set of machine learning tasks, increasing
generalization and out-of-distribution robustness. Thus, infusing neural
networks with additional human knowledge yields a best-of-both-worlds
representation that is both more consistent with human cognition and more
practically useful, thus paving the way toward more robust, interpretable, and
human-like artificial intelligence systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative neural networks for characteristic functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Brück
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide a simulation algorithm to simulate from a (multivariate)
characteristic function, which is only accessible in a black-box format. The
method is based on a generative neural network, whose loss function exploits a
specific representation of the Maximum-Mean-Discrepancy metric to directly
incorporate the targeted characteristic function. The algorithm is universal in
the sense that it is independent of the dimension and that it does not require
any assumptions on the given characteristic function. Furthermore, finite
sample guarantees on the approximation quality in terms of the Maximum-Mean
Discrepancy metric are derived. The method is illustrated in a simulation
study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Role of Deep Learning Regularizations on Actors in Offline RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Denis Tarasov, Anja Surina, Caglar Gulcehre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning regularization techniques, such as dropout, layer
normalization, or weight decay, are widely adopted in the construction of
modern artificial neural networks, often resulting in more robust training
processes and improved generalization capabilities. However, in the domain of
Reinforcement Learning (RL), the application of these techniques has been
limited, usually applied to value function estimators, and may result in
detrimental effects. This issue is even more pronounced in offline RL settings,
which bear greater similarity to supervised learning but have received less
attention. Recent work in continuous offline RL has demonstrated that while we
can build sufficiently powerful critic networks, the generalization of actor
networks remains a bottleneck. In this study, we empirically show that applying
standard regularization techniques to actor networks in offline RL actor-critic
algorithms yields improvements of 6% on average across two algorithms and three
different continuous D4RL domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/DT6A/ActoReg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoReEcho: Continuous Representation Learning for 2D+time
  Echocardiography Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fadillah Adamsyah Maani, Numan Saeed, Aleksandr Matsun, Mohammad Yaqub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) models have been advancing automatic medical image
analysis on various modalities, including echocardiography, by offering a
comprehensive end-to-end training pipeline. This approach enables DL models to
regress ejection fraction (EF) directly from 2D+time echocardiograms, resulting
in superior performance. However, the end-to-end training pipeline makes the
learned representations less explainable. The representations may also fail to
capture the continuous relation among echocardiogram clips, indicating the
existence of spurious correlations, which can negatively affect the
generalization. To mitigate this issue, we propose CoReEcho, a novel training
framework emphasizing continuous representations tailored for direct EF
regression. Our extensive experiments demonstrate that CoReEcho: 1) outperforms
the current state-of-the-art (SOTA) on the largest echocardiography dataset
(EchoNet-Dynamic) with MAE of 3.90 & R2 of 82.44, and 2) provides robust and
generalizable features that transfer more effectively in related downstream
tasks. The code is publicly available at https://github.com/fadamsyah/CoReEcho.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scikit-fingerprints: easy and efficient computation of molecular
  fingerprints in Python 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Adamczyk, Piotr Ludynia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present \skfp, a Python package for computation of molecular
fingerprints for applications in chemoinformatics. Our library offers an
industry-standard scikit-learn interface, allowing intuitive usage and easy
integration with machine learning pipelines. It is also highly optimized,
featuring parallel computation that enables efficient processing of large
molecular datasets. Currently, \skfp~stands as the most feature-rich library in
the open source Python ecosystem, offering over 30 molecular fingerprints. Our
library simplifies chemoinformatics tasks based on molecular fingerprints,
including molecular property prediction and virtual screening. It is also
flexible, highly efficient, and fully open source.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Learning in Biomedical Applications: A Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15189v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15189v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petr Ryšavý, Xiaoyu He, Jakub Mareček
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning causal relationships between a set of variables is a challenging
problem in computer science. Many existing artificial benchmark datasets are
based on sampling from causal models and thus contain residual information that
the ${R} ^2$-sortability can identify. Here, we present a benchmark for methods
in causal learning using time series. The presented dataset is not
${R}^2$-sortable and is based on a real-world scenario of the Krebs cycle that
is used in cells to release energy. We provide four scenarios of learning,
including short and long time series, and provide guidance so that testing is
unified between possible users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBMAP: Clustering-based manifold approximation and projection for
  dimensionality reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17940v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17940v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berat Dogan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimensionality reduction methods are employed to decrease data
dimensionality, either to enhance machine learning performance or to facilitate
data visualization in two or three-dimensional spaces. These methods typically
fall into two categories: feature selection and feature transformation. Feature
selection retains significant features, while feature transformation projects
data into a lower-dimensional space, with linear and nonlinear methods. While
nonlinear methods excel in preserving local structures and capturing nonlinear
relationships, they may struggle with interpreting global structures and can be
computationally intensive. Recent algorithms, such as the t-SNE, UMAP, TriMap,
and PaCMAP prioritize preserving local structures, often at the expense of
accurately representing global structures, leading to clusters being spread out
more in lower-dimensional spaces. Moreover, these methods heavily rely on
hyperparameters, making their results sensitive to parameter settings. To
address these limitations, this study introduces a clustering-based approach,
namely CBMAP (Clustering-Based Manifold Approximation and Projection), for
dimensionality reduction. CBMAP aims to preserve both global and local
structures, ensuring that clusters in lower-dimensional spaces closely resemble
those in high-dimensional spaces. Experimental evaluations on benchmark
datasets demonstrate CBMAP's efficacy, offering speed, scalability, and minimal
reliance on hyperparameters. Importantly, CBMAP enables low-dimensional
projection of test data, addressing a critical need in machine learning
applications. CBMAP is made freely available at
https://github.com/doganlab/cbmap and can be installed from the Python Package
Directory (PyPI) software repository with the command pip install cbmap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 10 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conformal Predictive Systems Under Covariate Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15018v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15018v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jef Jonkers, Glenn Van Wallendael, Luc Duchateau, Sofie Van Hoecke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformal Predictive Systems (CPS) offer a versatile framework for
constructing predictive distributions, allowing for calibrated inference and
informative decision-making. However, their applicability has been limited to
scenarios adhering to the Independent and Identically Distributed (IID) model
assumption. This paper extends CPS to accommodate scenarios characterized by
covariate shifts. We therefore propose Weighted CPS (WCPS), akin to Weighted
Conformal Prediction (WCP), leveraging likelihood ratios between training and
testing covariate distributions. This extension enables the construction of
nonparametric predictive distributions capable of handling covariate shifts. We
present theoretical underpinnings and conjectures regarding the validity and
efficacy of WCPS and demonstrate its utility through empirical evaluations on
both synthetic and real-world datasets. Our simulation experiments indicate
that WCPS are probabilistically calibrated under covariate shift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 13th Symposium on Conformal and Probabilistic
  Prediction with Applications (COPA), 9-11 September 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Dataset</span> of Pathloss and ToA Radio Maps With Localization Application 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.11777v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.11777v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Çağkan Yapar, Ron Levie, Gitta Kutyniok, Giuseppe Caire
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we present a collection of radio map datasets in dense urban
setting, which we generated and made publicly available. The datasets include
simulated pathloss/received signal strength (RSS) and time of arrival (ToA)
radio maps over a large collection of realistic dense urban setting in real
city maps. The two main applications of the presented dataset are 1) learning
methods that predict the pathloss from input city maps (namely, deep
learning-based simulations), and, 2) wireless localization. The fact that the
RSS and ToA maps are computed by the same simulations over the same city maps
allows for a fair comparison of the RSS and ToA-based localization methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Statistical Theory of Deep Learning: Approximation, Training
  Dynamics, and Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07187v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07187v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Namjoon Suh, Guang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we review the literature on statistical theories of neural
networks from three perspectives: approximation, training dynamics and
generative models. In the first part, results on excess risks for neural
networks are reviewed in the nonparametric framework of regression (and
classification in Appendix~{\color{blue}B}). These results rely on explicit
constructions of neural networks, leading to fast convergence rates of excess
risks. Nonetheless, their underlying analysis only applies to the global
minimizer in the highly non-convex landscape of deep neural networks. This
motivates us to review the training dynamics of neural networks in the second
part. Specifically, we review papers that attempt to answer ``how the neural
network trained via gradient-based methods finds the solution that can
generalize well on unseen data.'' In particular, two well-known paradigms are
reviewed: the Neural Tangent Kernel (NTK) paradigm, and Mean-Field (MF)
paradigm. Last but not least, we review the most recent theoretical
advancements in generative models including Generative Adversarial Networks
(GANs), diffusion models, and in-context learning (ICL) in the Large Language
Models (LLMs) from two perpsectives reviewed previously, i.e., approximation
and training dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 2 figures. Invited for review in Annual Review of
  Statistics and Its Application</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $Δ\text{-}{\rm OPE}$: Off-Policy Estimation with Pairs of Policies <span class="chip">RecSys '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Jeunen, Aleksei Ustimenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The off-policy paradigm casts recommendation as a counterfactual
decision-making task, allowing practitioners to unbiasedly estimate online
metrics using offline data. This leads to effective evaluation metrics, as well
as learning procedures that directly optimise online success. Nevertheless, the
high variance that comes with unbiasedness is typically the crux that
complicates practical applications. An important insight is that the difference
between policy values can often be estimated with significantly reduced
variance, if said policies have positive covariance. This allows us to
formulate a pairwise off-policy estimation task: $\Delta\text{-}{\rm OPE}$.
  $\Delta\text{-}{\rm OPE}$ subsumes the common use-case of estimating
improvements of a learnt policy over a production policy, using data collected
by a stochastic logging policy. We introduce $\Delta\text{-}{\rm OPE}$ methods
based on the widely used Inverse Propensity Scoring estimator and its
extensions. Moreover, we characterise a variance-optimal additive control
variate that further enhances efficiency. Simulated, offline, and online
experiments show that our methods significantly improve performance for both
evaluation and learning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a short paper in the 2024 ACM Conference on Recommender
  Systems (RecSys '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CataractBot: An <span class="highlight-title">LLM</span>-Powered Expert-in-the-Loop Chatbot for Cataract
  Patients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04620v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04620v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pragnya Ramjee, Bhuvan Sachdeva, Satvik Golechha, Shreyas Kulkarni, Geeta Fulari, Kaushik Murali, Mohit Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The healthcare landscape is evolving, with patients seeking reliable
information about their health conditions and available treatment options.
Despite the abundance of information sources, the digital age overwhelms
individuals with excess, often inaccurate information. Patients primarily trust
medical professionals, highlighting the need for expert-endorsed health
information. However, increased patient loads on experts has led to reduced
communication time, impacting information sharing. To address this gap, we
develop CataractBot, an experts-in-the-loop chatbot powered by LLMs, in
collaboration with an eye hospital in India. CataractBot answers cataract
surgery related questions instantly by querying a curated knowledge base, and
provides expert-verified responses asynchronously. It has multimodal and
multilingual capabilities. In an in-the-wild deployment study with 55
participants, CataractBot proved valuable, providing anytime accessibility,
saving time, accommodating diverse literacy levels, alleviating power
differences, and adding a privacy layer between patients and doctors. Users
reported that their trust in the system was established through expert
verification. Broadly, our results could inform future work on designing
expert-mediated LLM bots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Objective Recommendation via Multivariate Policy Learning <span class="chip">RecSys '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02141v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02141v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivier Jeunen, Jatin Mandav, Ivan Potapov, Nakul Agarwal, Sourabh Vaid, Wenzhe Shi, Aleksei Ustimenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world recommender systems often need to balance multiple objectives when
deciding which recommendations to present to users. These include behavioural
signals (e.g. clicks, shares, dwell time), as well as broader objectives (e.g.
diversity, fairness). Scalarisation methods are commonly used to handle this
balancing task, where a weighted average of per-objective reward signals
determines the final score used for ranking. Naturally, how these weights are
computed exactly, is key to success for any online platform. We frame this as a
decision-making task, where the scalarisation weights are actions taken to
maximise an overall North Star reward (e.g. long-term user retention or
growth). We extend existing policy learning methods to the continuous
multivariate action domain, proposing to maximise a pessimistic lower bound on
the North Star reward that the learnt policy will yield. Typical lower bounds
based on normal approximations suffer from insufficient coverage, and we
propose an efficient and effective policy-dependent correction for this. We
provide guidance to design stochastic data collection policies, as well as
highly sensitive reward signals. Empirical observations from simulations,
offline and online experiments highlight the efficacy of our deployed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a full paper in the 2024 ACM Conference on Recommender
  Systems (RecSys '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Thermodynamic Integration: Free Energies from Energy-based
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02313v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02313v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bálint Máté, François Fleuret, Tristan Bereau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thermodynamic integration (TI) offers a rigorous method for estimating
free-energy differences by integrating over a sequence of interpolating
conformational ensembles. However, TI calculations are computationally
expensive and typically limited to coupling a small number of degrees of
freedom due to the need to sample numerous intermediate ensembles with
sufficient conformational-space overlap. In this work, we propose to perform TI
along an alchemical pathway represented by a trainable neural network, which we
term Neural TI. Critically, we parametrize a time-dependent Hamiltonian
interpolating between the interacting and non-interacting systems, and optimize
its gradient using a score matching objective. The ability of the resulting
energy-based diffusion model to sample all intermediate ensembles allows us to
perform TI from a single reference calculation. We apply our method to
Lennard-Jones fluids, where we report accurate calculations of the excess
chemical potential, demonstrating that Neural TI reproduces the underlying
changes in free energy without the need for simulations at interpolating
Hamiltonians.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Spectral Graph Neural Networks with Spatially Adaptive
  Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.09071v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.09071v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingwei Guo, Kaizhu Huang, Xinping Yi, Zixian Su, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded
in the spectral domain, their practical reliance on polynomial approximation
implies a profound linkage to the spatial domain. As previous studies rarely
examine spectral GNNs from the spatial perspective, their spatial-domain
interpretability remains elusive, e.g., what information is essentially encoded
by spectral GNNs in the spatial domain? In this paper, to answer this question,
we establish a theoretical connection between spectral filtering and spatial
aggregation, unveiling an intrinsic interaction that spectral filtering
implicitly leads the original graph to an adapted new graph, explicitly
computed for spatial aggregation. Both theoretical and empirical investigations
reveal that the adapted new graph not only exhibits non-locality but also
accommodates signed edge weights to reflect label consistency among nodes.
These findings thus highlight the interpretable role of spectral GNNs in the
spatial domain and inspire us to rethink graph spectral filters beyond the
fixed-order polynomials, which neglect global information. Built upon the
theoretical findings, we revisit the state-of-the-art spectral GNNs and propose
a novel Spatially Adaptive Filtering (SAF) framework, which leverages the
adapted new graph by spectral filtering for an auxiliary non-local aggregation.
Notably, our proposed SAF comprehensively models both node similarity and
dissimilarity from a global perspective, therefore alleviating persistent
deficiencies of GNNs related to long-range dependencies and graph heterophily.
Extensive experiments over 13 node classification benchmarks demonstrate the
superiority of our proposed framework to the state-of-the-art models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating analytical variability in fMRI results with style transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elodie Germani, Camille Maumet, Elisa Fromont
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach to improve the reproducibility of neuroimaging
results by converting statistic maps across different functional MRI pipelines.
We make the assumption that pipelines used to compute fMRI statistic maps can
be considered as a style component and we propose to use different generative
models, among which, Generative Adversarial Networks (GAN) and Diffusion Models
(DM) to convert statistic maps across different pipelines. We explore the
performance of multiple GAN frameworks, and design a new DM framework for
unsupervised multi-domain styletransfer. We constrain the generation of 3D fMRI
statistic maps using the latent space of an auxiliary classifier that
distinguishes statistic maps from different pipelines and extend traditional
sampling techniques used in DM to improve the transition performance. Our
experiments demonstrate that our proposed methods aresuccessful: pipelines can
indeed be transferred as a style component, providing animportant source of
data augmentation for future medical studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Preserving Federated Learning with Consistency via Knowledge
  Distillation Using Conditional Generator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangyang Luo, Shuai Wang, Xiang Li, Yunshi Lan, Ming Gao, Jinlong Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is gaining popularity as a distributed learning
framework that only shares model parameters or gradient updates and keeps
private data locally. However, FL is at risk of privacy leakage caused by
privacy inference attacks. And most existing privacy-preserving mechanisms in
FL conflict with achieving high performance and efficiency. Therefore, we
propose FedMD-CG, a novel FL method with highly competitive performance and
high-level privacy preservation, which decouples each client's local model into
a feature extractor and a classifier, and utilizes a conditional generator
instead of the feature extractor to perform server-side model aggregation. To
ensure the consistency of local generators and classifiers, FedMD-CG leverages
knowledge distillation to train local models and generators at both the latent
feature level and the logit level. Also, we construct additional classification
losses and design new diversity losses to enhance client-side training.
FedMD-CG is robust to data heterogeneity and does not require training extra
discriminators (like cGAN). We conduct extensive experiments on various image
classification tasks to validate the superiority of FedMD-CG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DFDG: Data-Free Dual-Generator Adversarial Distillation for One-Shot
  Federated Learning <span class="chip">ICDM2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07734v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07734v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangyang Luo, Shuai Wang, Yexuan Fu, Renrong Shao, Xiang Li, Yunshi Lan, Ming Gao, Jinlong Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a distributed machine learning scheme in which
clients jointly participate in the collaborative training of a global model by
sharing model information rather than their private datasets. In light of
concerns associated with communication and privacy, one-shot FL with a single
communication round has emerged as a de facto promising solution. However,
existing one-shot FL methods either require public datasets, focus on model
homogeneous settings, or distill limited knowledge from local models, making it
difficult or even impractical to train a robust global model. To address these
limitations, we propose a new data-free dual-generator adversarial distillation
method (namely DFDG) for one-shot FL, which can explore a broader local models'
training space via training dual generators. DFDG is executed in an adversarial
manner and comprises two parts: dual-generator training and dual-model
distillation. In dual-generator training, we delve into each generator
concerning fidelity, transferability and diversity to ensure its utility, and
additionally tailor the cross-divergence loss to lessen the overlap of dual
generators' output spaces. In dual-model distillation, the trained dual
generators work together to provide the training data for updates of the global
model. At last, our extensive experiments on various image classification tasks
show that DFDG achieves significant performance gains in accuracy compared to
SOTA baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICDM2024 main conference (long paper). arXiv admin note:
  substantial text overlap with arXiv:2309.13546</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Central Answer Modeling for an Embodied Multi-<span class="highlight-title">LLM</span> System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10918v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10918v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhrij Patel, Vishnu Sashank Dorbala, Amrit Singh Bedi, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied Question Answering (EQA) is an important problem, which involves an
agent exploring the environment to answer user queries. In the existing
literature, EQA has exclusively been studied in single-agent scenarios, where
exploration can be time-consuming and costly. In this work, we consider EQA in
a multi-agent framework involving multiple large language models (LLM) based
agents independently answering queries about a household environment. To
generate one answer for each query, we use the individual responses to train a
Central Answer Model (CAM) that aggregates responses for a robust answer. While
prior Question Answering (QA) work has used a central module based on answers
from multiple LLM-based experts, we specifically look at applying this
framework to embodied LLM-based agents that must physically explore the
environment first to become experts on their given environment to answer
questions. Our work is the first to utilize a central answer model framework
with embodied agents that must rely on exploring an unknown environment. We set
up a variation of EQA where instead of the agents exploring the environment
after the question is asked, the agents first explore the environment for a set
amount of time and then answer a set of queries. Using CAM, we observe a $46\%$
higher EQA accuracy when compared against aggregation methods for ensemble LLM,
such as voting schemes and debates. CAM does not require any form of agent
communication, alleviating it from the associated costs. We ablate CAM with
various nonlinear (neural network, random forest, decision tree, XGBoost) and
linear (logistic regression classifier, SVM) algorithms. We experiment in
various topological graph environments and examine the case where one of the
agents is malicious and purposes contribute responses it believes to be wrong.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 Figures, 5 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Networks for Parkinsons Disease Detection <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07884v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07884v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shakeel A. Sheikh, Yacouba Kaloga, Md Sahidullah, Ina Kodrasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the promising performance of state of the art approaches for
Parkinsons Disease (PD) detection, these approaches often analyze individual
speech segments in isolation, which can lead to suboptimal results. Dysarthric
cues that characterize speech impairments from PD patients are expected to be
related across segments from different speakers. Isolated segment analysis
fails to exploit these inter segment relationships. Additionally, not all
speech segments from PD patients exhibit clear dysarthric symptoms, introducing
label noise that can negatively affect the performance and generalizability of
current approaches. To address these challenges, we propose a novel PD
detection framework utilizing Graph Convolutional Networks (GCNs). By
representing speech segments as nodes and capturing the similarity between
segments through edges, our GCN model facilitates the aggregation of dysarthric
cues across the graph, effectively exploiting segment relationships and
mitigating the impact of label noise. Experimental results demonstrate
theadvantages of the proposed GCN model for PD detection and provide insights
into its underlying mechanisms
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regret Analysis for Randomized Gaussian Process Upper Confidence Bound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00979v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00979v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shion Takeno, Yu Inatsu, Masayuki Karasuyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian process upper confidence bound (GP-UCB) is a theoretically
established algorithm for Bayesian optimization (BO), where we assume the
objective function $f$ follows GP. One notable drawback of GP-UCB is that the
theoretical confidence parameter $\beta$ increased along with the iterations is
too large. To alleviate this drawback, this paper analyzes the randomized
variant of GP-UCB called improved randomized GP-UCB (IRGP-UCB), which uses the
confidence parameter generated from the shifted exponential distribution. We
analyze the expected regret and conditional expected regret, where the
expectation and the probability are taken respectively with $f$ and noises and
with the randomness of the BO algorithm. In both regret analyses, IRGP-UCB
achieves a sub-linear regret upper bound without increasing the confidence
parameter if the input domain is finite. Finally, we show numerical experiments
using synthetic and benchmark functions and real-world emulators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:2302.01511</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CCE: Sample Efficient Sparse Reward Policy Learning for Robotic
  Navigation via Confidence-Controlled Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06192v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06192v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhrij Patel, Kasun Weerakoon, Wesley A. Suttle, Alec Koppel, Brian M. Sadler, Tianyi Zhou, Amrit Singh Bedi, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Confidence-Controlled Exploration (CCE), a novel exploration
scheme designed to enhance the training sample efficiency of reinforcement
learning (RL) algorithms for sparse reward settings such as robot navigation.
Sparse rewards are common in RL and convenient to design and implement, but
typically hard to deal with due to the challenges of exploration. Existing
methods deploy regularization-based methods to deal with the exploration
challenges. However, it is hard to characterize the balance between exploration
and exploitation because regularization modifies the reward function itself,
hence changing the objective we are optimizing for. In contrast to
regularization-based approaches in the existing literature, our approach, CCE,
is based on a novel relationship we provide between gradient estimation and
policy entropy. CCE dynamically adjusts the number of samples of the gradient
update used during training to control exploration. Interestingly, CCE can be
applied to both existing on-policy and off-policy RL methods, which we
demonstrate by empirically validating its efficacy on three popular RL methods:
REINFORCE, Proximal Policy Optimization (PPO), and Soft Actor-Critic (SAC) for
goal-reaching robotic navigation tasks. We demonstrate through simulated and
real-world experiments that CCE outperforms conventional methods that employ
constant trajectory lengths and entropy regularization when constraining the
sample budget. For a fixed sample budget, CCE achieves an 18\% increase in
navigation success rate, a 20-38\% reduction in navigation path length, and a
9.32\% decrease in elevation costs. Furthermore, we showcase the versatility of
CCE by integrating it with the Clearpath Husky robot, illustrating its
applicability in complex outdoor environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ V2X-VLM: End-to-End V2X Cooperative Autonomous Driving Through Large
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwei You, Haotian Shi, Zhuoyu Jiang, Zilin Huang, Rui Gan, Keshu Wu, Xi Cheng, Xiaopeng Li, Bin Ran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in autonomous driving have increasingly focused on end-to-end
(E2E) systems that manage the full spectrum of driving tasks, from
environmental perception to vehicle navigation and control. This paper
introduces V2X-VLM, an innovative E2E vehicle-infrastructure cooperative
autonomous driving (VICAD) framework with Vehicle-to-Everything (V2X) systems
and large vision-language models (VLMs). V2X-VLM is designed to enhance
situational awareness, decision-making, and ultimate trajectory planning by
integrating multimodel data from vehicle-mounted cameras, infrastructure
sensors, and textual information. The contrastive learning method is further
employed to complement VLM by refining feature discrimination, assisting the
model to learn robust representations of the driving environment. Evaluations
on the DAIR-V2X dataset show that V2X-VLM outperforms state-of-the-art
cooperative autonomous driving methods, while additional tests on corner cases
validate its robustness in real-world driving conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model (<span class="highlight-title">LLM</span>) for Telecommunications: A Comprehensive
  <span class="highlight-title">Survey</span> on Principles, Key Techniques, and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.10825v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.10825v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhou, Chengming Hu, Ye Yuan, Yufei Cui, Yili Jin, Can Chen, Haolun Wu, Dun Yuan, Li Jiang, Di Wu, Xue Liu, Charlie Zhang, Xianbin Wang, Jiangchuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have received considerable attention recently
due to their outstanding comprehension and reasoning capabilities, leading to
great progress in many fields. The advancement of LLM techniques also offers
promising opportunities to automate many tasks in the telecommunication
(telecom) field. After pre-training and fine-tuning, LLMs can perform diverse
downstream tasks based on human instructions, paving the way to artificial
general intelligence (AGI)-enabled 6G. Given the great potential of LLM
technologies, this work aims to provide a comprehensive overview of LLM-enabled
telecom networks. In particular, we first present LLM fundamentals, including
model architecture, pre-training, fine-tuning, inference and utilization, model
evaluation, and telecom deployment. Then, we introduce LLM-enabled key
techniques and telecom applications in terms of generation, classification,
optimization, and prediction problems. Specifically, the LLM-enabled generation
applications include telecom domain knowledge, code, and network configuration
generation. After that, the LLM-based classification applications involve
network security, text, image, and traffic classification problems. Moreover,
multiple LLM-enabled optimization techniques are introduced, such as automated
reward function design for reinforcement learning and verbal reinforcement
learning. Furthermore, for LLM-aided prediction problems, we discussed
time-series prediction models and multi-modality prediction problems for
telecom. Finally, we highlight the challenges and identify the future
directions of LLM-enabled telecom networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graphical Structural Learning of rs-fMRI data in Heavy Smokers <span class="chip">CCS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08395v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08395v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiru Gong, Qimin Zhang, Huili Zheng, Zheyan Liu, Shaohan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies revealed structural and functional brain changes in heavy
smokers. However, the specific changes in topological brain connections are not
well understood. We used Gaussian Undirected Graphs with the graphical lasso
algorithm on rs-fMRI data from smokers and non-smokers to identify significant
changes in brain connections. Our results indicate high stability in the
estimated graphs and identify several brain regions significantly affected by
smoking, providing valuable insights for future clinical research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE CCSB 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Trading Devil: Robust backdoor attack via Stochastic investment models
  and Bayesian approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10719v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10719v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orson Mengara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing use of voice-activated systems and speech recognition
technologies, the danger of backdoor attacks on audio data has grown
significantly. This research looks at a specific type of attack, known as a
Stochastic investment-based backdoor attack (MarketBack), in which adversaries
strategically manipulate the stylistic properties of audio to fool speech
recognition systems. The security and integrity of machine learning models are
seriously threatened by backdoor attacks, in order to maintain the reliability
of audio applications and systems, the identification of such attacks becomes
crucial in the context of audio data. Experimental results demonstrated that
MarketBack is feasible to achieve an average attack success rate close to 100%
in seven victim models when poisoning less than 1% of the training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>(Last update!, a constructive comment from arxiv led to this latest
  update ) Stochastic investment models and a Bayesian approach to better
  modeling of uncertainty : adversarial machine learning or Stochastic market.
  arXiv admin note: substantial text overlap with arXiv:2402.05967 (see this
  link to the paper by : Orson Mengara)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14608v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14608v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large models represent a groundbreaking advancement in multiple application
fields, enabling remarkable achievements across various tasks. However, their
unprecedented scale comes with significant computational costs. These models,
often consisting of billions of parameters, require vast amounts of
computational resources for execution. Especially, the expansive scale and
computational demands pose considerable challenges when customizing them for
particular downstream tasks, particularly over the hardware platforms
constrained by computational capabilities. Parameter Efficient Fine-Tuning
(PEFT) provides a practical solution by efficiently adjusting the large models
over the various downstream tasks. In particular, PEFT refers to the process of
adjusting the parameters of a pre-trained large model to adapt it to a specific
task or domain while minimizing the number of additional parameters introduced
or computational resources required. This approach is particularly important
when dealing with large-scale language models with high parameter counts, as
fine-tuning these models from scratch can be computationally expensive and
resource-intensive, posing considerable challenges in the supporting system
platform design. In this survey, we present comprehensive studies of various
PEFT algorithms, examining their performance and computational overhead.
Moreover, we provide an overview of applications developed using different PEFT
algorithms and discuss common techniques employed to mitigate computation costs
for PEFT. In addition to providing an extensive survey from an algorithmic
standpoint, we also examine various real-world system designs to investigate
the implementation costs associated with different PEFT approaches. This survey
serves as a valuable resource for researchers aiming to understand both the
PEFT algorithm and its system implementation, offering detailed ......
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 12 figures. Due to word limit, the abstract here is
  truncated. The full abstract is available in the PDF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Efficient Learning-Based Solver for Two-Stage DC Optimal Power Flow
  with Feasibility Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.01409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.01409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Zhang, Daniel Tabas, Baosen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the scenario-based two-stage stochastic DC optimal
power flow (OPF) problem for optimal and reliable dispatch when the load is
facing uncertainty. Although this problem is a linear program, it remains
computationally challenging to solve due to the large number of scenarios
needed to accurately represent the uncertainties. To mitigate the computational
issues, many techniques have been proposed to approximate the second-stage
decisions so they can be dealt more efficiently. The challenge of finding good
policies to approximate the second-stage decisions is that these solutions need
to be feasible, which has been difficult to achieve with existing policies.
  To address these challenges, this paper proposes a learning method to solve
the two-stage problem in a more efficient and optimal way. A technique called
the gauge map is incorporated into the learning architecture design to
guarantee the learned solutions' feasibility to the network constraints.
Namely, we can design policies that are feed forward functions and only output
feasible solutions. Simulation results on standard IEEE systems show that,
compared to iterative solvers and the widely used affine policy, our proposed
method not only learns solutions of good quality but also accelerates the
computation by orders of magnitude.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exact Recovery Guarantees for Parameterized Non-linear System
  Identification Problem under Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00276v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00276v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haixiang Zhang, Baturalp Yalcin, Javad Lavaei, Eduardo D. Sontag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study the system identification problem for parameterized
non-linear systems using basis functions under adversarial attacks. Motivated
by the LASSO-type estimators, we analyze the exact recovery property of a
non-smooth estimator, which is generated by solving an embedded $\ell_1$-loss
minimization problem. First, we derive necessary and sufficient conditions for
the well-specifiedness of the estimator and the uniqueness of global solutions
to the underlying optimization problem. Next, we provide exact recovery
guarantees for the estimator under two different scenarios of boundedness and
Lipschitz continuity of the basis functions. The non-asymptotic exact recovery
is guaranteed with high probability, even when there are more severely
corrupted data than clean data. Finally, we numerically illustrate the validity
of our theory. This is the first study on the sample complexity analysis of a
non-smooth estimator for the non-linear system identification problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing the Power of Federated Learning in Federated Contextual
  Bandits <span class="chip">NeurIPS
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengshuai Shi, Ruida Zhou, Kun Yang, Cong Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has demonstrated great potential in revolutionizing
distributed machine learning, and tremendous efforts have been made to extend
it beyond the original focus on supervised learning. Among many directions,
federated contextual bandits (FCB), a pivotal integration of FL and sequential
decision-making, has garnered significant attention in recent years. Despite
substantial progress, existing FCB approaches have largely employed their
tailored FL components, often deviating from the canonical FL framework.
Consequently, even renowned algorithms like FedAvg remain under-utilized in
FCB, let alone other FL advancements. Motivated by this disconnection, this
work takes one step towards building a tighter relationship between the
canonical FL study and the investigations on FCB. In particular, a novel FCB
design, termed FedIGW, is proposed to leverage a regression-based CB algorithm,
i.e., inverse gap weighting. Compared with existing FCB approaches, the
proposed FedIGW design can better harness the entire spectrum of FL
innovations, which is concretely reflected as (1) flexible incorporation of
(both existing and forthcoming) FL protocols; (2) modularized plug-in of FL
analyses in performance guarantees; (3) seamless integration of FL appendages
(such as personalization, robustness, and privacy). We substantiate these
claims through rigorous theoretical analyses and empirical evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Transactions on Machine Learning Research (07/2024); a
  preliminary version appeared in the Multi-Agent Security Workshop at NeurIPS
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span> Whisperer: An Inconspicuous Attack to Bias <span class="highlight-title">LLM</span> Responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiran Lin, Anna Gerchanovsky, Omer Akgul, Lujo Bauer, Matt Fredrikson, Zifan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Writing effective prompts for large language models (LLM) can be unintuitive
and burdensome. In response, services that optimize or suggest prompts have
emerged. While such services can reduce user effort, they also introduce a
risk: the prompt provider can subtly manipulate prompts to produce heavily
biased LLM responses. In this work, we show that subtle synonym replacements in
prompts can increase the likelihood (by a difference up to 78%) that LLMs
mention a target concept (e.g., a brand, political party, nation). We
substantiate our observations through a user study, showing our adversarially
perturbed prompts 1) are indistinguishable from unaltered prompts by humans, 2)
push LLMs to recommend target concepts more often, and 3) make users more
likely to notice target concepts, all without arousing suspicion. The
practicality of this attack has the potential to undermine user autonomy. Among
other measures, we recommend implementing warnings against using prompts from
untrusted parties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-Train</span>ing and Personalized Fine-Tuning via Over-the-Air Federated
  Meta-Learning: Convergence-Generalization Trade-Offs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11569v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11569v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haifeng Wen, Hong Xing, Osvaldo Simeone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For modern artificial intelligence (AI) applications such as large language
models (LLMs), the training paradigm has recently shifted to pre-training
followed by fine-tuning. Furthermore, owing to dwindling open repositories of
data and thanks to efforts to democratize access to AI models, pre-training is
expected to increasingly migrate from the current centralized deployments to
federated learning (FL) implementations. Meta-learning provides a general
framework in which pre-training and fine-tuning can be formalized.
Meta-learning-based personalized FL (meta-pFL) moves beyond basic
personalization by targeting generalization to new agents and tasks. This paper
studies the generalization performance of meta-pFL for a wireless setting in
which the agents participating in the pre-training phase, i.e., meta-learning,
are connected via a shared wireless channel to the server. Adopting
over-the-air computing, we study the trade-off between generalization to new
agents and tasks, on the one hand, and convergence, on the other hand. The
trade-off arises from the fact that channel impairments may enhance
generalization, while degrading convergence. Extensive numerical results
validate the theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 7 figures, submitted for possible journal publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Policy Learning for Balancing Short-Term and Long-Term Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.03329v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.03329v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wu, Ziyu Shen, Feng Xie, Zhongyao Wang, Chunchen Liu, Yan Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empirical researchers and decision-makers spanning various domains frequently
seek profound insights into the long-term impacts of interventions. While the
significance of long-term outcomes is undeniable, an overemphasis on them may
inadvertently overshadow short-term gains. Motivated by this, this paper
formalizes a new framework for learning the optimal policy that effectively
balances both long-term and short-term rewards, where some long-term outcomes
are allowed to be missing. In particular, we first present the identifiability
of both rewards under mild assumptions. Next, we deduce the semiparametric
efficiency bounds, along with the consistency and asymptotic normality of their
estimators. We also reveal that short-term outcomes, if associated, contribute
to improving the estimator of the long-term reward. Based on the proposed
estimators, we develop a principled policy learning approach and further derive
the convergence rates of regret and estimation errors associated with the
learned policy. Extensive experiments are conducted to validate the
effectiveness of the proposed method, demonstrating its practical
applicability.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamHead: Learning Spatial-Temporal Correspondence via Hierarchical
  Diffusion for Audio-driven Talking Head Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fa-Ting Hong, Yunfei Liu, Yu Li, Changyin Zhou, Fei Yu, Dan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven talking head synthesis strives to generate lifelike video
portraits from provided audio. The diffusion model, recognized for its superior
quality and robust generalization, has been explored for this task. However,
establishing a robust correspondence between temporal audio cues and
corresponding spatial facial expressions with diffusion models remains a
significant challenge in talking head generation. To bridge this gap, we
present DreamHead, a hierarchical diffusion framework that learns
spatial-temporal correspondences in talking head synthesis without compromising
the model's intrinsic quality and adaptability.~DreamHead learns to predict
dense facial landmarks from audios as intermediate signals to model the spatial
and temporal correspondences.~Specifically, a first hierarchy of
audio-to-landmark diffusion is first designed to predict temporally smooth and
accurate landmark sequences given audio sequence signals. Then, a second
hierarchy of landmark-to-image diffusion is further proposed to produce
spatially consistent facial portrait videos, by modeling spatial
correspondences between the dense facial landmark and appearance. Extensive
experiments show that proposed DreamHead can effectively learn spatial-temporal
consistency with the designed hierarchical diffusion and produce high-fidelity
audio-driven talking head videos for multiple identities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fit and Prune: Fast and Training-free Visual Token Pruning for
  Multi-modal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Ye, Qiong Wu, Wenhao Lin, Yiyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in Multimodal Large Language Models(MLLMs) often use large
image tokens to compensate the visual shortcoming of MLLMs, which not only
exhibits obvious redundancy but also greatly exacerbates the already high
computation. Token pruning is an effective solution for speeding up MLLMs, but
when and how to drop tokens still remains a challenge. In this paper, we
propose a novel and training-free approach for the effective visual token
pruning of MLLMs, termed FitPrune, which can quickly produce a complete pruning
recipe for MLLMs according to a pre-defined budget. Specifically, FitPrune
considers token pruning as a statistical problem of MLLM and its objective is
to find out an optimal pruning scheme that can minimize the divergence of the
attention distributions before and after pruning. In practice, FitPrune can be
quickly accomplished based on the attention statistics from a small batch of
inference data, avoiding the expensive trials of MLLMs. According to the
pruning recipe, an MLLM can directly remove the redundant visual tokens of
different examples during inference. To validate FitPrune, we apply it to a set
of recent MLLMs, including LLaVA-1.5, LLaVA-HR and LLaVA-NEXT, and conduct
extensive experiments on a set of benchmarks. The experimental results show
that our FitPrune can not only reduce the computational complexity to a large
extent, while retaining high performance, e.g., -54.9% FLOPs for LLaVA-NEXT
with only 0.5% accuracy drop. Notably, the pruning recipe can be obtained in
about 5 minutes. Our code is available at https://github.com/ywh187/FitPrune.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross: A Delay Based Congestion Control Method for RTP Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songyang Zhang, Changpeng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  After more than a decade of development, real time communication (RTC) for
video telephony has made significantly progress. However, emerging high-quality
RTC applications with high definition and high frame rate requires sufficient
bandwidth. The default congestion control mechanism specifically tuned for
video telephony leaves plenty of room for optimization under high-rate
scenarios. It is necessary to develop new rate control solutions to utilize
bandwidth efficiently and to provide better experience for such services. A
delay-based congestion control method called Cross is proposed, which regulates
rate based on queue load with a multiplicative increase and multiplicative
decrease fashion. A simulation module is developed to validate the
effectiveness of these congestion control algorithms for RTC services. The
module is released with the hope to provide convenience for RTC research
community. Simulation results demonstrate that Cross can achieve low queuing
delay and maintain high channel utilization under random loss environments.
Online deployment shows that Cross can reduce the video freezing ratio by up to
58.45\% on average when compared with a benchmark algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaQo: Towards a Query-Based Coach in Expressive Music Performance
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Zhang, Vincent Cheung, Hayato Nishioka, Simon Dixon, Shinichi Furuya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in music understanding has extensively explored composition-level
attributes such as key, genre, and instrumentation through advanced
representations, leading to cross-modal applications using large language
models. However, aspects of musical performance such as stylistic expression
and technique remain underexplored, along with the potential of using large
language models to enhance educational outcomes with customized feedback. To
bridge this gap, we introduce LLaQo, a Large Language Query-based music coach
that leverages audio language modeling to provide detailed and formative
assessments of music performances. We also introduce instruction-tuned
query-response datasets that cover a variety of performance dimensions from
pitch accuracy to articulation, as well as contextual performance understanding
(such as difficulty and performance techniques). Utilizing AudioMAE encoder and
Vicuna-7b LLM backend, our model achieved state-of-the-art (SOTA) results in
predicting teachers' performance ratings, as well as in identifying piece
difficulty and playing techniques. Textual responses from LLaQo was moreover
rated significantly higher compared to other baseline models in a user study
using audio-text matching. Our proposed model can thus provide informative
answers to open-ended questions related to musical performance from audio data.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-15T00:00:00Z">2024-09-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">43</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Acquiring Pronunciation Knowledge from Transcribed Speech Audio via
  Multi-task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Sun, Korin Richmond
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has shown the feasibility and benefit of bootstrapping an
integrated sequence-to-sequence (Seq2Seq) linguistic frontend from a
traditional pipeline-based frontend for text-to-speech (TTS). To overcome the
fixed lexical coverage of bootstrapping training data, previous work has
proposed to leverage easily accessible transcribed speech audio as an
additional training source for acquiring novel pronunciation knowledge for
uncovered words, which relies on an auxiliary ASR model as part of a cumbersome
implementation flow. In this work, we propose an alternative method to leverage
transcribed speech audio as an additional training source, based on multi-task
learning (MTL). Experiments show that, compared to a baseline Seq2Seq frontend,
the proposed MTL-based method reduces PER from 2.5% to 1.6% for those word
types covered exclusively in transcribed speech audio, achieving a similar
performance to the previous method but with a much simpler implementation flow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constructing a Singing Style Caption <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunjong Ok, Jaeho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singing voice synthesis and conversion have emerged as significant subdomains
of voice generation, leading to much demands on prompt-conditioned generation.
Unlike common voice data, generating a singing voice requires an understanding
of various associated vocal and musical characteristics, such as the vocal tone
of the singer or emotional expressions. However, existing open-source
audio-text datasets for voice generation tend to capture only a very limited
range of attributes, often missing musical characteristics of the audio. To
fill this gap, we introduce S2Cap, an audio-text pair dataset with a diverse
set of attributes. S2Cap consists of pairs of textual prompts and music audio
samples with a wide range of vocal and musical attributes, including pitch,
volume, tempo, mood, singer's gender and age, and musical genre and emotional
expression. Utilizing S2Cap, we suggest an effective novel baseline algorithm
for singing style captioning. Singing style captioning is a relative task to
voice generation that generates text descriptions of vocal characteristics,
which we first suggested. First, to mitigate the misalignment between the audio
encoder and the text decoder, we present a novel mechanism called CRESCENDO,
which utilizes positive-pair similarity learning to synchronize the embedding
spaces of a pretrained audio encoder to get similar embeddings with a text
encoder. We additionally supervise the model using the singer's voice, which is
demixed by the accompaniment. This supervision allows the model to more
accurately capture vocal characteristics, leading to improved singing style
captions that better reflect the style of the singer. The dataset and the codes
are available at \bulurl{https://github.com/HJ-Ok/S2cap}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Benchmark <span class="highlight-title">Dataset</span> with Larger Context for Non-Factoid Question
  Answering over Islamic Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faiza Qamar, Seemab Latif, Rabia Latif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accessing and comprehending religious texts, particularly the Quran (the
sacred scripture of Islam) and Ahadith (the corpus of the sayings or traditions
of the Prophet Muhammad), in today's digital era necessitates efficient and
accurate Question-Answering (QA) systems. Yet, the scarcity of QA systems
tailored specifically to the detailed nature of inquiries about the Quranic
Tafsir (explanation, interpretation, context of Quran for clarity) and Ahadith
poses significant challenges. To address this gap, we introduce a comprehensive
dataset meticulously crafted for QA purposes within the domain of Quranic
Tafsir and Ahadith. This dataset comprises a robust collection of over 73,000
question-answer pairs, standing as the largest reported dataset in this
specialized domain. Importantly, both questions and answers within the dataset
are meticulously enriched with contextual information, serving as invaluable
resources for training and evaluating tailored QA systems. However, while this
paper highlights the dataset's contributions and establishes a benchmark for
evaluating QA performance in the Quran and Ahadith domains, our subsequent
human evaluation uncovered critical insights regarding the limitations of
existing automatic evaluation techniques. The discrepancy between automatic
evaluation metrics, such as ROUGE scores, and human assessments became
apparent. The human evaluation indicated significant disparities: the model's
verdict consistency with expert scholars ranged between 11% to 20%, while its
contextual understanding spanned a broader spectrum of 50% to 90%. These
findings underscore the necessity for evaluation techniques that capture the
nuances and complexities inherent in understanding religious texts, surpassing
the limitations of traditional automatic metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GP-GPT: Large Language Model for Gene-Phenotype Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjun Lyu, Zihao Wu, Lu Zhang, Jing Zhang, Yiwei Li, Wei Ruan, Zhengliang Liu, Xiaowei Yu, Chao Cao, Tong Chen, Minheng Chen, Yan Zhuang, Xiang Li, Rongjie Liu, Chao Huang, Wentao Li, Tianming Liu, Dajiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained large language models(LLMs) have attracted increasing attention
in biomedical domains due to their success in natural language processing.
However, the complex traits and heterogeneity of multi-sources genomics data
pose significant challenges when adapting these models to the bioinformatics
and biomedical field. To address these challenges, we present GP-GPT, the first
specialized large language model for genetic-phenotype knowledge representation
and genomics relation analysis. Our model is fine-tuned in two stages on a
comprehensive corpus composed of over 3,000,000 terms in genomics, proteomics,
and medical genetics, derived from multiple large-scale validated datasets and
scientific publications. GP-GPT demonstrates proficiency in accurately
retrieving medical genetics information and performing common genomics analysis
tasks, such as genomics information retrieval and relationship determination.
Comparative experiments across domain-specific tasks reveal that GP-GPT
outperforms state-of-the-art LLMs, including Llama2, Llama3 and GPT-4. These
results highlight GP-GPT's potential to enhance genetic disease relation
research and facilitate accurate and efficient analysis in the fields of
genomics and medical genetics. Our investigation demonstrated the subtle
changes of bio-factor entities' representations in the GP-GPT, which suggested
the opportunities for the application of LLMs to advancing gene-phenotype
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Inference with Large Language Model: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal inference has been a pivotal challenge across diverse domains such as
medicine and economics, demanding a complicated integration of human knowledge,
mathematical reasoning, and data mining capabilities. Recent advancements in
natural language processing (NLP), particularly with the advent of large
language models (LLMs), have introduced promising opportunities for traditional
causal inference tasks. This paper reviews recent progress in applying LLMs to
causal inference, encompassing various tasks spanning different levels of
causation. We summarize the main causal problems and approaches, and present a
comparison of their evaluation results in different causal scenarios.
Furthermore, we discuss key findings and outline directions for future
research, underscoring the potential implications of integrating LLMs in
advancing causal inference methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning Paths with Reference Objects Elicit Quantitative Spatial
  Reasoning in Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances demonstrating vision-language models' (VLMs)
abilities to describe complex relationships in images using natural language,
their capability to quantitatively reason about object sizes and distances
remains underexplored. In this work, we introduce a manually annotated
benchmark, Q-Spatial Bench, with 271 questions across five categories designed
for quantitative spatial reasoning and systematically investigate the
performance of state-of-the-art VLMs on this task. Our analysis reveals that
reasoning about distances between objects is particularly challenging for SoTA
VLMs; however, some VLMs significantly outperform others, with an over 40-point
gap between the two best performing models. We also make the surprising
observation that the success rate of the top-performing VLM increases by 19
points when a reasoning path using a reference object emerges naturally in the
response. Inspired by this observation, we develop a zero-shot prompting
technique, SpatialPrompt, that encourages VLMs to answer quantitative spatial
questions using reference objects as visual cues. By instructing VLMs to use
reference objects in their reasoning paths via SpatialPrompt, Gemini 1.5 Pro,
Gemini 1.5 Flash, and GPT-4V improve their success rates by over 40, 20, and 30
points, respectively. We emphasize that these significant improvements are
obtained without needing more data, model architectural modifications, or
fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELMI: Interactive and Intelligent Sign Language Translation of Lyrics
  for Song Signing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suhyeon Yoo, Khai N. Truong, Young-Ho Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  d/Deaf and hearing song-signers become prevalent on video-sharing platforms,
but translating songs into sign language remains cumbersome and inaccessible.
Our formative study revealed the challenges song-signers face, including
semantic, syntactic, expressive, and rhythmic considerations in translations.
We present ELMI, an accessible song-signing tool that assists in translating
lyrics into sign language. ELMI enables users to edit glosses line-by-line,
with real-time synced lyric highlighting and music video snippets. Users can
also chat with a large language model-driven AI to discuss meaning, glossing,
emoting, and timing. Through an exploratory study with 13 song-signers, we
examined how ELMI facilitates their workflows and how song-signers leverage and
receive an LLM-driven chat for translation. Participants successfully adopted
ELMI to song-signing, with active discussions on the fly. They also reported
improved confidence and independence in their translations, finding ELMI
encouraging, constructive, and informative. We discuss design implications for
leveraging LLMs in culturally sensitive song-signing translations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages excluding reference and appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking <span class="highlight-title">LLM</span>s in Political Content Text-Annotation: Proof-of-Concept
  with Toxicity and Incivility Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bastián González-Bustamante
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article benchmarked the ability of OpenAI's GPTs and a number of
open-source LLMs to perform annotation tasks on political content. We used a
novel protest event dataset comprising more than three million digital
interactions and created a gold standard that includes ground-truth labels
annotated by human coders about toxicity and incivility on social media. We
included in our benchmark Google's Perspective algorithm, which, along with
GPTs, was employed throughout their respective APIs while the open-source LLMs
were deployed locally. The findings show that Perspective API using a laxer
threshold, GPT-4o, and Nous Hermes 2 Mixtral outperform other LLM's zero-shot
classification annotations. In addition, Nous Hermes 2 and Mistral OpenOrca,
with a smaller number of parameters, are able to perform the task with high
performance, being attractive options that could offer good trade-offs between
performance, implementing costs and computing time. Ancillary findings using
experiments setting different temperature levels show that although GPTs tend
to show not only excellent computing time but also overall good levels of
reliability, only open-source LLMs ensure full reproducibility in the
annotation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper prepared for delivery at the 8th Monash-Warwick-Zurich
  Text-as-Data Workshop, September 16-17, 2024: 11 pages, 3 tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PersonaMark: Personalized <span class="highlight-title">LLM</span> watermarking for model protection and user
  attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuehan Zhang, Peizhuo Lv, Yinpeng Liu, Yongqiang Ma, Wei Lu, Xiaofeng Wang, Xiaozhong Liu, Jiawei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of LLMs brings both convenience and potential threats.
As costumed and private LLMs are widely applied, model copyright protection has
become important. Text watermarking is emerging as a promising solution to
AI-generated text detection and model protection issues. However, current text
watermarks have largely ignored the critical need for injecting different
watermarks for different users, which could help attribute the watermark to a
specific individual. In this paper, we explore the personalized text
watermarking scheme for LLM copyright protection and other scenarios, ensuring
accountability and traceability in content generation. Specifically, we propose
a novel text watermarking method PersonaMark that utilizes sentence structure
as the hidden medium for the watermark information and optimizes the
sentence-level generation algorithm to minimize disruption to the model's
natural generation process. By employing a personalized hashing function to
inject unique watermark signals for different users, personalized watermarked
text can be obtained. Since our approach performs on sentence level instead of
token probability, the text quality is highly preserved. The injection process
of unique watermark signals for different users is time-efficient for a large
number of users with the designed multi-user hashing function. As far as we
know, we achieved personalized text watermarking for the first time through
this. We conduct an extensive evaluation of four different LLMs in terms of
perplexity, sentiment polarity, alignment, readability, etc. The results
demonstrate that our method maintains performance with minimal perturbation to
the model's behavior, allows for unbiased insertion of watermark information,
and exhibits strong watermark recognition capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Control With Human-Like Reasoning: Exploring Language Model
  Embodied Air Traffic Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justas Andriuškevičius, Junzi Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in language models have created new opportunities in air
traffic control studies. The current focus is primarily on text and
language-based use cases. However, these language models may offer a higher
potential impact in the air traffic control domain, thanks to their ability to
interact with air traffic environments in an embodied agent form. They also
provide a language-like reasoning capability to explain their decisions, which
has been a significant roadblock for the implementation of automatic air
traffic control.
  This paper investigates the application of a language model-based agent with
function-calling and learning capabilities to resolve air traffic conflicts
without human intervention. The main components of this research are
foundational large language models, tools that allow the agent to interact with
the simulator, and a new concept, the experience library. An innovative part of
this research, the experience library, is a vector database that stores
synthesized knowledge that agents have learned from interactions with the
simulations and language models.
  To evaluate the performance of our language model-based agent, both
open-source and closed-source models were tested. The results of our study
reveal significant differences in performance across various configurations of
the language model-based agents. The best-performing configuration was able to
solve almost all 120 but one imminent conflict scenarios, including up to four
aircraft at the same time. Most importantly, the agents are able to provide
human-level text explanations on traffic situations and conflict resolution
strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlpaPICO: <span class="highlight-title">Extraction</span> of PICO Frames from Clinical Trial Documents Using
  <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madhusudan Ghosh, Shrimon Mukherjee, Asmit Ganguly, Partha Basuchowdhuri, Sudip Kumar Naskar, Debasis Ganguly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been a surge in the publication of clinical trial
reports, making it challenging to conduct systematic reviews. Automatically
extracting Population, Intervention, Comparator, and Outcome (PICO) from
clinical trial studies can alleviate the traditionally time-consuming process
of manually scrutinizing systematic reviews. Existing approaches of PICO frame
extraction involves supervised approach that relies on the existence of
manually annotated data points in the form of BIO label tagging. Recent
approaches, such as In-Context Learning (ICL), which has been shown to be
effective for a number of downstream NLP tasks, require the use of labeled
examples. In this work, we adopt ICL strategy by employing the pretrained
knowledge of Large Language Models (LLMs), gathered during the pretraining
phase of an LLM, to automatically extract the PICO-related terminologies from
clinical trial documents in unsupervised set up to bypass the availability of
large number of annotated data instances. Additionally, to showcase the highest
effectiveness of LLM in oracle scenario where large number of annotated samples
are available, we adopt the instruction tuning strategy by employing Low Rank
Adaptation (LORA) to conduct the training of gigantic model in low resource
environment for the PICO frame extraction task. Our empirical results show that
our proposed ICL-based framework produces comparable results on all the version
of EBM-NLP datasets and the proposed instruction tuned version of our framework
produces state-of-the-art results on all the different EBM-NLP datasets. Our
project is available at \url{https://github.com/shrimonmuke0202/AlpaPICO.git}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Methods</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Open-Source Large Language Models for Native Language
  Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yee Man Ng, Ilia Markov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Native Language Identification (NLI) - the task of identifying the native
language (L1) of a person based on their writing in the second language (L2) -
has applications in forensics, marketing, and second language acquisition.
Historically, conventional machine learning approaches that heavily rely on
extensive feature engineering have outperformed transformer-based language
models on this task. Recently, closed-source generative large language models
(LLMs), e.g., GPT-4, have demonstrated remarkable performance on NLI in a
zero-shot setting, including promising results in open-set classification.
However, closed-source LLMs have many disadvantages, such as high costs and
undisclosed nature of training data. This study explores the potential of using
open-source LLMs for NLI. Our results indicate that open-source LLMs do not
reach the accuracy levels of closed-source LLMs when used out-of-the-box.
However, when fine-tuned on labeled training data, open-source LLMs can achieve
performance comparable to that of commercial LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Gender Bias in Large Language Models: Using Teacher's
  Evaluation in Higher Education As an Example 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanning Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates gender bias in Large Language Model (LLM)-generated
teacher evaluations in higher education setting, focusing on evaluations
produced by GPT-4 across six academic subjects. By applying a comprehensive
analytical framework that includes Odds Ratio (OR) analysis, Word Embedding
Association Test (WEAT), sentiment analysis, and contextual analysis, this
paper identified patterns of gender-associated language reflecting societal
stereotypes. Specifically, words related to approachability and support were
used more frequently for female instructors, while words related to
entertainment were predominantly used for male instructors, aligning with the
concepts of communal and agentic behaviors. The study also found moderate to
strong associations between male salient adjectives and male names, though
career and family words did not distinctly capture gender biases. These
findings align with prior research on societal norms and stereotypes,
reinforcing the notion that LLM-generated text reflects existing biases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple HMM with <span class="highlight-title">Self-Supervised</span> Representations for Phone Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gene-Ping Yang, Hao Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent advance in self-supervised representations, unsupervised
phonetic segmentation remains challenging. Most approaches focus on improving
phonetic representations with self-supervised learning, with the hope that the
improvement can transfer to phonetic segmentation. In this paper, contrary to
recent approaches, we show that peak detection on Mel spectrograms is a strong
baseline, better than many self-supervised approaches. Based on this finding,
we propose a simple hidden Markov model that uses self-supervised
representations and features at the boundaries for phone segmentation. Our
results demonstrate consistent improvements over previous approaches, with a
generalized formulation allowing versatile design adaptations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SLT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards understanding evolution of science through language model series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Dong, Zhuoqi Lyu, Qing Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce AnnualBERT, a series of language models designed specifically to
capture the temporal evolution of scientific text. Deviating from the
prevailing paradigms of subword tokenizations and "one model to rule them all",
AnnualBERT adopts whole words as tokens and is composed of a base RoBERTa model
pretrained from scratch on the full-text of 1.7 million arXiv papers published
until 2008 and a collection of progressively trained models on arXiv papers at
an annual basis. We demonstrate the effectiveness of AnnualBERT models by
showing that they not only have comparable performances in standard tasks but
also achieve state-of-the-art performances on domain-specific NLP tasks as well
as link prediction tasks in the arXiv citation network. We then utilize probing
tasks to quantify the models' behavior in terms of representation learning and
forgetting as time progresses. Our approach enables the pretrained models to
not only improve performances on scientific text processing tasks but also to
provide insights into the development of scientific discourse over time. The
series of the models is available at https://huggingface.co/jd445/AnnualBERTs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confidence Estimation for <span class="highlight-title">LLM</span>-Based Dialogue State Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Jyun Sun, Suvodip Dey, Dilek Hakkani-Tur, Gokhan Tur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimation of a model's confidence on its outputs is critical for
Conversational AI systems based on large language models (LLMs), especially for
reducing hallucination and preventing over-reliance. In this work, we provide
an exhaustive exploration of methods, including approaches proposed for open-
and closed-weight LLMs, aimed at quantifying and leveraging model uncertainty
to improve the reliability of LLM-generated responses, specifically focusing on
dialogue state tracking (DST) in task-oriented dialogue systems (TODS).
Regardless of the model type, well-calibrated confidence scores are essential
to handle uncertainties, thereby improving model performance. We evaluate four
methods for estimating confidence scores based on softmax, raw token scores,
verbalized confidences, and a combination of these methods, using the area
under the curve (AUC) metric to assess calibration, with higher AUC indicating
better calibration. We also enhance these with a self-probing mechanism,
proposed for closed models. Furthermore, we assess these methods using an
open-weight model fine-tuned for the task of DST, achieving superior joint goal
accuracy (JGA). Our findings also suggest that fine-tuning open-weight LLMs can
result in enhanced AUC performance, indicating better confidence score
calibration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Text Annotation through Rationale-Driven Collaborative
  Few-Shot <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfei Wu, Xubin Wang, Weijia Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The traditional data annotation process is often labor-intensive,
time-consuming, and susceptible to human bias, which complicates the management
of increasingly complex datasets. This study explores the potential of large
language models (LLMs) as automated data annotators to improve efficiency and
consistency in annotation tasks. By employing rationale-driven collaborative
few-shot prompting techniques, we aim to improve the performance of LLMs in
text annotation. We conduct a rigorous evaluation of six LLMs across four
benchmark datasets, comparing seven distinct methodologies. Our results
demonstrate that collaborative methods consistently outperform traditional
few-shot techniques and other baseline approaches, particularly in complex
annotation tasks. Our work provides valuable insights and a robust framework
for leveraging collaborative learning methods to tackle challenging text
annotation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking KenLM: Good and Bad Model Ensembles for Efficient Text
  Quality Filtering in Large Web Corpora 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yungi Kim, Hyunsoo Ha, Sukyung Lee, Jihoo Kim, Seonghoon Yang, Chanjun Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing demand for substantial amounts of high-quality data to
train large language models (LLMs), efficiently filtering large web corpora has
become a critical challenge. For this purpose, KenLM, a lightweight
n-gram-based language model that operates on CPUs, is widely used. However, the
traditional method of training KenLM utilizes only high-quality data and,
consequently, does not explicitly learn the linguistic patterns of low-quality
data. To address this issue, we propose an ensemble approach that leverages two
contrasting KenLMs: (i) Good KenLM, trained on high-quality data; and (ii) Bad
KenLM, trained on low-quality data. Experimental results demonstrate that our
approach significantly reduces noisy content while preserving high-quality
content compared to the traditional KenLM training method. This indicates that
our method can be a practical solution with minimal computational overhead for
resource-constrained environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Data-Centric RLHF: Simple Metrics for Preference <span class="highlight-title">Dataset</span>
  Comparison 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Judy Hanwen Shen, Archit Sharma, Jun Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of aligning language models to human preferences requires data that
reveal these preferences. Ideally, time and money can be spent carefully
collecting and tailoring bespoke preference data to each downstream
application. However, in practice, a select few publicly available preference
datasets are often used to train reward models for reinforcement learning from
human feedback (RLHF). While new preference datasets are being introduced with
increasing frequency, there are currently no existing efforts to measure and
compare these datasets. In this paper, we systematically study preference
datasets through three perspectives: scale, label noise, and information
content. We propose specific metrics for each of these perspectives and uncover
different axes of comparison for a better understanding of preference datasets.
Our work is a first step towards a data-centric approach to alignment by
providing perspectives that aid in training efficiency and iterative data
collection for RLHF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Statistical Significance in Human Evaluation of Automatic
  Metrics via Soft Pairwise Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Thompson, Nitika Mathur, Daniel Deutsch, Huda Khayrallah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selecting an automatic metric that best emulates human judgments is often
non-trivial, because there is no clear definition of "best emulates." A
meta-metric is required to compare the human judgments to the automatic metric
judgments, and metric rankings depend on the choice of meta-metric. We propose
Soft Pairwise Accuracy (SPA), a new meta-metric that builds on Pairwise
Accuracy (PA) but incorporates the statistical significance of both the human
judgments and the metric judgments. SPA allows for more fine-grained
comparisons between systems than a simplistic binary win/loss, and addresses a
number of shortcomings with PA: it is more stable with respect to both the
number of systems and segments used for evaluation, it mitigates the issue of
metric ties due to quantization, and it produces more statistically significant
results. SPA was selected as the official system-level metric for the 2024 WMT
metric shared task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ValueCompass: A Framework of Fundamental Values for Human-AI Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hua Shen, Tiffany Knearem, Reshmi Ghosh, Yu-Ju Yang, Tanushree Mitra, Yun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AI systems become more advanced, ensuring their alignment with a diverse
range of individuals and societal values becomes increasingly critical. But how
can we capture fundamental human values and assess the degree to which AI
systems align with them? We introduce ValueCompass, a framework of fundamental
values, grounded in psychological theory and a systematic review, to identify
and evaluate human-AI alignment. We apply ValueCompass to measure the value
alignment of humans and language models (LMs) across four real-world vignettes:
collaborative writing, education, public sectors, and healthcare. Our findings
uncover risky misalignment between humans and LMs, such as LMs agreeing with
values like "Choose Own Goals", which are largely disagreed by humans. We also
observe values vary across vignettes, underscoring the necessity for
context-aware AI alignment strategies. This work provides insights into the
design space of human-AI alignment, offering foundations for developing AI that
responsibly reflects societal values and ethics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for
  Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyao Li, Wei Xia, Kounianhua Du, Xinyi Dai, Ruiming Tang, Yasheng Wang, Yong Yu, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM agents enhanced by tree search algorithms have yielded notable
performances in code generation. However, current search algorithms in this
domain suffer from low search quality due to several reasons: 1) Ineffective
design of the search space for the high-reasoning demands of code generation
tasks, 2) Inadequate integration of code feedback with the search algorithm,
and 3) Poor handling of negative feedback during the search, leading to reduced
search efficiency and quality. To address these challenges, we propose to
search for the reasoning process of the code and use the detailed feedback of
code execution to refine erroneous thoughts during the search. In this paper,
we introduce RethinkMCTS, which employs the Monte Carlo Tree Search (MCTS)
algorithm to conduct thought-level searches before generating code, thereby
exploring a wider range of strategies. More importantly, we construct verbal
feedback from fine-grained code execution feedback to refine erroneous thoughts
during the search. This ensures that the search progresses along the correct
reasoning paths, thus improving the overall search quality of the tree by
leveraging execution feedback. Through extensive experiments, we demonstrate
that RethinkMCTS outperforms previous search-based and feedback-based code
generation baselines. On the HumanEval dataset, it improves the pass@1 of
GPT-3.5-turbo from 70.12 to 89.02 and GPT-4o-mini from 87.20 to 94.51. It
effectively conducts more thorough exploration through thought-level searches
and enhances the search quality of the entire tree by incorporating rethink
operation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thesis proposal: Are We Losing Textual Diversity to Natural Language
  Processing? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josef Jon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis argues that the currently widely used Natural Language Processing
algorithms possibly have various limitations related to the properties of the
texts they handle and produce. With the wide adoption of these tools in rapid
progress, we must ask what these limitations are and what are the possible
implications of integrating such tools even more deeply into our daily lives.
  As a testbed, we have chosen the task of Neural Machine Translation (NMT).
Nevertheless, we aim for general insights and outcomes, applicable even to
current Large Language Models (LLMs). We ask whether the algorithms used in NMT
have inherent inductive biases that are beneficial for most types of inputs but
might harm the processing of untypical texts. To explore this hypothesis, we
define a set of measures to quantify text diversity based on its statistical
properties, like uniformity or rhythmicity of word-level surprisal, on multiple
scales (sentence, discourse, language). We then conduct a series of experiments
to investigate whether NMT systems struggle with maintaining the diversity of
such texts, potentially reducing the richness of the language generated by
these systems, compared to human translators.
  We search for potential causes of these limitations rooted in training
objectives and decoding algorithms. Our ultimate goal is to develop
alternatives that do not enforce uniformity in the distribution of statistical
properties in the output and that allow for better global planning of the
translation, taking into account the intrinsic ambiguity of the translation
task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kernelized Concept Erasure <span class="chip">EMNLP22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12191v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12191v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shauli Ravfogel, Francisco Vargas, Yoav Goldberg, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The representation space of neural models for textual data emerges in an
unsupervised manner during training. Understanding how those representations
encode human-interpretable concepts is a fundamental problem. One prominent
approach for the identification of concepts in neural representations is
searching for a linear subspace whose erasure prevents the prediction of the
concept from the representations. However, while many linear erasure algorithms
are tractable and interpretable, neural networks do not necessarily represent
concepts in a linear manner. To identify non-linearly encoded concepts, we
propose a kernelization of a linear minimax game for concept erasure. We
demonstrate that it is possible to prevent specific non-linear adversaries from
predicting the concept. However, the protection does not transfer to different
nonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded
concept remains an open problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a long paper in EMNLP22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-Shot Safety Alignment for Large Language Models via Optimal
  Dualization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinmeng Huang, Shuo Li, Edgar Dobriban, Osbert Bastani, Hamed Hassani, Dongsheng Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing safety concerns surrounding Large Language Models (LLMs) raise an
urgent need to align them with diverse human preferences to simultaneously
enhance their helpfulness and safety. A promising approach is to enforce safety
constraints through Reinforcement Learning from Human Feedback (RLHF). For such
constrained RLHF, common Lagrangian-based primal-dual policy optimization
methods are computationally expensive and often unstable. This paper presents a
dualization perspective that reduces constrained alignment to an equivalent
unconstrained alignment problem. We do so by pre-optimizing a smooth and convex
dual function that has a closed form. This shortcut eliminates the need for
cumbersome primal-dual policy iterations, thus greatly reducing the
computational burden and improving training stability. Our strategy leads to
two practical algorithms in model-based and preference-based scenarios (MoCAN
and PeCAN, respectively). A broad range of experiments demonstrate the
effectiveness of our methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-tuning <span class="highlight-title">Pre-train</span>ed Language Models for Few-shot Intent Detection:
  Supervised <span class="highlight-title">Pre-train</span>ing and Isotropization <span class="chip">NAACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.07208v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.07208v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haode Zhang, Haowen Liang, Yuwei Zhang, Liming Zhan, Xiaolei Lu, Albert Y. S. Lam, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is challenging to train a good intent classifier for a task-oriented
dialogue system with only a few annotations. Recent studies have shown that
fine-tuning pre-trained language models with a small amount of labeled
utterances from public benchmarks in a supervised manner is extremely helpful.
However, we find that supervised pre-training yields an anisotropic feature
space, which may suppress the expressive power of the semantic representations.
Inspired by recent research in isotropization, we propose to improve supervised
pre-training by regularizing the feature space towards isotropy. We propose two
regularizers based on contrastive learning and correlation matrix respectively,
and demonstrate their effectiveness through extensive experiments. Our main
finding is that it is promising to regularize supervised pre-training with
isotropization to further improve the performance of few-shot intent detection.
The source code can be found at https://github.com/fanolabs/isoIntentBert-main.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2022, oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs.
  Continual <span class="highlight-title">Pre-train</span>ing <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05278v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05278v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haode Zhang, Haowen Liang, Liming Zhan, Albert Y. S. Lam, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the task of few-shot intent detection, which involves training a
deep learning model to classify utterances based on their underlying intents
using only a small amount of labeled data. The current approach to address this
problem is through continual pre-training, i.e., fine-tuning pre-trained
language models (PLMs) on external resources (e.g., conversational corpora,
public intent detection datasets, or natural language understanding datasets)
before using them as utterance encoders for training an intent classifier. In
this paper, we show that continual pre-training may not be essential, since the
overfitting problem of PLMs on this task may not be as serious as expected.
Specifically, we find that directly fine-tuning PLMs on only a handful of
labeled examples already yields decent results compared to methods that employ
continual pre-training, and the performance gap diminishes rapidly as the
number of labeled data increases. To maximize the utilization of the limited
available data, we propose a context augmentation method and leverage
sequential self-distillation to boost performance. Comprehensive experiments on
real-world benchmarks show that given only two or more labeled samples per
class, direct fine-tuning outperforms many strong baselines that utilize
external data sources for continual pre-training. The code can be found at
https://github.com/hdzhang-code/DFTPlus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023, Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Steering Conversational Large Language Models for Long Emotional Support
  Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navid Madani, Sougata Saha, Rohini Srihari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we address the challenge of enabling large language models
(LLMs) to consistently adhere to emotional support strategies in extended
conversations. We focus on the steerability of the Llama-2 and Llama-3 suite of
models, examining their ability to maintain these strategies throughout
interactions. To assess this, we introduce the Strategy Relevant Attention
(SRA) metric, which quantifies the model's adherence to the prompted strategy
through attention maps. To facilitate our study, we create a
strategy-conditioned synthetic conversational dataset derived from the ESConv
dataset. We also propose various baselines informed by our proposed SRA metric
to address the challenge and propose a fine-tuned model that significantly
enhances the steerability of the base model in following the strategy
throughout the conversation. The code and data are publicly available on our
GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Minimizing PLM-Based Few-Shot Intent Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haode Zhang, Albert Y. S. Lam, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has demonstrated the feasibility of training efficient intent
detectors based on pre-trained language model~(PLM) with limited labeled data.
However, deploying these detectors in resource-constrained environments such as
mobile devices poses challenges due to their large sizes. In this work, we aim
to address this issue by exploring techniques to minimize the size of PLM-based
intent detectors trained with few-shot data. Specifically, we utilize large
language models (LLMs) for data augmentation, employ a cutting-edge model
compression method for knowledge distillation, and devise a vocabulary pruning
mechanism called V-Prune. Through these approaches, we successfully achieve a
compression ratio of 21 in model memory usage, including both Transformer and
the vocabulary, while maintaining almost identical performance levels on four
real-world benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The 2nd FutureDial Challenge: Dialog Systems with Retrieval Augmented
  Generation (FutureDial-RAG) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Cai, Si Chen, Yuxuan Wu, Yi Huang, Junlan Feng, Zhijian Ou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, increasing research interests have focused on retrieval augmented
generation (RAG) to mitigate hallucination for large language models (LLMs).
Following this trend, we launch the FutureDial-RAG challenge at SLT 2024, which
aims at promoting the study of RAG for dialog systems. The challenge builds
upon the MobileCS2 dataset, a real-life customer service datasets with nearly
3000 high-quality dialogs containing annotations for knowledge base query and
corresponding results. Over the dataset, we define two tasks, track 1 for
knowledge retrieval and track 2 for response generation, which are core
research questions in dialog systems with RAG. We build baseline systems for
the two tracks and design metrics to measure whether the systems can perform
accurate retrieval and generate informative and coherent response. The baseline
results show that it is very challenging to perform well on the two tasks,
which encourages the participating teams and the community to study how to make
better use of RAG for real-life dialog systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SLT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span> Honeypot: Leveraging Large Language Models as Advanced Interactive
  Honeypot Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08234v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08234v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hakan T. Otal, M. Abdullah Canbaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of cyber threats necessitates innovative solutions for
detecting and analyzing malicious activity. Honeypots, which are decoy systems
designed to lure and interact with attackers, have emerged as a critical
component in cybersecurity. In this paper, we present a novel approach to
creating realistic and interactive honeypot systems using Large Language Models
(LLMs). By fine-tuning a pre-trained open-source language model on a diverse
dataset of attacker-generated commands and responses, we developed a honeypot
capable of sophisticated engagement with attackers. Our methodology involved
several key steps: data collection and processing, prompt engineering, model
selection, and supervised fine-tuning to optimize the model's performance.
Evaluation through similarity metrics and live deployment demonstrated that our
approach effectively generates accurate and informative responses. The results
highlight the potential of LLMs to revolutionize honeypot technology, providing
cybersecurity professionals with a powerful tool to detect and analyze
malicious activity, thereby enhancing overall security infrastructure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongGenBench: Benchmarking Long-Form Generation in Long Context <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02076v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02076v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Wu, Ming Shan Hee, Zhiqing Hu, Roy Ka-Wei Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In evaluating the long-context capabilities of large language models (LLMs),
benchmarks such as "Needle-in-a-Haystack" (NIAH), Ruler, and Needlebench are
commonly used. While these benchmarks measure how well models understand
long-context input sequences, they do not effectively gauge the quality of
long-form text generation--a critical aspect for applications such as design
proposals and creative writing. To address this gap, we have introduced a new
long-form text evaluation benchmark, LongGenBench, which tests models' ability
to identify specific events within generated long text sequences. In this
benchmark, we prompt long-context LMs to create long-form text that must
include particular events or constraints and evaluate their ability to
incorporate these elements. We evaluated ten long-context LMs across four
distinct scenarios, three types of prompt instructions, and two different
generation-length settings (16K and 32K). Although these models perform well on
NIAH benchmarks, none demonstrated satisfactory performance on the
LongGenBench, raising concerns about their ability to generate coherent
long-form text that follows instructions. Additionally, as the length of the
generated text increases, all models exhibit a significant drop in performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress; Github: https://github.com/mozhu621/LongGenBench/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Run Luo, Haonan Zhang, Longze Chen, Ting-En Lin, Xiong Liu, Yuchuan Wu, Min Yang, Minzheng Wang, Pengpeng Zeng, Lianli Gao, Heng Tao Shen, Yunshui Li, Xiaobo Xia, Fei Huang, Jingkuan Song, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Multimodal Large Language Models (MLLMs) has seen
significant advancements with increasing demands in various fields (e.g.,
multimodal agents, embodied intelligence). While model-driven approaches
attempt to enhance MLLMs capabilities through diverse architectures, the gains
have become increasingly marginal. Conversely, data-driven methods, which scale
up image-text instruction data, are more effective but face limited data
diversity and complexity challenges. The absence of high-quality data
constitutes a significant development barrier for MLLMs. To address the data
quality bottleneck, we propose MMEvol, a novel multimodal instruction data
evolution framework. This framework iteratively improve data quality through a
refined combination of fine-grained perception, cognitive reasoning, and
interaction evolution, generating a more complex and diverse image-text
instruction dataset that empowers MLLMs with enhanced capabilities. Beginning
with an initial set of instructions, SEED-163K, we utilize MMEvol to
systematically broaden the diversity of instruction types, extend visual
reasoning steps to improve cognitive reasoning abilities, and thoroughly
explore fine-grained information within images to enhance visual understanding
and robustness. To comprehensively evaluate the effectiveness of our approach,
we conduct extensive qualitative analysis and quantitative experiments across
13 vision-language tasks. Compared to baseline models trained with the initial
seed data, the results demonstrate that our method achieves an average accuracy
improvement of 3.1 percentage points. Furthermore, our approach reaches
state-of-the-art (SOTA) performance in nine tasks using significantly less data
compared to state-of-the-art models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>s and the Human Condition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08403v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08403v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Wallis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Theory based AI research has had a hard time recently and the aim here is to
propose a model of what LLMs are actually doing when they impress us with their
language skills. The model integrates three established theories of human
decision-making from philosophy, sociology, and computer science. The paper
starts with the collective understanding of reasoning from the early days of AI
research - primarily because that model is how we humans think we think, and is
the most accessible. It then describes what is commonly thought of as "reactive
systems" which is the position taken by many philosophers and indeed many
contemporary AI researchers. The third component to the proposed model is from
sociology and, although not flattering to our modern ego, provides an
explanation to a puzzle that for many years has occupied those of us working on
conversational user interfaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Significant edits mainly to give the paper a single purpose - removed
  discussion of the mechanism - but just generally tighter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoD, Towards an Interpretable Medical Agent using Chain of Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junying Chen, Chi Gui, Anningzhe Gao, Ke Ji, Xidong Wang, Xiang Wan, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of medical diagnosis has undergone a significant transformation
with the advent of large language models (LLMs), yet the challenges of
interpretability within these models remain largely unaddressed. This study
introduces Chain-of-Diagnosis (CoD) to enhance the interpretability of
LLM-based medical diagnostics. CoD transforms the diagnostic process into a
diagnostic chain that mirrors a physician's thought process, providing a
transparent reasoning pathway. Additionally, CoD outputs the disease confidence
distribution to ensure transparency in decision-making. This interpretability
makes model diagnostics controllable and aids in identifying critical symptoms
for inquiry through the entropy reduction of confidences. With CoD, we
developed DiagnosisGPT, capable of diagnosing 9604 diseases. Experimental
results demonstrate that DiagnosisGPT outperforms other LLMs on diagnostic
benchmarks. Moreover, DiagnosisGPT provides interpretability while ensuring
controllability in diagnostic rigor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HuatuoGPT-II, One-stage Training for Medical Adaption of <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junying Chen, Xidong Wang, Ke Ji, Anningzhe Gao, Feng Jiang, Shunian Chen, Hongbo Zhang, Dingjie Song, Wenya Xie, Chuyi Kong, Jianquan Li, Xiang Wan, Haizhou Li, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting a language model into a specific domain, a.k.a `domain adaption', is
a common practice when specialized knowledge, e.g. medicine, is not
encapsulated in a general language model like Llama2. The challenge lies in the
heterogeneity of data across the two training stages, as it varies in
languages, genres, or formats. To tackle this and simplify the learning
protocol, we propose to transform heterogeneous data, from the both
pre-training and supervised stages, into a unified, simple input-output pair
format. We validate the new protocol in the domains where proprietary LLMs like
ChatGPT perform relatively poorly, such as Traditional Chinese Medicine. The
developed model, HuatuoGPT-II, has shown state-of-the-art performance in
Chinese medicine domain on a number of benchmarks, e.g. medical licensing
exams. It even outperforms proprietary models like ChatGPT and GPT-4 in some
aspects, especially in Traditional Chinese Medicine. Expert manual evaluations
further validate HuatuoGPT-II's advantages over existing LLMs. Notably,
HuatuoGPT-II was benchmarked in a fresh Chinese National Medical Licensing
Examination where it achieved the best performance, showcasing not only its
effectiveness but also its generalization capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Large Language Models Really Good Logical Reasoners? A Comprehensive
  Evaluation and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09841v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09841v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, Erik Cambria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logical reasoning consistently plays a fundamental and significant role in
the domains of knowledge engineering and artificial intelligence. Recently,
Large Language Models (LLMs) have emerged as a noteworthy innovation in natural
language processing (NLP). However, the question of whether LLMs can
effectively address the task of logical reasoning, which requires gradual
cognitive inference similar to human intelligence, remains unanswered. To this
end, we aim to bridge this gap and provide comprehensive evaluations in this
paper. Firstly, to offer systematic evaluations, we select fifteen typical
logical reasoning datasets and organize them into deductive, inductive,
abductive and mixed-form reasoning settings. Considering the comprehensiveness
of evaluations, we include 3 early-era representative LLMs and 4 trending LLMs.
Secondly, different from previous evaluations relying only on simple metrics
(e.g., \emph{accuracy}), we propose fine-level evaluations in objective and
subjective manners, covering both answers and explanations, including
\emph{answer correctness}, \emph{explain correctness}, \emph{explain
completeness} and \emph{explain redundancy}. Additionally, to uncover the
logical flaws of LLMs, problematic cases will be attributed to five error types
from two dimensions, i.e., \emph{evidence selection process} and
\emph{reasoning process}. Thirdly, to avoid the influences of knowledge bias
and concentrate purely on benchmarking the logical reasoning capability of
LLMs, we propose a new dataset with neutral content. Based on the in-depth
evaluations, this paper finally forms a general evaluation scheme of logical
reasoning capability from six dimensions (i.e., \emph{Correct},
\emph{Rigorous}, \emph{Self-aware}, \emph{Active}, \emph{Oriented} and \emph{No
hallucination}). It reflects the pros and cons of LLMs and gives guiding
directions for future works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into
  Multimodal <span class="highlight-title">LLM</span>s at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of multimodal large language models (MLLMs), such as
GPT-4V, has led to significant advancements. However, these models still face
challenges in medical multimodal capabilities due to limitations in the
quantity and quality of medical vision-text data, stemming from data privacy
concerns and high annotation costs. While pioneering approaches utilize
PubMed's large-scale, de-identified medical image-text pairs to address these
limitations, they still fall short due to inherent data noise. To tackle this,
we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in
an 'unblinded' capacity to denoise and reformat the data, resulting in the
creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our
validation demonstrates that: (1) PubMedVision can significantly enhance the
medical multimodal capabilities of current MLLMs, showing significant
improvement in benchmarks including the MMMU Health & Medicine track; (2)
manual checks by medical experts and empirical results validate the superior
data quality of our dataset compared to other data construction methods. Using
PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows
superior performance in medical multimodal scenarios among open-source MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparison of Large Language Models for Generating Contextually Relevant
  Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20578v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20578v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivo Lodovico Molina, Valdemar Švábenský, Tsubasa Minematsu, Li Chen, Fumiya Okubo, Atsushi Shimada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the effectiveness of Large Language Models (LLMs) for
Automatic Question Generation in educational settings. Three LLMs are compared
in their ability to create questions from university slide text without
fine-tuning. Questions were obtained in a two-step pipeline: first, answer
phrases were extracted from slides using Llama 2-Chat 13B; then, the three
models generated questions for each answer. To analyze whether the questions
would be suitable in educational applications for students, a survey was
conducted with 46 students who evaluated a total of 246 questions across five
metrics: clarity, relevance, difficulty, slide relation, and question-answer
alignment. Results indicate that GPT-3.5 and Llama 2-Chat 13B outperform Flan
T5 XXL by a small margin, particularly in terms of clarity and question-answer
alignment. GPT-3.5 especially excels at tailoring questions to match the input
answers. The contribution of this research is the analysis of the capacity of
LLMs for Automatic Question Generation in education.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Springer ECTEL 2024 conference proceedings, see
  https://doi.org/10.1007/978-3-031-72312-4_18</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ YOLO-Stutter: End-to-end Region-Wise Speech Dysfluency Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15297v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15297v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanru Zhou, Anshul Kashyap, Steve Li, Ayati Sharma, Brittany Morin, David Baquirin, Jet Vonk, Zoe Ezzes, Zachary Miller, Maria Luisa Gorno Tempini, Jiachen Lian, Gopala Krishna Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dysfluent speech detection is the bottleneck for disordered speech analysis
and spoken language learning. Current state-of-the-art models are governed by
rule-based systems which lack efficiency and robustness, and are sensitive to
template design. In this paper, we propose YOLO-Stutter: a first end-to-end
method that detects dysfluencies in a time-accurate manner. YOLO-Stutter takes
imperfect speech-text alignment as input, followed by a spatial feature
aggregator, and a temporal dependency extractor to perform region-wise boundary
and class predictions. We also introduce two dysfluency corpus, VCTK-Stutter
and VCTK-TTS, that simulate natural spoken dysfluencies including repetition,
block, missing, replacement, and prolongation. Our end-to-end method achieves
state-of-the-art performance with a minimum number of trainable parameters for
on both simulated data and real aphasia speech. Code and datasets are
open-sourced at https://github.com/rorizzz/YOLO-Stutter
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Interspeech 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04307v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04307v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiying Zhu, Yiming Yang, Zhiqing Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations pose a significant challenge to the reliability of large
language models (LLMs) in critical domains. Recent benchmarks designed to
assess LLM hallucinations within conventional NLP tasks, such as
knowledge-intensive question answering (QA) and summarization, are insufficient
for capturing the complexities of user-LLM interactions in dynamic, real-world
settings. To address this gap, we introduce HaluEval-Wild, the first benchmark
specifically designed to evaluate LLM hallucinations in the wild. We
meticulously collect challenging (adversarially filtered by Alpaca) user
queries from ShareGPT, an existing real-world user-LLM interaction datasets, to
evaluate the hallucination rates of various LLMs. Upon analyzing the collected
queries, we categorize them into five distinct types, which enables a
fine-grained analysis of the types of hallucinations LLMs exhibit, and
synthesize the reference answers with the powerful GPT-4 model and
retrieval-augmented generation (RAG). Our benchmark offers a novel approach
towards enhancing our comprehension of and improving LLM reliability in
scenarios reflective of real-world interactions. Our benchmark is available at
https://github.com/HaluEval-Wild/HaluEval-Wild.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparative Analysis of Encoder-Based NER and Large Language Models for
  Skill <span class="highlight-title">Extraction</span> from Russian Job Vacancies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19816v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19816v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikita Matkin, Aleksei Smirnov, Mikhail Usanin, Egor Ivanov, Kirill Sobyanin, Sofiia Paklina, Petr Parshakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The labor market is undergoing rapid changes, with increasing demands on job
seekers and a surge in job openings. Identifying essential skills and
competencies from job descriptions is challenging due to varying employer
requirements and the omission of key skills. This study addresses these
challenges by comparing traditional Named Entity Recognition (NER) methods
based on encoders with Large Language Models (LLMs) for extracting skills from
Russian job vacancies. Using a labeled dataset of 4,000 job vacancies for
training and 1,472 for testing, the performance of both approaches is
evaluated. Results indicate that traditional NER models, especially DeepPavlov
RuBERT NER tuned, outperform LLMs across various metrics including accuracy,
precision, recall, and inference time. The findings suggest that traditional
NER models provide more effective and efficient solutions for skill extraction,
enhancing job requirement clarity and aiding job seekers in aligning their
qualifications with employer expectations. This research contributes to the
field of natural language processing (NLP) and its application in the labor
market, particularly in non-English contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Sequencing of Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02578v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02578v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Gervers, Gelila Tilahun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We outline an unsupervised method for temporal rank ordering of sets of
historical documents, namely American State of the Union Addresses and DEEDS, a
corpus of medieval English property transfer documents. Our method relies upon
effectively capturing the gradual change in word usage via a bandwidth estimate
for the non-parametric Generalized Linear Models (Fan, Heckman, and Wand,
1995). The number of possible rank orders needed to search through for cost
functions related to the bandwidth can be quite large, even for a small set of
documents. We tackle this problem of combinatorial optimization using the
Simulated Annealing algorithm, which allows us to obtain the optimal document
temporal orders. Our rank ordering method significantly improved the temporal
sequencing of both corpora compared to a randomly sequenced baseline. This
unsupervised approach should enable the temporal ordering of undated document
sets.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">80</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRIN: Zero-Shot Metric Depth with Pixel-Level Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vitor Guizilini, Pavel Tokmakov, Achal Dave, Rares Ambrus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction from a single image is a long-standing problem in computer
vision. Learning-based methods address its inherent scale ambiguity by
leveraging increasingly large labeled and unlabeled datasets, to produce
geometric priors capable of generating accurate predictions across domains. As
a result, state of the art approaches show impressive performance in zero-shot
relative and metric depth estimation. Recently, diffusion models have exhibited
remarkable scalability and generalizable properties in their learned
representations. However, because these models repurpose tools originally
designed for image generation, they can only operate on dense ground-truth,
which is not available for most depth labels, especially in real-world
settings. In this paper we present GRIN, an efficient diffusion model designed
to ingest sparse unstructured training data. We use image features with 3D
geometric positional encodings to condition the diffusion process both globally
and locally, generating depth predictions at a pixel-level. With comprehensive
experiments across eight indoor and outdoor datasets, we show that GRIN
establishes a new state of the art in zero-shot metric monocular depth
estimation even when trained from scratch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resolving Inconsistent Semantics in Multi-<span class="highlight-title">Dataset</span> Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qilong Zhangli, Di Liu, Abhishek Aich, Dimitris Metaxas, Samuel Schulter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging multiple training datasets to scale up image segmentation models
is beneficial for increasing robustness and semantic understanding. Individual
datasets have well-defined ground truth with non-overlapping mask layouts and
mutually exclusive semantics. However, merging them for multi-dataset training
disrupts this harmony and leads to semantic inconsistencies; for example, the
class "person" in one dataset and class "face" in another will require
multilabel handling for certain pixels. Existing methods struggle with this
setting, particularly when evaluated on label spaces mixed from the individual
training sets. To overcome these issues, we introduce a simple yet effective
multi-dataset training approach by integrating language-based embeddings of
class names and label space-specific query embeddings. Our method maintains
high performance regardless of the underlying inconsistencies between training
datasets. Notably, on four benchmark datasets with label space inconsistencies
during inference, we outperform previous methods by 1.6% mIoU for semantic
segmentation, 9.1% PQ for panoptic segmentation, 12.1% AP for instance
segmentation, and 3.0% in the newly proposed PIQ metric.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Kinetic Manipulation of the Latent Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Porres
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The latent space of many generative models are rich in unexplored valleys and
mountains. The majority of tools used for exploring them are so far limited to
Graphical User Interfaces (GUIs). While specialized hardware can be used for
this task, we show that a simple feature extraction of pre-trained
Convolutional Neural Networks (CNNs) from a live RGB camera feed does a very
good job at manipulating the latent space with simple changes in the scene,
with vast room for improvement. We name this new paradigm Visual-reactive
Interpolation, and the full code can be found at
https://github.com/PDillis/stylegan3-fun.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Physical-World Adversarial Attack on Traffic Sign
  Recognition: A Commercial Systems Perspective <span class="chip">NDSS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ningfei Wang, Shaoyuan Xie, Takami Sato, Yunpeng Luo, Kaidi Xu, Qi Alfred Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic Sign Recognition (TSR) is crucial for safe and correct driving
automation. Recent works revealed a general vulnerability of TSR models to
physical-world adversarial attacks, which can be low-cost, highly deployable,
and capable of causing severe attack effects such as hiding a critical traffic
sign or spoofing a fake one. However, so far existing works generally only
considered evaluating the attack effects on academic TSR models, leaving the
impacts of such attacks on real-world commercial TSR systems largely unclear.
In this paper, we conduct the first large-scale measurement of physical-world
adversarial attacks against commercial TSR systems. Our testing results reveal
that it is possible for existing attack works from academia to have highly
reliable (100\%) attack success against certain commercial TSR system
functionality, but such attack capabilities are not generalizable, leading to
much lower-than-expected attack success rates overall. We find that one
potential major factor is a spatial memorization design that commonly exists in
today's commercial TSR systems. We design new attack success metrics that can
mathematically model the impacts of such design on the TSR system-level attack
success, and use them to revisit existing attacks. Through these efforts, we
uncover 7 novel observations, some of which directly challenge the observations
or claims in prior works due to the introduction of the new metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NDSS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracking Virtual Meetings in the Wild: Re-identification in
  Multi-Participant Virtual Meetings <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oriel Perl, Ido Leshem, Uria Franko, Yuval Goldman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, workplaces and educational institutes have widely adopted
virtual meeting platforms. This has led to a growing interest in analyzing and
extracting insights from these meetings, which requires effective detection and
tracking of unique individuals. In practice, there is no standardization in
video meetings recording layout, and how they are captured across the different
platforms and services. This, in turn, creates a challenge in acquiring this
data stream and analyzing it in a uniform fashion. Our approach provides a
solution to the most general form of video recording, usually consisting of a
grid of participants (\cref{fig:videomeeting}) from a single video source with
no metadata on participant locations, while using the least amount of
constraints and assumptions as to how the data was acquired. Conventional
approaches often use YOLO models coupled with tracking algorithms, assuming
linear motion trajectories akin to that observed in CCTV footage. However, such
assumptions fall short in virtual meetings, where participant video feed window
can abruptly change location across the grid. In an organic video meeting
setting, participants frequently join and leave, leading to sudden, non-linear
movements on the video grid. This disrupts optical flow-based tracking methods
that depend on linear motion. Consequently, standard object detection and
tracking methods might mistakenly assign multiple participants to the same
tracker. In this paper, we introduce a novel approach to track and re-identify
participants in remote video meetings, by utilizing the spatio-temporal priors
arising from the data in our domain. This, in turn, increases tracking
capabilities compared to the use of general object tracking. Our approach
reduces the error rate by 95% on average compared to YOLO-based tracking
methods as a baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Template-based Multi-Domain Face Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudh Nanduri, Rama Chellappa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable performance of deep neural networks for face detection
and recognition tasks in the visible spectrum, their performance on more
challenging non-visible domains is comparatively still lacking. While
significant research has been done in the fields of domain adaptation and
domain generalization, in this paper we tackle scenarios in which these methods
have limited applicability owing to the lack of training data from target
domains. We focus on the problem of single-source (visible) and multi-target
(SWIR, long-range/remote, surveillance, and body-worn) face recognition task.
We show through experiments that a good template generation algorithm becomes
crucial as the complexity of the target domain increases. In this context, we
introduce a template generation algorithm called Norm Pooling (and a variant
known as Sparse Pooling) and show that it outperforms average pooling across
different domains and networks, on the IARPA JANUS Benchmark Multi-domain Face
(IJB-MDF) dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCB 2024 - Special Session on Recognition at Long Range and from
  High Altitude</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NARF24: Estimating Articulated Object Structure for Implicit Rendering <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stanley Lewis, Tom Gao, Odest Chadwicke Jenkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Articulated objects and their representations pose a difficult problem for
robots. These objects require not only representations of geometry and texture,
but also of the various connections and joint parameters that make up each
articulation. We propose a method that learns a common Neural Radiance Field
(NeRF) representation across a small number of collected scenes. This
representation is combined with a parts-based image segmentation to produce an
implicit space part localization, from which the connectivity and joint
parameters of the articulated object can be estimated, thus enabling
configuration-conditioned rendering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>extended abstract as submitted to ICRA@40 anniversary conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Famba-V: Fast Vision Mamba with Cross-Layer Token Fusion <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Shen, Zhongwei Wan, Xin Wang, Mi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mamba and Vision Mamba (Vim) models have shown their potential as an
alternative to methods based on Transformer architecture. This work introduces
Fast Mamba for Vision (Famba-V), a cross-layer token fusion technique to
enhance the training efficiency of Vim models. The key idea of Famba-V is to
identify and fuse similar tokens across different Vim layers based on a suit of
cross-layer strategies instead of simply applying token fusion uniformly across
all the layers that existing works propose. We evaluate the performance of
Famba-V on CIFAR-100. Our results show that Famba-V is able to enhance the
training efficiency of Vim models by reducing both training time and peak
memory usage during training. Moreover, the proposed cross-layer strategies
allow Famba-V to deliver superior accuracy-efficiency trade-offs. These results
all together demonstrate Famba-V as a promising efficiency enhancement
technique for Vim models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version of ECCV 2024 The Fourth Workshop on
  Computational Aspects of Deep Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Abnormal <span class="highlight-title">Event</span> Detection In Videos Using Deep Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darshan Venkatrayappa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abnormal event detection or anomaly detection in surveillance videos is
currently a challenge because of the diversity of possible events. Due to the
lack of anomalous events at training time, anomaly detection requires the
design of learning methods without supervision. In this work we propose an
unsupervised approach for video anomaly detection with the aim to jointly
optimize the objectives of the deep neural network and the anomaly detection
task using a hybrid architecture. Initially, a convolutional autoencoder is
pre-trained in an unsupervised manner with a fusion of depth, motion and
appearance features. In the second step, we utilize the encoder part of the
pre-trained autoencoder and extract the embeddings of the fused input. Now, we
jointly train/ fine tune the encoder to map the embeddings to a hypercenter.
Thus, embeddings of normal data fall near the hypercenter, whereas embeddings
of anomalous data fall far away from the hypercenter.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain and Content Adaptive Convolutions for Cross-Domain Adenocarcinoma
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frauke Wilm, Mathias Öttl, Marc Aubreville, Katharina Breininger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in computer-aided diagnosis for histopathology have been
largely driven by the use of deep learning models for automated image analysis.
While these networks can perform on par with medical experts, their performance
can be impeded by out-of-distribution data. The Cross-Organ and Cross-Scanner
Adenocarcinoma Segmentation (COSAS) challenge aimed to address the task of
cross-domain adenocarcinoma segmentation in the presence of morphological and
scanner-induced domain shifts. In this paper, we present a U-Net-based
segmentation framework designed to tackle this challenge. Our approach achieved
segmentation scores of 0.8020 for the cross-organ track and 0.8527 for the
cross-scanner track on the final challenge test sets, ranking it the
best-performing submission.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Topology Refinement for Medical Image Segmentation with
  Polynomial Feature Synthesis <span class="chip">MICCAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu Li, Hanchun Wang, Matthew Baugh, Qiang Ma, Weitong Zhang, Cheng Ouyang, Daniel Rueckert, Bernhard Kainz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although existing medical image segmentation methods provide impressive
pixel-wise accuracy, they often neglect topological correctness, making their
segmentations unusable for many downstream tasks. One option is to retrain such
models whilst including a topology-driven loss component. However, this is
computationally expensive and often impractical. A better solution would be to
have a versatile plug-and-play topology refinement method that is compatible
with any domain-specific segmentation pipeline. Directly training a
post-processing model to mitigate topological errors often fails as such models
tend to be biased towards the topological errors of a target segmentation
network. The diversity of these errors is confined to the information provided
by a labelled training set, which is especially problematic for small datasets.
Our method solves this problem by training a model-agnostic topology refinement
network with synthetic segmentations that cover a wide variety of topological
errors. Inspired by the Stone-Weierstrass theorem, we synthesize
topology-perturbation masks with randomly sampled coefficients of orthogonal
polynomial bases, which ensures a complete and unbiased representation.
Practically, we verified the efficiency and effectiveness of our methods as
being compatible with multiple families of polynomial bases, and show evidence
that our universal plug-and-play topology refinement network outperforms both
existing topology-driven learning-based and post-processing methods. We also
show that combining our method with learning-based models provides an
effortless add-on, which can further improve the performance of existing
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 27th International Conference on Medical Image
  Computing and Computer Assisted Intervention (MICCAI 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiple Rotation Averaging with Constrained Reweighting Deep Matrix
  Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqi Li, Jihua Zhu, Yifan Xie, Naiwen Hu, Mingchen Zhu, Zhongyu Li, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple rotation averaging plays a crucial role in computer vision and
robotics domains. The conventional optimization-based methods optimize a
nonlinear cost function based on certain noise assumptions, while most previous
learning-based methods require ground truth labels in the supervised training
process. Recognizing the handcrafted noise assumption may not be reasonable in
all real-world scenarios, this paper proposes an effective rotation averaging
method for mining data patterns in a learning manner while avoiding the
requirement of labels. Specifically, we apply deep matrix factorization to
directly solve the multiple rotation averaging problem in unconstrained linear
space. For deep matrix factorization, we design a neural network model, which
is explicitly low-rank and symmetric to better suit the background of multiple
rotation averaging. Meanwhile, we utilize a spanning tree-based edge filtering
to suppress the influence of rotation outliers. What's more, we also adopt a
reweighting scheme and dynamic depth selection strategy to further improve the
robustness. Our method synthesizes the merit of both optimization-based and
learning-based methods. Experimental results on various datasets validate the
effectiveness of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reasoning Paths with Reference Objects Elicit Quantitative Spatial
  Reasoning in Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances demonstrating vision-language models' (VLMs)
abilities to describe complex relationships in images using natural language,
their capability to quantitatively reason about object sizes and distances
remains underexplored. In this work, we introduce a manually annotated
benchmark, Q-Spatial Bench, with 271 questions across five categories designed
for quantitative spatial reasoning and systematically investigate the
performance of state-of-the-art VLMs on this task. Our analysis reveals that
reasoning about distances between objects is particularly challenging for SoTA
VLMs; however, some VLMs significantly outperform others, with an over 40-point
gap between the two best performing models. We also make the surprising
observation that the success rate of the top-performing VLM increases by 19
points when a reasoning path using a reference object emerges naturally in the
response. Inspired by this observation, we develop a zero-shot prompting
technique, SpatialPrompt, that encourages VLMs to answer quantitative spatial
questions using reference objects as visual cues. By instructing VLMs to use
reference objects in their reasoning paths via SpatialPrompt, Gemini 1.5 Pro,
Gemini 1.5 Flash, and GPT-4V improve their success rates by over 40, 20, and 30
points, respectively. We emphasize that these significant improvements are
obtained without needing more data, model architectural modifications, or
fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Lesion Segmentation in PET/CT Imaging with Deep Learning and
  Advanced Data Preprocessing Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Liu, Qiaoyi Xue, Youdan Feng, Tianming Xu, Kaixin Shen, Chuyun Shen, Yuhang Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The escalating global cancer burden underscores the critical need for precise
diagnostic tools in oncology. This research employs deep learning to enhance
lesion segmentation in PET/CT imaging, utilizing a dataset of 900 whole-body
FDG-PET/CT and 600 PSMA-PET/CT studies from the AutoPET challenge III. Our
methodical approach includes robust preprocessing and data augmentation
techniques to ensure model robustness and generalizability. We investigate the
influence of non-zero normalization and modifications to the data augmentation
pipeline, such as the introduction of RandGaussianSharpen and adjustments to
the Gamma transform parameter. This study aims to contribute to the
standardization of preprocessing and augmentation strategies in PET/CT imaging,
potentially improving the diagnostic accuracy and the personalized management
of cancer patients. Our code will be open-sourced and available at
https://github.com/jiayiliu-pku/DC2024.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Underwater Image Enhancement via Dehazing and Color Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengqin Wu, Shuai Yu, Qingson Hu, Jingxiang Xu, Lijun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of marine engineering projects such as marine
resource extraction and oceanic surveys, underwater visual imaging and analysis
has become a critical technology. Unfortunately, due to the inevitable
non-linear attenuation of light in underwater environments, underwater images
and videos often suffer from low contrast, blurriness, and color degradation,
which significantly complicate the subsequent research. Existing underwater
image enhancement methods often treat the haze and color cast as a unified
degradation process and disregard their independence and interdependence, which
limits the performance improvement. Here, we propose a Vision Transformer
(ViT)-based network (referred to as WaterFormer) to improve the underwater
image quality. WaterFormer contains three major components: a dehazing block
(DehazeFormer Block) to capture the self-correlated haze features and extract
deep-level features, a Color Restoration Block (CRB) to capture self-correlated
color cast features, and a Channel Fusion Block (CFB) to capture fusion
features within the network. To ensure authenticity, a soft reconstruction
layer based on the underwater imaging physics model is included. To improve the
quality of the enhanced images, we introduce the Chromatic Consistency Loss and
Sobel Color Loss to train the network. Comprehensive experimental results
demonstrate that WaterFormer outperforms other state-of-the-art methods in
enhancing underwater images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiFSD: Ego-Centric Fully Sparse Paradigm with Uncertainty Denoising and
  Iterative Refinement for Efficient End-to-End Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haisheng Su, Wei Wu, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current end-to-end autonomous driving methods resort to unifying modular
designs for various tasks (e.g. perception, prediction and planning). Although
optimized in a planning-oriented spirit with a fully differentiable framework,
existing end-to-end driving systems without ego-centric designs still suffer
from unsatisfactory performance and inferior efficiency, owing to the
rasterized scene representation learning and redundant information
transmission. In this paper, we revisit the human driving behavior and propose
an ego-centric fully sparse paradigm, named DiFSD, for end-to-end self-driving.
Specifically, DiFSD mainly consists of sparse perception, hierarchical
interaction and iterative motion planner. The sparse perception module performs
detection, tracking and online mapping based on sparse representation of the
driving scene. The hierarchical interaction module aims to select the Closest
In-Path Vehicle / Stationary (CIPV / CIPS) from coarse to fine, benefiting from
an additional geometric prior. As for the iterative motion planner, both
selected interactive agents and ego-vehicle are considered for joint motion
prediction, where the output multi-modal ego-trajectories are optimized in an
iterative fashion. Besides, both position-level motion diffusion and
trajectory-level planning denoising are introduced for uncertainty modeling,
thus facilitating the training stability and convergence of the whole
framework. Extensive experiments conducted on nuScenes dataset demonstrate the
superior planning performance and great efficiency of DiFSD, which
significantly reduces the average L2 error by \textbf{66\%} and collision rate
by \textbf{77\%} than UniAD while achieves \textbf{8.2$\times$} faster running
efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizing Alignment Paradigm of Text-to-Image Generation with
  Preferences through $f$-divergence Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Sun, Bo Xia, Yongzhe Chang, Xueqian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) has recently expanded its successful
application from aligning large language models (LLMs) to aligning
text-to-image models with human preferences, which has generated considerable
interest within the community. However, we have observed that these approaches
rely solely on minimizing the reverse Kullback-Leibler divergence during
alignment process between the fine-tuned model and the reference model,
neglecting the incorporation of other divergence constraints. In this study, we
focus on extending reverse Kullback-Leibler divergence in the alignment
paradigm of text-to-image models to $f$-divergence, which aims to garner better
alignment performance as well as good generation diversity. We provide the
generalized formula of the alignment paradigm under the $f$-divergence
condition and thoroughly analyze the impact of different divergence constraints
on alignment process from the perspective of gradient fields. We conduct
comprehensive evaluation on image-text alignment performance, human value
alignment performance and generation diversity performance under different
divergence constraints, and the results indicate that alignment based on
Jensen-Shannon divergence achieves the best trade-off among them. The option of
divergence employed for aligning text-to-image models significantly impacts the
trade-off between alignment performance (especially human value alignment) and
generation diversity, which highlights the necessity of selecting an
appropriate divergence for practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer
  setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoyi Xue, Youdan Feng, Jiayi Liu, Tianming Xu, Kaixin Shen, Chuyun Shen, Yuhang Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores a workflow for automated segmentation of lesions in FDG
and PSMA PET/CT images. Due to the substantial differences in image
characteristics between FDG and PSMA, specialized preprocessing steps are
required. Utilizing YOLOv8 for data classification, the FDG and PSMA images are
preprocessed separately before feeding them into the segmentation models,
aiming to improve lesion segmentation accuracy. The study focuses on evaluating
the performance of automated segmentation workflow for multitracer PET images.
The findings are expected to provide critical insights for enhancing diagnostic
workflows and patient-specific treatment plans. Our code will be open-sourced
and available at https://github.com/jiayiliu-pku/AP2024.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MesonGS: Post-training Compression of 3D Gaussians via Efficient
  Attribute Transformation <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuzhao Xie, Weixiang Zhang, Chen Tang, Yunpeng Bai, Rongwei Lu, Shijia Ge, Zhi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting demonstrates excellent quality and speed in novel view
synthesis. Nevertheless, the huge file size of the 3D Gaussians presents
challenges for transmission and storage. Current works design compact models to
replace the substantial volume and attributes of 3D Gaussians, along with
intensive training to distill information. These endeavors demand considerable
training time, presenting formidable hurdles for practical deployment. To this
end, we propose MesonGS, a codec for post-training compression of 3D Gaussians.
Initially, we introduce a measurement criterion that considers both
view-dependent and view-independent factors to assess the impact of each
Gaussian point on the rendering output, enabling the removal of insignificant
points. Subsequently, we decrease the entropy of attributes through two
transformations that complement subsequent entropy coding techniques to enhance
the file compression rate. More specifically, we first replace rotation
quaternions with Euler angles; then, we apply region adaptive hierarchical
transform to key attributes to reduce entropy. Lastly, we adopt finer-grained
quantization to avoid excessive information loss. Moreover, a well-crafted
finetune scheme is devised to restore quality. Extensive experiments
demonstrate that MesonGS significantly reduces the size of 3D Gaussians while
preserving competitive quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures, ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Single-Lens Controllable Depth-of-Field Imaging via All-in-Focus
  Aberration Correction and Monocular Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolong Qian, Qi Jiang, Yao Gao, Shaohua Gao, Zhonghua Yi, Lei Sun, Kai Wei, Haifeng Li, Kailun Yang, Kaiwei Wang, Jian Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable Depth-of-Field (DoF) imaging commonly produces amazing visual
effects based on heavy and expensive high-end lenses. However, confronted with
the increasing demand for mobile scenarios, it is desirable to achieve a
lightweight solution with Minimalist Optical Systems (MOS). This work centers
around two major limitations of MOS, i.e., the severe optical aberrations and
uncontrollable DoF, for achieving single-lens controllable DoF imaging via
computational methods. A Depth-aware Controllable DoF Imaging (DCDI) framework
is proposed equipped with All-in-Focus (AiF) aberration correction and
monocular depth estimation, where the recovered image and corresponding depth
map are utilized to produce imaging results under diverse DoFs of any high-end
lens via patch-wise convolution. To address the depth-varying optical
degradation, we introduce a Depth-aware Degradation-adaptive Training (DA2T)
scheme. At the dataset level, a Depth-aware Aberration MOS (DAMOS) dataset is
established based on the simulation of Point Spread Functions (PSFs) under
different object distances. Additionally, we design two plug-and-play
depth-aware mechanisms to embed depth information into the aberration image
recovery for better tackling depth-aware degradation. Furthermore, we propose a
storage-efficient Omni-Lens-Field model to represent the 4D PSF library of
various lenses. With the predicted depth map, recovered image, and depth-aware
PSF map inferred by Omni-Lens-Field, single-lens controllable DoF imaging is
achieved. Comprehensive experimental results demonstrate that the proposed
framework enhances the recovery performance, and attains impressive single-lens
controllable DoF imaging results, providing a seminal baseline for this field.
The source code and the established dataset will be publicly available at
https://github.com/XiaolongQian/DCDI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code and the established dataset will be publicly
  available at https://github.com/XiaolongQian/DCDI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahriar Rifat, Jonathan Ashdown, Francesco Restuccia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test Time Adaptation (TTA) has emerged as a practical solution to mitigate
the performance degradation of Deep Neural Networks (DNNs) in the presence of
corruption/ noise affecting inputs. Existing approaches in TTA continuously
adapt the DNN, leading to excessive resource consumption and performance
degradation due to accumulation of error stemming from lack of supervision. In
this work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to
address such issues. Our key approach is to proactively learn latent
representations of some corruption types, each one associated with a
sub-network state tailored to correctly classify inputs affected by that
corruption. After deployment, DARDA adapts the DNN to previously unseen
corruptions in an unsupervised fashion by (i) estimating the latent
representation of the ongoing corruption; (ii) selecting the sub-network whose
associated corruption is the closest in the latent space to the ongoing
corruption; and (iii) adapting DNN state, so that its representation matches
the ongoing corruption. This way, DARDA is more resource efficient and can
swiftly adapt to new distributions caused by different corruptions without
requiring a large variety of input data. Through experiments with two popular
mobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA
reduces energy consumption and average cache memory footprint respectively by
1.74x and 2.64x with respect to the state of the art, while increasing the
performance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explore the Hallucination on Low-level Perception for M<span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinan Sun, Zicheng Zhang, Haoning Wu, Xiaohong Liu, Weisi Lin, Guangtao Zhai, Xiongkuo Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of Multi-modality Large Language Models (MLLMs) has
significantly influenced various aspects of industry and daily life, showcasing
impressive capabilities in visual perception and understanding. However, these
models also exhibit hallucinations, which limit their reliability as AI
systems, especially in tasks involving low-level visual perception and
understanding. We believe that hallucinations stem from a lack of explicit
self-awareness in these models, which directly impacts their overall
performance. In this paper, we aim to define and evaluate the self-awareness of
MLLMs in low-level visual perception and understanding tasks. To this end, we
present QL-Bench, a benchmark settings to simulate human responses to low-level
vision, investigating self-awareness in low-level visual perception through
visual question answering related to low-level attributes such as clarity and
lighting. Specifically, we construct the LLSAVisionQA dataset, comprising 2,990
single images and 1,999 image pairs, each accompanied by an open-ended question
about its low-level features. Through the evaluation of 15 MLLMs, we
demonstrate that while some models exhibit robust low-level visual
capabilities, their self-awareness remains relatively underdeveloped. Notably,
for the same model, simpler questions are often answered more accurately than
complex ones. However, self-awareness appears to improve when addressing more
challenging questions. We hope that our benchmark will motivate further
research, particularly focused on enhancing the self-awareness of MLLMs in
tasks involving low-level visual perception and understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Two-factor Representation for Magnetic Resonance Image
  Super-resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weifeng Wei, Heng Chen, Pengxiang Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic Resonance Imaging (MRI) requires a trade-off between resolution,
signal-to-noise ratio, and scan time, making high-resolution (HR) acquisition
challenging. Therefore, super-resolution for MR image is a feasible solution.
However, most existing methods face challenges in accurately learning a
continuous volumetric representation from low-resolution image or require HR
image for supervision. To solve these challenges, we propose a novel method for
MR image super-resolution based on two-factor representation. Specifically, we
factorize intensity signals into a linear combination of learnable basis and
coefficient factors, enabling efficient continuous volumetric representation
from low-resolution MR image. Besides, we introduce a coordinate-based encoding
to capture structural relationships between sparse voxels, facilitating smooth
completion in unobserved regions. Experiments on BraTS 2019 and MSSEG 2016
datasets demonstrate that our method achieves state-of-the-art performance,
providing superior visual fidelity and robustness, particularly in large
up-sampling scale MR image super-resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precise Pick-and-Place using Score-Based Diffusion Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shih-Wei Guo, Tsu-Ching Hsiao, Yu-Lun Liu, Chun-Yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel coarse-to-fine continuous pose diffusion
method to enhance the precision of pick-and-place operations within robotic
manipulation tasks. Leveraging the capabilities of diffusion networks, we
facilitate the accurate perception of object poses. This accurate perception
enhances both pick-and-place success rates and overall manipulation precision.
Our methodology utilizes a top-down RGB image projected from an RGB-D camera
and adopts a coarse-to-fine architecture. This architecture enables efficient
learning of coarse and fine models. A distinguishing feature of our approach is
its focus on continuous pose estimation, which enables more precise object
manipulation, particularly concerning rotational angles. In addition, we employ
pose and color augmentation techniques to enable effective training with
limited data. Through extensive experiments in simulated and real-world
scenarios, as well as an ablation study, we comprehensively evaluate our
proposed methodology. Taken together, the findings validate its effectiveness
in achieving high-precision pick-and-place tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures. Project webpage:
  https://tony2guo.github.io/precise-pick-and-place/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MFCLIP: Multi-modal Fine-grained CLIP for Generalizable Diffusion Face
  Forgery Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaning Zhang, Tianyi Wang, Zitong Yu, Zan Gao, Linlin Shen, Shengyong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of photo-realistic face generation methods has raised
significant concerns in society and academia, highlighting the urgent need for
robust and generalizable face forgery detection (FFD) techniques. Although
existing approaches mainly capture face forgery patterns using image modality,
other modalities like fine-grained noises and texts are not fully explored,
which limits the generalization capability of the model. In addition, most FFD
methods tend to identify facial images generated by GAN, but struggle to detect
unseen diffusion-synthesized ones. To address the limitations, we aim to
leverage the cutting-edge foundation model, contrastive language-image
pre-training (CLIP), to achieve generalizable diffusion face forgery detection
(DFFD). In this paper, we propose a novel multi-modal fine-grained CLIP
(MFCLIP) model, which mines comprehensive and fine-grained forgery traces
across image-noise modalities via language-guided face forgery representation
learning, to facilitate the advancement of DFFD. Specifically, we devise a
fine-grained language encoder (FLE) that extracts fine global language features
from hierarchical text prompts. We design a multi-modal vision encoder (MVE) to
capture global image forgery embeddings as well as fine-grained noise forgery
patterns extracted from the richest patch, and integrate them to mine general
visual forgery traces. Moreover, we build an innovative plug-and-play sample
pair attention (SPA) method to emphasize relevant negative pairs and suppress
irrelevant ones, allowing cross-modality sample pairs to conduct more flexible
alignment. Extensive experiments and visualizations show that our model
outperforms the state of the arts on different settings like cross-generator,
cross-forgery, and cross-dataset evaluations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finetuning CLIP to Reason about Pairwise Differences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Sam, Devin Willmott, Joao D. Semedo, J. Zico Kolter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) such as CLIP are trained via contrastive
learning between text and image pairs, resulting in aligned image and text
embeddings that are useful for many downstream tasks. A notable drawback of
CLIP, however, is that the resulting embedding space seems to lack some of the
structure of their purely text-based alternatives. For instance, while text
embeddings have been long noted to satisfy \emph{analogies} in embedding space
using vector arithmetic, CLIP has no such property. In this paper, we propose
an approach to natively train CLIP in a contrastive manner to reason about
differences in embedding space. We finetune CLIP so that the differences in
image embedding space correspond to \emph{text descriptions of the image
differences}, which we synthetically generate with large language models on
image-caption paired datasets. We first demonstrate that our approach yields
significantly improved capabilities in ranking images by a certain attribute
(e.g., elephants are larger than cats), which is useful in retrieval or
constructing attribute-based classifiers, and improved zeroshot classification
performance on many downstream image classification tasks. In addition, our
approach enables a new mechanism for inference that we refer to as comparative
prompting, where we leverage prior knowledge of text descriptions of
differences between classes of interest, achieving even larger performance
gains in classification. Finally, we illustrate that the resulting embeddings
obey a larger degree of geometric properties in embedding space, such as in
text-to-image generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Visual Priors: Unsupervised Learning of Scene
  Interpretations with Compositional Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09716v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09716v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krzysztof Krawiec, Antoni Nowinowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contemporary deep learning architectures lack principled means for capturing
and handling fundamental visual concepts, like objects, shapes, geometric
transforms, and other higher-level structures. We propose a neurosymbolic
architecture that uses a domain-specific language to capture selected priors of
image formation, including object shape, appearance, categorization, and
geometric transforms. We express template programs in that language and learn
their parameterization with features extracted from the scene by a
convolutional neural network. When executed, the parameterized program produces
geometric primitives which are rendered and assessed for correspondence with
the scene content and trained via auto-association with gradient. We confront
our approach with a baseline method on a synthetic benchmark and demonstrate
its capacity to disentangle selected aspects of the image formation process,
learn from small data, correct inference in the presence of noise, and
out-of-sample generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Pre-Train</span>ing for 3D Hand Pose Estimation with Contrastive Learning on
  Large-Scale Hand Images in the Wild <span class="chip">ECCV24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nie Lin, Takehiko Ohkawa, Mingfang Zhang, Yifei Huang, Ryosuke Furuta, Yoichi Sato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a contrastive learning framework based on in-the-wild hand images
tailored for pre-training 3D hand pose estimators, dubbed HandCLR. Pre-training
on large-scale images achieves promising results in various tasks, but prior 3D
hand pose pre-training methods have not fully utilized the potential of diverse
hand images accessible from in-the-wild videos. To facilitate scalable
pre-training, we first prepare an extensive pool of hand images from
in-the-wild videos and design our method with contrastive learning.
Specifically, we collected over 2.0M hand images from recent human-centric
videos, such as 100DOH and Ego4D. To extract discriminative information from
these images, we focus on the similarity of hands; pairs of similar hand poses
originating from different samples, and propose a novel contrastive learning
method that embeds similar hand pairs closer in the latent space. Our
experiments demonstrate that our method outperforms conventional contrastive
learning approaches that produce positive pairs sorely from a single image with
data augmentation. We achieve significant improvements over the
state-of-the-art method in various datasets, with gains of 15% on FreiHand, 10%
on DexYCB, and 4% on AssemblyHands.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>HANDS@ECCV24 (Extended Abstracts)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELSA: Exploiting Layer-wise N:M Sparsity for Vision Transformer
  Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning-Chi Huang, Chi-Chih Chang, Wei-Cheng Lin, Endri Taka, Diana Marculescu, Kai-Chiang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $N{:}M$ sparsity is an emerging model compression method supported by more
and more accelerators to speed up sparse matrix multiplication in deep neural
networks. Most existing $N{:}M$ sparsity methods compress neural networks with
a uniform setting for all layers in a network or heuristically determine the
layer-wise configuration by considering the number of parameters in each layer.
However, very few methods have been designed for obtaining a layer-wise
customized $N{:}M$ sparse configuration for vision transformers (ViTs), which
usually consist of transformer blocks involving the same number of parameters.
In this work, to address the challenge of selecting suitable sparse
configuration for ViTs on $N{:}M$ sparsity-supporting accelerators, we propose
ELSA, Exploiting Layer-wise $N{:}M$ Sparsity for ViTs. Considering not only all
$N{:}M$ sparsity levels supported by a given accelerator but also the expected
throughput improvement, our methodology can reap the benefits of accelerators
supporting mixed sparsity by trading off negligible accuracy loss with both
memory usage and inference time reduction for ViT models. For instance, our
approach achieves a noteworthy 2.9$\times$ reduction in FLOPs for both Swin-B
and DeiT-B with only a marginal degradation of accuracy on ImageNet. Our code
will be released upon paper acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synergistic Spotting and Recognition of Micro-Expression via Temporal
  State Transition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bochao Zou, Zizheng Guo, Wenfeng Qin, Xin Li, Kangsheng Wang, Huimin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Micro-expressions are involuntary facial movements that cannot be consciously
controlled, conveying subtle cues with substantial real-world applications. The
analysis of micro-expressions generally involves two main tasks: spotting
micro-expression intervals in long videos and recognizing the emotions
associated with these intervals. Previous deep learning methods have primarily
relied on classification networks utilizing sliding windows. However, fixed
window sizes and window-level hard classification introduce numerous
constraints. Additionally, these methods have not fully exploited the potential
of complementary pathways for spotting and recognition. In this paper, we
present a novel temporal state transition architecture grounded in the state
space model, which replaces conventional window-level classification with
video-level regression. Furthermore, by leveraging the inherent connections
between spotting and recognition tasks, we propose a synergistic strategy that
enhances overall analysis performance. Extensive experiments demonstrate that
our method achieves state-of-the-art performance. The codes and pre-trained
models are available at https://github.com/zizheng-guo/ME-TST.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E-Commerce Inpainting with Mask Guidance in Controlnet for Reducing
  Overcompletion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guandong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  E-commerce image generation has always been one of the core demands in the
e-commerce field. The goal is to restore the missing background that matches
the main product given. In the post-AIGC era, diffusion models are primarily
used to generate product images, achieving impressive results. This paper
systematically analyzes and addresses a core pain point in diffusion model
generation: overcompletion, which refers to the difficulty in maintaining
product features. We propose two solutions: 1. Using an instance mask
fine-tuned inpainting model to mitigate this phenomenon; 2. Adopting a
train-free mask guidance approach, which incorporates refined product masks as
constraints when combining ControlNet and UNet to generate the main product,
thereby avoiding overcompletion of the product. Our method has achieved
promising results in practical applications and we hope it can serve as an
inspiring technical report in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reliable Multi-View Learning with Conformal Prediction for Aortic
  Stenosis Classification in Echocardiography <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Nan Gu, Michael Tsang, Hooman Vaseli, Teresa Tsang, Purang Abolmaesumi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fundamental problem with ultrasound-guided diagnosis is that the acquired
images are often 2-D cross-sections of a 3-D anatomy, potentially missing
important anatomical details. This limitation leads to challenges in ultrasound
echocardiography, such as poor visualization of heart valves or foreshortening
of ventricles. Clinicians must interpret these images with inherent
uncertainty, a nuance absent in machine learning's one-hot labels. We propose
Re-Training for Uncertainty (RT4U), a data-centric method to introduce
uncertainty to weakly informative inputs in the training set. This simple
approach can be incorporated to existing state-of-the-art aortic stenosis
classification methods to further improve their accuracy. When combined with
conformal prediction techniques, RT4U can yield adaptively sized prediction
sets which are guaranteed to contain the ground truth class to a high accuracy.
We validate the effectiveness of RT4U on three diverse datasets: a public
(TMED-2) and a private AS dataset, along with a CIFAR-10-derived toy dataset.
Results show improvement on all the datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone any post-submission improvements or
  corrections. The Version of Record of this contribution is published in:
  International Conference on Medical Image Computing and Computer-Assisted
  Intervention (MICCAI), Springer (2024) under the same title</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Methodological <span class="highlight-title">Survey</span> of Human Activity Recognition
  Across Divers Data Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungpil Shin, Najmul Hassan, Abu Saleh Musa Miah1, Satoshi Nishimura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Activity Recognition (HAR) systems aim to understand human behaviour
and assign a label to each action, attracting significant attention in computer
vision due to their wide range of applications. HAR can leverage various data
modalities, such as RGB images and video, skeleton, depth, infrared, point
cloud, event stream, audio, acceleration, and radar signals. Each modality
provides unique and complementary information suited to different application
scenarios. Consequently, numerous studies have investigated diverse approaches
for HAR using these modalities. This paper presents a comprehensive survey of
the latest advancements in HAR from 2014 to 2024, focusing on machine learning
(ML) and deep learning (DL) approaches categorized by input data modalities. We
review both single-modality and multi-modality techniques, highlighting
fusion-based and co-learning frameworks. Additionally, we cover advancements in
hand-crafted action features, methods for recognizing human-object
interactions, and activity detection. Our survey includes a detailed dataset
description for each modality and a summary of the latest HAR systems, offering
comparative results on benchmark datasets. Finally, we provide insightful
observations and propose effective future research directions in HAR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SITSMamba for Crop Classification based on Satellite Image Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09673v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09673v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolei Qin, Xin Su, Liangpei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Satellite image time series (SITS) data provides continuous observations over
time, allowing for the tracking of vegetation changes and growth patterns
throughout the seasons and years. Numerous deep learning (DL) approaches using
SITS for crop classification have emerged recently, with the latest approaches
adopting Transformer for SITS classification. However, the quadratic complexity
of self-attention in Transformer poses challenges for classifying long time
series. While the cutting-edge Mamba architecture has demonstrated strength in
various domains, including remote sensing image interpretation, its capacity to
learn temporal representations in SITS data remains unexplored. Moreover, the
existing SITS classification methods often depend solely on crop labels as
supervision signals, which fails to fully exploit the temporal information. In
this paper, we proposed a Satellite Image Time Series Mamba (SITSMamba) method
for crop classification based on remote sensing time series data. The proposed
SITSMamba contains a spatial encoder based on Convolutional Neural Networks
(CNN) and a Mamba-based temporal encoder. To exploit richer temporal
information from SITS, we design two branches of decoder used for different
tasks. The first branch is a crop Classification Branch (CBranch), which
includes a ConvBlock to decode the feature to a crop map. The second branch is
a SITS Reconstruction Branch that uses a Linear layer to transform the encoded
feature to predict the original input values. Furthermore, we design a
Positional Weight (PW) applied to the RBranch to help the model learn rich
latent knowledge from SITS. We also design two weighting factors to control the
balance of the two branches during training. The code of SITSMamba is available
at: https://github.com/XiaoleiQinn/SITSMamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Hyperspectral and Multispectral Image Blind Fusion Based on
  Deep Tucker Decomposition Network with Spatial-Spectral Manifold Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Wang, Yang Xu, Zebin Wu, Zhihui Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral and multispectral image fusion aims to generate high spectral
and spatial resolution hyperspectral images (HR-HSI) by fusing high-resolution
multispectral images (HR-MSI) and low-resolution hyperspectral images (LR-HSI).
However, existing fusion methods encounter challenges such as unknown
degradation parameters, incomplete exploitation of the correlation between
high-dimensional structures and deep image features. To overcome these issues,
in this article, an unsupervised blind fusion method for hyperspectral and
multispectral images based on Tucker decomposition and spatial spectral
manifold learning (DTDNML) is proposed. We design a novel deep Tucker
decomposition network that maps LR-HSI and HR-MSI into a consistent feature
space, achieving reconstruction through decoders with shared parameter. To
better exploit and fuse spatial-spectral features in the data, we design a core
tensor fusion network that incorporates a spatial spectral attention mechanism
for aligning and fusing features at different scales. Furthermore, to enhance
the capacity in capturing global information, a Laplacian-based
spatial-spectral manifold constraints is introduced in shared-decoders.
Sufficient experiments have validated that this method enhances the accuracy
and efficiency of hyperspectral and multispectral fusion on different remote
sensing datasets. The source code is available at
https://github.com/Shawn-H-Wang/DTDNML.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TNNLS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EditBoard: Towards A Comprehensive Evaluation Benchmark for Text-based
  Video Editing Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupeng Chen, Penglin Chen, Xiaoyu Zhang, Yixian Huang, Qian Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of diffusion models has significantly advanced
AI-generated content (AIGC), particularly in Text-to-Image (T2I) and
Text-to-Video (T2V) generation. Text-based video editing, leveraging these
generative capabilities, has emerged as a promising field, enabling precise
modifications to videos based on text prompts. Despite the proliferation of
innovative video editing models, there is a conspicuous lack of comprehensive
evaluation benchmarks that holistically assess these models' performance across
various dimensions. Existing evaluations are limited and inconsistent,
typically summarizing overall performance with a single score, which obscures
models' effectiveness on individual editing tasks. To address this gap, we
propose EditBoard, the first comprehensive evaluation benchmark for text-based
video editing models. EditBoard encompasses nine automatic metrics across four
dimensions, evaluating models on four task categories and introducing three new
metrics to assess fidelity. This task-oriented benchmark facilitates objective
evaluation by detailing model performance and providing insights into each
model's strengths and weaknesses. By open-sourcing EditBoard, we aim to
standardize evaluation and advance the development of robust video editing
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SparX: A Sparse Cross-Layer Connection Mechanism for Hierarchical Vision
  Mamba and Transformer Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Lou, Yunxiang Fu, Yizhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the capability of dynamic state space models (SSMs) in capturing
long-range dependencies with near-linear computational complexity, Mamba has
shown notable performance in NLP tasks. This has inspired the rapid development
of Mamba-based vision models, resulting in promising results in visual
recognition tasks. However, such models are not capable of distilling features
across layers through feature aggregation, interaction, and selection.
Moreover, existing cross-layer feature aggregation methods designed for CNNs or
ViTs are not practical in Mamba-based models due to high computational costs.
Therefore, this paper aims to introduce an efficient cross-layer feature
aggregation mechanism for Mamba-based vision backbone networks. Inspired by the
Retinal Ganglion Cells (RGCs) in the human visual system, we propose a new
sparse cross-layer connection mechanism termed SparX to effectively improve
cross-layer feature interaction and reuse. Specifically, we build two different
types of network layers: ganglion layers and normal layers. The former has
higher connectivity and complexity, enabling multi-layer feature aggregation
and interaction in an input-dependent manner. In contrast, the latter has lower
connectivity and complexity. By interleaving these two types of layers, we
design a new vision backbone network with sparsely cross-connected layers,
achieving an excellent trade-off among model size, computational cost, memory
cost, and accuracy in comparison to its counterparts. For instance, with fewer
parameters, SparX-Mamba-T improves the top-1 accuracy of VMamba-T from 82.5% to
83.5%, while SparX-Swin-T achieves a 1.3% increase in top-1 accuracy compared
to Swin-T. Extensive experimental results demonstrate that our new connection
mechanism possesses both superior performance and generalization capabilities
on various vision tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code will be publicly available at: https://github.com/LMMMEng/SparX</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Framework For Text Detection From Natural Scene Images With
  Complex Background 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Basavaraj Kaladagi, Jagadeesh Pujari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing texts from camera images is a known hard problem because of the
difficulties in text detection from the varied and complicated background. In
this paper we propose a novel and efficient method to detect text region from
images with complex background using Wavelet Transforms. The framework uses
Wavelet Transformation of the original image in its grayscale form followed by
Sub-band filtering. Then Region clustering technique is applied using centroids
of the regions, further Bounding box is fitted to each region thus identifying
the text regions. This method is much sophisticated and efficient than the
previous methods as it doesn't stick to a particular font size of the text
thus, making it generalized. The sample set used for experimental purpose
consists of 50 images with varying backgrounds. Images with edge prominence are
considered. Furthermore, our method can be easily customized for applications
with different scopes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models Grasp <span class="highlight-title">Event</span> Signals? Exploring Pure Zero-Shot
  <span class="highlight-title">Event</span>-based Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongyou Yu, Qiang Qu, Xiaoming Chen, Chen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in event-based zero-shot object recognition have
demonstrated promising results. However, these methods heavily depend on
extensive training and are inherently constrained by the characteristics of
CLIP. To the best of our knowledge, this research is the first study to explore
the understanding capabilities of large language models (LLMs) for event-based
visual content. We demonstrate that LLMs can achieve event-based object
recognition without additional training or fine-tuning in conjunction with
CLIP, effectively enabling pure zero-shot event-based recognition.
Particularly, we evaluate the ability of GPT-4o / 4turbo and two other
open-source LLMs to directly recognize event-based visual content. Extensive
experiments are conducted across three benchmark datasets, systematically
assessing the recognition accuracy of these models. The results show that LLMs,
especially when enhanced with well-designed prompts, significantly improve
event-based zero-shot recognition performance. Notably, GPT-4o outperforms the
compared models and exceeds the recognition accuracy of state-of-the-art
event-based zero-shot methods on N-ImageNet by five orders of magnitude. The
implementation of this paper is available at
\url{https://github.com/ChrisYu-Zz/Pure-event-based-recognition-based-LLM}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Weakly-Supervised Object Detection on Static Images through
  (Hallucinated) Motion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cagri Gungor, Adriana Kovashka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While motion has garnered attention in various tasks, its potential as a
modality for weakly-supervised object detection (WSOD) in static images remains
unexplored. Our study introduces an approach to enhance WSOD methods by
integrating motion information. This method involves leveraging hallucinated
motion from static images to improve WSOD on image datasets, utilizing a
Siamese network for enhanced representation learning with motion, addressing
camera motion through motion normalization, and selectively training images
based on object motion. Experimental validation on the COCO and YouTube-BB
datasets demonstrates improvements over a state-of-the-art method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Audio Narrations to Strengthen Domain Generalization in
  Multimodal First-Person Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cagri Gungor, Adriana Kovashka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  First-person activity recognition is rapidly growing due to the widespread
use of wearable cameras but faces challenges from domain shifts across
different environments, such as varying objects or background scenes. We
propose a multimodal framework that improves domain generalization by
integrating motion, audio, and appearance features. Key contributions include
analyzing the resilience of audio and motion features to domain shifts, using
audio narrations for enhanced audio-text alignment, and applying consistency
ratings between audio and visual narrations to optimize the impact of audio in
recognition during training. Our approach achieves state-of-the-art performance
on the ARGO1M dataset, effectively generalizing across unseen scenarios and
locations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TextureDiffusion: Target <span class="highlight-title">Prompt</span> Disentangled Editing for Various Texture
  Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Su, Junhao Zhuang, Chun Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, text-guided image editing has achieved significant success.
However, existing methods can only apply simple textures like wood or gold when
changing the texture of an object. Complex textures such as cloud or fire pose
a challenge. This limitation stems from that the target prompt needs to contain
both the input image content and <texture>, restricting the texture
representation. In this paper, we propose TextureDiffusion, a tuning-free image
editing method applied to various texture transfer. Initially, the target
prompt is directly set to "<texture>", making the texture disentangled from the
input image content to enhance texture representation. Subsequently, query
features in self-attention and features in residual blocks are utilized to
preserve the structure of the input image. Finally, to maintain the background,
we introduce an edit localization technique which blends the self-attention
results and the intermediate latents. Comprehensive experiments demonstrate
that TextureDiffusion can harmoniously transfer various textures with excellent
structure and background preservation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamMover: Leveraging the Prior of Diffusion Models for Image
  Interpolation with Large Motion <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liao Shen, Tianqi Liu, Huiqiang Sun, Xinyi Ye, Baopu Li, Jianming Zhang, Zhiguo Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of generating intermediate images from image pairs with
large motion while maintaining semantic consistency. Due to the large motion,
the intermediate semantic information may be absent in input images. Existing
methods either limit to small motion or focus on topologically similar objects,
leading to artifacts and inconsistency in the interpolation results. To
overcome this challenge, we delve into pre-trained image diffusion models for
their capabilities in semantic cognition and representations, ensuring
consistent expression of the absent intermediate semantic representations with
the input. To this end, we propose DreamMover, a novel image interpolation
framework with three main components: 1) A natural flow estimator based on the
diffusion model that can implicitly reason about the semantic correspondence
between two images. 2) To avoid the loss of detailed information during fusion,
our key insight is to fuse information in two parts, high-level space and
low-level space. 3) To enhance the consistency between the generated images and
input, we propose the self-attention concatenation and replacement approach.
Lastly, we present a challenging benchmark dataset InterpBench to evaluate the
semantic consistency of generated results. Extensive experiments demonstrate
the effectiveness of our method. Our project is available at
https://dreamm0ver.github.io .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Shot Learning for Pose-Guided Person Image Synthesis in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Fan, Tao Chen, Mingjie Wang, Rui Ma, Qiang Tang, Zili Yi, Qian Wang, Liang Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Pose-Guided Person Image Synthesis (PGPIS) methods depend heavily on
large amounts of labeled triplet data to train the generator in a supervised
manner. However, they often falter when applied to in-the-wild samples,
primarily due to the distribution gap between the training datasets and
real-world test samples. While some researchers aim to enhance model
generalizability through sophisticated training procedures, advanced
architectures, or by creating more diverse datasets, we adopt the test-time
fine-tuning paradigm to customize a pre-trained Text2Image (T2I) model.
However, naively applying test-time tuning results in inconsistencies in facial
identities and appearance attributes. To address this, we introduce a Visual
Consistency Module (VCM), which enhances appearance consistency by combining
the face, text, and image embedding. Our approach, named OnePoseTrans, requires
only a single source image to generate high-quality pose transfer results,
offering greater stability than state-of-the-art data-driven methods. For each
test case, OnePoseTrans customizes a model in around 48 seconds with an NVIDIA
V100 GPU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLCONet: Learning Multi-source Perception Representation for Camouflaged
  Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanguang Sun, Hanyu Xuan, Jian Yang, Lei Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, biological perception has been a powerful tool for handling the
camouflaged object detection (COD) task. However, most existing methods are
heavily dependent on the local spatial information of diverse scales from
convolutional operations to optimize initial features. A commonly neglected
point in these methods is the long-range dependencies between feature pixels
from different scale spaces that can help the model build a global structure of
the object, inducing a more precise image representation. In this paper, we
propose a novel Global-Local Collaborative Optimization Network, called
GLCONet. Technically, we first design a collaborative optimization strategy
from the perspective of multi-source perception to simultaneously model the
local details and global long-range relationships, which can provide features
with abundant discriminative information to boost the accuracy in detecting
camouflaged objects. Furthermore, we introduce an adjacent reverse decoder that
contains cross-layer aggregation and reverse optimization to integrate
complementary information from different levels for generating high-quality
representations. Extensive experiments demonstrate that the proposed GLCONet
method with different backbones can effectively activate potentially
significant pixels in an image, outperforming twenty state-of-the-art methods
on three public COD datasets. The source code is available at:
\https://github.com/CSYSI/GLCONet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TNNLS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NEVLP: Noise-Robust Framework for Efficient Vision-Language <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyi Tao, Zhuoyue Wang, Hang Zhang, Lun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of Vision Language Models (VLMs) on various vision-language tasks
heavily relies on pre-training with large scale web-crawled datasets. However,
the noisy and incomplete nature of web data makes dataset scale crucial for
performance, rendering end-to-end training increasingly prohibitive. In this
paper, we propose NEVLP, a noise-robust framework for efficient vision-language
pre-training that requires less pre-training data. Specifically, we bridge the
modality gap between a frozen image encoder and a large language model with a
transformer and introduce two innovative learning strategies: noise-adaptive
learning and concept-enhanced learning to mitigate the impact of noise. In
noise-adaptive learning, we estimate the noise probability of each image-text
pair based on the transformer's memorization effect and employ noise-adaptive
regularization on image-text contrastive learning to condition cross-modal
alignment. In concept-enhanced learning, we enrich incomplete text by
incorporating visual concepts (objects in the image) to provide prior
information about existing objects for image-text matching and image-grounded
text generation, thereby mitigating text incompletion. Our framework
effectively utilizes noisy web data and achieves state-of-the-art performance
with less pre-training data across a wide range of vision-language tasks,
including image-text retrieval, image captioning, and visual question
answering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bias Begets Bias: The Impact of Biased Embeddings on Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahil Kuchlous, Marvin Li, Jeffrey G. Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing adoption of Text-to-Image (TTI) systems, the social biases
of these models have come under increased scrutiny. Herein we conduct a
systematic investigation of one such source of bias for diffusion models:
embedding spaces. First, because traditional classifier-based fairness
definitions require true labels not present in generative modeling, we propose
statistical group fairness criteria based on a model's internal representation
of the world. Using these definitions, we demonstrate theoretically and
empirically that an unbiased text embedding space for input prompts is a
necessary condition for representationally balanced diffusion models, meaning
the distribution of generated images satisfy diversity requirements with
respect to protected attributes. Next, we investigate the impact of biased
embeddings on evaluating the alignment between generated images and prompts, a
process which is commonly used to assess diffusion models. We find that biased
multimodal embeddings like CLIP can result in lower alignment scores for
representationally balanced TTI models, thus rewarding unfair behavior.
Finally, we develop a theoretical framework through which biases in alignment
evaluation can be studied and propose bias mitigation methods. By specifically
adapting the perspective of embedding spaces, we establish new fairness
conditions for diffusion model development and evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Transferable Features for Implicit Neural Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kushal Vyas, Ahmed Imtiaz Humayun, Aniket Dashpute, Richard G. Baraniuk, Ashok Veeraraghavan, Guha Balakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit neural representations (INRs) have demonstrated success in a variety
of applications, including inverse problems and neural rendering. An INR is
typically trained to capture one signal of interest, resulting in learned
neural features that are highly attuned to that signal. Assumed to be less
generalizable, we explore the aspect of transferability of such learned neural
features for fitting similar signals. We introduce a new INR training
framework, STRAINER that learns transferrable features for fitting INRs to new
signals from a given distribution, faster and with better reconstruction
quality. Owing to the sequential layer-wise affine operations in an INR, we
propose to learn transferable representations by sharing initial encoder layers
across multiple INRs with independent decoder layers. At test time, the learned
encoder representations are transferred as initialization for an otherwise
randomly initialized INR. We find STRAINER to yield extremely powerful
initialization for fitting images from the same domain and allow for $\approx
+10dB$ gain in signal quality early on compared to an untrained INR itself.
STRAINER also provides a simple way to encode data-driven priors in INRs. We
evaluate STRAINER on multiple in-domain and out-of-domain signal fitting tasks
and inverse problems and further provide detailed analysis and discussion on
the transferability of STRAINER's features. Our demo can be accessed at
https://colab.research.google.com/drive/1fBZAwqE8C_lrRPAe-hQZJTWrMJuAKtG2?usp=sharing .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dawei Yan, Pengcheng Li, Yang Li, Hao Chen, Qingguo Chen, Weihua Luo, Wei Dong, Qingsen Yan, Haokui Zhang, Chunhua Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, inspired by the success of vision-language models (VLMs), an
increasing number of researchers are focusing on improving VLMs and have
achieved promising results. However, most existing methods concentrate on
optimizing the connector and enhancing the language model component, while
neglecting improvements to the vision encoder itself. In contrast, we propose
Text Guided LLaVA (TG-LLaVA) in this paper, which optimizes VLMs by guiding the
vision encoder with text, offering a new and orthogonal optimization direction.
Specifically, inspired by the purpose-driven logic inherent in human behavior,
we use learnable latent embeddings as a bridge to analyze textual instruction
and add the analysis results to the vision encoder as guidance, refining it.
Subsequently, another set of latent embeddings extracts additional detailed
text-guided information from high-resolution local patches as auxiliary
information. Finally, with the guidance of text, the vision encoder can extract
text-related features, similar to how humans focus on the most relevant parts
of an image when considering a question. This results in generating better
answers. Experiments on various datasets validate the effectiveness of the
proposed method. Remarkably, without the need for additional training data, our
propsoed method can bring more benefits to the baseline (LLaVA-1.5) compared
with other concurrent methods. Furthermore, the proposed method consistently
brings improvement in different settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car
  Damage Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04743v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04743v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teerapong Panboonyuen, Naphat Nithisopa, Panin Pienroj, Laphonchai Jirachuphun, Chaiwasut Watthanasirikrit, Naruepon Pornwiriyakul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating car damages from misfortune is critical to the car insurance
industry. However, the accuracy is still insufficient for real-world
applications since the deep learning network is not designed for car damage
images as inputs, and its segmented masks are still very coarse. This paper
presents MARS (Mask Attention Refinement with Sequential quadtree nodes) for
car damage instance segmentation. Our MARS represents self-attention mechanisms
to draw global dependencies between the sequential quadtree nodes layer and
quadtree transformer to recalibrate channel weights and predict highly accurate
instance masks. Our extensive experiments demonstrate that MARS outperforms
state-of-the-art (SOTA) instance segmentation methods on three popular
benchmarks such as Mask R-CNN [9], PointRend [13], and Mask Transfiner [12], by
a large margin of +1.3 maskAP-based R50-FPN backbone and +2.3 maskAP-based
R101-FPN backbone on Thai car-damage dataset. Our demos are available at
https://github.com/kaopanboonyuen/MARS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages. arXiv admin note: substantial text overlap with
  arXiv:2111.13673 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Layout Agnostic Scene Text Image Synthesis with Diffusion Models <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01062v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01062v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qilong Zhangli, Jindong Jiang, Di Liu, Licheng Yu, Xiaoliang Dai, Ankit Ramchandani, Guan Pang, Dimitris N. Metaxas, Praveen Krishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While diffusion models have significantly advanced the quality of image
generation their capability to accurately and coherently render text within
these images remains a substantial challenge. Conventional diffusion-based
methods for scene text generation are typically limited by their reliance on an
intermediate layout output. This dependency often results in a constrained
diversity of text styles and fonts an inherent limitation stemming from the
deterministic nature of the layout generation phase. To address these
challenges this paper introduces SceneTextGen a novel diffusion-based model
specifically designed to circumvent the need for a predefined layout stage. By
doing so SceneTextGen facilitates a more natural and varied representation of
text. The novelty of SceneTextGen lies in its integration of three key
components: a character-level encoder for capturing detailed typographic
properties coupled with a character-level instance segmentation model and a
word-level spotting model to address the issues of unwanted text generation and
minor character inaccuracies. We validate the performance of our method by
demonstrating improved character recognition rates on generated images across
different public visual text datasets in comparison to both standard diffusion
based methods and text specific methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR), 2024, pp. 7496-7506</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoCOCO: Robustness Benchmark of MS-COCO to Stress-test Image-Text
  Matching Models <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10727v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10727v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seulki Park, Daeho Um, Hajung Yoon, Sanghyuk Chun, Sangdoo Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the extensive use of vision-language models in various downstream tasks,
evaluating their robustness is crucial. In this paper, we propose a benchmark
for assessing the robustness of vision-language models. We believe that a
robust model should properly understand both linguistic and visual semantics
and be resilient to explicit variations. In pursuit of this goal, we create new
variants of texts and images in the MS-COCO test set and re-evaluate the
state-of-the-art (SOTA) models with the new data. Specifically, we alter the
meaning of text by replacing a word, and generate visually altered images that
maintain some visual context while introducing noticeable pixel changes through
image mixing techniques.Our evaluations on the proposed benchmark reveal
substantial performance degradation in many SOTA models (e.g., Image-to-Text
Recall@1: 81.9\% $\rightarrow$ 48.4\% in BLIP, 66.1\% $\rightarrow$ 37.6\% in
VSE$\infty$), with the models often favoring the altered texts/images over the
original ones. This indicates the current vision-language models struggle with
subtle changes and often fail to understand the overall context of texts and
images. Based on these findings, we propose semantic contrastive loss and
visual contrastive loss to learn more robust embedding. Datasets and code are
available at {\url{https://github.com/pseulki/rococo}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV Synthetic Data for Computer Vision Workshop (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatial Transformer Network YOLO Model for Agricultural Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Zambre, Ekdev Rajkitkul, Akshatha Mohan, Joshua Peeples
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection plays a crucial role in the field of computer vision by
autonomously locating and identifying objects of interest. The You Only Look
Once (YOLO) model is an effective single-shot detector. However, YOLO faces
challenges in cluttered or partially occluded scenes and can struggle with
small, low-contrast objects. We propose a new method that integrates spatial
transformer networks (STNs) into YOLO to improve performance. The proposed
STN-YOLO aims to enhance the model's effectiveness by focusing on important
areas of the image and improving the spatial invariance of the model before the
detection process. Our proposed method improved object detection performance
both qualitatively and quantitatively. We explore the impact of different
localization networks within the STN module as well as the robustness of the
model across different spatial transformations. We apply the STN-YOLO on
benchmark datasets for Agricultural object detection as well as a new dataset
from a state-of-the-art plant phenotyping greenhouse facility. Our code and
dataset are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, accepted to 2024 IEEE International Conference on
  Machine Learning and Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Style Transfer: From Stitching to Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhe Xu, Zhuoer Wang, Yihan Zhang, Yizhou Liu, Zhaoyue Wang, Zhihao Xu, Muhan Zhao, Huaiying Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article compares two style transfer methods in image processing: the
traditional method, which synthesizes new images by stitching together small
patches from existing images, and a modern machine learning-based approach that
uses a segmentation network to isolate foreground objects and apply style
transfer solely to the background. The traditional method excels in creating
artistic abstractions but can struggle with seamlessness, whereas the machine
learning method preserves the integrity of foreground elements while enhancing
the background, offering improved aesthetic quality and computational
efficiency. Our study indicates that machine learning-based methods are more
suited for real-world applications where detail preservation in foreground
elements is essential.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Neural Networks for 2D MRI Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14875v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14875v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lohith Konathala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification is vital for safety-critical Deep Learning
applications like medical image segmentation. We introduce BA U-Net, an
uncertainty-aware model for MRI segmentation that integrates Bayesian Neural
Networks with Attention Mechanisms. BA U-Net delivers accurate, interpretable
results, crucial for reliable pathology screening. Evaluated on BraTS 2020,
this model addresses the critical need for confidence estimation in deep
learning-based medical imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, conference-paper style</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenSU3D: Open World 3D Scene Understanding using Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14279v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14279v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafay Mohiuddin, Sai Manoj Prakhya, Fiona Collins, Ziyuan Liu, André Borrmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel, scalable approach for constructing open
set, instance-level 3D scene representations, advancing open world
understanding of 3D environments. Existing methods require pre-constructed 3D
scenes and face scalability issues due to per-point feature vector learning,
limiting their efficacy with complex queries. Our method overcomes these
limitations by incrementally building instance-level 3D scene representations
using 2D foundation models, efficiently aggregating instance-level details such
as masks, feature vectors, names, and captions. We introduce fusion schemes for
feature vectors to enhance their contextual knowledge and performance on
complex queries. Additionally, we explore large language models for robust
automatic annotation and spatial reasoning tasks. We evaluate our proposed
approach on multiple scenes from ScanNet and Replica datasets demonstrating
zero-shot generalization capabilities, exceeding current state-of-the-art
methods in open world 3D scene understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://opensu3d.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mismatched: Evaluating the Limits of Image Matching Approaches and
  Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sierra Bonilla, Chiara Di Vece, Rema Daher, Xinwei Ju, Danail Stoyanov, Francisco Vasconcelos, Sophia Bano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Three-dimensional (3D) reconstruction from two-dimensional images is an
active research field in computer vision, with applications ranging from
navigation and object tracking to segmentation and three-dimensional modeling.
Traditionally, parametric techniques have been employed for this task. However,
recent advancements have seen a shift towards learning-based methods. Given the
rapid pace of research and the frequent introduction of new image matching
methods, it is essential to evaluate them. In this paper, we present a
comprehensive evaluation of various image matching methods using a
structure-from-motion pipeline. We assess the performance of these methods on
both in-domain and out-of-domain datasets, identifying key limitations in both
the methods and benchmarks. We also investigate the impact of edge detection as
a pre-processing step. Our analysis reveals that image matching for 3D
reconstruction remains an open challenge, necessitating careful selection and
tuning of models for specific scenarios, while also highlighting mismatches in
how metrics currently represent method performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COBRA -- COnfidence score Based on shape Regression Analysis for
  method-independent quality assessment of object pose estimation from single
  images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16471v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16471v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panagiotis Sapoutzoglou, George Giapitzakis, George Terzakis, Maria Pateraki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a generic algorithm for scoring pose estimation methods that rely
on single image semantic analysis. The algorithm employs a lightweight putative
shape representation using a combination of multiple Gaussian Processes. Each
Gaussian Process (GP) yields distance normal distributions from multiple
reference points in the object's coordinate system to its surface, thus
providing a geometric evaluation framework for scoring predicted poses. Our
confidence measure comprises the average mixture probability of pixel
back-projections onto the shape template. In the reported experiments, we
compare the accuracy of our GP based representation of objects versus the
actual geometric models and demonstrate the ability of our method to capture
the influence of outliers as opposed to the corresponding intrinsic measures
that ship with the segmentation and pose estimation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View
  Planning <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicong Pan, Liren Jin, Xuying Huang, Cyrill Stachniss, Marija Popović, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object reconstruction is relevant for many autonomous robotic tasks that
require interaction with the environment. A key challenge in such scenarios is
planning view configurations to collect informative measurements for
reconstructing an initially unknown object. One-shot view planning enables
efficient data collection by predicting view configurations and planning the
globally shortest path connecting all views at once. However, prior knowledge
about the object is required to conduct one-shot view planning. In this work,
we propose a novel one-shot view planning approach that utilizes the powerful
3D generation capabilities of diffusion models as priors. By incorporating such
geometric priors into our pipeline, we achieve effective one-shot view planning
starting with only a single RGB image of the object to be reconstructed. Our
planning experiments in simulation and real-world setups indicate that our
approach balances well between object reconstruction quality and movement cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Sicong Pan and Liren Jin have equal contribution. Publication to
  appear in IEEE/RSJ International Conference on Intelligent Robots and Systems
  (IROS), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MpoxMamba: A Grouped Mamba-based Lightweight Hybrid Network for Mpox
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04218v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04218v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubiao Yue, Jun Xue, Haihuang Liang, Zhenzhang Li, Yufeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the lack of effective mpox detection tools, the mpox virus continues
to spread worldwide and has once again been declared a public health emergency
of international concern by the World Health Organization. Lightweight deep
learning model-based detection systems are crucial to alleviate mpox outbreaks
since they are suitable for widespread deployment, especially in
resource-limited scenarios. However, the key to its successful application
depends on ensuring that the model can effectively model local features and
long-range dependencies in mpox lesions while maintaining lightweight. Inspired
by the success of Mamba in modeling long-range dependencies and its linear
complexity, we proposed a lightweight hybrid architecture called MpoxMamba for
efficient mpox detection. MpoxMamba utilizes depth-wise separable convolutions
to extract local feature representations in mpox skin lesions and greatly
enhances the model's ability to model the global contextual information by
grouped Mamba modules. Notably, MpoxMamba's parameter size and FLOPs are 0.77M
and 0.53G, respectively. Experimental results on two widely recognized
benchmark datasets demonstrate that MpoxMamba outperforms state-of-the-art
lightweight models and existing mpox detection methods. Importantly, we
developed a web-based online application to provide free mpox detection
(http://5227i971s5.goho.co:30290). The source codes of MpoxMamba are available
at https://github.com/YubiaoYue/MpoxMamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoboSense: Large-scale <span class="highlight-title">Dataset</span> and Benchmark for Multi-sensor Low-speed
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15503v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15503v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haisheng Su, Feixiang Song, Cong Ma, Wei Wu, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust object detection and tracking under arbitrary sight of view is
challenging yet essential for the development of Autonomous Vehicle technology.
With the growing demand of unmanned function vehicles, near-field scene
understanding becomes an important research topic in the areas of low-speed
autonomous driving. Due to the complexity of driving conditions and diversity
of near obstacles such as blind spots and high occlusion, the perception
capability of near-field environment is still inferior than its farther
counterpart. To further enhance the intelligent ability of unmanned vehicles,
in this paper, we construct a multimodal data collection platform based on 3
main types of sensors (Camera, LiDAR and Fisheye), which supports flexible
sensor configurations to enable dynamic sight of view for ego vehicle, either
global view or local view. Meanwhile, a large-scale multi-sensor dataset is
built, named RoboSense, to facilitate near-field scene understanding. RoboSense
contains more than 133K synchronized data with 1.4M 3D bounding box and IDs
annotated in the full $360^{\circ}$ view, forming 216K trajectories across 7.6K
temporal sequences. It has $270\times$ and $18\times$ as many annotations of
near-field obstacles within 5$m$ as the previous single-vehicle datasets such
as KITTI and nuScenes. Moreover, we define a novel matching criterion for
near-field 3D perception and prediction metrics. Based on RoboSense, we
formulate 6 popular tasks to facilitate the future development of related
research, where the detailed data analysis as well as benchmarks are also
provided accordingly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Space3D-Bench: Spatial 3D Question Answering Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16662v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16662v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emilia Szymanska, Mihai Dusmanu, Jan-Willem Buurlage, Mahdi Rad, Marc Pollefeys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering questions about the spatial properties of the environment poses
challenges for existing language and vision foundation models due to a lack of
understanding of the 3D world notably in terms of relationships between
objects. To push the field forward, multiple 3D Q&A datasets were proposed
which, overall, provide a variety of questions, but they individually focus on
particular aspects of 3D reasoning or are limited in terms of data modalities.
To address this, we present Space3D-Bench - a collection of 1000 general
spatial questions and answers related to scenes of the Replica dataset which
offers a variety of data modalities: point clouds, posed RGB-D images,
navigation meshes and 3D object detections. To ensure that the questions cover
a wide range of 3D objectives, we propose an indoor spatial questions taxonomy
inspired by geographic information systems and use it to balance the dataset
accordingly. Moreover, we provide an assessment system that grades natural
language responses based on predefined ground-truth answers by leveraging a
Vision Language Model's comprehension of both text and images to compare the
responses with ground-truth textual information or relevant visual data.
Finally, we introduce a baseline called RAG3D-Chat integrating the world
understanding of foundation models with rich context retrieval, achieving an
accuracy of 67% on the proposed dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Shot is Enough for Sequential Infrared Small Target Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingbing Dan, Meihui Li, Tao Tang, Jing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infrared small target sequences exhibit strong similarities between frames
and contain rich contextual information, which motivates us to achieve
sequential infrared small target segmentation (IRSTS) with minimal data.
Inspired by the success of Segment Anything Model (SAM) across various
downstream tasks, we propose a one-shot and training-free method that perfectly
adapts SAM's zero-shot generalization capability to sequential IRSTS.
Specifically, we first obtain a confidence map through local feature matching
(LFM). The highest point in the confidence map is used as the prompt to replace
the manual prompt. Then, to address the over-segmentation issue caused by the
domain gap, we design the point prompt-centric focusing (PPCF) module.
Subsequently, to prevent miss and false detections, we introduce the
triple-level ensemble (TLE) module to produce the final mask. Experiments
demonstrate that our method requires only one shot to achieve comparable
performance to state-of-the-art IRSTS methods and significantly outperforms
other one-shot segmentation methods. Moreover, ablation studies confirm the
robustness of our method in the type of annotations and the selection of
reference images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoBEV: Elevating Roadside 3D Object Detection with Depth and Height
  Complementarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02815v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02815v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Shi, Chengshan Pang, Jiaming Zhang, Kailun Yang, Yuhao Wu, Huajian Ni, Yining Lin, Rainer Stiefelhagen, Kaiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Roadside camera-driven 3D object detection is a crucial task in intelligent
transportation systems, which extends the perception range beyond the
limitations of vision-centric vehicles and enhances road safety. While previous
studies have limitations in using only depth or height information, we find
both depth and height matter and they are in fact complementary. The depth
feature encompasses precise geometric cues, whereas the height feature is
primarily focused on distinguishing between various categories of height
intervals, essentially providing semantic context. This insight motivates the
development of Complementary-BEV (CoBEV), a novel end-to-end monocular 3D
object detection framework that integrates depth and height to construct robust
BEV representations. In essence, CoBEV estimates each pixel's depth and height
distribution and lifts the camera features into 3D space for lateral fusion
using the newly proposed two-stage complementary feature selection (CFS)
module. A BEV feature distillation framework is also seamlessly integrated to
further enhance the detection accuracy from the prior knowledge of the
fusion-modal CoBEV teacher. We conduct extensive experiments on the public 3D
detection benchmarks of roadside camera-based DAIR-V2X-I and Rope3D, as well as
the private Supremind-Road dataset, demonstrating that CoBEV not only achieves
the accuracy of the new state-of-the-art, but also significantly advances the
robustness of previous methods in challenging long-distance scenarios and noisy
camera disturbance, and enhances generalization by a large margin in
heterologous settings with drastic changes in scene and camera parameters. For
the first time, the vehicle AP score of a camera model reaches 80% on
DAIR-V2X-I in terms of easy mode. The source code will be made publicly
available at https://github.com/MasterHow/CoBEV.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Image Processing (TIP). The source
  code will be made publicly available at https://github.com/MasterHow/CoBEV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge-enhanced Visual-Language <span class="highlight-title">Pretrain</span>ing for Computational
  Pathology <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Zhou, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Weidi Xie, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the problem of visual representation learning for
computational pathology, by exploiting large-scale image-text pairs gathered
from public resources, along with the domain-specific knowledge in pathology.
Specifically, we make the following contributions: (i) We curate a pathology
knowledge tree that consists of 50,470 informative attributes for 4,718
diseases requiring pathology diagnosis from 32 human tissues. To our knowledge,
this is the first comprehensive structured pathology knowledge base; (ii) We
develop a knowledge-enhanced visual-language pretraining approach, where we
first project pathology-specific knowledge into latent embedding space via a
language model, and use it to guide the visual representation learning; (iii)
We conduct thorough experiments to validate the effectiveness of our proposed
components, demonstrating significant performance improvement on various
downstream tasks, including cross-modal retrieval, zero-shot classification on
pathology patches, and zero-shot tumor subtyping on whole slide images (WSIs).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV2024(Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transformer-based stereo-aware 3D object detection from binocular images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11906v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11906v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Sun, Yanwei Pang, Jiale Cao, Jin Xie, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have shown promising progress in various visual object detection
tasks, including monocular 2D/3D detection and surround-view 3D detection. More
importantly, the attention mechanism in the Transformer model and the 3D
information extraction in binocular stereo are both similarity-based. However,
directly applying existing Transformer-based detectors to binocular stereo 3D
object detection leads to slow convergence and significant precision drops. We
argue that a key cause of that defect is that existing Transformers ignore the
binocular-stereo-specific image correspondence information. In this paper, we
explore the model design of Transformers in binocular 3D object detection,
focusing particularly on extracting and encoding task-specific image
correspondence information. To achieve this goal, we present TS3D, a
Transformer-based Stereo-aware 3D object detector. In the TS3D, a
Disparity-Aware Positional Encoding (DAPE) module is proposed to embed the
image correspondence information into stereo features. The correspondence is
encoded as normalized sub-pixel-level disparity and is used in conjunction with
sinusoidal 2D positional encoding to provide the 3D location information of the
scene. To enrich multi-scale stereo features, we propose a Stereo Preserving
Feature Pyramid Network (SPFPN). The SPFPN is designed to preserve the
correspondence information while fusing intra-scale and aggregating cross-scale
stereo features. Our proposed TS3D achieves a 41.29% Moderate Car detection
average precision on the KITTI test set and takes 88 ms to detect objects from
each binocular image pair. It is competitive with advanced counterparts in
terms of both precision and inference speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE T-ITS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenFace: A Large-Scale Fine-Grained Face Forgery Benchmark and Cross
  Appearance-Edge Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02003v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02003v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaning Zhang, Zitong Yu, Tianyi Wang, Xiaobin Huang, Linlin Shen, Zan Gao, Jianfeng Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of photorealistic generators has reached a critical
juncture where the discrepancy between authentic and manipulated images is
increasingly indistinguishable. Thus, benchmarking and advancing techniques
detecting digital manipulation become an urgent issue. Although there have been
a number of publicly available face forgery datasets, the forgery faces are
mostly generated using GAN-based synthesis technology, which does not involve
the most recent technologies like diffusion. The diversity and quality of
images generated by diffusion models have been significantly improved and thus
a much more challenging face forgery dataset shall be used to evaluate SOTA
forgery detection literature. In this paper, we propose a large-scale, diverse,
and fine-grained high-fidelity dataset, namely GenFace, to facilitate the
advancement of deepfake detection, which contains a large number of forgery
faces generated by advanced generators such as the diffusion-based model and
more detailed labels about the manipulation approaches and adopted generators.
In addition to evaluating SOTA approaches on our benchmark, we design an
innovative cross appearance-edge learning (CAEL) detector to capture
multi-grained appearance and edge global representations, and detect
discriminative and general forgery traces. Moreover, we devise an
appearance-edge cross-attention (AECA) module to explore the various
integrations across two domains. Extensive experiment results and
visualizations show that our detection model outperforms the state of the arts
on different settings like cross-generator, cross-forgery, and cross-dataset
evaluations. Code and datasets will be available at
\url{https://github.com/Jenine-321/GenFace
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Information Forensics and Security</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EEPPR: <span class="highlight-title">Event</span>-based Estimation of Periodic Phenomena Rate using
  Correlation in 3D 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06899v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06899v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Kolář, Radim Špetlík, Jiří Matas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel method for measuring the rate of periodic phenomena (e.g.,
rotation, flicker, and vibration), by an event camera, a device asynchronously
reporting brightness changes at independently operating pixels with high
temporal resolution. The approach assumes that for a periodic phenomenon, a
highly similar set of events is generated within a spatio-temporal window at a
time difference corresponding to its period. The sets of similar events are
detected by a correlation in the spatio-temporal event stream space. The
proposed method, EEPPR, is evaluated on a dataset of 12 sequences of periodic
phenomena, i.e. flashing light and vibration, and periodic motion, e.g.,
rotation, ranging from 3.2 Hz to 2 kHz (equivalent to 192 - 120 000 RPM). EEPPR
significantly outperforms published methods on this dataset, achieving a mean
relative error of 0.1%, setting new state-of-the-art. The dataset and codes are
publicly available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figues, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.13560v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.13560v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaohu Xing, Tian Ye, Yijun Yang, Guang Liu, Lei Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Transformer architecture has shown a remarkable ability in modeling
global relationships. However, it poses a significant computational challenge
when processing high-dimensional medical images. This hinders its development
and widespread adoption in this task. Mamba, as a State Space Model (SSM),
recently emerged as a notable manner for long-range dependencies in sequential
modeling, excelling in natural language processing filed with its remarkable
memory efficiency and computational speed. Inspired by its success, we
introduce SegMamba, a novel 3D medical image \textbf{Seg}mentation
\textbf{Mamba} model, designed to effectively capture long-range dependencies
within whole volume features at every scale. Our SegMamba, in contrast to
Transformer-based methods, excels in whole volume feature modeling from a state
space model standpoint, maintaining superior processing speed, even with volume
features at a resolution of {$64\times 64\times 64$}. Comprehensive experiments
on the BraTS2023 dataset demonstrate the effectiveness and efficiency of our
SegMamba. The code for SegMamba is available at:
https://github.com/ge-xing/SegMamba
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code has released</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dreaming is All You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01633v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01633v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingze Ni, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In classification tasks, achieving a harmonious balance between exploration
and precision is of paramount importance. To this end, this research introduces
two novel deep learning models, SleepNet and DreamNet, to strike this balance.
SleepNet seamlessly integrates supervised learning with unsupervised ``sleep"
stages using pre-trained encoder models. Dedicated neurons within SleepNet are
embedded in these unsupervised features, forming intermittent ``sleep" blocks
that facilitate exploratory learning. Building upon the foundation of SleepNet,
DreamNet employs full encoder-decoder frameworks to reconstruct the hidden
states, mimicking the human "dreaming" process. This reconstruction process
enables further exploration and refinement of the learned representations.
Moreover, the principle ideas of our SleepNet and DreamNet are generic and can
be applied to both computer vision and natural language processing downstream
tasks. Through extensive empirical evaluations on diverse image and text
datasets, SleepNet and DreanNet have demonstrated superior performance compared
to state-of-the-art models, showcasing the strengths of unsupervised
exploration and supervised precision afforded by our innovative approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Traffic Object Detection in Variable Illumination with
  RGB-<span class="highlight-title">Event</span> Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00436v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00436v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanwen Liu, Nan Yang, Yang Wang, Yuke Li, Xiangmo Zhao, Fei-Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic object detection under variable illumination is challenging due to
the information loss caused by the limited dynamic range of conventional
frame-based cameras. To address this issue, we introduce bio-inspired event
cameras and propose a novel Structure-aware Fusion Network (SFNet) that
extracts sharp and complete object structures from the event stream to
compensate for the lost information in images through cross-modality fusion,
enabling the network to obtain illumination-robust representations for traffic
object detection. Specifically, to mitigate the sparsity or blurriness issues
arising from diverse motion states of traffic objects in fixed-interval event
sampling methods, we propose the Reliable Structure Generation Network (RSGNet)
to generate Speed Invariant Frames (SIF), ensuring the integrity and sharpness
of object structures. Next, we design a novel Adaptive Feature Complement
Module (AFCM) which guides the adaptive fusion of two modality features to
compensate for the information loss in the images by perceiving the global
lightness distribution of the images, thereby generating illumination-robust
representations. Finally, considering the lack of large-scale and high-quality
annotations in the existing event-based object detection datasets, we build a
DSEC-Det dataset, which consists of 53 sequences with 63,931 images and more
than 208,000 labels for 8 classes. Extensive experimental results demonstrate
that our proposed SFNet can overcome the perceptual boundaries of conventional
cameras and outperform the frame-based method by 8.0% in mAP50 and 5.9% in
mAP50:95. Our code and dataset will be available at
https://github.com/YN-Yang/SFNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE T-ITS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PMT: Progressive Mean Teacher via Exploring Temporal Consistency for
  Semi-Supervised Medical Image Segmentation <span class="chip">ECCV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Gao, Sanping Zhou, Le Wang, Nanning Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning has emerged as a widely adopted technique in the
field of medical image segmentation. The existing works either focuses on the
construction of consistency constraints or the generation of pseudo labels to
provide high-quality supervisory signals, whose main challenge mainly comes
from how to keep the continuous improvement of model capabilities. In this
paper, we propose a simple yet effective semi-supervised learning framework,
termed Progressive Mean Teachers (PMT), for medical image segmentation, whose
goal is to generate high-fidelity pseudo labels by learning robust and diverse
features in the training process. Specifically, our PMT employs a standard mean
teacher to penalize the consistency of the current state and utilizes two sets
of MT architectures for co-training. The two sets of MT architectures are
individually updated for prolonged periods to maintain stable model diversity
established through performance gaps generated by iteration differences.
Additionally, a difference-driven alignment regularizer is employed to expedite
the alignment of lagging models with the representation capabilities of leading
models. Furthermore, a simple yet effective pseudo-label filtering algorithm is
employed for facile evaluation of models and selection of high-fidelity
pseudo-labels outputted when models are operating at high performance for
co-training purposes. Experimental results on two datasets with different
modalities, i.e., CT and MRI, demonstrate that our method outperforms the
state-of-the-art medical image segmentation approaches across various
dimensions. The code is available at https://github.com/Axi404/PMT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convolutional Unscented Kalman Filter for Multi-Object Tracking with
  Outliers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.01380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.01380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqi Liu, Wenhan Cao, Chang Liu, Tianyi Zhang, Shengbo Eben Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-object tracking (MOT) is an essential technique for navigation in
autonomous driving. In tracking-by-detection systems, biases, false positives,
and misses, which are referred to as outliers, are inevitable due to complex
traffic scenarios. Recent tracking methods are based on filtering algorithms
that overlook these outliers, leading to reduced tracking accuracy or even loss
of the objects trajectory. To handle this challenge, we adopt a probabilistic
perspective, regarding the generation of outliers as misspecification between
the actual distribution of measurement data and the nominal measurement model
used for filtering. We further demonstrate that, by designing a convolutional
operation, we can mitigate this misspecification. Incorporating this operation
into the widely used unscented Kalman filter (UKF) in commonly adopted tracking
algorithms, we derive a variant of the UKF that is robust to outliers, called
the convolutional UKF (ConvUKF). We show that ConvUKF maintains the Gaussian
conjugate property, thus allowing for real-time tracking. We also prove that
ConvUKF has a bounded tracking error in the presence of outliers, which implies
robust stability. The experimental results on the KITTI and nuScenes datasets
show improved accuracy compared to representative baseline algorithms for MOT
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Intelligent Vehicles</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into
  Multimodal <span class="highlight-title">LLM</span>s at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of multimodal large language models (MLLMs), such as
GPT-4V, has led to significant advancements. However, these models still face
challenges in medical multimodal capabilities due to limitations in the
quantity and quality of medical vision-text data, stemming from data privacy
concerns and high annotation costs. While pioneering approaches utilize
PubMed's large-scale, de-identified medical image-text pairs to address these
limitations, they still fall short due to inherent data noise. To tackle this,
we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in
an 'unblinded' capacity to denoise and reformat the data, resulting in the
creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our
validation demonstrates that: (1) PubMedVision can significantly enhance the
medical multimodal capabilities of current MLLMs, showing significant
improvement in benchmarks including the MMMU Health & Medicine track; (2)
manual checks by medical experts and empirical results validate the superior
data quality of our dataset compared to other data construction methods. Using
PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows
superior performance in medical multimodal scenarios among open-source MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HMAFlow: Learning More Accurate Optical Flow via Hierarchical Motion
  Field Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dianbo Ma, Kousuke Imamura, Ziyan Gao, Xiangjie Wang, Satoshi Yamane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical flow estimation is a fundamental and long-standing visual task. In
this work, we present a novel method, dubbed HMAFlow, to improve optical flow
estimation in challenging scenes, particularly those involving small objects.
The proposed model mainly consists of two core components: a Hierarchical
Motion Field Alignment (HMA) module and a Correlation Self-Attention (CSA)
module. In addition, we rebuild 4D cost volumes by employing a Multi-Scale
Correlation Search (MCS) layer and replacing average pooling in common cost
volumes with a search strategy utilizing multiple search ranges. Experimental
results demonstrate that our model achieves the best generalization performance
compared to other state-of-the-art methods. Specifically, compared with RAFT,
our method achieves relative error reductions of 14.2% and 3.4% on the clean
pass and final pass of the Sintel online benchmark, respectively. On the KITTI
test benchmark, HMAFlow surpasses RAFT and GMA in the Fl-all metric by relative
margins of 6.8% and 7.7%, respectively. To facilitate future research, our code
will be made available at https://github.com/BooTurbo/HMAFlow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, Afshin Dehghan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose SlowFast-LLaVA (or SF-LLaVA for short), a training-free video
large language model (LLM) that can jointly capture detailed spatial semantics
and long-range temporal context without exceeding the token budget of commonly
used LLMs. This is realized by using a two-stream SlowFast design of inputs for
Video LLMs to aggregate features from sampled frames in an effective way.
Specifically, the Slow pathway extracts features at a low frame rate while
keeping as much spatial detail as possible (e.g., with 12x24 tokens), and the
Fast pathway operates on a high frame rate but uses a larger spatial pooling
stride (e.g., downsampling 6x) to focus on the motion cues. As a result, this
design allows us to adequately capture both spatial and temporal features that
are beneficial for detailed video understanding. Experimental results show that
SF-LLaVA outperforms existing training-free methods on a wide range of video
tasks. On some benchmarks, it achieves comparable or even better performance
compared to state-of-the-art Video LLMs that are fine-tuned on video datasets.
Code has been made available at: https://github.com/apple/ml-slowfast-llava.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Matrix <span class="highlight-title">Information</span> Theory for <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17326v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17326v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhang, Zhiquan Tan, Jingqin Yang, Weiran Huang, Yang Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The maximum entropy encoding framework provides a unified perspective for
many non-contrastive learning methods like SimSiam, Barlow Twins, and MEC.
Inspired by this framework, we introduce Matrix-SSL, a novel approach that
leverages matrix information theory to interpret the maximum entropy encoding
loss as matrix uniformity loss. Furthermore, Matrix-SSL enhances the maximum
entropy encoding method by seamlessly incorporating matrix alignment loss,
directly aligning covariance matrices in different branches. Experimental
results reveal that Matrix-SSL outperforms state-of-the-art methods on the
ImageNet dataset under linear evaluation settings and on MS-COCO for transfer
learning tasks. Specifically, when performing transfer learning tasks on
MS-COCO, our method outperforms previous SOTA methods such as MoCo v2 and BYOL
up to 3.3% with only 400 epochs compared to 800 epochs pre-training. We also
try to introduce representation learning into the language modeling regime by
fine-tuning a 7B model using matrix cross-entropy loss, with a margin of 3.1%
on the GSM8K dataset over the standard cross-entropy loss. Code available at
https://github.com/yifanzhang-pro/Matrix-SSL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-Language Meets the Skeleton: Progressively Distillation with
  Cross-Modal Knowledge for 3D Action Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Chen, Tian He, Junfeng Fu, Ling Wang, Jingcai Guo, Ting Hu, Hong Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeleton-based action representation learning aims to interpret and
understand human behaviors by encoding the skeleton sequences, which can be
categorized into two primary training paradigms: supervised learning and
self-supervised learning. However, the former one-hot classification requires
labor-intensive predefined action categories annotations, while the latter
involves skeleton transformations (e.g., cropping) in the pretext tasks that
may impair the skeleton structure. To address these challenges, we introduce a
novel skeleton-based training framework (C$^2$VL) based on Cross-modal
Contrastive learning that uses the progressive distillation to learn
task-agnostic human skeleton action representation from the Vision-Language
knowledge prompts. Specifically, we establish the vision-language action
concept space through vision-language knowledge prompts generated by
pre-trained large multimodal models (LMMs), which enrich the fine-grained
details that the skeleton action space lacks. Moreover, we propose the
intra-modal self-similarity and inter-modal cross-consistency softened targets
in the cross-modal representation learning process to progressively control and
guide the degree of pulling vision-language knowledge prompts and corresponding
skeletons closer. These soft instance discrimination and self-knowledge
distillation strategies contribute to the learning of better skeleton-based
action representations from the noisy skeleton-vision-language pairs. During
the inference phase, our method requires only the skeleton data as the input
for action recognition and no longer for vision-language prompts. Extensive
experiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets demonstrate
that our method outperforms the previous methods and achieves state-of-the-art
results. Code is available at: https://github.com/cseeyangchen/C2VL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pseudo-Labeling Based Practical Semi-Supervised Meta-Training for
  Few-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06817v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06817v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingping Dong, Tianran Ouyang, Shengcai Liao, Bo Du, Ling Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing few-shot learning (FSL) methods require a large amount of
labeled data in meta-training, which is a major limit. To reduce the
requirement of labels, a semi-supervised meta-training (SSMT) setting has been
proposed for FSL, which includes only a few labeled samples and numbers of
unlabeled samples in base classes. However, existing methods under this setting
require class-aware sample selection from the unlabeled set, which violates the
assumption of unlabeled set. In this paper, we propose a practical
semi-supervised meta-training setting with truly unlabeled data to facilitate
the applications of FSL in realistic scenarios. To better utilize both the
labeled and truly unlabeled data, we propose a simple and effective
meta-training framework, called pseudo-labeling based meta-learning (PLML).
Firstly, we train a classifier via common semi-supervised learning (SSL) and
use it to obtain the pseudo-labels of unlabeled data. Then we build few-shot
tasks from labeled and pseudo-labeled data and design a novel finetuning method
with feature smoothing and noise suppression to better learn the FSL model from
noise labels. Surprisingly, through extensive experiments across two FSL
datasets, we find that this simple meta-training framework effectively prevents
the performance degradation of various FSL models under limited labeled data,
and also significantly outperforms the state-of-the-art SSMT models. Besides,
benefiting from meta-training, our method also improves two representative SSL
algorithms as well.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IEEE Transactions on Image Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Random Walk on Pixel Manifolds for Anomaly Segmentation of Complex
  Driving Scenes <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17961v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17961v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelong Zeng, Kaname Tomite
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In anomaly segmentation for complex driving scenes, state-of-the-art
approaches utilize anomaly scoring functions to calculate anomaly scores. For
these functions, accurately predicting the logits of inlier classes for each
pixel is crucial for precisely inferring the anomaly score. However, in
real-world driving scenarios, the diversity of scenes often results in
distorted manifolds of pixel embeddings in the space. This effect is not
conducive to directly using the pixel embeddings for the logit prediction
during inference, a concern overlooked by existing methods. To address this
problem, we propose a novel method called Random Walk on Pixel Manifolds
(RWPM). RWPM utilizes random walks to reveal the intrinsic relationships among
pixels to refine the pixel embeddings. The refined pixel embeddings alleviate
the distortion of manifolds, improving the accuracy of anomaly scores. Our
extensive experiments show that RWPM consistently improve the performance of
the existing anomaly segmentation methods and achieve the best results. Code is
available at: \url{https://github.com/ZelongZeng/RWPM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proximal Ranking Policy Optimization for Practical Safety in
  Counterfactual Learning to Rank <span class="chip">RecSys 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Gupta, Harrie Oosterhuis, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual learning to rank (CLTR) can be risky and, in various
circumstances, can produce sub-optimal models that hurt performance when
deployed. Safe CLTR was introduced to mitigate these risks when using inverse
propensity scoring to correct for position bias. However, the existing safety
measure for CLTR is not applicable to state-of-the-art CLTR methods, cannot
handle trust bias, and relies on specific assumptions about user behavior. We
propose a novel approach, proximal ranking policy optimization (PRPO), that
provides safety in deployment without assumptions about user behavior. PRPO
removes incentives for learning ranking behavior that is too dissimilar to a
safe ranking model. Thereby, PRPO imposes a limit on how much learned models
can degrade performance metrics, without relying on any specific user
assumptions. Our experiments show that PRPO provides higher performance than
the existing safe inverse propensity scoring approach. PRPO always maintains
safety, even in maximally adversarial situations. By avoiding assumptions, PRPO
is the first method with unconditional safety in deployment that translates to
robust safety for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the CONSEQUENCES 2024 workshop, co-located with ACM
  RecSys 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CROSS-JEM: Accurate and Efficient Cross-encoders for Short-text Ranking
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhawna Paliwal, Deepak Saini, Mudit Dhawan, Siddarth Asokan, Nagarajan Natarajan, Surbhi Aggarwal, Pankaj Malhotra, Jian Jiao, Manik Varma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ranking a set of items based on their relevance to a given query is a core
problem in search and recommendation. Transformer-based ranking models are the
state-of-the-art approaches for such tasks, but they score each query-item
independently, ignoring the joint context of other relevant items. This leads
to sub-optimal ranking accuracy and high computational costs. In response, we
propose Cross-encoders with Joint Efficient Modeling (CROSS-JEM), a novel
ranking approach that enables transformer-based models to jointly score
multiple items for a query, maximizing parameter utilization. CROSS-JEM
leverages (a) redundancies and token overlaps to jointly score multiple items,
that are typically short-text phrases arising in search and recommendations,
and (b) a novel training objective that models ranking probabilities. CROSS-JEM
achieves state-of-the-art accuracy and over 4x lower ranking latency over
standard cross-encoders. Our contributions are threefold: (i) we highlight the
gap between the ranking application's need for scoring thousands of items per
query and the limited capabilities of current cross-encoders; (ii) we introduce
CROSS-JEM for joint efficient scoring of multiple items per query; and (iii) we
demonstrate state-of-the-art accuracy on standard public datasets and a
proprietary dataset. CROSS-JEM opens up new directions for designing tailored
early-attention-based ranking models that incorporate strict production
constraints such as item multiplicity and latency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring Recency Bias In Sequential Recommendation Systems <span class="chip">RecSys
  '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeonglyul Oh, Sungzoon Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recency bias in a sequential recommendation system refers to the overly high
emphasis placed on recent items within a user session. This bias can diminish
the serendipity of recommendations and hinder the system's ability to capture
users' long-term interests, leading to user disengagement. We propose a simple
yet effective novel metric specifically designed to quantify recency bias. Our
findings also demonstrate that high recency bias measured in our proposed
metric adversely impacts recommendation performance too, and mitigating it
results in improved recommendation performances across all models evaluated in
our experiments, thus highlighting the importance of measuring recency bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the CONSEQUENCES '24 workshop, co-located with ACM RecSys
  '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlpaPICO: <span class="highlight-title">Extraction</span> of PICO Frames from Clinical Trial Documents Using
  <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madhusudan Ghosh, Shrimon Mukherjee, Asmit Ganguly, Partha Basuchowdhuri, Sudip Kumar Naskar, Debasis Ganguly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been a surge in the publication of clinical trial
reports, making it challenging to conduct systematic reviews. Automatically
extracting Population, Intervention, Comparator, and Outcome (PICO) from
clinical trial studies can alleviate the traditionally time-consuming process
of manually scrutinizing systematic reviews. Existing approaches of PICO frame
extraction involves supervised approach that relies on the existence of
manually annotated data points in the form of BIO label tagging. Recent
approaches, such as In-Context Learning (ICL), which has been shown to be
effective for a number of downstream NLP tasks, require the use of labeled
examples. In this work, we adopt ICL strategy by employing the pretrained
knowledge of Large Language Models (LLMs), gathered during the pretraining
phase of an LLM, to automatically extract the PICO-related terminologies from
clinical trial documents in unsupervised set up to bypass the availability of
large number of annotated data instances. Additionally, to showcase the highest
effectiveness of LLM in oracle scenario where large number of annotated samples
are available, we adopt the instruction tuning strategy by employing Low Rank
Adaptation (LORA) to conduct the training of gigantic model in low resource
environment for the PICO frame extraction task. Our empirical results show that
our proposed ICL-based framework produces comparable results on all the version
of EBM-NLP datasets and the proposed instruction tuned version of our framework
produces state-of-the-art results on all the different EBM-NLP datasets. Our
project is available at \url{https://github.com/shrimonmuke0202/AlpaPICO.git}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Methods</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Diffusion Models for Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianghao Lin, Jiaqi Liu, Jiachen Zhu, Yunjia Xi, Chengkai Liu, Yangtian Zhang, Yong Yu, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While traditional recommendation techniques have made significant strides in
the past decades, they still suffer from limited generalization performance
caused by factors like inadequate collaborative signals, weak latent
representations, and noisy data. In response, diffusion models (DMs) have
emerged as promising solutions for recommender systems due to their robust
generative capabilities, solid theoretical foundations, and improved training
stability. To this end, in this paper, we present the first comprehensive
survey on diffusion models for recommendation, and draw a bird's-eye view from
the perspective of the whole pipeline in real-world recommender systems. We
systematically categorize existing research works into three primary domains:
(1) diffusion for data engineering & encoding, focusing on data augmentation
and representation enhancement; (2) diffusion as recommender models, employing
diffusion models to directly estimate user preferences and rank items; and (3)
diffusion for content presentation, utilizing diffusion models to generate
personalized content such as fashion and advertisement creatives. Our taxonomy
highlights the unique strengths of diffusion models in capturing complex data
distributions and generating high-quality, diverse samples that closely align
with user preferences. We also summarize the core characteristics of the
adapting diffusion models for recommendation, and further identify key areas
for future exploration, which helps establish a roadmap for researchers and
practitioners seeking to advance recommender systems through the innovative
application of diffusion models. To further facilitate the research community
of recommender systems based on diffusion models, we actively maintain a GitHub
repository for papers and other related resources in this rising direction
https://github.com/CHIANGEL/Awesome-Diffusion-for-RecSys.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">73</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRIN: Zero-Shot Metric Depth with Pixel-Level Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vitor Guizilini, Pavel Tokmakov, Achal Dave, Rares Ambrus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction from a single image is a long-standing problem in computer
vision. Learning-based methods address its inherent scale ambiguity by
leveraging increasingly large labeled and unlabeled datasets, to produce
geometric priors capable of generating accurate predictions across domains. As
a result, state of the art approaches show impressive performance in zero-shot
relative and metric depth estimation. Recently, diffusion models have exhibited
remarkable scalability and generalizable properties in their learned
representations. However, because these models repurpose tools originally
designed for image generation, they can only operate on dense ground-truth,
which is not available for most depth labels, especially in real-world
settings. In this paper we present GRIN, an efficient diffusion model designed
to ingest sparse unstructured training data. We use image features with 3D
geometric positional encodings to condition the diffusion process both globally
and locally, generating depth predictions at a pixel-level. With comprehensive
experiments across eight indoor and outdoor datasets, we show that GRIN
establishes a new state of the art in zero-shot metric monocular depth
estimation even when trained from scratch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Wage Disparities Using Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyon Vafa, Susan Athey, David M. Blei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One thread of empirical work in social science focuses on decomposing group
differences in outcomes into unexplained components and components explained by
observable factors. In this paper, we study gender wage decompositions, which
require estimating the portion of the gender wage gap explained by career
histories of workers. Classical methods for decomposing the wage gap employ
simple predictive models of wages which condition on a small set of simple
summaries of labor history. The problem is that these predictive models cannot
take advantage of the full complexity of a worker's history, and the resulting
decompositions thus suffer from omitted variable bias (OVB), where covariates
that are correlated with both gender and wages are not included in the model.
Here we explore an alternative methodology for wage gap decomposition that
employs powerful foundation models, such as large language models, as the
predictive engine. Foundation models excel at making accurate predictions from
complex, high-dimensional inputs. We use a custom-built foundation model,
designed to predict wages from full labor histories, to decompose the gender
wage gap. We prove that the way such models are usually trained might still
lead to OVB, but develop fine-tuning algorithms that empirically mitigate this
issue. Our model captures a richer representation of career history than simple
models and predicts wages more accurately. In detail, we first provide a novel
set of conditions under which an estimator of the wage gap based on a
fine-tuned foundation model is $\sqrt{n}$-consistent. Building on the theory,
we then propose methods for fine-tuning foundation models that minimize OVB.
Using data from the Panel Study of Income Dynamics, we find that history
explains more of the gender wage gap than standard econometric models can
measure, and we identify elements of history that are important for reducing
OVB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Fraud Detection: Integrating Reinforcement Learning into Graph
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Dong, Jianhua Yao, Jiajing Wang, Yingbin Liang, Shuhan Liao, Minheng Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial fraud refers to the act of obtaining financial benefits through
dishonest means. Such behavior not only disrupts the order of the financial
market but also harms economic and social development and breeds other illegal
and criminal activities. With the popularization of the internet and online
payment methods, many fraudulent activities and money laundering behaviors in
life have shifted from offline to online, posing a great challenge to
regulatory authorities. How to efficiently detect these financial fraud
activities has become an urgent issue that needs to be resolved. Graph neural
networks are a type of deep learning model that can utilize the interactive
relationships within graph structures, and they have been widely applied in the
field of fraud detection. However, there are still some issues. First,
fraudulent activities only account for a very small part of transaction
transfers, leading to an inevitable problem of label imbalance in fraud
detection. At the same time, fraudsters often disguise their behavior, which
can have a negative impact on the final prediction results. In addition,
existing research has overlooked the importance of balancing neighbor
information and central node information. For example, when the central node
has too many neighbors, the features of the central node itself are often
neglected. Finally, fraud activities and patterns are constantly changing over
time, so considering the dynamic evolution of graph edge relationships is also
very important.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flexible Diffusion Scopes with Parameterized Laplacian for Heterophilic
  Graph Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qincheng Lu, Jiaqi Zhu, Sitao Luan, Xiao-Wen Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of Graph Neural Networks (GNNs) to capture long-range and global
topology information is limited by the scope of conventional graph Laplacian,
leading to unsatisfactory performance on some datasets, particularly on
heterophilic graphs. To address this limitation, we propose a new class of
parameterized Laplacian matrices, which provably offers more flexibility in
controlling the diffusion distance between nodes than the conventional graph
Laplacian, allowing long-range information to be adaptively captured through
diffusion on graph. Specifically, we first prove that the diffusion distance
and spectral distance on graph have an order-preserving relationship. With this
result, we demonstrate that the parameterized Laplacian can accelerate the
diffusion of long-range information, and the parameters in the Laplacian enable
flexibility of the diffusion scopes. Based on the theoretical results, we
propose topology-guided rewiring mechanism to capture helpful long-range
neighborhood information for heterophilic graphs. With this mechanism and the
new Laplacian, we propose two GNNs with flexible diffusion scopes: namely the
Parameterized Diffusion based Graph Convolutional Networks (PD-GCN) and Graph
Attention Networks (PD-GAT). Synthetic experiments reveal the high correlations
between the parameters of the new Laplacian and the performance of
parameterized GNNs under various graph homophily levels, which verifies that
our new proposed GNNs indeed have the ability to adjust the parameters to
adaptively capture the global information for different levels of heterophilic
graphs. They also outperform the state-of-the-art (SOTA) models on 6 out of 7
real-world benchmark datasets, which further confirms their superiority.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2403.01475</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leiden-Fusion Partitioning Method for Effective Distributed Training of
  Graph Embeddings <span class="chip">ECML-PKDD 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhe Bai, Camelia Constantin, Hubert Naacke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the area of large-scale training of graph embeddings, effective training
frameworks and partitioning methods are critical for handling large networks.
However, they face two major challenges: 1) existing synchronized distributed
frameworks require continuous communication to access information from other
machines, and 2) the inability of current partitioning methods to ensure that
subgraphs remain connected components without isolated nodes, which is
essential for effective training of GNNs since training relies on information
aggregation from neighboring nodes. To address these issues, we introduce a
novel partitioning method, named Leiden-Fusion, designed for large-scale
training of graphs with minimal communication. Our method extends the Leiden
community detection algorithm with a greedy algorithm that merges the smallest
communities with highly connected neighboring communities. Our method
guarantees that, for an initially connected graph, each partition is a densely
connected subgraph with no isolated nodes. After obtaining the partitions, we
train a GNN for each partition independently, and finally integrate all
embeddings for node classification tasks, which significantly reduces the need
for network communication and enhances the efficiency of distributed graph
training. We demonstrate the effectiveness of our method through extensive
evaluations on several benchmark datasets, achieving high efficiency while
preserving the quality of the graph embeddings for node classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 2024 European Conference on Machine Learning and
  Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proximal Ranking Policy Optimization for Practical Safety in
  Counterfactual Learning to Rank <span class="chip">RecSys 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Gupta, Harrie Oosterhuis, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual learning to rank (CLTR) can be risky and, in various
circumstances, can produce sub-optimal models that hurt performance when
deployed. Safe CLTR was introduced to mitigate these risks when using inverse
propensity scoring to correct for position bias. However, the existing safety
measure for CLTR is not applicable to state-of-the-art CLTR methods, cannot
handle trust bias, and relies on specific assumptions about user behavior. We
propose a novel approach, proximal ranking policy optimization (PRPO), that
provides safety in deployment without assumptions about user behavior. PRPO
removes incentives for learning ranking behavior that is too dissimilar to a
safe ranking model. Thereby, PRPO imposes a limit on how much learned models
can degrade performance metrics, without relying on any specific user
assumptions. Our experiments show that PRPO provides higher performance than
the existing safe inverse propensity scoring approach. PRPO always maintains
safety, even in maximally adversarial situations. By avoiding assumptions, PRPO
is the first method with unconditional safety in deployment that translates to
robust safety for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the CONSEQUENCES 2024 workshop, co-located with ACM
  RecSys 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Continuous Kernels with Sparse Fourier Domain Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09875v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09875v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clayton Harper, Luke Wood, Peter Gerstoft, Eric C. Larson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address three key challenges in learning continuous kernel
representations: computational efficiency, parameter efficiency, and spectral
bias. Continuous kernels have shown significant potential, but their practical
adoption is often limited by high computational and memory demands.
Additionally, these methods are prone to spectral bias, which impedes their
ability to capture high-frequency details. To overcome these limitations, we
propose a novel approach that leverages sparse learning in the Fourier domain.
Our method enables the efficient scaling of continuous kernels, drastically
reduces computational and memory requirements, and mitigates spectral bias by
exploiting the Gibbs phenomenon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constructing a Singing Style Caption <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunjong Ok, Jaeho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singing voice synthesis and conversion have emerged as significant subdomains
of voice generation, leading to much demands on prompt-conditioned generation.
Unlike common voice data, generating a singing voice requires an understanding
of various associated vocal and musical characteristics, such as the vocal tone
of the singer or emotional expressions. However, existing open-source
audio-text datasets for voice generation tend to capture only a very limited
range of attributes, often missing musical characteristics of the audio. To
fill this gap, we introduce S2Cap, an audio-text pair dataset with a diverse
set of attributes. S2Cap consists of pairs of textual prompts and music audio
samples with a wide range of vocal and musical attributes, including pitch,
volume, tempo, mood, singer's gender and age, and musical genre and emotional
expression. Utilizing S2Cap, we suggest an effective novel baseline algorithm
for singing style captioning. Singing style captioning is a relative task to
voice generation that generates text descriptions of vocal characteristics,
which we first suggested. First, to mitigate the misalignment between the audio
encoder and the text decoder, we present a novel mechanism called CRESCENDO,
which utilizes positive-pair similarity learning to synchronize the embedding
spaces of a pretrained audio encoder to get similar embeddings with a text
encoder. We additionally supervise the model using the singer's voice, which is
demixed by the accompaniment. This supervision allows the model to more
accurately capture vocal characteristics, leading to improved singing style
captions that better reflect the style of the singer. The dataset and the codes
are available at \bulurl{https://github.com/HJ-Ok/S2cap}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Out-of-distribution Generalization for Graph Machine
  Learning from a Causal View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph machine learning (GML) has been successfully applied across a wide
range of tasks. Nonetheless, GML faces significant challenges in generalizing
over out-of-distribution (OOD) data, which raises concerns about its wider
applicability. Recent advancements have underscored the crucial role of
causality-driven approaches in overcoming these generalization challenges.
Distinct from traditional GML methods that primarily rely on statistical
dependencies, causality-focused strategies delve into the underlying causal
mechanisms of data generation and model prediction, thus significantly
improving the generalization of GML across different environments. This paper
offers a thorough review of recent progress in causality-involved GML
generalization. We elucidate the fundamental concepts of employing causality to
enhance graph model generalization and categorize the various approaches,
providing detailed descriptions of their methodologies and the connections
among them. Furthermore, we explore the incorporation of causality in other
related important areas of trustworthy GML, such as explanation, fairness, and
robustness. Concluding with a discussion on potential future research
directions, this review seeks to articulate the continuing development and
future potential of causality in enhancing the trustworthiness of graph machine
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Benchmark <span class="highlight-title">Dataset</span> with Larger Context for Non-Factoid Question
  Answering over Islamic Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faiza Qamar, Seemab Latif, Rabia Latif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accessing and comprehending religious texts, particularly the Quran (the
sacred scripture of Islam) and Ahadith (the corpus of the sayings or traditions
of the Prophet Muhammad), in today's digital era necessitates efficient and
accurate Question-Answering (QA) systems. Yet, the scarcity of QA systems
tailored specifically to the detailed nature of inquiries about the Quranic
Tafsir (explanation, interpretation, context of Quran for clarity) and Ahadith
poses significant challenges. To address this gap, we introduce a comprehensive
dataset meticulously crafted for QA purposes within the domain of Quranic
Tafsir and Ahadith. This dataset comprises a robust collection of over 73,000
question-answer pairs, standing as the largest reported dataset in this
specialized domain. Importantly, both questions and answers within the dataset
are meticulously enriched with contextual information, serving as invaluable
resources for training and evaluating tailored QA systems. However, while this
paper highlights the dataset's contributions and establishes a benchmark for
evaluating QA performance in the Quran and Ahadith domains, our subsequent
human evaluation uncovered critical insights regarding the limitations of
existing automatic evaluation techniques. The discrepancy between automatic
evaluation metrics, such as ROUGE scores, and human assessments became
apparent. The human evaluation indicated significant disparities: the model's
verdict consistency with expert scholars ranged between 11% to 20%, while its
contextual understanding spanned a broader spectrum of 50% to 90%. These
findings underscore the necessity for evaluation techniques that capture the
nuances and complexities inherent in understanding religious texts, surpassing
the limitations of traditional automatic metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Diffusion Models for Controllable RNA Sequence Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixuan Huang, Yukang Yang, Kaidi Fu, Yanyi Chu, Le Cong, Mengdi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents RNAdiffusion, a latent diffusion model for generating and
optimizing discrete RNA sequences. RNA is a particularly dynamic and versatile
molecule in biological processes. RNA sequences exhibit high variability and
diversity, characterized by their variable lengths, flexible three-dimensional
structures, and diverse functions. We utilize pretrained BERT-type models to
encode raw RNAs into token-level biologically meaningful representations. A
Q-Former is employed to compress these representations into a fixed-length set
of latent vectors, with an autoregressive decoder trained to reconstruct RNA
sequences from these latent variables. We then develop a continuous diffusion
model within this latent space. To enable optimization, we train reward
networks to estimate functional properties of RNA from the latent variables. We
employ gradient-based guidance during the backward diffusion process, aiming to
generate RNA sequences that are optimized for higher rewards. Empirical
experiments confirm that RNAdiffusion generates non-coding RNAs that align with
natural distributions across various biological indicators. We fine-tuned the
diffusion model on untranslated regions (UTRs) of mRNA and optimize sample
sequences for protein translation efficiencies. Our guided diffusion model
effectively generates diverse UTR sequences with high Mean Ribosome Loading
(MRL) and Translation Efficiency (TE), surpassing baselines. These results hold
promise for studies on RNA sequence-function relationships, protein synthesis,
and enhancing therapeutic RNA design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simpler Alternative to Variational Regularized Counterfactual Risk
  Minimization <span class="chip">RecSys
  '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hua Chang Bakker, Shashank Gupta, Harrie Oosterhuis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variance regularized counterfactual risk minimization (VRCRM) has been
proposed as an alternative off-policy learning (OPL) method. VRCRM method uses
a lower-bound on the $f$-divergence between the logging policy and the target
policy as regularization during learning and was shown to improve performance
over existing OPL alternatives on multi-label classification tasks. In this
work, we revisit the original experimental setting of VRCRM and propose to
minimize the $f$-divergence directly, instead of optimizing for the lower bound
using a $f$-GAN approach. Surprisingly, we were unable to reproduce the results
reported in the original setting. In response, we propose a novel simpler
alternative to f-divergence optimization by minimizing a direct approximation
of f-divergence directly, instead of a $f$-GAN based lower bound. Experiments
showed that minimizing the divergence using $f$-GANs did not work as expected,
whereas our proposed novel simpler alternative works better empirically.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the CONSEQUENCES '24 workshop, co-located with ACM RecSys
  '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PROSE-FD: A Multimodal PDE Foundation Model for Learning Multiple
  Operators for Forecasting Fluid Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Liu, Jingmin Sun, Xinjie He, Griffin Pinney, Zecheng Zhang, Hayden Schaeffer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose PROSE-FD, a zero-shot multimodal PDE foundational model for
simultaneous prediction of heterogeneous two-dimensional physical systems
related to distinct fluid dynamics settings. These systems include shallow
water equations and the Navier-Stokes equations with incompressible and
compressible flow, regular and complex geometries, and different buoyancy
settings. This work presents a new transformer-based multi-operator learning
approach that fuses symbolic information to perform operator-based data
prediction, i.e. non-autoregressive. By incorporating multiple modalities in
the inputs, the PDE foundation model builds in a pathway for including
mathematical descriptions of the physical behavior. We pre-train our foundation
model on 6 parametric families of equations collected from 13 datasets,
including over 60K trajectories. Our model outperforms popular operator
learning, computer vision, and multi-physics models, in benchmark forward
prediction tasks. We test our architecture choices with ablation studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning in Adversarial Environments: Testbed Design and
  Poisoning Resilience in Cybersecurity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Jian Huang, Bekzod Iskandarov, Mizanur Rahman, Hakan T. Otal, M. Abdullah Canbaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the design and implementation of a Federated Learning
(FL) testbed, focusing on its application in cybersecurity and evaluating its
resilience against poisoning attacks. Federated Learning allows multiple
clients to collaboratively train a global model while keeping their data
decentralized, addressing critical needs for data privacy and security,
particularly in sensitive fields like cybersecurity. Our testbed, built using
the Flower framework, facilitates experimentation with various FL frameworks,
assessing their performance, scalability, and ease of integration. Through a
case study on federated intrusion detection systems, we demonstrate the
testbed's capabilities in detecting anomalies and securing critical
infrastructure without exposing sensitive network data. Comprehensive poisoning
tests, targeting both model and data integrity, evaluate the system's
robustness under adversarial conditions. Our results show that while federated
learning enhances data privacy and distributed learning, it remains vulnerable
to poisoning attacks, which must be mitigated to ensure its reliability in
real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Data Quality through Self-learning on Imbalanced Financial
  Risk Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Sun, Zixuan Qin, Shun Zhang, Yuexian Wang, Li Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the financial risk domain, particularly in credit default prediction and
fraud detection, accurate identification of high-risk class instances is
paramount, as their occurrence can have significant economic implications.
Although machine learning models have gained widespread adoption for risk
prediction, their performance is often hindered by the scarcity and diversity
of high-quality data. This limitation stems from factors in datasets such as
small risk sample sizes, high labeling costs, and severe class imbalance, which
impede the models' ability to learn effectively and accurately forecast
critical events. This study investigates data pre-processing techniques to
enhance existing financial risk datasets by introducing TriEnhance, a
straightforward technique that entails: (1) generating synthetic samples
specifically tailored to the minority class, (2) filtering using binary
feedback to refine samples, and (3) self-learning with pseudo-labels. Our
experiments across six benchmark datasets reveal the efficacy of TriEnhance,
with a notable focus on improving minority class calibration, a key factor for
developing more robust financial risk prediction systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BEnDEM:A Boltzmann Sampler Based on Bootstrapped Denoising Energy
  Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        RuiKang OuYang, Bo Qiang, José Miguel Hernández-Lobato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing an efficient sampler capable of generating independent and
identically distributed (IID) samples from a Boltzmann distribution is a
crucial challenge in scientific research, e.g. molecular dynamics. In this
work, we intend to learn neural samplers given energy functions instead of data
sampled from the Boltzmann distribution. By learning the energies of the noised
data, we propose a diffusion-based sampler, ENERGY-BASED DENOISING ENERGY
MATCHING, which theoretically has lower variance and more complexity compared
to related works. Furthermore, a novel bootstrapping technique is applied to
EnDEM to balance between bias and variance. We evaluate EnDEM and BEnDEM on a
2-dimensional 40 Gaussian Mixture Model (GMM) and a 4-particle double-welling
potential (DW-4). The experimental results demonstrate that BEnDEM can achieve
state-of-the-art performance while being more robust.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Rate Optimization for Deep Neural Networks Using Lipschitz
  Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Padma Priyanka, Sheetal Kalyani, Avhishek Chatterjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning rate is a crucial parameter in training of neural networks. A
properly tuned learning rate leads to faster training and higher test accuracy.
In this paper, we propose a Lipschitz bandit-driven approach for tuning the
learning rate of neural networks. The proposed approach is compared with the
popular HyperOpt technique used extensively for hyperparameter optimization and
the recently developed bandit-based algorithm BLiE. The results for multiple
neural network architectures indicate that our method finds a better learning
rate using a) fewer evaluations and b) lesser number of epochs per evaluation,
when compared to both HyperOpt and BLiE. Thus, the proposed approach enables
more efficient training of neural networks, leading to lower training time and
lesser computational cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RandALO: Out-of-sample risk estimation in no time flat 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parth T. Nobel, Daniel LeJeune, Emmanuel J. Candès
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating out-of-sample risk for models trained on large high-dimensional
datasets is an expensive but essential part of the machine learning process,
enabling practitioners to optimally tune hyperparameters. Cross-validation (CV)
serves as the de facto standard for risk estimation but poorly trades off high
bias ($K$-fold CV) for computational cost (leave-one-out CV). We propose a
randomized approximate leave-one-out (RandALO) risk estimator that is not only
a consistent estimator of risk in high dimensions but also less computationally
expensive than $K$-fold CV. We support our claims with extensive simulations on
synthetic and real data and provide a user-friendly Python package implementing
RandALO available on PyPI as randalo and at https://github.com/cvxgrp/randalo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rewind-to-Delete: Certified Machine Unlearning for Nonconvex Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09778v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09778v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqiao Mu, Diego Klabjan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning algorithms aim to efficiently remove data from a model
without retraining it from scratch, in order to enforce data privacy, remove
corrupted or outdated data, or respect a user's ``right to be forgotten."
Certified machine unlearning is a strong theoretical guarantee that quantifies
the extent to which data is erased from the model weights. Most prior works in
certified unlearning focus on models trained on convex or strongly convex loss
functions, which benefit from convenient convergence guarantees and the
existence of global minima. For nonconvex objectives, existing algorithms rely
on limiting assumptions and expensive computations that hinder practical
implementations. In this work, we propose a simple first-order algorithm for
unlearning on general nonconvex loss functions which unlearns by ``rewinding"
to an earlier step during the learning process and then performs gradient
descent on the loss function of the retained data points. Our algorithm is
black-box, in that it can be directly applied to models pretrained with vanilla
gradient descent with no prior consideration of unlearning. We prove
$(\epsilon, \delta)$ certified unlearning and performance guarantees that
establish the privacy-utility-complexity tradeoff of our algorithm, with
special consideration for nonconvex functions that satisfy the
Polyak-Lojasiewicz inequality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Multi-view Graph Anomaly Detection with Similarity-Guided
  Contrastive Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lecheng Zheng, John R. Birge, Yifang Zhang, Jingrui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection on graphs plays an important role in many real-world
applications. Usually, these data are composed of multiple types (e.g., user
information and transaction records for financial data), thus exhibiting view
heterogeneity. Therefore, it can be challenging to leverage such multi-view
information and learn the graph's contextual information to identify rare
anomalies. To tackle this problem, many deep learning-based methods utilize
contrastive learning loss as a regularization term to learn good
representations. However, many existing contrastive-based methods show that
traditional contrastive learning losses fail to consider the semantic
information (e.g., class membership information). In addition, we theoretically
show that clustering-based contrastive learning also easily leads to a
sub-optimal solution. To address these issues, in this paper, we proposed an
autoencoder-based clustering framework regularized by a similarity-guided
contrastive loss to detect anomalous nodes. Specifically, we build a similarity
map to help the model learn robust representations without imposing a hard
margin constraint between the positive and negative pairs. Theoretically, we
show that the proposed similarity-guided loss is a variant of contrastive
learning loss, and how it alleviates the issue of unreliable pseudo-labels with
the connection to graph spectral clustering. Experimental results on several
datasets demonstrate the effectiveness and efficiency of our proposed
framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of Centrifugal Clutches in Two-Speed Automatic Transmissions
  with Deep Learning-Based Engagement Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo-Yi Lin, Kai Chun Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive numerical analysis of centrifugal clutch
systems integrated with a two-speed automatic transmission, a key component in
automotive torque transfer. Centrifugal clutches enable torque transmission
based on rotational speed without external controls. The study systematically
examines various clutch configurations effects on transmission dynamics,
focusing on torque transfer, upshifting, and downshifting behaviors under
different conditions. A Deep Neural Network (DNN) model predicts clutch
engagement using parameters such as spring preload and shoe mass, offering an
efficient alternative to complex simulations. The integration of deep learning
and numerical modeling provides critical insights for optimizing clutch
designs, enhancing transmission performance and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Optimality of (Accelerated) SGD for High-Dimensional Quadratic
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haihan Zhang, Yuanshi Liu, Qianwen Chen, Cong Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic gradient descent (SGD) is a widely used algorithm in machine
learning, particularly for neural network training. Recent studies on SGD for
canonical quadratic optimization or linear regression show it attains well
generalization under suitable high-dimensional settings. However, a fundamental
question -- for what kinds of high-dimensional learning problems SGD and its
accelerated variants can achieve optimality has yet to be well studied. This
paper investigates SGD with two essential components in practice: exponentially
decaying step size schedule and momentum. We establish the convergence upper
bound for momentum accelerated SGD (ASGD) and propose concrete classes of
learning problems under which SGD or ASGD achieves min-max optimal convergence
rates. The characterization of the target function is based on standard
power-law decays in (functional) linear regression. Our results unveil new
insights for understanding the learning bias of SGD: (i) SGD is efficient in
learning ``dense'' features where the corresponding weights are subject to an
infinity norm constraint; (ii) SGD is efficient for easy problem without
suffering from the saturation effect; (iii) momentum can accelerate the
convergence rate by order when the learning problem is relatively hard. To our
knowledge, this is the first work to clearly identify the optimal boundary of
SGD versus ASGD for the problem under mild settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OML-AD: Online Machine Learning for Anomaly Detection in Time Series
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Wette, Florian Heinrichs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series are ubiquitous and occur naturally in a variety of applications
-- from data recorded by sensors in manufacturing processes, over financial
data streams to climate data. Different tasks arise, such as regression,
classification or segmentation of the time series. However, to reliably solve
these challenges, it is important to filter out abnormal observations that
deviate from the usual behavior of the time series. While many anomaly
detection methods exist for independent data and stationary time series, these
methods are not applicable to non-stationary time series. To allow for
non-stationarity in the data, while simultaneously detecting anomalies, we
propose OML-AD, a novel approach for anomaly detection (AD) based on online
machine learning (OML). We provide an implementation of OML-AD within the
Python library River and show that it outperforms state-of-the-art baseline
methods in terms of accuracy and computational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Challenges and Pitfalls to Recommendations and Opportunities:
  Implementing Federated Learning in Healthcare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Li, Pengcheng Xu, Junjie Hu, Zeyu Tang, Guang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning holds great potential for enabling large-scale healthcare
research and collaboration across multiple centres while ensuring data privacy
and security are not compromised. Although numerous recent studies suggest or
utilize federated learning based methods in healthcare, it remains unclear
which ones have potential clinical utility. This review paper considers and
analyzes the most recent studies up to May 2024 that describe federated
learning based methods in healthcare. After a thorough review, we find that the
vast majority are not appropriate for clinical use due to their methodological
flaws and/or underlying biases which include but are not limited to privacy
concerns, generalization issues, and communication costs. As a result, the
effectiveness of federated learning in healthcare is significantly compromised.
To overcome these challenges, we provide recommendations and promising
opportunities that might be implemented to resolve these problems and improve
the quality of model development in federated learning with healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring Recency Bias In Sequential Recommendation Systems <span class="chip">RecSys
  '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeonglyul Oh, Sungzoon Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recency bias in a sequential recommendation system refers to the overly high
emphasis placed on recent items within a user session. This bias can diminish
the serendipity of recommendations and hinder the system's ability to capture
users' long-term interests, leading to user disengagement. We propose a simple
yet effective novel metric specifically designed to quantify recency bias. Our
findings also demonstrate that high recency bias measured in our proposed
metric adversely impacts recommendation performance too, and mitigating it
results in improved recommendation performances across all models evaluated in
our experiments, thus highlighting the importance of measuring recency bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the CONSEQUENCES '24 workshop, co-located with ACM RecSys
  '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finetuning CLIP to Reason about Pairwise Differences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dylan Sam, Devin Willmott, Joao D. Semedo, J. Zico Kolter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) such as CLIP are trained via contrastive
learning between text and image pairs, resulting in aligned image and text
embeddings that are useful for many downstream tasks. A notable drawback of
CLIP, however, is that the resulting embedding space seems to lack some of the
structure of their purely text-based alternatives. For instance, while text
embeddings have been long noted to satisfy \emph{analogies} in embedding space
using vector arithmetic, CLIP has no such property. In this paper, we propose
an approach to natively train CLIP in a contrastive manner to reason about
differences in embedding space. We finetune CLIP so that the differences in
image embedding space correspond to \emph{text descriptions of the image
differences}, which we synthetically generate with large language models on
image-caption paired datasets. We first demonstrate that our approach yields
significantly improved capabilities in ranking images by a certain attribute
(e.g., elephants are larger than cats), which is useful in retrieval or
constructing attribute-based classifiers, and improved zeroshot classification
performance on many downstream image classification tasks. In addition, our
approach enables a new mechanism for inference that we refer to as comparative
prompting, where we leverage prior knowledge of text descriptions of
differences between classes of interest, achieving even larger performance
gains in classification. Finally, we illustrate that the resulting embeddings
obey a larger degree of geometric properties in embedding space, such as in
text-to-image generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELSA: Exploiting Layer-wise N:M Sparsity for Vision Transformer
  Acceleration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning-Chi Huang, Chi-Chih Chang, Wei-Cheng Lin, Endri Taka, Diana Marculescu, Kai-Chiang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $N{:}M$ sparsity is an emerging model compression method supported by more
and more accelerators to speed up sparse matrix multiplication in deep neural
networks. Most existing $N{:}M$ sparsity methods compress neural networks with
a uniform setting for all layers in a network or heuristically determine the
layer-wise configuration by considering the number of parameters in each layer.
However, very few methods have been designed for obtaining a layer-wise
customized $N{:}M$ sparse configuration for vision transformers (ViTs), which
usually consist of transformer blocks involving the same number of parameters.
In this work, to address the challenge of selecting suitable sparse
configuration for ViTs on $N{:}M$ sparsity-supporting accelerators, we propose
ELSA, Exploiting Layer-wise $N{:}M$ Sparsity for ViTs. Considering not only all
$N{:}M$ sparsity levels supported by a given accelerator but also the expected
throughput improvement, our methodology can reap the benefits of accelerators
supporting mixed sparsity by trading off negligible accuracy loss with both
memory usage and inference time reduction for ViT models. For instance, our
approach achieves a noteworthy 2.9$\times$ reduction in FLOPs for both Swin-B
and DeiT-B with only a marginal degradation of accuracy on ImageNet. Our code
will be released upon paper acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlpaPICO: <span class="highlight-title">Extraction</span> of PICO Frames from Clinical Trial Documents Using
  <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Madhusudan Ghosh, Shrimon Mukherjee, Asmit Ganguly, Partha Basuchowdhuri, Sudip Kumar Naskar, Debasis Ganguly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been a surge in the publication of clinical trial
reports, making it challenging to conduct systematic reviews. Automatically
extracting Population, Intervention, Comparator, and Outcome (PICO) from
clinical trial studies can alleviate the traditionally time-consuming process
of manually scrutinizing systematic reviews. Existing approaches of PICO frame
extraction involves supervised approach that relies on the existence of
manually annotated data points in the form of BIO label tagging. Recent
approaches, such as In-Context Learning (ICL), which has been shown to be
effective for a number of downstream NLP tasks, require the use of labeled
examples. In this work, we adopt ICL strategy by employing the pretrained
knowledge of Large Language Models (LLMs), gathered during the pretraining
phase of an LLM, to automatically extract the PICO-related terminologies from
clinical trial documents in unsupervised set up to bypass the availability of
large number of annotated data instances. Additionally, to showcase the highest
effectiveness of LLM in oracle scenario where large number of annotated samples
are available, we adopt the instruction tuning strategy by employing Low Rank
Adaptation (LORA) to conduct the training of gigantic model in low resource
environment for the PICO frame extraction task. Our empirical results show that
our proposed ICL-based framework produces comparable results on all the version
of EBM-NLP datasets and the proposed instruction tuned version of our framework
produces state-of-the-art results on all the different EBM-NLP datasets. Our
project is available at \url{https://github.com/shrimonmuke0202/AlpaPICO.git}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Methods</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GFlowNet <span class="highlight-title">Pretrain</span>ing with Inexpensive Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohit Pandey, Gopeshh Subbaraj, Emmanuel Bengio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Flow Networks (GFlowNets), a class of generative models have
recently emerged as a suitable framework for generating diverse and
high-quality molecular structures by learning from unnormalized reward
distributions. Previous works in this direction often restrict exploration by
using predefined molecular fragments as building blocks, limiting the chemical
space that can be accessed. In this work, we introduce Atomic GFlowNets
(A-GFNs), a foundational generative model leveraging individual atoms as
building blocks to explore drug-like chemical space more comprehensively. We
propose an unsupervised pre-training approach using offline drug-like molecule
datasets, which conditions A-GFNs on inexpensive yet informative molecular
descriptors such as drug-likeliness, topological polar surface area, and
synthetic accessibility scores. These properties serve as proxy rewards,
guiding A-GFNs towards regions of chemical space that exhibit desirable
pharmacological properties. We further our method by implementing a
goal-conditioned fine-tuning process, which adapts A-GFNs to optimize for
specific target properties. In this work, we pretrain A-GFN on the ZINC15
offline dataset and employ robust evaluation metrics to show the effectiveness
of our approach when compared to other relevant baseline methods in drug
design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting building types and functions at transnational scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Fill, Michael Eichelbeck, Michael Ebner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building-specific knowledge such as building type and function information is
important for numerous energy applications. However, comprehensive datasets
containing this information for individual households are missing in many
regions of Europe. For the first time, we investigate whether it is feasible to
predict building types and functional classes at a European scale based on only
open GIS datasets available across countries. We train a graph neural network
(GNN) classifier on a large-scale graph dataset consisting of OpenStreetMap
(OSM) buildings across the EU, Norway, Switzerland, and the UK. To efficiently
perform training using the large-scale graph, we utilize localized subgraphs. A
graph transformer model achieves a high Cohen's kappa coefficient of 0.754 when
classifying buildings into 9 classes, and a very high Cohen's kappa coefficient
of 0.844 when classifying buildings into the residential and non-residential
classes. The experimental results imply three core novel contributions to
literature. Firstly, we show that building classification across multiple
countries is possible using a multi-source dataset consisting of information
about 2D building shape, land use, degree of urbanization, and countries as
input, and OSM tags as ground truth. Secondly, our results indicate that GNN
models that consider contextual information about building neighborhoods
improve predictive performance compared to models that only consider individual
buildings and ignore the neighborhood. Thirdly, we show that training with GNNs
on localized subgraphs instead of standard GNNs improves performance for the
task of building classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extrapolative ML Models for Copolymers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Israrul H. Hashmi,  Himanshu, Rahul Karmakar, Tarak K Patra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models have been progressively used for predicting materials
properties. These models can be built using pre-existing data and are useful
for rapidly screening the physicochemical space of a material, which is
astronomically large. However, ML models are inherently interpolative, and
their efficacy for searching candidates outside a material's known range of
property is unresolved. Moreover, the performance of an ML model is intricately
connected to its learning strategy and the volume of training data. Here, we
determine the relationship between the extrapolation ability of an ML model,
the size and range of its training dataset, and its learning approach. We focus
on a canonical problem of predicting the properties of a copolymer as a
function of the sequence of its monomers. Tree search algorithms, which learn
the similarity between polymer structures, are found to be inefficient for
extrapolation. Conversely, the extrapolation capability of neural networks and
XGBoost models, which attempt to learn the underlying functional correlation
between the structure and property of polymers, show strong correlations with
the volume and range of training data. These findings have important
implications on ML-based new material development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Safe Neural Networks with Global SDP Bounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Soletskyi, David "davidad" Dalrymple
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to training neural networks with formal
safety guarantees using semidefinite programming (SDP) for verification. Our
method focuses on verifying safety over large, high-dimensional input regions,
addressing limitations of existing techniques that focus on adversarial
robustness bounds. We introduce an ADMM-based training scheme for an accurate
neural network classifier on the Adversarial Spheres dataset, achieving
provably perfect recall with input dimensions up to $d=40$. This work advances
the development of reliable neural network verification methods for
high-dimensional systems, with potential applications in safe RL policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Dimensionality in 2D Rectangle Packing Problem under
  Reinforcement Learning Schema 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Waldemar Kołodziejczyk, Mariusz Kaleta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the application of Reinforcement Learning (RL) to the
two-dimensional rectangular packing problem. We propose a reduced
representation of the state and action spaces that allow us for high
granularity. Leveraging UNet architecture and Proximal Policy Optimization
(PPO), we achieved a model that is comparable to the MaxRect heuristic.
However, our approach has great potential to be generalized to nonrectangular
packing problems and complex constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5th Polish Conference on Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Selection Through Model Sorting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Ali Hajiani, Babak Seyfe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach to select the best model of the data. Based on
the exclusive properties of the nested models, we find the most parsimonious
model containing the risk minimizer predictor. We prove the existence of
probable approximately correct (PAC) bounds on the difference of the minimum
empirical risk of two successive nested models, called successive empirical
excess risk (SEER). Based on these bounds, we propose a model order selection
method called nested empirical risk (NER). By the sorted NER (S-NER) method to
sort the models intelligently, the minimum risk decreases. We construct a test
that predicts whether expanding the model decreases the minimum risk or not.
With a high probability, the NER and S-NER choose the true model order and the
most parsimonious model containing the risk minimizer predictor, respectively.
We use S-NER model selection in the linear regression and show that, the S-NER
method without any prior information can outperform the accuracy of feature
sorting algorithms like orthogonal matching pursuit (OMP) that aided with prior
knowledge of the true model order. Also, in the UCR data set, the NER method
reduces the complexity of the classification of UCR datasets dramatically, with
a negligible loss of accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 4 figures, submitted to IEEE Transactions on Pattern
  Analysis and Machine Intelligence, October 26, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KAN v.s. MLP for Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haihong Guo, Fengxin Li, Jiao Li, Hongyan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kolmogorov-Arnold Networks (KAN) is an emerging neural network architecture
in machine learning. It has greatly interested the research community about
whether KAN can be a promising alternative of the commonly used Multi-Layer
Perceptions (MLP). Experiments in various fields demonstrated that KAN-based
machine learning can achieve comparable if not better performance than
MLP-based methods, but with much smaller parameter scales and are more
explainable. In this paper, we explore the incorporation of KAN into the actor
and critic networks for offline reinforcement learning (RL). We evaluated the
performance, parameter scales, and training efficiency of various KAN and MLP
based conservative Q-learning (CQL) on the the classical D4RL benchmark for
offline RL. Our study demonstrates that KAN can achieve performance close to
the commonly used MLP with significantly fewer parameters. This provides us an
option to choose the base networks according to the requirements of the offline
RL tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages,2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional sampling within generative diffusion models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Zhao, Ziwei Luo, Jens Sjölund, Thomas B. Schön
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative diffusions are a powerful class of Monte Carlo samplers that
leverage bridging Markov processes to approximate complex, high-dimensional
distributions, such as those found in image processing and language models.
Despite their success in these domains, an important open challenge remains:
extending these techniques to sample from conditional distributions, as
required in, for example, Bayesian inverse problems. In this paper, we present
a comprehensive review of existing computational approaches to conditional
sampling within generative diffusion models. Specifically, we highlight key
methodologies that either utilise the joint distribution, or rely on
(pre-trained) marginal distributions with explicit likelihoods, to construct
conditional generative samplers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COSCO: A Sharpness-Aware Training Framework for Few-shot Multivariate
  Time Series Classification <span class="chip">CIKM '24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesus Barreda, Ashley Gomez, Ruben Puga, Kaixiong Zhou, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate time series classification is an important task with widespread
domains of applications. Recently, deep neural networks (DNN) have achieved
state-of-the-art performance in time series classification. However, they often
require large expert-labeled training datasets which can be infeasible in
practice. In few-shot settings, i.e. only a limited number of samples per class
are available in training data, DNNs show a significant drop in testing
accuracy and poor generalization ability. In this paper, we propose to address
these problems from an optimization and a loss function perspective.
Specifically, we propose a new learning framework named COSCO consisting of a
sharpness-aware minimization (SAM) optimization and a Prototypical loss
function to improve the generalization ability of DNN for multivariate time
series classification problems under few-shot setting. Our experiments
demonstrate our proposed method outperforms the existing baseline methods. Our
source code is available at: https://github.com/JRB9/COSCO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, CIKM '24 Short Paper Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extract and Diffuse: Latent Integration for Improved Diffusion-based
  Speech and Vocal Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudong Yang, Zhan Liu, Wenyi Yu, Guangzhi Sun, Qiuqiang Kong, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based generative models have recently achieved remarkable results
in speech and vocal enhancement due to their ability to model complex speech
data distributions. While these models generalize well to unseen acoustic
environments, they may not achieve the same level of fidelity as the
discriminative models specifically trained to enhance particular acoustic
conditions. In this paper, we propose Ex-Diff, a novel score-based diffusion
model that integrates the latent representations produced by a discriminative
model to improve speech and vocal enhancement, which combines the strengths of
both generative and discriminative models. Experimental results on the widely
used MUSDB dataset show relative improvements of 3.7% in SI-SDR and 10.0% in
SI-SIR compared to the baseline diffusion model for speech and vocal
enhancement tasks, respectively. Additionally, case studies are provided to
further illustrate and analyze the complementary nature of generative and
discriminative models in this context.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Framework For Text Detection From Natural Scene Images With
  Complex Background 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Basavaraj Kaladagi, Jagadeesh Pujari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing texts from camera images is a known hard problem because of the
difficulties in text detection from the varied and complicated background. In
this paper we propose a novel and efficient method to detect text region from
images with complex background using Wavelet Transforms. The framework uses
Wavelet Transformation of the original image in its grayscale form followed by
Sub-band filtering. Then Region clustering technique is applied using centroids
of the regions, further Bounding box is fitted to each region thus identifying
the text regions. This method is much sophisticated and efficient than the
previous methods as it doesn't stick to a particular font size of the text
thus, making it generalized. The sample set used for experimental purpose
consists of 50 images with varying backgrounds. Images with edge prominence are
considered. Furthermore, our method can be easily customized for applications
with different scopes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Simplicity Bias towards Compositional Mappings via
  Learning Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Ren, Danica J. Sutherland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obtaining compositional mappings is important for the model to generalize
well compositionally. To better understand when and how to encourage the model
to learn such mappings, we study their uniqueness through different
perspectives. Specifically, we first show that the compositional mappings are
the simplest bijections through the lens of coding length (i.e., an upper bound
of their Kolmogorov complexity). This property explains why models having such
mappings can generalize well. We further show that the simplicity bias is
usually an intrinsic property of neural network training via gradient descent.
That partially explains why some models spontaneously generalize well when they
are trained appropriately.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HJ-sampler: A Bayesian sampler for inverse problems of a stochastic
  process by leveraging Hamilton-Jacobi PDEs and score-based generative models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingwei Meng, Zongren Zou, Jérôme Darbon, George Em Karniadakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The interplay between stochastic processes and optimal control has been
extensively explored in the literature. With the recent surge in the use of
diffusion models, stochastic processes have increasingly been applied to sample
generation. This paper builds on the log transform, known as the Cole-Hopf
transform in Brownian motion contexts, and extends it within a more abstract
framework that includes a linear operator. Within this framework, we found that
the well-known relationship between the Cole-Hopf transform and optimal
transport is a particular instance where the linear operator acts as the
infinitesimal generator of a stochastic process. We also introduce a novel
scenario where the linear operator is the adjoint of the generator, linking to
Bayesian inference under specific initial and terminal conditions. Leveraging
this theoretical foundation, we develop a new algorithm, named the HJ-sampler,
for Bayesian inference for the inverse problem of a stochastic differential
equation with given terminal observations. The HJ-sampler involves two stages:
(1) solving the viscous Hamilton-Jacobi partial differential equations, and (2)
sampling from the associated stochastic optimal control problem. Our proposed
algorithm naturally allows for flexibility in selecting the numerical solver
for viscous HJ PDEs. We introduce two variants of the solver: the
Riccati-HJ-sampler, based on the Riccati method, and the SGM-HJ-sampler, which
utilizes diffusion models. We demonstrate the effectiveness and flexibility of
the proposed methods by applying them to solve Bayesian inverse problems
involving various stochastic processes and prior distributions, including
applications that address model misspecifications and quantifying model
uncertainty.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Audio Narrations to Strengthen Domain Generalization in
  Multimodal First-Person Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cagri Gungor, Adriana Kovashka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  First-person activity recognition is rapidly growing due to the widespread
use of wearable cameras but faces challenges from domain shifts across
different environments, such as varying objects or background scenes. We
propose a multimodal framework that improves domain generalization by
integrating motion, audio, and appearance features. Key contributions include
analyzing the resilience of audio and motion features to domain shifts, using
audio narrations for enhanced audio-text alignment, and applying consistency
ratings between audio and visual narrations to optimize the impact of audio in
recognition during training. Our approach achieves state-of-the-art performance
on the ARGO1M dataset, effectively generalizing across unseen scenarios and
locations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Data-Centric RLHF: Simple Metrics for Preference <span class="highlight-title">Dataset</span>
  Comparison 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Judy Hanwen Shen, Archit Sharma, Jun Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of aligning language models to human preferences requires data that
reveal these preferences. Ideally, time and money can be spent carefully
collecting and tailoring bespoke preference data to each downstream
application. However, in practice, a select few publicly available preference
datasets are often used to train reward models for reinforcement learning from
human feedback (RLHF). While new preference datasets are being introduced with
increasing frequency, there are currently no existing efforts to measure and
compare these datasets. In this paper, we systematically study preference
datasets through three perspectives: scale, label noise, and information
content. We propose specific metrics for each of these perspectives and uncover
different axes of comparison for a better understanding of preference datasets.
Our work is a first step towards a data-centric approach to alignment by
providing perspectives that aid in training efficiency and iterative data
collection for RLHF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-World Test-Time Training: Self-Training with Contrast Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houcheng Su, Mengzhu Wang, Jiao Li, Bingli Wang, Daixian Liu, Zeheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional test-time training (TTT) methods, while addressing domain shifts,
often assume a consistent class set, limiting their applicability in real-world
scenarios characterized by infinite variety. Open-World Test-Time Training
(OWTTT) addresses the challenge of generalizing deep learning models to unknown
target domain distributions, especially in the presence of strong
Out-of-Distribution (OOD) data. Existing TTT methods often struggle to maintain
performance when confronted with strong OOD data. In OWTTT, the focus has
predominantly been on distinguishing between overall strong and weak OOD data.
However, during the early stages of TTT, initial feature extraction is hampered
by interference from strong OOD and corruptions, resulting in diminished
contrast and premature classification of certain classes as strong OOD. To
address this, we introduce Open World Dynamic Contrastive Learning (OWDCL), an
innovative approach that utilizes contrastive learning to augment positive
sample pairs. This strategy not only bolsters contrast in the early stages but
also significantly enhances model robustness in subsequent stages. In
comparison datasets, our OWDCL model has produced the most advanced
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine learning assisted screening of metal binary alloys for anode
  materials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyue Shi, Linming Zhou, Yuhui Huang, Yongjun Wu, Zijian Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the dynamic and rapidly advancing battery field, alloy anode materials are
a focal point due to their superior electrochemical performance. Traditional
screening methods are inefficient and time-consuming. Our research introduces a
machine learning-assisted strategy to expedite the discovery and optimization
of these materials. We compiled a vast dataset from the MP and AFLOW databases,
encompassing tens of thousands of alloy compositions and properties. Utilizing
a CGCNN, we accurately predicted the potential and specific capacity of alloy
anodes, validated against experimental data. This approach identified
approximately 120 low potential and high specific capacity alloy anodes
suitable for various battery systems including Li, Na, K, Zn, Mg, Ca, and
Al-based. Our method not only streamlines the screening of battery anode
materials but also propels the advancement of battery material research and
innovation in energy storage technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages include SI, 5 figures in main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bias Begets Bias: The Impact of Biased Embeddings on Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahil Kuchlous, Marvin Li, Jeffrey G. Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing adoption of Text-to-Image (TTI) systems, the social biases
of these models have come under increased scrutiny. Herein we conduct a
systematic investigation of one such source of bias for diffusion models:
embedding spaces. First, because traditional classifier-based fairness
definitions require true labels not present in generative modeling, we propose
statistical group fairness criteria based on a model's internal representation
of the world. Using these definitions, we demonstrate theoretically and
empirically that an unbiased text embedding space for input prompts is a
necessary condition for representationally balanced diffusion models, meaning
the distribution of generated images satisfy diversity requirements with
respect to protected attributes. Next, we investigate the impact of biased
embeddings on evaluating the alignment between generated images and prompts, a
process which is commonly used to assess diffusion models. We find that biased
multimodal embeddings like CLIP can result in lower alignment scores for
representationally balanced TTI models, thus rewarding unfair behavior.
Finally, we develop a theoretical framework through which biases in alignment
evaluation can be studied and propose bias mitigation methods. By specifically
adapting the perspective of embedding spaces, we establish new fairness
conditions for diffusion model development and evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Astrometric Binary Classification Via Artificial Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joe Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With nearly two billion stars observed and their corresponding astrometric
parameters evaluated in the recent Gaia mission, the number of astrometric
binary candidates have risen significantly. Due to the surplus of astrometric
data, the current computational methods employed to inspect these astrometric
binary candidates are both computationally expensive and cannot be executed in
a reasonable time frame. In light of this, a machine learning (ML) technique to
automatically classify whether a set of stars belong to an astrometric binary
pair via an artificial neural network (ANN) is proposed. Using data from Gaia
DR3, the ANN was trained and tested on 1.5 million highly probable true and
visual binaries, considering the proper motions, parallaxes, and angular and
physical separations as features. The ANN achieves high classification scores,
with an accuracy of 99.3%, a precision rate of 0.988, a recall rate of 0.991,
and an AUC of 0.999, indicating that the utilized ML technique is a highly
effective method for classifying astrometric binaries. Thus, the proposed ANN
is a promising alternative to the existing methods for the classification of
astrometric binaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Astrophysical Journal (ApJ)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rail-only: A Low-Cost High-Performance Network for Training <span class="highlight-title">LLM</span>s with
  Trillion Parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12169v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12169v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyang Wang, Manya Ghobadi, Kayvon Shakeri, Ying Zhang, Naader Hasani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a low-cost network architecture for training large
language models (LLMs) at hyperscale. We study the optimal parallelization
strategy of LLMs and propose a novel datacenter network design tailored to
LLM's unique communication pattern. We show that LLM training generates sparse
communication patterns in the network and, therefore, does not require
any-to-any full-bisection network to complete efficiently. As a result, our
design eliminates the spine layer in traditional GPU clusters. We name this
design a Rail-only network and demonstrate that it achieves the same training
performance while reducing the network cost by 38% to 77% and network power
consumption by 37% to 75% compared to a conventional GPU datacenter. Our
architecture also supports Mixture-of-Expert (MoE) models with all-to-all
communication through forwarding, with only 8.2% to 11.2% completion time
overhead for all-to-all traffic. We study the failure robustness of Rail-only
networks and provide insights into the performance impact of different network
and training parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Preserving Hierarchical Model-Distributed Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18353v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18353v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemeh Jafarian Dehkordi, Yasaman Keshtkarjahromi, Hulya Seferoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on designing a privacy-preserving Machine Learning (ML)
inference protocol for a hierarchical setup, where clients own/generate data,
model owners (cloud servers) have a pre-trained ML model, and edge servers
perform ML inference on clients' data using the cloud server's ML model. Our
goal is to speed up ML inference while providing privacy to both data and the
ML model. Our approach (i) uses model-distributed inference (model
parallelization) at the edge servers and (ii) reduces the amount of
communication to/from the cloud server. Our privacy-preserving hierarchical
model-distributed inference, privateMDI design uses additive secret sharing and
linearly homomorphic encryption to handle linear calculations in the ML
inference, and garbled circuit and a novel three-party oblivious transfer are
used to handle non-linear functions. privateMDI consists of offline and online
phases. We designed these phases in a way that most of the data exchange is
done in the offline phase while the communication overhead of the online phase
is reduced. In particular, there is no communication to/from the cloud server
in the online phase, and the amount of communication between the client and
edge servers is minimized. The experimental results demonstrate that privateMDI
significantly reduces the ML inference time as compared to the baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kernelized Concept Erasure <span class="chip">EMNLP22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.12191v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.12191v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shauli Ravfogel, Francisco Vargas, Yoav Goldberg, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The representation space of neural models for textual data emerges in an
unsupervised manner during training. Understanding how those representations
encode human-interpretable concepts is a fundamental problem. One prominent
approach for the identification of concepts in neural representations is
searching for a linear subspace whose erasure prevents the prediction of the
concept from the representations. However, while many linear erasure algorithms
are tractable and interpretable, neural networks do not necessarily represent
concepts in a linear manner. To identify non-linearly encoded concepts, we
propose a kernelization of a linear minimax game for concept erasure. We
demonstrate that it is possible to prevent specific non-linear adversaries from
predicting the concept. However, the protection does not transfer to different
nonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded
concept remains an open problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a long paper in EMNLP22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hypothesis on Black Swan in Unchanging Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18422v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18422v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunin Lee, Chanwoo Park, David Abel, Ming Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Black swan events are statistically rare occurrences that carry extremely
high risks. A typical view of defining black swan events is heavily assumed to
originate from an unpredictable time-varying environments; however, the
community lacks a comprehensive definition of black swan events. To this end,
this paper challenges that the standard view is incomplete and claims that
high-risk, statistically rare events can also occur in unchanging environments
due to human misperception of their value and likelihood, which we call as
spatial black swan event. We first carefully categorize black swan events,
focusing on spatial black swan events, and mathematically formalize the
definition of black swan events. We hope these definitions can pave the way for
the development of algorithms to prevent such events by rationally correcting
human perception.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Authorship was updated</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatial Transformer Network YOLO Model for Agricultural Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Zambre, Ekdev Rajkitkul, Akshatha Mohan, Joshua Peeples
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection plays a crucial role in the field of computer vision by
autonomously locating and identifying objects of interest. The You Only Look
Once (YOLO) model is an effective single-shot detector. However, YOLO faces
challenges in cluttered or partially occluded scenes and can struggle with
small, low-contrast objects. We propose a new method that integrates spatial
transformer networks (STNs) into YOLO to improve performance. The proposed
STN-YOLO aims to enhance the model's effectiveness by focusing on important
areas of the image and improving the spatial invariance of the model before the
detection process. Our proposed method improved object detection performance
both qualitatively and quantitatively. We explore the impact of different
localization networks within the STN module as well as the robustness of the
model across different spatial transformations. We apply the STN-YOLO on
benchmark datasets for Agricultural object detection as well as a new dataset
from a state-of-the-art plant phenotyping greenhouse facility. Our code and
dataset are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, accepted to 2024 IEEE International Conference on
  Machine Learning and Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Note on the Convergence of Denoising Diffusion Probabilistic Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sokhna Diarra Mbacke, Omar Rivasplata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are one of the most important families of deep generative
models. In this note, we derive a quantitative upper bound on the Wasserstein
distance between the data-generating distribution and the distribution learned
by a diffusion model. Unlike previous works in this field, our result does not
make assumptions on the learned score function. Moreover, our bound holds for
arbitrary data-generating distributions on bounded instance spaces, even those
without a density w.r.t. the Lebesgue measure, and the upper bound does not
suffer from exponential dependencies. Our main result builds upon the recent
work of Mbacke et al. (2023) and our proofs are elementary.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at TMLR in 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Early Detection of Network Service Degradation: An Intra-Flow Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06637v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06637v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Balint Bicski, Adrian Pekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents a novel method for predicting service degradation (SD)
in computer networks by leveraging early flow features. Our approach focuses on
the observable (O) segments of network flows, particularly analyzing Packet
Inter-Arrival Time (PIAT) values and other derived metrics, to infer the
behavior of non-observable (NO) segments. Through a comprehensive evaluation,
we identify an optimal O/NO split threshold of 10 observed delay samples,
balancing prediction accuracy and resource utilization. Evaluating models
including Logistic Regression, XGBoost, and Multi-Layer Perceptron, we find
XGBoost outperforms others, achieving an F1-score of 0.74, balanced accuracy of
0.84, and AUROC of 0.97. Our findings highlight the effectiveness of
incorporating comprehensive early flow features and the potential of our method
to offer a practical solution for monitoring network traffic in
resource-constrained environments. This approach ensures enhanced user
experience and network performance by preemptively addressing potential SD,
providing the basis for a robust framework for maintaining high-quality network
services.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>reduced to 5 pages; accepted as a short paper for presentation at the
  CNSM 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-Shot Safety Alignment for Large Language Models via Optimal
  Dualization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinmeng Huang, Shuo Li, Edgar Dobriban, Osbert Bastani, Hamed Hassani, Dongsheng Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing safety concerns surrounding Large Language Models (LLMs) raise an
urgent need to align them with diverse human preferences to simultaneously
enhance their helpfulness and safety. A promising approach is to enforce safety
constraints through Reinforcement Learning from Human Feedback (RLHF). For such
constrained RLHF, common Lagrangian-based primal-dual policy optimization
methods are computationally expensive and often unstable. This paper presents a
dualization perspective that reduces constrained alignment to an equivalent
unconstrained alignment problem. We do so by pre-optimizing a smooth and convex
dual function that has a closed form. This shortcut eliminates the need for
cumbersome primal-dual policy iterations, thus greatly reducing the
computational burden and improving training stability. Our strategy leads to
two practical algorithms in model-based and preference-based scenarios (MoCAN
and PeCAN, respectively). A broad range of experiments demonstrate the
effectiveness of our methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conformal Off-Policy Prediction for Multi-Agent Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Kuipers, Renukanandan Tumu, Shuo Yang, Milad Kazemi, Rahul Mangharam, Nicola Paoletti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Off-Policy Prediction (OPP), i.e., predicting the outcomes of a target policy
using only data collected under a nominal (behavioural) policy, is a paramount
problem in data-driven analysis of safety-critical systems where the deployment
of a new policy may be unsafe. To achieve dependable off-policy predictions,
recent work on Conformal Off-Policy Prediction (COPP) leverage the conformal
prediction framework to derive prediction regions with probabilistic guarantees
under the target process. Existing COPP methods can account for the
distribution shifts induced by policy switching, but are limited to
single-agent systems and scalar outcomes (e.g., rewards). In this work, we
introduce MA-COPP, the first conformal prediction method to solve OPP problems
involving multi-agent systems, deriving joint prediction regions for all
agents' trajectories when one or more ego agents change their policies. Unlike
the single-agent scenario, this setting introduces higher complexity as the
distribution shifts affect predictions for all agents, not just the ego agents,
and the prediction task involves full multi-dimensional trajectories, not just
reward values. A key contribution of MA-COPP is to avoid enumeration or
exhaustive search of the output space of agent trajectories, which is instead
required by existing COPP methods to construct the prediction region. We
achieve this by showing that an over-approximation of the true joint prediction
region (JPR) can be constructed, without enumeration, from the maximum density
ratio of the JPR trajectories. We evaluate the effectiveness of MA-COPP in
multi-agent systems from the PettingZoo library and the F1TENTH autonomous
racing environment, achieving nominal coverage in higher dimensions and various
shift settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the 63rd IEEE Conference on Decision and
  Control (CDC) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sarah Frank-Wolfe: Methods for Constrained Optimization with Best Rates
  and Practical Features <span class="chip">ICML 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11737v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11737v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandr Beznosikov, David Dobre, Gauthier Gidel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Frank-Wolfe (FW) method is a popular approach for solving optimization
problems with structured constraints that arise in machine learning
applications. In recent years, stochastic versions of FW have gained
popularity, motivated by large datasets for which the computation of the full
gradient is prohibitively expensive. In this paper, we present two new variants
of the FW algorithms for stochastic finite-sum minimization. Our algorithms
have the best convergence guarantees of existing stochastic FW approaches for
both convex and non-convex objective functions. Our methods do not have the
issue of permanently collecting large batches, which is common to many
stochastic projection-free approaches. Moreover, our second approach does not
require either large batches or full deterministic gradients, which is a
typical weakness of many techniques for finite-sum problems. The faster
theoretical rates of our approaches are confirmed experimentally.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appears in: the 41st International Conference on Machine Learning
  (ICML 2024). 26 pages, 2 algorithms, 5 figures, 2 tables. Reference:
  https://proceedings.mlr.press/v235/beznosikov24a.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-Series Forecasting and Sequence Learning Using Memristor-based
  Reservoir System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13347v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13347v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullah M. Zyarah, Dhireesha Kudithipudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pushing the frontiers of time-series information processing in the
ever-growing domain of edge devices with stringent resources has been impeded
by the systems' ability to process information and learn locally on the device.
Local processing and learning of time-series information typically demand
intensive computations and massive storage as the process involves retrieving
information and tuning hundreds of parameters back in time. In this work, we
developed a memristor-based echo state network accelerator that features
efficient temporal data processing and in-situ online learning. The proposed
design is benchmarked using various datasets involving real-world tasks, such
as forecasting the load energy consumption and weather conditions. The
experimental results illustrate that the hardware model experiences a marginal
degradation in performance as compared to the software counterpart. This is
mainly attributed to the limited precision and dynamic range of network
parameters when emulated using memristor devices. The proposed system is
evaluated for lifespan, robustness, and energy-delay product. It is observed
that the system demonstrates reasonable robustness for device failure below
10%, which may occur due to stuck-at faults. Furthermore, 247X reduction in
energy consumption is achieved when compared to a custom CMOS digital design
implemented at the same technology node.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span> Honeypot: Leveraging Large Language Models as Advanced Interactive
  Honeypot Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08234v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08234v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hakan T. Otal, M. Abdullah Canbaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of cyber threats necessitates innovative solutions for
detecting and analyzing malicious activity. Honeypots, which are decoy systems
designed to lure and interact with attackers, have emerged as a critical
component in cybersecurity. In this paper, we present a novel approach to
creating realistic and interactive honeypot systems using Large Language Models
(LLMs). By fine-tuning a pre-trained open-source language model on a diverse
dataset of attacker-generated commands and responses, we developed a honeypot
capable of sophisticated engagement with attackers. Our methodology involved
several key steps: data collection and processing, prompt engineering, model
selection, and supervised fine-tuning to optimize the model's performance.
Evaluation through similarity metrics and live deployment demonstrated that our
approach effectively generates accurate and informative responses. The results
highlight the potential of LLMs to revolutionize honeypot technology, providing
cybersecurity professionals with a powerful tool to detect and analyze
malicious activity, thereby enhancing overall security infrastructure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bringing motion taxonomies to continuous domains via GPLVM on hyperbolic
  manifolds <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01672v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01672v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noémie Jaquier, Leonel Rozo, Miguel González-Duque, Viacheslav Borovitskiy, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion taxonomies serve as high-level hierarchical abstractions that
classify how humans move and interact with their environment. They have proven
useful to analyse grasps, manipulation skills, and whole-body support poses.
Despite substantial efforts devoted to design their hierarchy and underlying
categories, their use remains limited. This may be attributed to the lack of
computational models that fill the gap between the discrete hierarchical
structure of the taxonomy and the high-dimensional heterogeneous data
associated to its categories. To overcome this problem, we propose to model
taxonomy data via hyperbolic embeddings that capture the associated
hierarchical structure. We achieve this by formulating a novel Gaussian process
hyperbolic latent variable model that incorporates the taxonomy structure
through graph-based priors on the latent space and distance-preserving back
constraints. We validate our model on three different human motion taxonomies
to learn hyperbolic embeddings that faithfully preserve the original graph
structure. We show that our model properly encodes unseen data from existing or
new taxonomy categories, and outperforms its Euclidean and VAE-based
counterparts. Finally, through proof-of-concept experiments, we show that our
model may be used to generate realistic trajectories between the learned
embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Intl. Conference on Machine Learning (ICML), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Tuning Adaptive Stochastic Optimizers: Determining the Optimal
  Hyperparameter $ε$ via Gradient Magnitude Histogram Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11532v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11532v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo Silva, Paul Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic optimizers play a crucial role in the successful training of deep
neural network models. To achieve optimal model performance, designers must
carefully select both model and optimizer hyperparameters. However, this
process is frequently demanding in terms of computational resources and
processing time. While it is a well-established practice to tune the entire set
of optimizer hyperparameters for peak performance, there is still a lack of
clarity regarding the individual influence of hyperparameters mislabeled as
"low priority", including the safeguard factor $\epsilon$ and decay rate
$\beta$, in leading adaptive stochastic optimizers like the Adam optimizer. In
this manuscript, we introduce a new framework based on the empirical
probability density function of the loss' gradient magnitude, termed as the
"gradient magnitude histogram", for a thorough analysis of adaptive stochastic
optimizers and the safeguard hyperparameter $\epsilon$. This framework reveals
and justifies valuable relationships and dependencies among hyperparameters in
connection to optimal performance across diverse tasks, such as classification,
language modeling and machine translation. Furthermore, we propose a novel
algorithm using gradient magnitude histograms to automatically estimate a
refined and accurate search space for the optimal safeguard hyperparameter
$\epsilon$, surpassing the conventional trial-and-error methodology by
establishing a worst-case search space that is two times narrower.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PolyLUT-Add: FPGA-based LUT Inference with Wide Inputs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binglei Lou, Richard Rademacher, David Boland, Philip H. W. Leong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  FPGAs have distinct advantages as a technology for deploying deep neural
networks (DNNs) at the edge. Lookup Table (LUT) based networks, where neurons
are directly modeled using LUTs, help maximize this promise of offering
ultra-low latency and high area efficiency on FPGAs. Unfortunately, LUT
resource usage scales exponentially with the number of inputs to the LUT,
restricting PolyLUT to small LUT sizes. This work introduces PolyLUT-Add, a
technique that enhances neuron connectivity by combining $A$ PolyLUT
sub-neurons via addition to improve accuracy. Moreover, we describe a novel
architecture to improve its scalability. We evaluated our implementation over
the MNIST, Jet Substructure classification, and Network Intrusion Detection
benchmark and found that for similar accuracy, PolyLUT-Add achieves a LUT
reduction of $2.0-13.9\times$ with a $1.2-1.6\times$ decrease in latency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code for this paper is available at:
  https://github.com/bingleilou/PolyLUT-Add</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dreaming is All You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01633v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01633v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingze Ni, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In classification tasks, achieving a harmonious balance between exploration
and precision is of paramount importance. To this end, this research introduces
two novel deep learning models, SleepNet and DreamNet, to strike this balance.
SleepNet seamlessly integrates supervised learning with unsupervised ``sleep"
stages using pre-trained encoder models. Dedicated neurons within SleepNet are
embedded in these unsupervised features, forming intermittent ``sleep" blocks
that facilitate exploratory learning. Building upon the foundation of SleepNet,
DreamNet employs full encoder-decoder frameworks to reconstruct the hidden
states, mimicking the human "dreaming" process. This reconstruction process
enables further exploration and refinement of the learned representations.
Moreover, the principle ideas of our SleepNet and DreamNet are generic and can
be applied to both computer vision and natural language processing downstream
tasks. Through extensive empirical evaluations on diverse image and text
datasets, SleepNet and DreanNet have demonstrated superior performance compared
to state-of-the-art models, showcasing the strengths of unsupervised
exploration and supervised precision afforded by our innovative approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiating and Integrating ZX Diagrams with Applications to Quantum
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.13250v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.13250v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quanlong Wang, Richie Yeung, Mark Koch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ZX-calculus has proved to be a useful tool for quantum technology with a wide
range of successful applications. Most of these applications are of an
algebraic nature. However, other tasks that involve differentiation and
integration remain unreachable with current ZX techniques. Here we elevate ZX
to an analytical perspective by realising differentiation and integration
entirely within the framework of ZX-calculus. We explicitly illustrate the new
analytic framework of ZX-calculus by applying it in context of quantum machine
learning for the analysis of barren plateaus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Quantum</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A POD-TANN approach for the multiscale modeling of materials and
  macroelement derivation in geomechanics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07165v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07165v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Piunno, Ioannis Stefanou, Cristina Jommi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel approach that combines Proper Orthogonal
Decomposition (POD) with Thermodynamics-based Artificial Neural Networks (TANN)
to capture the macroscopic behavior of complex inelastic systems and derive
macroelements in geomechanics. The methodology leverages POD to extract
macroscopic Internal State Variables (ISVs) from microscopic state information,
thereby enriching the macroscopic state description used to train an energy
potential network within the TANN framework. The thermodynamic consistency
provided by TANN, combined with the hierarchical nature of POD, allows for
accurate modeling of complex, non-linear material behavior and reliable
macroscopic geomechanical systems responses. The effectiveness of this approach
is validated through applications of increasing complexity, demonstrating its
capability to handle various material behaviors and microstructural topologies.
These applications include the homogenization of continuous inelastic
representative unit cells (RUCs) and the derivation of a macroelement for a
geotechnical system involving a monopile in a clay layer subjected to
horizontal loading. The results indicate that the proposed POD-TANN methodology
not only achieves high accuracy in reproducing stress-strain responses, but
also significantly reduces computational costs, making it a practical tool for
the multiscale modeling of heterogeneous inelastic systems, and the efficient
derivation of macroelements for complex geomechanical problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 14 figures, Submitted to International Journal for
  Numerical and Analytical Methods in Geomechanics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoD, Towards an Interpretable Medical Agent using Chain of Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junying Chen, Chi Gui, Anningzhe Gao, Ke Ji, Xidong Wang, Xiang Wan, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of medical diagnosis has undergone a significant transformation
with the advent of large language models (LLMs), yet the challenges of
interpretability within these models remain largely unaddressed. This study
introduces Chain-of-Diagnosis (CoD) to enhance the interpretability of
LLM-based medical diagnostics. CoD transforms the diagnostic process into a
diagnostic chain that mirrors a physician's thought process, providing a
transparent reasoning pathway. Additionally, CoD outputs the disease confidence
distribution to ensure transparency in decision-making. This interpretability
makes model diagnostics controllable and aids in identifying critical symptoms
for inquiry through the entropy reduction of confidences. With CoD, we
developed DiagnosisGPT, capable of diagnosing 9604 diseases. Experimental
results demonstrate that DiagnosisGPT outperforms other LLMs on diagnostic
benchmarks. Moreover, DiagnosisGPT provides interpretability while ensuring
controllability in diagnostic rigor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HuatuoGPT-II, One-stage Training for Medical Adaption of <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junying Chen, Xidong Wang, Ke Ji, Anningzhe Gao, Feng Jiang, Shunian Chen, Hongbo Zhang, Dingjie Song, Wenya Xie, Chuyi Kong, Jianquan Li, Xiang Wan, Haizhou Li, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting a language model into a specific domain, a.k.a `domain adaption', is
a common practice when specialized knowledge, e.g. medicine, is not
encapsulated in a general language model like Llama2. The challenge lies in the
heterogeneity of data across the two training stages, as it varies in
languages, genres, or formats. To tackle this and simplify the learning
protocol, we propose to transform heterogeneous data, from the both
pre-training and supervised stages, into a unified, simple input-output pair
format. We validate the new protocol in the domains where proprietary LLMs like
ChatGPT perform relatively poorly, such as Traditional Chinese Medicine. The
developed model, HuatuoGPT-II, has shown state-of-the-art performance in
Chinese medicine domain on a number of benchmarks, e.g. medical licensing
exams. It even outperforms proprietary models like ChatGPT and GPT-4 in some
aspects, especially in Traditional Chinese Medicine. Expert manual evaluations
further validate HuatuoGPT-II's advantages over existing LLMs. Notably,
HuatuoGPT-II was benchmarked in a fresh Chinese National Medical Licensing
Examination where it achieved the best performance, showcasing not only its
effectiveness but also its generalization capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into
  Multimodal <span class="highlight-title">LLM</span>s at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19280v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19280v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of multimodal large language models (MLLMs), such as
GPT-4V, has led to significant advancements. However, these models still face
challenges in medical multimodal capabilities due to limitations in the
quantity and quality of medical vision-text data, stemming from data privacy
concerns and high annotation costs. While pioneering approaches utilize
PubMed's large-scale, de-identified medical image-text pairs to address these
limitations, they still fall short due to inherent data noise. To tackle this,
we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in
an 'unblinded' capacity to denoise and reformat the data, resulting in the
creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our
validation demonstrates that: (1) PubMedVision can significantly enhance the
medical multimodal capabilities of current MLLMs, showing significant
improvement in benchmarks including the MMMU Health & Medicine track; (2)
manual checks by medical experts and empirical results validate the superior
data quality of our dataset compared to other data construction methods. Using
PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows
superior performance in medical multimodal scenarios among open-source MLLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balancing Act: Prioritization Strategies for <span class="highlight-title">LLM</span>-Designed Restless
  Bandit Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12112v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12112v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shresth Verma, Niclas Boehmer, Lingkai Kong, Milind Tambe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs are increasingly used to design reward functions based on human
preferences in Reinforcement Learning (RL). We focus on LLM-designed rewards
for Restless Multi-Armed Bandits, a framework for allocating limited resources
among agents. In applications such as public health, this approach empowers
grassroots health workers to tailor automated allocation decisions to community
needs. In the presence of multiple agents, altering the reward function based
on human preferences can impact subpopulations very differently, leading to
complex tradeoffs and a multi-objective resource allocation problem. We are the
first to present a principled method termed Social Choice Language Model for
dealing with these tradeoffs for LLM-designed rewards for multiagent planners
in general and restless bandits in particular. The novel part of our model is a
transparent and configurable selection component, called an adjudicator,
external to the LLM that controls complex tradeoffs via a user-selected social
welfare function. Our experiments demonstrate that our model reliably selects
more effective, aligned, and balanced reward functions compared to purely
LLM-based approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mallows-DPO: Fine-Tune Your <span class="highlight-title">LLM</span> with Preference Dispersions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxian Chen, Hanyang Zhao, Henry Lam, David Yao, Wenpin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) has recently emerged as a popular
approach to improve reinforcement learning with human feedback (RLHF), leading
to better techniques to fine-tune large language models (LLM). A weakness of
DPO, however, lies in its lack of capability to characterize the diversity of
human preferences. Inspired by Mallows' theory of preference ranking, we
develop in this paper a new approach, the Mallows-DPO. A distinct feature of
this approach is a dispersion index, which reflects the dispersion of human
preference to prompts. We show that existing DPO models can be reduced to
special cases of this dispersion index, thus unified with Mallows-DPO. More
importantly, we demonstrate (empirically) how to use this dispersion index to
enhance the performance of DPO in a broad array of benchmark tasks, from
synthetic bandit selection to controllable generations and dialogues, while
maintaining great generalization capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pathformer: Multi-scale Transformers with Adaptive Pathways for Time
  Series Forecasting <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05956v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05956v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Chen, Yingying Zhang, Yunyao Cheng, Yang Shu, Yihang Wang, Qingsong Wen, Bin Yang, Chenjuan Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers for time series forecasting mainly model time series from
limited or fixed scales, making it challenging to capture different
characteristics spanning various scales. We propose Pathformer, a multi-scale
Transformer with adaptive pathways. It integrates both temporal resolution and
temporal distance for multi-scale modeling. Multi-scale division divides the
time series into different temporal resolutions using patches of various sizes.
Based on the division of each scale, dual attention is performed over these
patches to capture global correlations and local details as temporal
dependencies. We further enrich the multi-scale Transformer with adaptive
pathways, which adaptively adjust the multi-scale modeling process based on the
varying temporal dynamics of the input, improving the accuracy and
generalization of Pathformer. Extensive experiments on eleven real-world
datasets demonstrate that Pathformer not only achieves state-of-the-art
performance by surpassing all current models but also exhibits stronger
generalization abilities under various transfer scenarios. The code is made
available at https://github.com/decisionintelligence/pathformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 12th International Conference on Learning
  Representations (ICLR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Matrix <span class="highlight-title">Information</span> Theory for <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17326v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17326v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhang, Zhiquan Tan, Jingqin Yang, Weiran Huang, Yang Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The maximum entropy encoding framework provides a unified perspective for
many non-contrastive learning methods like SimSiam, Barlow Twins, and MEC.
Inspired by this framework, we introduce Matrix-SSL, a novel approach that
leverages matrix information theory to interpret the maximum entropy encoding
loss as matrix uniformity loss. Furthermore, Matrix-SSL enhances the maximum
entropy encoding method by seamlessly incorporating matrix alignment loss,
directly aligning covariance matrices in different branches. Experimental
results reveal that Matrix-SSL outperforms state-of-the-art methods on the
ImageNet dataset under linear evaluation settings and on MS-COCO for transfer
learning tasks. Specifically, when performing transfer learning tasks on
MS-COCO, our method outperforms previous SOTA methods such as MoCo v2 and BYOL
up to 3.3% with only 400 epochs compared to 800 epochs pre-training. We also
try to introduce representation learning into the language modeling regime by
fine-tuning a 7B model using matrix cross-entropy loss, with a margin of 3.1%
on the GSM8K dataset over the standard cross-entropy loss. Code available at
https://github.com/yifanzhang-pro/Matrix-SSL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-Language Meets the Skeleton: Progressively Distillation with
  Cross-Modal Knowledge for 3D Action Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Chen, Tian He, Junfeng Fu, Ling Wang, Jingcai Guo, Ting Hu, Hong Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeleton-based action representation learning aims to interpret and
understand human behaviors by encoding the skeleton sequences, which can be
categorized into two primary training paradigms: supervised learning and
self-supervised learning. However, the former one-hot classification requires
labor-intensive predefined action categories annotations, while the latter
involves skeleton transformations (e.g., cropping) in the pretext tasks that
may impair the skeleton structure. To address these challenges, we introduce a
novel skeleton-based training framework (C$^2$VL) based on Cross-modal
Contrastive learning that uses the progressive distillation to learn
task-agnostic human skeleton action representation from the Vision-Language
knowledge prompts. Specifically, we establish the vision-language action
concept space through vision-language knowledge prompts generated by
pre-trained large multimodal models (LMMs), which enrich the fine-grained
details that the skeleton action space lacks. Moreover, we propose the
intra-modal self-similarity and inter-modal cross-consistency softened targets
in the cross-modal representation learning process to progressively control and
guide the degree of pulling vision-language knowledge prompts and corresponding
skeletons closer. These soft instance discrimination and self-knowledge
distillation strategies contribute to the learning of better skeleton-based
action representations from the noisy skeleton-vision-language pairs. During
the inference phase, our method requires only the skeleton data as the input
for action recognition and no longer for vision-language prompts. Extensive
experiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets demonstrate
that our method outperforms the previous methods and achieves state-of-the-art
results. Code is available at: https://github.com/cseeyangchen/C2VL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Global Perspective on the Past, Present, and Future of Video Streaming
  over Starlink 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liz Izhikevich, Reese Enghardt, Te-Yuan Huang, Renata Teixeira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents the first global analysis of on-demand video streaming
over Low Earth Orbit (LEO) satellite networks, using data from over one million
households across 85 countries. We highlight Starlink's role as a major LEO
provider, enhancing connectivity in underserved regions. Our findings reveal
that while overall video quality on Starlink matches that of traditional
networks, the inherent variability in LEO conditions -- such as throughput
fluctuations and packet loss -- leads to an increase in bitrate switches and
rebuffers. To further improve the quality of experience for the LEO community,
we manipulate existing congestion control and adaptive bitrate streaming
algorithms using simulation and real A/B tests deployed on over one million
households. Our results underscore the need for video streaming and congestion
control algorithms to adapt to rapidly evolving network landscapes, ensuring
high-quality service across diverse and dynamic network types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Video to Audio Mapper with Visual Scene Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingjing Yi, Ming Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-to-audio (V2A) generation aims to produce corresponding audio given
silent video inputs. This task is particularly challenging due to the
cross-modality and sequential nature of the audio-visual features involved.
Recent works have made significant progress in bridging the domain gap between
video and audio, generating audio that is semantically aligned with the video
content. However, a critical limitation of these approaches is their inability
to effectively recognize and handle multiple scenes within a video, often
leading to suboptimal audio generation in such cases. In this paper, we first
reimplement a state-of-the-art V2A model with a slightly modified light-weight
architecture, achieving results that outperform the baseline. We then propose
an improved V2A model that incorporates a scene detector to address the
challenge of switching between multiple visual scenes. Results on VGGSound show
that our model can recognize and handle multiple scenes within a video and
achieve superior performance against the baseline for both fidelity and
relevance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-view Hypergraph-based Contrastive Learning Model for Cold-Start
  Micro-video Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sisuo Lyu, Xiuze Zhou, Xuming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread use of mobile devices and the rapid growth of micro-video
platforms such as TikTok and Kwai, the demand for personalized micro-video
recommendation systems has significantly increased. Micro-videos typically
contain diverse information, such as textual metadata, visual cues (e.g., cover
images), and dynamic video content, significantly affecting user interaction
and engagement patterns. However, most existing approaches often suffer from
the problem of over-smoothing, which limits their ability to capture
comprehensive interaction information effectively. Additionally, cold-start
scenarios present ongoing challenges due to sparse interaction data and the
underutilization of available interaction signals.
  To address these issues, we propose a Multi-view Hypergraph-based Contrastive
learning model for cold-start micro-video Recommendation (MHCR). MHCR
introduces a multi-view multimodal feature extraction layer to capture
interaction signals from various perspectives and incorporates multi-view
self-supervised learning tasks to provide additional supervisory signals.
Through extensive experiments on two real-world datasets, we show that MHCR
significantly outperforms existing video recommendation models and effectively
mitigates cold-start challenges. Our code is available at
https://anonymous.4open.science/r/MHCR-02EF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Foundation Models for Music Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjun Li, Ying Cai, Ziyang Wu, Wenyi Zhang, Yifan Chen, Rundong Qi, Mengqi Dong, Peigen Chen, Xiao Dong, Fenghao Shi, Lei Guo, Junwei Han, Bao Ge, Tianming Liu, Lin Gan, Tuo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music is essential in daily life, fulfilling emotional and entertainment
needs, and connecting us personally, socially, and culturally. A better
understanding of music can enhance our emotions, cognitive skills, and cultural
connections. The rapid advancement of artificial intelligence (AI) has
introduced new ways to analyze music, aiming to replicate human understanding
of music and provide related services. While the traditional models focused on
audio features and simple tasks, the recent development of large language
models (LLMs) and foundation models (FMs), which excel in various fields by
integrating semantic information and demonstrating strong reasoning abilities,
could capture complex musical features and patterns, integrate music with
language and incorporate rich musical, emotional and psychological knowledge.
Therefore, they have the potential in handling complex music understanding
tasks from a semantic perspective, producing outputs closer to human
perception. This work, to our best knowledge, is one of the early reviews of
the intersection of AI techniques and music understanding. We investigated,
analyzed, and tested recent large-scale music foundation models in respect of
their music comprehension abilities. We also discussed their limitations and
proposed possible future directions, offering insights for researchers in this
field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-Language Meets the Skeleton: Progressively Distillation with
  Cross-Modal Knowledge for 3D Action Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Chen, Tian He, Junfeng Fu, Ling Wang, Jingcai Guo, Ting Hu, Hong Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeleton-based action representation learning aims to interpret and
understand human behaviors by encoding the skeleton sequences, which can be
categorized into two primary training paradigms: supervised learning and
self-supervised learning. However, the former one-hot classification requires
labor-intensive predefined action categories annotations, while the latter
involves skeleton transformations (e.g., cropping) in the pretext tasks that
may impair the skeleton structure. To address these challenges, we introduce a
novel skeleton-based training framework (C$^2$VL) based on Cross-modal
Contrastive learning that uses the progressive distillation to learn
task-agnostic human skeleton action representation from the Vision-Language
knowledge prompts. Specifically, we establish the vision-language action
concept space through vision-language knowledge prompts generated by
pre-trained large multimodal models (LMMs), which enrich the fine-grained
details that the skeleton action space lacks. Moreover, we propose the
intra-modal self-similarity and inter-modal cross-consistency softened targets
in the cross-modal representation learning process to progressively control and
guide the degree of pulling vision-language knowledge prompts and corresponding
skeletons closer. These soft instance discrimination and self-knowledge
distillation strategies contribute to the learning of better skeleton-based
action representations from the noisy skeleton-vision-language pairs. During
the inference phase, our method requires only the skeleton data as the input
for action recognition and no longer for vision-language prompts. Extensive
experiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets demonstrate
that our method outperforms the previous methods and achieves state-of-the-art
results. Code is available at: https://github.com/cseeyangchen/C2VL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-14T00:00:00Z">2024-09-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">50</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASR Error Correction using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rao Ma, Mengjie Qian, Mark Gales, Kate Knill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Error correction (EC) models play a crucial role in refining Automatic Speech
Recognition (ASR) transcriptions, enhancing the readability and quality of
transcriptions. Without requiring access to the underlying code or model
weights, EC can improve performance and provide domain adaptation for black-box
ASR systems. This work investigates the use of large language models (LLMs) for
error correction across diverse scenarios. 1-best ASR hypotheses are commonly
used as the input to EC models. We propose building high-performance EC models
using ASR N-best lists which should provide more contextual information for the
correction process. Additionally, the generation process of a standard EC model
is unrestricted in the sense that any output sequence can be generated. For
some scenarios, such as unseen domains, this flexibility may impact
performance. To address this, we introduce a constrained decoding approach
based on the N-best list or an ASR lattice. Finally, most EC models are trained
for a specific ASR system requiring retraining whenever the underlying ASR
system is changed. This paper explores the ability of EC models to operate on
the output of different ASR systems. This concept is further extended to
zero-shot error correction using LLMs, such as ChatGPT. Experiments on three
standard datasets demonstrate the efficacy of our proposed methods for both
Transducer and attention-based encoder-decoder ASR systems. In addition, the
proposed method can serve as an effective method for model ensembling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Audio, Speech and Language
  Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Planning Transformer: Long-Horizon Offline Reinforcement Learning with
  Planning Tokens <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09513v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09513v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Clinton, Robert Lieck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised learning approaches to offline reinforcement learning,
particularly those utilizing the Decision Transformer, have shown effectiveness
in continuous environments and for sparse rewards. However, they often struggle
with long-horizon tasks due to the high compounding error of auto-regressive
models. To overcome this limitation, we go beyond next-token prediction and
introduce Planning Tokens, which contain high-level, long time-scale
information about the agent's future. Predicting dual time-scale tokens at
regular intervals enables our model to use these long-horizon Planning Tokens
as a form of implicit planning to guide its low-level policy and reduce
compounding error. This architectural modification significantly enhances
performance on long-horizon tasks, establishing a new state-of-the-art in
complex D4RL environments. Additionally, we demonstrate that Planning Tokens
improve the interpretability of the model's policy through the interpretable
plan visualisations and attention map.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, Submitted to AAAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Retrieval-Augmentation and Parameter-Efficient Fine-Tuning for
  Privacy-Preserving Personalization of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Salemi, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Privacy-preserving methods for personalizing large language models (LLMs) are
relatively under-explored. There are two schools of thought on this topic: (1)
generating personalized outputs by personalizing the input prompt through
retrieval augmentation from the user's personal information (RAG-based
methods), and (2) parameter-efficient fine-tuning of LLMs per user that
considers efficiency and space limitations (PEFT-based methods). This paper
presents the first systematic comparison between two approaches on a wide range
of personalization tasks using seven diverse datasets. Our results indicate
that RAG-based and PEFT-based personalization methods on average yield 14.92%
and 1.07% improvements over the non-personalized LLM, respectively. We find
that combining RAG with PEFT elevates these improvements to 15.98%.
Additionally, we identify a positive correlation between the amount of user
data and PEFT's effectiveness, indicating that RAG is a better choice for
cold-start users (i.e., user's with limited personal data).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uddessho: An Extensive Benchmark <span class="highlight-title">Dataset</span> for Multimodal Author Intent
  Classification in Low-Resource Bangla Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatema Tuj Johora Faria, Mukaffi Bin Moin, Md. Mahfuzur Rahman, Md Morshed Alam Shanto, Asif Iftekher Fahim, Md. Moinul Hoque
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing popularity of daily information sharing and acquisition
on the Internet, this paper introduces an innovative approach for intent
classification in Bangla language, focusing on social media posts where
individuals share their thoughts and opinions. The proposed method leverages
multimodal data with particular emphasis on authorship identification, aiming
to understand the underlying purpose behind textual content, especially in the
context of varied user-generated posts on social media. Current methods often
face challenges in low-resource languages like Bangla, particularly when author
traits intricately link with intent, as observed in social media posts. To
address this, we present the Multimodal-based Author Bangla Intent
Classification (MABIC) framework, utilizing text and images to gain deeper
insights into the conveyed intentions. We have created a dataset named
"Uddessho," comprising 3,048 instances sourced from social media. Our
methodology comprises two approaches for classifying textual intent and
multimodal author intent, incorporating early fusion and late fusion
techniques. In our experiments, the unimodal approach achieved an accuracy of
64.53% in interpreting Bangla textual intent. In contrast, our multimodal
approach significantly outperformed traditional unimodal methods, achieving an
accuracy of 76.19%. This represents an improvement of 11.66%. To our best
knowledge, this is the first research work on multimodal-based author intent
classification for low-resource Bangla language social media posts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in "18th International Conference on
  Information Technology and Applications (ICITA 2024)"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic4Health: Generating Annotated Synthetic Clinical Letters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Libo Ren, Samuel Belkadi, Lifeng Han, Warren Del-Pinto, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since clinical letters contain sensitive information, clinical-related
datasets can not be widely applied in model training, medical research, and
teaching. This work aims to generate reliable, various, and de-identified
synthetic clinical letters. To achieve this goal, we explored different
pre-trained language models (PLMs) for masking and generating text. After that,
we worked on Bio\_ClinicalBERT, a high-performing model, and experimented with
different masking strategies. Both qualitative and quantitative methods were
used for evaluation. Additionally, a downstream task, Named Entity Recognition
(NER), was also implemented to assess the usability of these synthetic letters.
  The results indicate that 1) encoder-only models outperform encoder-decoder
models. 2) Among encoder-only models, those trained on general corpora perform
comparably to those trained on clinical data when clinical information is
preserved. 3) Additionally, preserving clinical entities and document structure
better aligns with our objectives than simply fine-tuning the model. 4)
Furthermore, different masking strategies can impact the quality of synthetic
clinical letters. Masking stopwords has a positive impact, while masking nouns
or verbs has a negative effect. 5) For evaluation, BERTScore should be the
primary quantitative evaluation metric, with other metrics serving as
supplementary references. 6) Contextual information does not significantly
impact the models' understanding, so the synthetic clinical letters have the
potential to replace the original ones in downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ongoing work, 48 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keeping Humans in the Loop: Human-Centered Automated Annotation with
  Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Pangakis, Samuel Wolken
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated text annotation is a compelling use case for generative large
language models (LLMs) in social media research. Recent work suggests that LLMs
can achieve strong performance on annotation tasks; however, these studies
evaluate LLMs on a small number of tasks and likely suffer from contamination
due to a reliance on public benchmark datasets. Here, we test a human-centered
framework for responsibly evaluating artificial intelligence tools used in
automated annotation. We use GPT-4 to replicate 27 annotation tasks across 11
password-protected datasets from recently published computational social
science articles in high-impact journals. For each task, we compare GPT-4
annotations against human-annotated ground-truth labels and against annotations
from separate supervised classification models fine-tuned on human-generated
labels. Although the quality of LLM labels is generally high, we find
significant variation in LLM performance across tasks, even within datasets.
Our findings underscore the importance of a human-centered workflow and careful
evaluation standards: Automated annotations significantly diverge from human
judgment in numerous scenarios, despite various optimization strategies such as
prompt tuning. Grounding automated annotation in validation labels generated by
humans is essential for responsible evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking the Influence of Source Code on Test Case Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Huang, Jie M. Zhang, Mingzhe Du, Mark Harman, Heming Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have been widely applied to assist test
generation with the source code under test provided as the context. This paper
aims to answer the question: If the source code under test is incorrect, will
LLMs be misguided when generating tests? The effectiveness of test cases is
measured by their accuracy, coverage, and bug detection effectiveness. Our
evaluation results with five open- and six closed-source LLMs on four datasets
demonstrate that incorrect code can significantly mislead LLMs in generating
correct, high-coverage, and bug-revealing tests. For instance, in the HumanEval
dataset, LLMs achieve 80.45% test accuracy when provided with task descriptions
and correct code, but only 57.12% when given task descriptions and incorrect
code. For the APPS dataset, prompts with correct code yield tests that detect
39.85% of the bugs, while prompts with incorrect code detect only 19.61%. These
findings have important implications for the deployment of LLM-based testing:
using it on mature code may help protect against future regression, but on
early-stage immature code, it may simply bake in errors. Our findings also
underscore the need for further research to improve LLMs resilience against
incorrect code in generating reliable and bug-revealing tests.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing <span class="highlight-title">LLM</span> Problem Solving with REAP: Reflection, Explicit Problem
  Deconstruction, and Advanced <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Lingo, Martin Arroyo, Rajeev Chhajer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have transformed natural language processing,
yet improving their problem-solving capabilities, particularly for complex,
reasoning-intensive tasks, remains a persistent challenge. This paper
introduces the REAP (Reflection, Explicit Problem Deconstruction, and Advanced
Prompting) method, an innovative approach within the dynamic context generation
framework. REAP guides LLMs through reflection on the query, deconstructing it
into manageable components, and generating relevant context to enhance the
solution process. We evaluated REAP using a dataset designed to expose LLM
limitations, comparing zero-shot prompting with REAP-enhanced prompts across
six state-of-the-art models: OpenAI's o1-preview, o1-mini, GPT-4o, GPT-4o-mini,
Google's Gemini 1.5 Pro, and Claude 3.5 Sonnet. The results demonstrate notable
performance gains, with o1-mini improving by 40.97%, GPT-4o by 66.26%, and
GPT-4o-mini by 112.93%. Despite the already strong baseline performance of
OpenAI's o1-preview, modest gains were observed. Beyond performance
improvements, REAP offers a cost-effective solution; for example, GPT-4o-mini,
which is approximately 100 times cheaper than o1-preview, delivered competitive
results. REAP also improves the clarity of model outputs, making it easier for
humans to understand the reasoning behind the results and simplifying the
process of identifying and addressing any issues. These findings demonstrate
REAP's potential to greatly improve the capabilities of LLMs, providing both
better performance and increased cost-efficiency across a wide range of
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>524 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constructive Approach to Bidirectional Causation between Qualia
  Structure and Language Emergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tadahiro Taniguchi, Masafumi Oizumi, Noburo Saji, Takato Horii, Naotsugu Tsuchiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel perspective on the bidirectional causation
between language emergence and relational structure of subjective experiences,
termed qualia structure, and lays out the constructive approach to the
intricate dependency between the two. We hypothesize that languages with
distributional semantics, e.g., syntactic-semantic structures, may have emerged
through the process of aligning internal representations among individuals, and
such alignment of internal representations facilitates more structured
language. This mutual dependency is suggested by the recent advancements in AI
and symbol emergence robotics, and collective predictive coding (CPC)
hypothesis, in particular. Computational studies show that neural network-based
language models form systematically structured internal representations, and
multimodal language models can share representations between language and
perceptual information. This perspective suggests that language emergence
serves not only as a mechanism creating a communication tool but also as a
mechanism for allowing people to realize shared understanding of qualitative
experiences. The paper discusses the implications of this bidirectional
causation in the context of consciousness studies, linguistics, and cognitive
science, and outlines future constructive research directions to further
explore this dynamic relationship between language emergence and qualia
structure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Diverse and Efficient Audio Captioning via Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manjie Xu, Chenxing Li, Xinyi Tu, Yong Ren, Ruibo Fu, Wei Liang, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Diffusion-based Audio Captioning (DAC), a non-autoregressive
diffusion model tailored for diverse and efficient audio captioning. Although
existing captioning models relying on language backbones have achieved
remarkable success in various captioning tasks, their insufficient performance
in terms of generation speed and diversity impede progress in audio
understanding and multimedia applications. Our diffusion-based framework offers
unique advantages stemming from its inherent stochasticity and holistic context
modeling in captioning. Through rigorous evaluation, we demonstrate that DAC
not only achieves SOTA performance levels compared to existing benchmarks in
the caption quality, but also significantly outperforms them in terms of
generation speed and diversity. The success of DAC illustrates that text
generation can also be seamlessly integrated with audio and visual generation
tasks using a diffusion backbone, paving the way for a unified, audio-related
generative model across different modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://sites.google.com/view/diffusion-audio-captioning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating <span class="highlight-title">Event</span>-oriented Attribution for Movies via Two-Stage
  Prefix-Enhanced Multimodal <span class="highlight-title">LLM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanjie Lyu, Tong Xu, Zihan Niu, Bo Peng, Jing Ke, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prosperity of social media platforms has raised the urgent demand for
semantic-rich services, e.g., event and storyline attribution. However, most
existing research focuses on clip-level event understanding, primarily through
basic captioning tasks, without analyzing the causes of events across an entire
movie. This is a significant challenge, as even advanced multimodal large
language models (MLLMs) struggle with extensive multimodal information due to
limited context length. To address this issue, we propose a Two-Stage
Prefix-Enhanced MLLM (TSPE) approach for event attribution, i.e., connecting
associated events with their causal semantics, in movie videos. In the local
stage, we introduce an interaction-aware prefix that guides the model to focus
on the relevant multimodal information within a single clip, briefly
summarizing the single event. Correspondingly, in the global stage, we
strengthen the connections between associated events using an inferential
knowledge graph, and design an event-aware prefix that directs the model to
focus on associated events rather than all preceding clips, resulting in
accurate event attribution. Comprehensive evaluations of two real-world
datasets demonstrate that our framework outperforms state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Overcoming linguistic barriers in code assistants: creating a QLoRA
  adapter to improve support for Russian-language code writing instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        C. B. Pronin, A. V. Volosova, A. V. Ostroukh, Yu. N. Strogov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, an approach to training and evaluating an adapter model for
the popular language model "zephyr-7b-beta" is described. The adapter was
developed to improve the performance of the base model in tasks related to
programming and understanding the Russian language. Considering the high
quality of the original model in tasks in the English language, the goal of the
research was to expand its linguistic and technical spectrum. The proposed
adapter was trained using a large and diverse dataset, including
question-answer pairs related to programming, as well code-related texts in
Russian language. The applied training methodology ensures an improvement in
the model's quality of answers in understanding and generating Python code
based on Russian instructions. We evaluated the performance of the base model
with the installed adapter using various metrics, comparing it to the base
model as well as other state-of-the-art models in this field. The obtained
results showed significant improvement, both in tasks related to writing Python
code and in processing the Russian language, confirming the effectiveness of
the proposed adapter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Fine-Tuning of Large Language Models for Automated Medical
  Documentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Yi Leong, Yi Fan Gao, Ji Shuai, Uktu Pamuksuz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific research indicates that for every hour spent in direct patient
care, physicians spend nearly two additional hours on administrative tasks,
particularly on electronic health records (EHRs) and desk work. This excessive
administrative burden not only reduces the time available for patient care but
also contributes to physician burnout and inefficiencies in healthcare
delivery. To address these challenges, this study introduces MediGen, a
fine-tuned large language model (LLM) designed to automate the generation of
medical reports from medical dialogues. By leveraging state-of-the-art
methodologies for fine-tuning open-source pretrained models, including
LLaMA3-8B, MediGen achieves high accuracy in transcribing and summarizing
clinical interactions. The fine-tuned LLaMA3-8B model demonstrated promising
results, achieving a ROUGE score of 58% and a BERTScore-F1 of 72%, indicating
its effectiveness in generating accurate and clinically relevant medical
reports. These findings suggest that MediGen has the potential to significantly
reduce the administrative workload on physicians, improving both healthcare
efficiency and physician well-being.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 Figures, 3 Tables, This is a preprint version of the
  article. The final version will be published in the proceedings of the IEEE
  conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Compressive Memory-based Retrieval Approach for <span class="highlight-title">Event</span> <span class="highlight-title">Argument</span>
  <span class="highlight-title">Extraction</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanlong Liu, Enqi Zhang, Li Zhou, Dingyi Zeng, Shaohuan Cheng, Chen Zhang, Malu Zhang, Wenyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have demonstrated the effectiveness of retrieval augmentation in
the Event Argument Extraction (EAE) task. However, existing retrieval-based EAE
methods have two main limitations: (1) input length constraints and (2) the gap
between the retriever and the inference model. These issues limit the diversity
and quality of the retrieved information. In this paper, we propose a
Compressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the
two limitations mentioned above. Our compressive memory, designed as a dynamic
matrix that effectively caches retrieved information and supports continuous
updates, overcomes the limitations of the input length. Additionally, after
pre-loading all candidate demonstrations into the compressive memory, the model
further retrieves and filters relevant information from memory based on the
input query, bridging the gap between the retriever and the inference model.
Extensive experiments show that our method achieves new state-of-the-art
performance on three public datasets (RAMS, WikiEvents, ACE05), significantly
outperforming existing retrieval-based EAE methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yahan Tu, Rui Hu, Jitao Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucination poses a significant challenge for multimodal large language
models (MLLMs). However, existing benchmarks for evaluating hallucinations are
static, which can lead to potential data contamination. This paper introduces
ODE, an open-set, dynamic protocol for evaluating object existence
hallucinations in MLLMs. Our framework employs graph structures to model
associations between real-word concepts and generates novel samples for both
general and domain-specific scenarios. The dynamic combination of concepts,
along with various combination principles, ensures a broad sample distribution.
Experimental results show that MLLMs exhibit higher hallucination rates with
ODE-generated samples, effectively avoiding data contamination. Moreover, these
samples can also be used for fine-tuning to improve MLLM performance on
existing benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models "Grok" to Copy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Lv, Ruobing Xie, Xingwu Sun, Zhanhui Kang, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine the pre-training dynamics of language models, focusing on their
ability to copy text from preceding context--a fundamental skill for various
LLM applications, including in-context learning (ICL) and retrieval-augmented
generation (RAG). We propose a novel perspective that Transformer-based
language models develop copying abilities similarly to grokking, which refers
to sudden generalization on test set long after the model fit to the training
set. Our experiments yield three arguments: (1) The pre-training loss decreases
rapidly, while the context copying ability of models initially lags and then
abruptly saturates. (2) The speed of developing copying ability is independent
of the number of tokens trained, similarly to how grokking speed is unaffected
by dataset size as long as the data distribution is preserved. (3) Induction
heads, the attention heads responsible for copying, form from shallow to deep
layers during training, mirroring the development of circuits in deeper layers
during grokking. We contend that the connection between grokking and context
copying can provide valuable insights for more effective language model
training, ultimately improving in-context performance. For example, we
demonstrated that techniques that enhance grokking, such as regularization,
either accelerate or enhance the development of context copying.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An empirical evaluation of using ChatGPT to summarize disputes for
  recommending similar labor and employment cases in Chinese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-Hsien Wu, Chao-Lin Liu, Wei-Jie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a hybrid mechanism for recommending similar cases of labor and
employment litigations. The classifier determines the similarity based on the
itemized disputes of the two cases, that the courts prepared. We cluster the
disputes, compute the cosine similarity between the disputes, and use the
results as the features for the classification tasks. Experimental results
indicate that this hybrid approach outperformed our previous system, which
considered only the information about the clusters of the disputes. We replaced
the disputes that were prepared by the courts with the itemized disputes that
were generated by GPT-3.5 and GPT-4, and repeated the same experiments. Using
the disputes generated by GPT-4 led to better results. Although our classifier
did not perform as well when using the disputes that the ChatGPT generated, the
results were satisfactory. Hence, we hope that the future large-language models
will become practically useful.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 2 tables, the 18th Int'l Workshop on
  Juris-Informatics (JURISIN 2024), associated with the 16th JSAI International
  Symposium on AI (JSAI-isAI 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guiding Vision-Language Model Selection for Visual Question-Answering
  Across Tasks, Domains, and Knowledge Types 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neelabh Sinha, Vinija Jain, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question-Answering (VQA) has become a key use-case in several
applications to aid user experience, particularly after Vision-Language Models
(VLMs) achieving good results in zero-shot inference. But evaluating different
VLMs for an application requirement using a standardized framework in practical
settings is still challenging. This paper introduces a comprehensive framework
for evaluating VLMs tailored to VQA tasks in practical settings. We present a
novel dataset derived from established VQA benchmarks, annotated with task
types, application domains, and knowledge types, three key practical aspects on
which tasks can vary. We also introduce GoEval, a multimodal evaluation metric
developed using GPT-4o, achieving a correlation factor of 56.71% with human
judgments. Our experiments with ten state-of-the-art VLMs reveals that no
single model excelling universally, making appropriate selection a key design
decision. Proprietary models such as Gemini-1.5-Pro and GPT-4o-mini generally
outperform others, though open-source models like InternVL-2-8B and
CogVLM-2-Llama-3-19B demonstrate competitive strengths in specific contexts,
while providing additional advantages. This study guides the selection of VLMs
based on specific task requirements and resource constraints, and can also be
extended to other vision-language tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages + references + 6 pages of Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Is Wrong with My Model? Identifying Systematic Problems with
  Semantic Data Slicing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Yang, Yining Hong, Grace A. Lewis, Tongshuang Wu, Christian Kästner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models make mistakes, yet sometimes it is difficult to
identify the systematic problems behind the mistakes. Practitioners engage in
various activities, including error analysis, testing, auditing, and
red-teaming, to form hypotheses of what can go (or has gone) wrong with their
models. To validate these hypotheses, practitioners employ data slicing to
identify relevant examples. However, traditional data slicing is limited by
available features and programmatic slicing functions. In this work, we propose
SemSlicer, a framework that supports semantic data slicing, which identifies a
semantically coherent slice, without the need for existing features. SemSlicer
uses Large Language Models to annotate datasets and generate slices from any
user-defined slicing criteria. We show that SemSlicer generates accurate slices
with low cost, allows flexible trade-offs between different design dimensions,
reliably identifies under-performing data slices, and helps practitioners
identify useful data slices that reflect systematic problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing Correlations Between Intrinsic and Extrinsic Bias Metrics of
  Static Word Embeddings With Their Measuring Biases Aligned 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taisei Katô, Yusuke Miyao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine the abilities of intrinsic bias metrics of static word embeddings
to predict whether Natural Language Processing (NLP) systems exhibit biased
behavior. A word embedding is one of the fundamental NLP technologies that
represents the meanings of words through real vectors, and problematically, it
also learns social biases such as stereotypes. An intrinsic bias metric
measures bias by examining a characteristic of vectors, while an extrinsic bias
metric checks whether an NLP system trained with a word embedding is biased. A
previous study found that a common intrinsic bias metric usually does not
correlate with extrinsic bias metrics. However, the intrinsic and extrinsic
bias metrics did not measure the same bias in most cases, which makes us
question whether the lack of correlation is genuine. In this paper, we extract
characteristic words from datasets of extrinsic bias metrics and analyze
correlations with intrinsic bias metrics with those words to ensure both
metrics measure the same bias. We observed moderate to high correlations with
some extrinsic bias metrics but little to no correlations with the others. This
result suggests that intrinsic bias metrics can predict biased behavior in
particular settings but not in others. Experiment codes are available at
GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleash <span class="highlight-title">LLM</span>s Potential for Recommendation by Coordinating Twin-Tower
  Dynamic Semantic Token Generator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yin, Zhengxin Zeng, Mingzheng Li, Hao Yan, Chaozhuo Li, Weihao Han, Jianjin Zhang, Ruochen Liu, Allen Sun, Denvy Deng, Feng Sun, Qi Zhang, Shirui Pan, Senzhang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Owing to the unprecedented capability in semantic understanding and logical
reasoning, the pre-trained large language models (LLMs) have shown fantastic
potential in developing the next-generation recommender systems (RSs). However,
the static index paradigm adopted by current methods greatly restricts the
utilization of LLMs capacity for recommendation, leading to not only the
insufficient alignment between semantic and collaborative knowledge, but also
the neglect of high-order user-item interaction patterns. In this paper, we
propose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS
which adopts dynamic semantic index paradigm, targeting at resolving the above
problems simultaneously. To be more specific, we for the first time contrive a
dynamic knowledge fusion framework which integrates a twin-tower semantic token
generator into the LLM-based recommender, hierarchically allocating meaningful
semantic index for items and users, and accordingly predicting the semantic
index of target item. Furthermore, a dual-modality variational auto-encoder is
proposed to facilitate multi-grained alignment between semantic and
collaborative knowledge. Eventually, a series of novel tuning tasks specially
customized for capturing high-order user-item interaction patterns are proposed
to take advantages of user historical behavior. Extensive experiments across
three public datasets demonstrate the superiority of the proposed methodology
in developing LLM-based generative RSs. The proposed TTDS recommender achieves
an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric,
compared with the leading baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NovAScore: A New Automated Metric for Evaluating Document Level Novelty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Ai, Ziwei Gong, Harshsaiprasad Deshpande, Alexander Johnson, Emmy Phung, Ahmad Emami, Julia Hirschberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid expansion of online content has intensified the issue of
information redundancy, underscoring the need for solutions that can identify
genuinely new information. Despite this challenge, the research community has
seen a decline in focus on novelty detection, particularly with the rise of
large language models (LLMs). Additionally, previous approaches have relied
heavily on human annotation, which is time-consuming, costly, and particularly
challenging when annotators must compare a target document against a vast
number of historical documents. In this work, we introduce NovAScore (Novelty
Evaluation in Atomicity Score), an automated metric for evaluating
document-level novelty. NovAScore aggregates the novelty and salience scores of
atomic information, providing high interpretability and a detailed analysis of
a document's novelty. With its dynamic weight adjustment scheme, NovAScore
offers enhanced flexibility and an additional dimension to assess both the
novelty level and the importance of information within a document. Our
experiments show that NovAScore strongly correlates with human judgments of
novelty, achieving a 0.626 Point-Biserial correlation on the TAP-DLND 1.0
dataset and a 0.920 Pearson correlation on an internal human-annotated dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Training of Neural Networks at Arbitrary Precision and Sparsity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxi Ye, Grace Chu, Yanfeng Liu, Yichi Zhang, Lukasz Lew, Andrew Howard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discontinuous operations inherent in quantization and sparsification
introduce obstacles to backpropagation. This is particularly challenging when
training deep neural networks in ultra-low precision and sparse regimes. We
propose a novel, robust, and universal solution: a denoising affine transform
that stabilizes training under these challenging conditions. By formulating
quantization and sparsification as perturbations during training, we derive a
perturbation-resilient approach based on ridge regression. Our solution employs
a piecewise constant backbone model to ensure a performance lower bound and
features an inherent noise reduction mechanism to mitigate perturbation-induced
corruption. This formulation allows existing models to be trained at
arbitrarily low precision and sparsity levels with off-the-shelf recipes.
Furthermore, our method provides a novel perspective on training temporal
binary neural networks, contributing to ongoing efforts to narrow the gap
between artificial and biological neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autoregressive + Chain of Thought (CoT) $\simeq$ Recurrent: Recurrence's
  Role in Language Models and a Revist of Recurrent Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Zhang, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Transformer architecture excels in a variety of language modeling tasks,
outperforming traditional neural architectures such as RNN and LSTM. This is
partially due to its elimination of recurrent connections, which allows for
parallel training and a smoother flow of gradients. However, this move away
from recurrent structures places the Transformer model at the lower end of
Chomsky's computational hierarchy, imposing limitations on its computational
abilities. Consequently, even advanced Transformer-based models face
considerable difficulties in tasks like counting, string reversal, bracket
pairing, and multiplication. These tasks, though seemingly elementary, require
a level of computational complexity that exceeds the capabilities of the
Transformer architecture. Concurrently, the emergence of ``Chain of Thought"
(CoT) prompting has enabled Transformer-based language models to tackle tasks
that were previously impossible or poorly executed. Despite some previous
research primarily interpreting CoT from a psychological perspective, a
comprehensive understanding of \textit{why} CoT proves so effective in the
reasoning process remains elusive. In this work, we thoroughly investigate the
influence of recurrent structures in language models on their reasoning
abilities, shedding light on how the CoT approach can mimic recurrent
computation and act as a bridge between autoregression and recurrence. It is
this approximated recurrence that notably improves the model's performance and
computational capacity. Moreover, we revisit recent recurrent-based Transformer
model designs, focusing on their computational abilities through our proposed
concept of ``recurrence-completeness" and identify key theoretical limitations
in models like Linear Transformer and RWKV. Through this, we aim to provide
insight into the neural model architectures and prompt better model design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A New Era in Computational Pathology: A <span class="highlight-title">Survey</span> on Foundation and
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14496v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14496v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dibaloke Chanda, Milan Aryal, Nasim Yahya Soltani, Masoud Ganji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep learning have completely transformed the domain of
computational pathology (CPath), which in turn altered the diagnostic workflow
of pathologists by integrating foundation models (FMs) and vision-language
models (VLMs) in their assessment and decision-making process. FMs overcome the
limitations of existing deep learning approaches in CPath by learning a
representation space that can be adapted to a wide variety of downstream tasks
without explicit supervision. VLMs allow pathology reports written in natural
language to be used as a rich semantic information source to improve existing
models as well as generate predictions in natural language form. In this
survey, a holistic and systematic overview of recent innovations in FMs and
VLMs in CPath is presented. Furthermore, the tools, datasets and training
schemes for these models are summarized in addition to categorizing them into
distinct groups. This extensive survey highlights the current trends in CPath
and the way it is going to be transformed through FMs and VLMs in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 19 figures and 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anchored Preference Optimization and Contrastive Revisions: Addressing
  Underspecification in Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06266v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06266v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karel D'Oosterlinck, Winnie Xu, Chris Develder, Thomas Demeester, Amanpreet Singh, Christopher Potts, Douwe Kiela, Shikib Mehri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are often aligned using contrastive alignment
objectives and preference pair datasets. The interaction between model, paired
data, and objective makes alignment a complicated procedure, sometimes
producing subpar results. We study this and find that (i) preference data gives
a better learning signal when the underlying responses are contrastive, and
(ii) alignment objectives lead to better performance when they specify more
control over the model during training. Based on these insights, we introduce
Contrastive Learning from AI Revisions (CLAIR), a data-creation method which
leads to more contrastive preference pairs, and Anchored Preference
Optimization (APO), a controllable and more stable alignment objective. We
align Llama-3-8B-Instruct using various comparable datasets and alignment
objectives and measure MixEval-Hard scores, which correlate highly with human
judgments. The CLAIR preferences lead to the strongest performance out of all
datasets, and APO consistently outperforms less controllable objectives. Our
best model, trained on 32K CLAIR preferences with APO, improves
Llama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code
is available at https://github.com/ContextualAI/CLAIR_and_APO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid Attention-based Encoder-decoder Model for Efficient Language
  Model Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07369v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07369v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoshi Ling, Guoli Ye, Rui Zhao, Yifan Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The attention-based encoder-decoder (AED) speech recognition model has been
widely successful in recent years. However, the joint optimization of acoustic
model and language model in end-to-end manner has created challenges for text
adaptation. In particular, effective, quick and inexpensive adaptation with
text input has become a primary concern for deploying AED systems in the
industry. To address this issue, we propose a novel model, the hybrid
attention-based encoder-decoder (HAED) speech recognition model that preserves
the modularity of conventional hybrid automatic speech recognition systems. Our
HAED model separates the acoustic and language models, allowing for the use of
conventional text-based language model adaptation techniques. We demonstrate
that the proposed HAED model yields 23% relative Word Error Rate (WER)
improvements when out-of-domain text data is used for language model
adaptation, with only a minor degradation in WER on a general test set compared
with the conventional AED model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StateFlow: Enhancing <span class="highlight-title">LLM</span> Task-Solving through State-Driven Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11322v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11322v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, Qingyun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is a notable trend to use Large Language Models (LLMs) to tackle complex
tasks, e.g., tasks that require a sequence of actions and dynamic interaction
with tools and external environments. In this paper, we propose StateFlow, a
novel LLM-based task-solving paradigm that conceptualizes complex task-solving
processes as state machines. In StateFlow, we distinguish between "process
grounding" (via state and state transitions) and "sub-task solving" (through
actions within a state), enhancing control and interpretability of the
task-solving procedure. A state represents the status of a running process. The
transitions between states are controlled by heuristic rules or decisions made
by the LLM, allowing for a dynamic and adaptive progression. Upon entering a
state, a series of actions is executed, involving not only calling LLMs guided
by different prompts, but also the utilization of external tools as needed. Our
results show that StateFlow significantly enhances LLMs' efficiency. For
instance, StateFlow achieves 13% and 28% higher success rates compared to ReAct
in InterCode SQL and ALFWorld benchmark, with 5x and 3x less cost respectively.
We also show that StateFlow can be combined with iterative refining methods
like Reflexion to further improve performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seeing Like an AI: How <span class="highlight-title">LLM</span>s Apply (and Misapply) Wikipedia Neutrality
  Norms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04183v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04183v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Ashkinaze, Ruijia Guan, Laura Kurek, Eytan Adar, Ceren Budak, Eric Gilbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are trained on broad corpora and then used in
communities with specialized norms. Is providing LLMs with community rules
enough for models to follow these norms? We evaluate LLMs' capacity to detect
(Task 1) and correct (Task 2) biased Wikipedia edits according to Wikipedia's
Neutral Point of View (NPOV) policy. LLMs struggled with bias detection,
achieving only 64% accuracy on a balanced dataset. Models exhibited contrasting
biases (some under- and others over-predicted bias), suggesting distinct priors
about neutrality. LLMs performed better at generation, removing 79% of words
removed by Wikipedia editors. However, LLMs made additional changes beyond
Wikipedia editors' simpler neutralizations, resulting in high-recall but
low-precision editing. Interestingly, crowdworkers rated AI rewrites as more
neutral (70%) and fluent (61%) than Wikipedia-editor rewrites. Qualitative
analysis found LLMs sometimes applied NPOV more comprehensively than Wikipedia
editors but often made extraneous non-NPOV-related changes (such as grammar).
LLMs may apply rules in ways that resonate with the public but diverge from
community experts. While potentially effective for generation, LLMs may reduce
editor agency and increase moderation workload (e.g., verifying additions).
Even when rules are easy to articulate, having LLMs apply them like community
members may still be difficult.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Course-Skill Atlas: A national longitudinal <span class="highlight-title">dataset</span> of skills taught in
  U.S. higher education curricula 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Javadian Sabet, Sarah H. Bana, Renzhe Yu, Morgan R. Frank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Higher education plays a critical role in driving an innovative economy by
equipping students with knowledge and skills demanded by the workforce. While
researchers and practitioners have developed data systems to track detailed
occupational skills, such as those established by the U.S. Department of Labor
(DOL), much less effort has been made to document which of these skills are
being developed in higher education at a similar granularity. Here, we fill
this gap by presenting Course-Skill Atlas -- a longitudinal dataset of skills
inferred from over three million course syllabi taught at nearly three thousand
U.S. higher education institutions. To construct Course-Skill Atlas, we apply
natural language processing to quantify the alignment between course syllabi
and detailed workplace activities (DWAs) used by the DOL to describe
occupations. We then aggregate these alignment scores to create skill profiles
for institutions and academic majors. Our dataset offers a large-scale
representation of college education's role in preparing students for the labor
market. Overall, Course-Skill Atlas can enable new research on the source of
skills in the context of workforce development and provide actionable insights
for shaping the future of higher education to meet evolving labor demands,
especially in the face of new technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 14 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SSDM: Scalable Speech Dysfluency Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16221v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16221v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachen Lian, Xuanru Zhou, Zoe Ezzes, Jet Vonk, Brittany Morin, David Baquirin, Zachary Mille, Maria Luisa Gorno Tempini, Gopala Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech dysfluency modeling is the core module for spoken language learning,
and speech therapy. However, there are three challenges. First, current
state-of-the-art solutions suffer from poor scalability. Second, there is a
lack of a large-scale dysfluency corpus. Third, there is not an effective
learning framework. In this paper, we propose \textit{SSDM: Scalable Speech
Dysfluency Modeling}, which (1) adopts articulatory gestures as scalable forced
alignment; (2) introduces connectionist subsequence aligner (CSA) to achieve
dysfluency alignment; (3) introduces a large-scale simulated dysfluency corpus
called Libri-Dys; and (4) develops an end-to-end system by leveraging the power
of large language models (LLMs). We expect SSDM to serve as a standard in the
area of dysfluency modeling. Demo is available at
\url{https://eureka235.github.io}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ClarQ-<span class="highlight-title">LLM</span>: A Benchmark for Models Clarifying and Requesting <span class="highlight-title">Information</span>
  in Task-Oriented Dialog 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujian Gan, Changling Li, Jinxia Xie, Luou Wen, Matthew Purver, Massimo Poesio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ClarQ-LLM, an evaluation framework consisting of bilingual
English-Chinese conversation tasks, conversational agents and evaluation
metrics, designed to serve as a strong benchmark for assessing agents' ability
to ask clarification questions in task-oriented dialogues. The benchmark
includes 31 different task types, each with 10 unique dialogue scenarios
between information seeker and provider agents. The scenarios require the
seeker to ask questions to resolve uncertainty and gather necessary information
to complete tasks. Unlike traditional benchmarks that evaluate agents based on
fixed dialogue content, ClarQ-LLM includes a provider conversational agent to
replicate the original human provider in the benchmark. This allows both
current and future seeker agents to test their ability to complete information
gathering tasks through dialogue by directly interacting with our provider
agent. In tests, LLAMA3.1 405B seeker agent managed a maximum success rate of
only 60.05\%, showing that ClarQ-LLM presents a strong challenge for future
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M<span class="highlight-title">LLM</span>-Bench: Evaluating Multimodal <span class="highlight-title">LLM</span>s with Per-sample Criteria 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13951v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13951v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Ge, Shunian Chen, Guiming Hardy Chen, Junying Chen, Zhihong Chen, Nuo Chen, Wenya Xie, Shuo Yan, Chenghao Zhu, Ziyue Lin, Song Dingjie, Xidong Wang, Anningzhe Gao, Zhang Zhiyi, Jianquan Li, Xiang Wan, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have broadened the scope of AI
applications. Existing automatic evaluation methodologies for MLLMs are mainly
limited in evaluating queries without considering user experiences,
inadequately addressing the nuances of creative and associative multimodal
tasks. However, the open-ended and subjective nature of such tasks poses a
significant challenge to the evaluation methodology, where it is difficult to
define the ground-truth answers for them. To this end, in our paper, we propose
a new evaluation paradigm for MLLMs, which is evaluating MLLMs with per-sample
criteria using potent MLLM as the judge. To validate the feasibility and
effectiveness of this paradigm, we design a benchmark, dubbed MLLM-Bench, by
curating the evaluation samples across six comprehensive cognitive levels. We
benchmark 21 popular MLLMs in a pairwise-comparison fashion, showing diverse
performance across models. Moreover, the validity of our benchmark manifests
itself in reaching 88.02% agreement with human evaluation. We contend that the
proposed paradigm explores the potential of MLLMs as effective evaluation tools
with the help of per-sample criteria. See online leaderboard at
\url{https://mllm-bench.llmzoo.com}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaVA-OneVision: Easy Visual Task Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LLaVA-OneVision, a family of open large multimodal models (LMMs)
developed by consolidating our insights into data, models, and visual
representations in the LLaVA-NeXT blog series. Our experimental results
demonstrate that LLaVA-OneVision is the first single model that can
simultaneously push the performance boundaries of open LMMs in three important
computer vision scenarios: single-image, multi-image, and video scenarios.
Importantly, the design of LLaVA-OneVision allows strong transfer learning
across different modalities/scenarios, yielding new emerging capabilities. In
particular, strong video understanding and cross-scenario capabilities are
demonstrated through task transfer from images to videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Homepage:
  https://llava-vl.github.io/blog/2024-08-05-llava-onevision/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quest: Query-centric Data Synthesis Approach for Long-context Scaling of
  Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19846v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19846v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaochen Gao, Xing Wu, Qi Fu, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models, initially pre-trained with a limited context length,
can better handle longer texts by continuing training on a corpus with extended
contexts. However, obtaining effective long-context data is challenging due to
the scarcity and uneven distribution of long documents across different
domains. To address this issue, we propose a Query-centric data synthesis
method, abbreviated as Quest. Quest is an interpretable method based on the
observation that documents retrieved by similar queries are relevant but
low-redundant, thus well-suited for synthesizing long-context data. The method
is also scalable and capable of constructing large amounts of long-context
data. Using Quest, we synthesize a long-context dataset up to 128k context
length, significantly outperforming other data synthesis methods on multiple
long-context benchmark datasets. In addition, we further verify that the Quest
method is predictable through scaling law experiments, making it a reliable
solution for advancing long-context models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CodeAttack: Revealing Safety Generalization Challenges of Large Language
  Models via Code Completion <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07865v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07865v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has brought about
remarkable generative capabilities but also raised concerns about their
potential misuse. While strategies like supervised fine-tuning and
reinforcement learning from human feedback have enhanced their safety, these
methods primarily focus on natural languages, which may not generalize to other
domains. This paper introduces CodeAttack, a framework that transforms natural
language inputs into code inputs, presenting a novel environment for testing
the safety generalization of LLMs. Our comprehensive studies on
state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a
new and universal safety vulnerability of these models against code input:
CodeAttack bypasses the safety guardrails of all models more than 80\% of the
time. We find that a larger distribution gap between CodeAttack and natural
language leads to weaker safety generalization, such as encoding natural
language input with data structures. Furthermore, we give our hypotheses about
the success of CodeAttack: the misaligned bias acquired by LLMs during code
training, prioritizing code completion over avoiding the potential safety risk.
Finally, we analyze potential mitigation measures. These findings highlight new
safety risks in the code domain and the need for more robust safety alignment
algorithms to match the code capabilities of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL Findings 2024, Code is available at
  https://github.com/renqibing/CodeAttack</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Apollo: A Lightweight Multilingual Medical <span class="highlight-title">LLM</span> towards Democratizing
  Medical AI to 6B People 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03640v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03640v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xidong Wang, Nuo Chen, Junyin Chen, Yidong Wang, Guorui Zhen, Chunxian Zhang, Xiangbo Wu, Yan Hu, Anningzhe Gao, Xiang Wan, Haizhou Li, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the vast repository of global medical knowledge predominantly being
in English, local languages are crucial for delivering tailored healthcare
services, particularly in areas with limited medical resources. To extend the
reach of medical AI advancements to a broader population, we aim to develop
medical LLMs across the six most widely spoken languages, encompassing a global
population of 6.1 billion. This effort culminates in the creation of the
ApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the
multilingual medical benchmark, the released Apollo models, at various
relatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best
performance among models of equivalent size. Especially, Apollo-7B is the
state-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite
models could be used to improve the multi-lingual medical capabilities of
larger models without fine-tuning in a proxy-tuning fashion. We will
open-source training corpora, code, model weights and evaluation benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Video Temporal Dynamics with Cross-Modal Attention for Robust
  Audio-Visual Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungnyun Kim, Kangwook Jang, Sangmin Bae, Hoirin Kim, Se-Young Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-visual speech recognition (AVSR) aims to transcribe human speech using
both audio and video modalities. In practical environments with noise-corrupted
audio, the role of video information becomes crucial. However, prior works have
primarily focused on enhancing audio features in AVSR, overlooking the
importance of video features. In this study, we strengthen the video features
by learning three temporal dynamics in video data: context order, playback
direction, and the speed of video frames. Cross-modal attention modules are
introduced to enrich video features with audio information so that speech
variability can be taken into account when training on the video temporal
dynamics. Based on our approach, we achieve the state-of-the-art performance on
the LRS2 and LRS3 AVSR benchmarks for the noise-dominant settings. Our approach
excels in scenarios especially for babble and speech noise, indicating the
ability to distinguish the speech signal that should be recognized from lip
movements in the video modality. We support the validity of our methodology by
offering the ablation experiments for the temporal dynamics losses and the
cross-modal attention architecture design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SLT 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Multimodal Mis<span class="highlight-title">information</span> Detection with Logic Reasoning <span class="chip">ACL 23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Liu, Wenya Wang, Haoliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal misinformation on online social platforms is becoming a critical
concern due to increasing credibility and easier dissemination brought by
multimedia content, compared to traditional text-only information. While
existing multimodal detection approaches have achieved high performance, the
lack of interpretability hinders these systems' reliability and practical
deployment. Inspired by NeuralSymbolic AI which combines the learning ability
of neural networks with the explainability of symbolic learning, we propose a
novel logic-based neural model for multimodal misinformation detection which
integrates interpretable logic clauses to express the reasoning process of the
target task. To make learning effective, we parameterize symbolic logical
elements using neural representations, which facilitate the automatic
generation and evaluation of meaningful logic clauses. Additionally, to make
our framework generalizable across diverse misinformation sources, we introduce
five meta-predicates that can be instantiated with different correlations.
Results on three public datasets (Twitter, Weibo, and Sarcasm) demonstrate the
feasibility and versatility of our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Findings of ACL 23. 9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction
  Retriever <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16672v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16672v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Jha, Bo Wang, Michael Günther, Georgios Mastrapas, Saba Sturua, Isabelle Mohr, Andreas Koukounas, Mohammad Kalim Akram, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-vector dense models, such as ColBERT, have proven highly effective in
information retrieval. ColBERT's late interaction scoring approximates the
joint query-document attention seen in cross-encoders while maintaining
inference efficiency closer to traditional dense retrieval models, thanks to
its bi-encoder architecture and recent optimizations in indexing and search. In
this work we propose a number of incremental improvements to the ColBERT model
architecture and training pipeline, using methods shown to work in the more
mature single-vector embedding model training paradigm, particularly those that
apply to heterogeneous multilingual data or boost efficiency with little
tradeoff. Our new model, Jina-ColBERT-v2, demonstrates strong performance
across a range of English and multilingual retrieval tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, references at pp7,8; EMNLP workshop submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Puzzle Solving using Reasoning of Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11291v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11291v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panagiotis Giadikiaroglou, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring the capabilities of Large Language Models (LLMs) in puzzle solving
unveils critical insights into their potential and challenges in AI, marking a
significant step towards understanding their applicability in complex reasoning
tasks. This survey leverages a unique taxonomy -- dividing puzzles into
rule-based and rule-less categories -- to critically assess LLMs through
various methodologies, including prompting techniques, neuro-symbolic
approaches, and fine-tuning. Through a critical review of relevant datasets and
benchmarks, we assess LLMs' performance, identifying significant challenges in
complex puzzle scenarios. Our findings highlight the disparity between LLM
capabilities and human-like reasoning, particularly in those requiring advanced
logical inference. The survey underscores the necessity for novel strategies
and richer datasets to advance LLMs' puzzle-solving proficiency and contribute
to AI's logical reasoning and creative problem-solving advancements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chain of Empathy: Enhancing Empathetic Response of Large Language Models
  Based on Psychotherapy Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoon Kyung Lee, Inju Lee, Minjung Shin, Seoyeon Bae, Sowon Hahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel method, the Chain of Empathy (CoE) prompting, that
utilizes insights from psychotherapy to induce Large Language Models (LLMs) to
reason about human emotional states. This method is inspired by various
psychotherapy approaches including Cognitive Behavioral Therapy (CBT),
Dialectical Behavior Therapy (DBT), Person Centered Therapy (PCT), and Reality
Therapy (RT), each leading to different patterns of interpreting clients'
mental states. LLMs without reasoning generated predominantly exploratory
responses. However, when LLMs used CoE reasoning, we found a more comprehensive
range of empathetic responses aligned with the different reasoning patterns of
each psychotherapy model. The CBT based CoE resulted in the most balanced
generation of empathetic responses. The findings underscore the importance of
understanding the emotional context and how it affects human and AI
communication. Our research contributes to understanding how psychotherapeutic
models can be incorporated into LLMs, facilitating the development of
context-specific, safer, and empathetic AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What does it take to get state of the art in simultaneous
  speech-to-speech translation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00965v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00965v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Wilmet, Johnson Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an in-depth analysis of the latency characteristics
observed in simultaneous speech-to-speech model's performance, particularly
focusing on hallucination-induced latency spikes. By systematically
experimenting with various input parameters and conditions, we propose methods
to minimize latency spikes and improve overall performance. The findings
suggest that a combination of careful input management and strategic parameter
adjustments can significantly enhance speech-to-speech model's latency
behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On-Device Language Models: A Comprehensive <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00088v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00088v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Xu, Zhiyuan Li, Wei Chen, Qun Wang, Xin Gao, Qi Cai, Ziyuan Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large language models (LLMs) revolutionized natural language
processing applications, and running LLMs on edge devices has become
increasingly attractive for reasons including reduced latency, data
localization, and personalized user experiences. This comprehensive review
examines the challenges of deploying computationally expensive LLMs on
resource-constrained devices and explores innovative solutions across multiple
domains. The paper investigates the development of on-device language models,
their efficient architectures, including parameter sharing and modular designs,
as well as state-of-the-art compression techniques like quantization, pruning,
and knowledge distillation. Hardware acceleration strategies and collaborative
edge-cloud deployment approaches are analyzed, highlighting the intricate
balance between performance and resource utilization. Case studies of on-device
language models from major mobile manufacturers demonstrate real-world
applications and potential benefits. The review also addresses critical aspects
such as adaptive learning, multi-modal capabilities, and personalization. By
identifying key research directions and open challenges, this paper provides a
roadmap for future advancements in on-device language models, emphasizing the
need for interdisciplinary efforts to realize the full potential of ubiquitous,
intelligent computing while ensuring responsible and ethical deployment. For a
comprehensive review of research work and educational resources on on-device
large language models (LLMs), please visit
https://github.com/NexaAI/Awesome-LLMs-on-device. To download and run on-device
LLMs, visit https://www.nexaai.com/models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Resilient and Efficient <span class="highlight-title">LLM</span>s: A Comparative Study of Efficiency,
  Performance, and Adversarial Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04585v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04585v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojing Fan, Chunliang Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing demand for practical applications of Large Language
Models (LLMs), many attention-efficient models have been developed to balance
performance and computational cost. However, the adversarial robustness of
these models remains under-explored. In this work, we design a framework to
investigate the trade-off between efficiency, performance, and adversarial
robustness of LLMs and conduct extensive experiments on three prominent models
with varying levels of complexity and efficiency -- Transformer++, Gated Linear
Attention (GLA) Transformer, and MatMul-Free LM -- utilizing the GLUE and
AdvGLUE datasets. The AdvGLUE dataset extends the GLUE dataset with adversarial
samples designed to challenge model robustness. Our results show that while the
GLA Transformer and MatMul-Free LM achieve slightly lower accuracy on GLUE
tasks, they demonstrate higher efficiency and either superior or comparative
robustness on AdvGLUE tasks compared to Transformer++ across different attack
levels. These findings highlight the potential of simplified architectures to
achieve a compelling balance between efficiency, performance, and adversarial
robustness, offering valuable insights for applications where resource
constraints and resilience to adversarial attacks are critical.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Audit on the Perspectives and Challenges of Hallucinations in NLP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07461v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07461v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Narayanan Venkit, Tatiana Chakravorti, Vipul Gupta, Heidi Biggs, Mukund Srinath, Koustava Goswami, Sarah Rajtmajer, Shomir Wilson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We audit how hallucination in large language models (LLMs) is characterized
in peer-reviewed literature, using a critical examination of 103 publications
across NLP research. Through the examination of the literature, we identify a
lack of agreement with the term `hallucination' in the field of NLP.
Additionally, to compliment our audit, we conduct a survey with 171
practitioners from the field of NLP and AI to capture varying perspectives on
hallucination. Our analysis calls for the necessity of explicit definitions and
frameworks outlining hallucination within NLP, highlighting potential
challenges, and our survey inputs provide a thematic understanding of the
influence and ramifications of hallucination in society.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafeGen: Mitigating Sexually Explicit Content Generation in
  Text-to-Image Models <span class="chip">CCS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06666v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06666v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinfeng Li, Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) models, such as Stable Diffusion, have exhibited
remarkable performance in generating high-quality images from text descriptions
in recent years. However, text-to-image models may be tricked into generating
not-safe-for-work (NSFW) content, particularly in sexually explicit scenarios.
Existing countermeasures mostly focus on filtering inappropriate inputs and
outputs, or suppressing improper text embeddings, which can block sexually
explicit content (e.g., naked) but may still be vulnerable to adversarial
prompts -- inputs that appear innocent but are ill-intended. In this paper, we
present SafeGen, a framework to mitigate sexual content generation by
text-to-image models in a text-agnostic manner. The key idea is to eliminate
explicit visual representations from the model regardless of the text input. In
this way, the text-to-image model is resistant to adversarial prompts since
such unsafe visual representations are obstructed from within. Extensive
experiments conducted on four datasets and large-scale user studies demonstrate
SafeGen's effectiveness in mitigating sexually explicit content generation
while preserving the high-fidelity of benign images. SafeGen outperforms eight
state-of-the-art baseline methods and achieves 99.4% sexual content removal
performance. Furthermore, our constructed benchmark of adversarial prompts
provides a basis for future development and evaluation of anti-NSFW-generation
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM CCS 2024. Please cite this paper as "Xinfeng Li,
  Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu.
  SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image
  Models. In Proceedings of ACM Conference on Computer and Communications
  Security (CCS), 2024."</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tamper-Resistant Safeguards for Open-Weight <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00761v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00761v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishub Tamirisa, Bhrugu Bharathi, Long Phan, Andy Zhou, Alice Gatti, Tarun Suresh, Maxwell Lin, Justin Wang, Rowan Wang, Ron Arel, Andy Zou, Dawn Song, Bo Li, Dan Hendrycks, Mantas Mazeika
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid advances in the capabilities of large language models (LLMs) have
raised widespread concerns regarding their potential for malicious use.
Open-weight LLMs present unique challenges, as existing safeguards lack
robustness to tampering attacks that modify model weights. For example, recent
works have demonstrated that refusal and unlearning safeguards can be trivially
removed with a few steps of fine-tuning. These vulnerabilities necessitate new
approaches for enabling the safe release of open-weight LLMs. We develop a
method, called TAR, for building tamper-resistant safeguards into open-weight
LLMs such that adversaries cannot remove the safeguards even after thousands of
steps of fine-tuning. In extensive evaluations and red teaming analyses, we
find that our method greatly improves tamper-resistance while preserving benign
capabilities. Our results demonstrate that tamper-resistance is a tractable
problem, opening up a promising new avenue to improve the safety and security
of open-weight LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://www.tamper-resistant-safeguards.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty in Language Models: Assessment through Rank-Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.03163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.03163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinmeng Huang, Shuo Li, Mengxin Yu, Matteo Sesia, Hamed Hassani, Insup Lee, Osbert Bastani, Edgar Dobriban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) have shown promising performance in natural language
generation. However, as LMs often generate incorrect or hallucinated responses,
it is crucial to correctly quantify their uncertainty in responding to given
inputs. In addition to verbalized confidence elicited via prompting, many
uncertainty measures ($e.g.$, semantic entropy and affinity-graph-based
measures) have been proposed. However, these measures can differ greatly, and
it is unclear how to compare them, partly because they take values over
different ranges ($e.g.$, $[0,\infty)$ or $[0,1]$). In this work, we address
this issue by developing a novel and practical framework, termed
$Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs.
Our key tenet is that higher uncertainty (or lower confidence) should imply
lower generation quality, on average. Rank-calibration quantifies deviations
from this ideal relationship in a principled manner, without requiring ad hoc
binary thresholding of the correctness score ($e.g.$, ROUGE or METEOR). The
broad applicability and the granular interpretability of our methods are
demonstrated empirically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Detection of Toxic <span class="highlight-title">Prompt</span>s in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Liu, Junzhe Yu, Huijia Sun, Ling Shi, Gelei Deng, Yuqi Chen, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) like ChatGPT and Gemini have significantly
advanced natural language processing, enabling various applications such as
chatbots and automated content generation. However, these models can be
exploited by malicious individuals who craft toxic prompts to elicit harmful or
unethical responses. These individuals often employ jailbreaking techniques to
bypass safety mechanisms, highlighting the need for robust toxic prompt
detection methods. Existing detection techniques, both blackbox and whitebox,
face challenges related to the diversity of toxic prompts, scalability, and
computational efficiency. In response, we propose ToxicDetector, a lightweight
greybox method designed to efficiently detect toxic prompts in LLMs.
ToxicDetector leverages LLMs to create toxic concept prompts, uses embedding
vectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP)
classifier for prompt classification. Our evaluation on various versions of the
LLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector
achieves a high accuracy of 96.39\% and a low false positive rate of 2.00\%,
outperforming state-of-the-art methods. Additionally, ToxicDetector's
processing time of 0.0780 seconds per prompt makes it highly suitable for
real-time applications. ToxicDetector achieves high accuracy, efficiency, and
scalability, making it a practical method for toxic prompt detection in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 39th IEEE/ACM International Conference on Automated
  Software Engineering (ASE 2024)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating authenticity and quality of image captions via sentiment and
  semantic analyses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksei Krotov, Alison Tebo, Dylan K. Picart, Aaron Dean Algave
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growth of deep learning (DL) relies heavily on huge amounts of labelled
data for tasks such as natural language processing and computer vision.
Specifically, in image-to-text or image-to-image pipelines, opinion (sentiment)
may be inadvertently learned by a model from human-generated image captions.
Additionally, learning may be affected by the variety and diversity of the
provided captions. While labelling large datasets has largely relied on
crowd-sourcing or data-worker pools, evaluating the quality of such training
data is crucial.
  This study proposes an evaluation method focused on sentiment and semantic
richness. That method was applied to the COCO-MS dataset, comprising
approximately 150K images with segmented objects and corresponding
crowd-sourced captions. We employed pre-trained models (Twitter-RoBERTa-base
and BERT-base) to extract sentiment scores and variability of semantic
embeddings from captions. The relation of the sentiment score and semantic
variability with object categories was examined using multiple linear
regression. Results indicate that while most captions were neutral, about 6% of
the captions exhibited strong sentiment influenced by specific object
categories. Semantic variability of within-image captions remained low and
uncorrelated with object categories. Model-generated captions showed less than
1.5% of strong sentiment which was not influenced by object categories and did
not correlate with the sentiment of the respective human-generated captions.
This research demonstrates an approach to assess the quality of crowd- or
worker-sourced captions informed by image content.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Printed Circuit Board Defect Detection through Ensemble
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ka Nam Canaan Law, Mingshuo Yu, Lianglei Zhang, Yiyi Zhang, Peng Xu, Jerry Gao, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality control of printed circuit boards (PCBs) is paramount in
advancing electronic device technology. While numerous machine learning
methodologies have been utilized to augment defect detection efficiency and
accuracy, previous studies have predominantly focused on optimizing individual
models for specific defect types, often overlooking the potential synergies
between different approaches. This paper introduces a comprehensive inspection
framework leveraging an ensemble learning strategy to address this gap.
Initially, we utilize four distinct PCB defect detection models utilizing
state-of-the-art methods: EfficientDet, MobileNet SSDv2, Faster RCNN, and
YOLOv5. Each method is capable of identifying PCB defects independently.
Subsequently, we integrate these models into an ensemble learning framework to
enhance detection performance. A comparative analysis reveals that our ensemble
learning framework significantly outperforms individual methods, achieving a
95% accuracy in detecting diverse PCB defects. These findings underscore the
efficacy of our proposed ensemble learning framework in enhancing PCB quality
control processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MANGO: Disentangled Image Transformation Manifolds with Grouped
  Operators <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brighton Ancelin, Yenho Chen, Peimeng Guan, Chiraag Kaushik, Belen Martin-Urcelay, Alex Saad-Falcon, Nakul Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning semantically meaningful image transformations (i.e. rotation,
thickness, blur) directly from examples can be a challenging task. Recently,
the Manifold Autoencoder (MAE) proposed using a set of Lie group operators to
learn image transformations directly from examples. However, this approach has
limitations, as the learned operators are not guaranteed to be disentangled and
the training routine is prohibitively expensive when scaling up the model. To
address these limitations, we propose MANGO (transformation Manifolds with
Grouped Operators) for learning disentangled operators that describe image
transformations in distinct latent subspaces. Moreover, our approach allows
practitioners the ability to define which transformations they aim to model,
thus improving the semantic meaning of the learned operators. Through our
experiments, we demonstrate that MANGO enables composition of image
transformations and introduces a one-phase training routine that leads to a
100x speedup over prior works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE ICASSP 2025. This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Augmentation-based Model Re-adaptation Framework for Robust Image
  Segmentation <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheming Zuo, Joseph Smith, Jonathan Stonehouse, Boguslaw Obara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image segmentation is a crucial task in computer vision, with wide-ranging
applications in industry. The Segment Anything Model (SAM) has recently
attracted intensive attention; however, its application in industrial
inspection, particularly for segmenting commercial anti-counterfeit codes,
remains challenging. Unlike open-source datasets, industrial settings often
face issues such as small sample sizes and complex textures. Additionally,
computational cost is a key concern due to the varying number of trainable
parameters. To address these challenges, we propose an Augmentation-based Model
Re-adaptation Framework (AMRF). This framework leverages data augmentation
techniques during training to enhance the generalisation of segmentation
models, allowing them to adapt to newly released datasets with temporal
disparity. By observing segmentation masks from conventional models (FCN and
U-Net) and a pre-trained SAM model, we determine a minimal augmentation set
that optimally balances training efficiency and model performance. Our results
demonstrate that the fine-tuned FCN surpasses its baseline by 3.29% and 3.02%
in cropping accuracy, and 5.27% and 4.04% in classification accuracy on two
temporally continuous datasets. Similarly, the fine-tuned U-Net improves upon
its baseline by 7.34% and 4.94% in cropping, and 8.02% and 5.52% in
classification. Both models outperform the top-performing SAM models (ViT-Large
and ViT-Base) by an average of 11.75% and 9.01% in cropping accuracy, and 2.93%
and 4.83% in classification accuracy, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the European Conference on Computer Vision (ECCV) 2024
  workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery
  with SAM Empowerment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Hu, Janet Wang, Jihun Hamm, Rie R Yotsu, Zhengming Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current AI-assisted skin image diagnosis has achieved dermatologist-level
performance in classifying skin cancer, driven by rapid advancements in deep
learning architectures. However, unlike traditional vision tasks, skin images
in general present unique challenges due to the limited availability of
well-annotated datasets, complex variations in conditions, and the necessity
for detailed interpretations to ensure patient safety. Previous segmentation
methods have sought to reduce image noise and enhance diagnostic performance,
but these techniques require fine-grained, pixel-level ground truth masks for
training. In contrast, with the rise of foundation models, the Segment Anything
Model (SAM) has been introduced to facilitate promptable segmentation, enabling
the automation of the segmentation process with simple yet effective prompts.
Efforts applying SAM predominantly focus on dermatoscopy images, which present
more easily identifiable lesion boundaries than clinical photos taken with
smartphones. This limitation constrains the practicality of these approaches to
real-world applications. To overcome the challenges posed by noisy clinical
photos acquired via non-standardized protocols and to improve diagnostic
accessibility, we propose a novel Cross-Attentive Fusion framework for
interpretable skin lesion diagnosis. Our method leverages SAM to generate
visual concepts for skin diseases using prompts, integrating local visual
concepts with global image features to enhance model performance. Extensive
evaluation on two skin disease datasets demonstrates our proposed method's
effectiveness on lesion diagnosis and interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One missing piece in Vision and Language: A <span class="highlight-title">Survey</span> on Comics
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Vivoli, Andrey Barsky, Mohamed Ali Souibgui, Artemis LLabres, Marco Bertini, Dimosthenis Karatzas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models have recently evolved into versatile systems capable
of high performance across a range of tasks, such as document understanding,
visual question answering, and grounding, often in zero-shot settings. Comics
Understanding, a complex and multifaceted field, stands to greatly benefit from
these advances. Comics, as a medium, combine rich visual and textual
narratives, challenging AI models with tasks that span image classification,
object detection, instance segmentation, and deeper narrative comprehension
through sequential panels. However, the unique structure of comics --
characterized by creative variations in style, reading order, and non-linear
storytelling -- presents a set of challenges distinct from those in other
visual-language domains. In this survey, we present a comprehensive review of
Comics Understanding from both dataset and task perspectives. Our contributions
are fivefold: (1) We analyze the structure of the comics medium, detailing its
distinctive compositional elements; (2) We survey the widely used datasets and
tasks in comics research, emphasizing their role in advancing the field; (3) We
introduce the Layer of Comics Understanding (LoCU) framework, a novel taxonomy
that redefines vision-language tasks within comics and lays the foundation for
future work; (4) We provide a detailed review and categorization of existing
methods following the LoCU framework; (5) Finally, we highlight current
research challenges and propose directions for future exploration, particularly
in the context of vision-language models applied to comics. This survey is the
first to propose a task-oriented framework for comics intelligence and aims to
guide future research by addressing critical gaps in data availability and task
definition. A project associated with this survey is available at
https://github.com/emanuelevivoli/awesome-comics-understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review. project website:
  https://github.com/emanuelevivoli/awesome-comics-understanding</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Scale Grouped Prototypes for Interpretable Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Porta, Emanuele Dalsasso, Diego Marcos, Devis Tuia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prototypical part learning is emerging as a promising approach for making
semantic segmentation interpretable. The model selects real patches seen during
training as prototypes and constructs the dense prediction map based on the
similarity between parts of the test image and the prototypes. This improves
interpretability since the user can inspect the link between the predicted
output and the patterns learned by the model in terms of prototypical
information. In this paper, we propose a method for interpretable semantic
segmentation that leverages multi-scale image representation for prototypical
part learning. First, we introduce a prototype layer that explicitly learns
diverse prototypical parts at several scales, leading to multi-scale
representations in the prototype activation output. Then, we propose a sparse
grouping mechanism that produces multi-scale sparse groups of these
scale-specific prototypical parts. This provides a deeper understanding of the
interactions between multi-scale object representations while enhancing the
interpretability of the segmentation model. The experiments conducted on Pascal
VOC, Cityscapes, and ADE20K demonstrate that the proposed method increases
model sparsity, improves interpretability over existing prototype-based
methods, and narrows the performance gap with the non-interpretable counterpart
models. Code is available at github.com/eceo-epfl/ScaleProtoSeg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-<span class="highlight-title">Prompt</span>ing Polyp Segmentation in Colonoscopy using Hybrid Yolo-SAM 2
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mobina Mansoori, Sajjad Shahabodini, Jamshid Abouei, Konstantinos N. Plataniotis, Arash Mohammadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early diagnosis and treatment of polyps during colonoscopy are essential for
reducing the incidence and mortality of Colorectal Cancer (CRC). However, the
variability in polyp characteristics and the presence of artifacts in
colonoscopy images and videos pose significant challenges for accurate and
efficient polyp detection and segmentation. This paper presents a novel
approach to polyp segmentation by integrating the Segment Anything Model (SAM
2) with the YOLOv8 model. Our method leverages YOLOv8's bounding box
predictions to autonomously generate input prompts for SAM 2, thereby reducing
the need for manual annotations. We conducted exhaustive tests on five
benchmark colonoscopy image datasets and two colonoscopy video datasets,
demonstrating that our method exceeds state-of-the-art models in both image and
video segmentation tasks. Notably, our approach achieves high segmentation
accuracy using only bounding box annotations, significantly reducing annotation
time and effort. This advancement holds promise for enhancing the efficiency
and scalability of polyp detection in clinical settings
https://github.com/sajjad-sh33/YOLO_SAM2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAC-VO: Metrics-aware Covariance for Learning-based Stereo Visual
  Odometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Qiu, Yutian Chen, Zihao Zhang, Wenshan Wang, Sebastian Scherer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the MAC-VO, a novel learning-based stereo VO that leverages the
learned metrics-aware matching uncertainty for dual purposes: selecting
keypoint and weighing the residual in pose graph optimization. Compared to
traditional geometric methods prioritizing texture-affluent features like
edges, our keypoint selector employs the learned uncertainty to filter out the
low-quality features based on global inconsistency. In contrast to the
learning-based algorithms that model the scale-agnostic diagonal weight matrix
for covariance, we design a metrics-aware covariance model to capture the
spatial error during keypoint registration and the correlations between
different axes. Integrating this covariance model into pose graph optimization
enhances the robustness and reliability of pose estimation, particularly in
challenging environments with varying illumination, feature density, and motion
patterns. On public benchmark datasets, MAC-VO outperforms existing VO
algorithms and even some SLAM algorithms in challenging environments. The
covariance map also provides valuable information about the reliability of the
estimated poses, which can benefit decision-making for autonomous systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From FDG to PSMA: A Hitchhiker's Guide to Multitracer, Multicenter
  Lesion Segmentation in PET/CT Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Rokuss, Balint Kovacs, Yannick Kirchhoff, Shuhan Xiao, Constantin Ulrich, Klaus H. Maier-Hein, Fabian Isensee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated lesion segmentation in PET/CT scans is crucial for improving
clinical workflows and advancing cancer diagnostics. However, the task is
challenging due to physiological variability, different tracers used in PET
imaging, and diverse imaging protocols across medical centers. To address this,
the autoPET series was created to challenge researchers to develop algorithms
that generalize across diverse PET/CT environments. This paper presents our
solution for the autoPET III challenge, targeting multitracer, multicenter
generalization using the nnU-Net framework with the ResEncL architecture. Key
techniques include misalignment data augmentation and multi-modal pretraining
across CT, MR, and PET datasets to provide an initial anatomical understanding.
We incorporate organ supervision as a multitask approach, enabling the model to
distinguish between physiological uptake and tracer-specific patterns, which is
particularly beneficial in cases where no lesions are present. Compared to the
default nnU-Net, which achieved a Dice score of 57.61, or the larger ResEncL
(65.31) our model significantly improved performance with a Dice score of
68.40, alongside a reduction in false positive (FPvol: 7.82) and false negative
(FNvol: 10.35) volumes. These results underscore the effectiveness of combining
advanced network design, augmentation, pretraining, and multitask learning for
PET/CT lesion segmentation. Code is publicly available at
https://github.com/MIC-DKFZ/autopet-3-submission.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PRE: Vision-Language <span class="highlight-title">Prompt</span> Learning with Reparameterization Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07760v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07760v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thi Minh Anh Pham, An Duc Nguyen, Cephas Svosve, Vasileios Argyriou, Georgios Tzimiropoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pre-trained vision-language models such as CLIP have demonstrated great
potential in zero-shot transferability to downstream tasks. However, to attain
optimal performance, the manual selection of prompts is necessary to improve
alignment between the downstream image distribution and the textual class
descriptions. This manual prompt engineering is the major challenge for
deploying such models in practice since it requires domain expertise and is
extremely time-consuming. To avoid non-trivial prompt engineering, recent work
Context Optimization (CoOp) introduced the concept of prompt learning to the
vision domain using learnable textual tokens. While CoOp can achieve
substantial improvements over manual prompts, its learned context is worse
generalizable to wider unseen classes within the same dataset. In this work, we
present Prompt Learning with Reparameterization Encoder (PRE) - a simple and
efficient method that enhances the generalization ability of the learnable
prompt to unseen classes while maintaining the capacity to learn Base classes.
Instead of directly optimizing the prompts, PRE employs a prompt encoder to
reparameterize the input prompt embeddings, enhancing the exploration of
task-specific knowledge from few-shot samples. Experiments and extensive
ablation studies on 8 benchmarks demonstrate that our approach is an efficient
method for prompt learning. Specifically, PRE achieves a notable enhancement of
5.60% in average accuracy on New classes and 3% in Harmonic mean compared to
CoOp in the 16-shot setting, all achieved within a good training time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages excluding References and Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on the Robustness of Computer Vision Models against Common
  Corruptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06024v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06024v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunxin Wang, Raymond Veldhuis, Christoph Brune, Nicola Strisciuglio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of computer vision models are susceptible to unexpected
changes in input images caused by sensor errors or extreme imaging
environments, known as common corruptions (e.g. noise, blur, illumination
changes). These corruptions can significantly hinder the reliability of these
models when deployed in real-world scenarios, yet they are often overlooked
when testing model generalization and robustness. In this survey, we present a
comprehensive overview of methods that improve the robustness of computer
vision models against common corruptions. We categorize methods into three
groups based on the model components and training methods they target: data
augmentation, learning strategies, and network components. We release a unified
benchmark framework (available at
\url{https://github.com/nis-research/CorruptionBenchCV}) to compare robustness
performance across several datasets, and we address the inconsistencies of
evaluation practices in the literature. Our experimental analysis highlights
the base corruption robustness of popular vision backbones, revealing that
corruption robustness does not necessarily scale with model size and data size.
Large models gain negligible robustness improvements, considering the increased
computational requirements. To achieve generalizable and robust computer vision
models, we foresee the need of developing new learning strategies that
efficiently exploit limited data and mitigate unreliable learning behaviors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PuzzleAvatar: Assembling 3D Avatars from Personal Albums 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuliang Xiu, Yufei Ye, Zhen Liu, Dimitrios Tzionas, Michael J. Black
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating personalized 3D avatars is crucial for AR/VR. However, recent
text-to-3D methods that generate avatars for celebrities or fictional
characters, struggle with everyday people. Methods for faithful reconstruction
typically require full-body images in controlled settings. What if a user could
just upload their personal "OOTD" (Outfit Of The Day) photo collection and get
a faithful avatar in return? The challenge is that such casual photo
collections contain diverse poses, challenging viewpoints, cropped views, and
occlusion (albeit with a consistent outfit, accessories and hairstyle). We
address this novel "Album2Human" task by developing PuzzleAvatar, a novel model
that generates a faithful 3D avatar (in a canonical pose) from a personal OOTD
album, while bypassing the challenging estimation of body and camera pose. To
this end, we fine-tune a foundational vision-language model (VLM) on such
photos, encoding the appearance, identity, garments, hairstyles, and
accessories of a person into (separate) learned tokens and instilling these
cues into the VLM. In effect, we exploit the learned tokens as "puzzle pieces"
from which we assemble a faithful, personalized 3D avatar. Importantly, we can
customize avatars by simply inter-changing tokens. As a benchmark for this new
task, we collect a new dataset, called PuzzleIOI, with 41 subjects in a total
of nearly 1K OOTD configurations, in challenging partial photos with paired
ground-truth 3D bodies. Evaluation shows that PuzzleAvatar not only has high
reconstruction accuracy, outperforming TeCH and MVDreamBooth, but also a unique
scalability to album photos, and strong robustness. Our code and data are
publicly available for research purpose at https://puzzleavatar.is.tue.mpg.de/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Page: https://puzzleavatar.is.tue.mpg.de/, Code:
  https://github.com/YuliangXiu/PuzzleAvatar, Video:
  https://youtu.be/0hpXH2tVPk4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reduce, Reuse, Recycle: Compositional Generation with Energy-Based
  Diffusion Models and MCMC <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11552v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11552v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, Will Grathwohl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since their introduction, diffusion models have quickly become the prevailing
approach to generative modeling in many domains. They can be interpreted as
learning the gradients of a time-varying sequence of log-probability density
functions. This interpretation has motivated classifier-based and
classifier-free guidance as methods for post-hoc control of diffusion models.
In this work, we build upon these ideas using the score-based interpretation of
diffusion models, and explore alternative ways to condition, modify, and reuse
diffusion models for tasks involving compositional generation and guidance. In
particular, we investigate why certain types of composition fail using current
techniques and present a number of solutions. We conclude that the sampler (not
the model) is responsible for this failure and propose new samplers, inspired
by MCMC, which enable successful compositional generation. Further, we propose
an energy-based parameterization of diffusion models which enables the use of
new compositional operators and more sophisticated, Metropolis-corrected
samplers. Intriguingly we find these samplers lead to notable improvements in
compositional generation across a wide set of problems such as
classifier-guided ImageNet modeling and compositional text-to-image generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023, Project Webpage:
  https://energy-based-model.github.io/reduce-reuse-recycle/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaVA-OneVision: Easy Visual Task Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LLaVA-OneVision, a family of open large multimodal models (LMMs)
developed by consolidating our insights into data, models, and visual
representations in the LLaVA-NeXT blog series. Our experimental results
demonstrate that LLaVA-OneVision is the first single model that can
simultaneously push the performance boundaries of open LMMs in three important
computer vision scenarios: single-image, multi-image, and video scenarios.
Importantly, the design of LLaVA-OneVision allows strong transfer learning
across different modalities/scenarios, yielding new emerging capabilities. In
particular, strong video understanding and cross-scenario capabilities are
demonstrated through task transfer from images to videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Homepage:
  https://llava-vl.github.io/blog/2024-08-05-llava-onevision/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleash <span class="highlight-title">LLM</span>s Potential for Recommendation by Coordinating Twin-Tower
  Dynamic Semantic Token Generator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yin, Zhengxin Zeng, Mingzheng Li, Hao Yan, Chaozhuo Li, Weihao Han, Jianjin Zhang, Ruochen Liu, Allen Sun, Denvy Deng, Feng Sun, Qi Zhang, Shirui Pan, Senzhang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Owing to the unprecedented capability in semantic understanding and logical
reasoning, the pre-trained large language models (LLMs) have shown fantastic
potential in developing the next-generation recommender systems (RSs). However,
the static index paradigm adopted by current methods greatly restricts the
utilization of LLMs capacity for recommendation, leading to not only the
insufficient alignment between semantic and collaborative knowledge, but also
the neglect of high-order user-item interaction patterns. In this paper, we
propose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS
which adopts dynamic semantic index paradigm, targeting at resolving the above
problems simultaneously. To be more specific, we for the first time contrive a
dynamic knowledge fusion framework which integrates a twin-tower semantic token
generator into the LLM-based recommender, hierarchically allocating meaningful
semantic index for items and users, and accordingly predicting the semantic
index of target item. Furthermore, a dual-modality variational auto-encoder is
proposed to facilitate multi-grained alignment between semantic and
collaborative knowledge. Eventually, a series of novel tuning tasks specially
customized for capturing high-order user-item interaction patterns are proposed
to take advantages of user historical behavior. Extensive experiments across
three public datasets demonstrate the superiority of the proposed methodology
in developing LLM-based generative RSs. The proposed TTDS recommender achieves
an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric,
compared with the leading baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaborative Automatic Modulation Classification via Deep Edge
  Inference for Hierarchical Cognitive Radio Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07946v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07946v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaowei He, Peihao Dong, Fuhui Zhou, Qihui Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In hierarchical cognitive radio networks, edge or cloud servers utilize the
data collected by edge devices for modulation classification, which, however,
is faced with problems of the transmission overhead, data privacy, and
computation load. In this article, an edge learning (EL) based framework
jointly mobilizing the edge device and the edge server for intelligent
co-inference is proposed to realize the collaborative automatic modulation
classification (C-AMC) between them. A spectrum semantic compression neural
network (SSCNet) with the lightweight structure is designed for the edge device
to compress the collected raw data into a compact semantic message that is then
sent to the edge server via the wireless channel. On the edge server side, a
modulation classification neural network (MCNet) combining bidirectional long
short-term memory (Bi-LSTM) and multi-head attention layers is elaborated to
determine the modulation type from the noisy semantic message. By leveraging
the computation resources of both the edge device and the edge server, high
transmission overhead and risks of data privacy leakage are avoided. The
simulation results verify the effectiveness of the proposed C-AMC framework,
significantly reducing the model size and computational complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2407.20772</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction
  Retriever <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16672v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16672v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohan Jha, Bo Wang, Michael Günther, Georgios Mastrapas, Saba Sturua, Isabelle Mohr, Andreas Koukounas, Mohammad Kalim Akram, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-vector dense models, such as ColBERT, have proven highly effective in
information retrieval. ColBERT's late interaction scoring approximates the
joint query-document attention seen in cross-encoders while maintaining
inference efficiency closer to traditional dense retrieval models, thanks to
its bi-encoder architecture and recent optimizations in indexing and search. In
this work we propose a number of incremental improvements to the ColBERT model
architecture and training pipeline, using methods shown to work in the more
mature single-vector embedding model training paradigm, particularly those that
apply to heterogeneous multilingual data or boost efficiency with little
tradeoff. Our new model, Jina-ColBERT-v2, demonstrates strong performance
across a range of English and multilingual retrieval tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, references at pp7,8; EMNLP workshop submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LTP-MMF: Towards Long-term Provider Max-min Fairness Under
  Recommendation Feedback Loops 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.05902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.05902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Xu, Xiaopeng Ye, Jun Xu, Xiao Zhang, Weiran Shen, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-stakeholder recommender systems involve various roles, such as users,
and providers. Previous work pointed out that max-min fairness (MMF) is a
better metric to support weak providers. However, when considering MMF, the
features or parameters of these roles vary over time, how to ensure long-term
provider MMF has become a significant challenge. We observed that
recommendation feedback loops (named RFL) will greatly influence the provider
MMF in the long term. RFL means that recommender systems can only receive
feedback on exposed items from users and update recommender models
incrementally based on this feedback. When utilizing the feedback, the
recommender model will regard the unexposed items as negative. In this way, the
tail provider will not get the opportunity to be exposed, and its items will
always be considered negative samples. Such phenomena will become more and more
serious in RFL. To alleviate the problem, this paper proposes an online ranking
model named Long-Term Provider Max-min Fairness (named LTP-MMF). Theoretical
analysis shows that the long-term regret of LTP-MMF enjoys a sub-linear bound.
Experimental results on three public recommendation benchmarks demonstrated
that LTP-MMF can outperform the baselines in the long term.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted in TOIS 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prototypical <span class="highlight-title">Prompt</span>ing for Text-to-image Person Re-identification <span class="chip">ACM MM2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuanglin Yan, Jun Liu, Neng Dong, Liyan Zhang, Jinhui Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the problem of Text-to-Image Person Re-identification
(TIReID), which aims to find images of the same identity described by a text
sentence from a pool of candidate images. Benefiting from Vision-Language
Pre-training, such as CLIP (Contrastive Language-Image Pretraining), the TIReID
techniques have achieved remarkable progress recently. However, most existing
methods only focus on instance-level matching and ignore identity-level
matching, which involves associating multiple images and texts belonging to the
same person. In this paper, we propose a novel prototypical prompting framework
(Propot) designed to simultaneously model instance-level and identity-level
matching for TIReID. Our Propot transforms the identity-level matching problem
into a prototype learning problem, aiming to learn identity-enriched
prototypes. Specifically, Propot works by 'initialize, adapt, enrich, then
aggregate'. We first use CLIP to generate high-quality initial prototypes.
Then, we propose a domain-conditional prototypical prompting (DPP) module to
adapt the prototypes to the TIReID task using task-related information.
Further, we propose an instance-conditional prototypical prompting (IPP) module
to update prototypes conditioned on intra-modal and inter-modal instances to
ensure prototype diversity. Finally, we design an adaptive prototype
aggregation module to aggregate these prototypes, generating final
identity-enriched prototypes. With identity-enriched prototypes, we diffuse its
rich identity information to instances through prototype-to-instance
contrastive loss to facilitate identity-level matching. Extensive experiments
conducted on three benchmarks demonstrate the superiority of Propot compared to
existing TIReID methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Driven Virtual Teacher for Enhanced Educational Efficiency:
  Leveraging Large <span class="highlight-title">Pretrain</span> Models for Autonomous Error Analysis and Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianlong Xu, Yi-Fan Zhang, Zhendong Chu, Shen Wang, Qingsong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Students frequently make mistakes while solving mathematical problems, and
traditional error correction methods are both time-consuming and
labor-intensive. This paper introduces an innovative \textbf{V}irtual
\textbf{A}I \textbf{T}eacher system designed to autonomously analyze and
correct student \textbf{E}rrors (VATE). Leveraging advanced large language
models (LLMs), the system uses student drafts as a primary source for error
analysis, which enhances understanding of the student's learning process. It
incorporates sophisticated prompt engineering and maintains an error pool to
reduce computational overhead. The AI-driven system also features a real-time
dialogue component for efficient student interaction. Our approach demonstrates
significant advantages over traditional and machine learning-based error
correction methods, including reduced educational costs, high scalability, and
superior generalizability. The system has been deployed on the Squirrel AI
learning platform for elementary mathematics education, where it achieves
78.3\% accuracy in error analysis and shows a marked improvement in student
learning efficiency. Satisfaction surveys indicate a strong positive reception,
highlighting the system's potential to transform educational practices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prevailing Research Areas for Music AI in the Era of Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Megan Wei, Mateusz Modrzejewski, Aswin Sivaraman, Dorien Herremans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In tandem with the recent advancements in foundation model research, there
has been a surge of generative music AI applications within the past few years.
As the idea of AI-generated or AI-augmented music becomes more mainstream, many
researchers in the music AI community may be wondering what avenues of research
are left. With regards to music generative models, we outline the current areas
of research with significant room for exploration. Firstly, we pose the
question of foundational representation of these generative models and
investigate approaches towards explainability. Next, we discuss the current
state of music datasets and their limitations. We then overview different
generative models, forms of evaluating these models, and their computational
constraints/limitations. Subsequently, we highlight applications of these
generative models towards extensions to multiple modalities and integration
with artists' workflow as well as music education systems. Finally, we survey
the potential copyright implications of generative music and discuss strategies
for protecting the rights of musicians. While it is not meant to be exhaustive,
our survey calls to attention a variety of research directions enabled by music
foundation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MHAD: Multimodal Home Activity <span class="highlight-title">Dataset</span> with Multi-Angle Videos and
  Synchronized Physiological Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yu, Jintao Fei, Xinyi Liu, Yang Yao, Jun Zhao, Guoxin Wang, Xin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-based physiology, exemplified by remote photoplethysmography (rPPG),
extracts physiological signals such as pulse and respiration by analyzing
subtle changes in video recordings. This non-contact, real-time monitoring
method holds great potential for home settings. Despite the valuable
contributions of public benchmark datasets to this technology, there is
currently no dataset specifically designed for passive home monitoring.
Existing datasets are often limited to close-up, static, frontal recordings and
typically include only 1-2 physiological signals. To advance video-based
physiology in real home settings, we introduce the MHAD dataset. It comprises
1,440 videos from 40 subjects, capturing 6 typical activities from 3 angles in
a real home environment. Additionally, 5 physiological signals were recorded,
making it a comprehensive video-based physiology dataset. MHAD is compatible
with the rPPG-toolbox and has been validated using several unsupervised and
supervised methods. Our dataset is publicly available at
https://github.com/jdh-algo/MHAD-Dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSCLAP: Domain-Specific Contrastive Language-Audio <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09289v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09289v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengqiang Liu, Da Liu, Anna Wang, Zhiyu Zhang, Jie Gao, Yali Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing real-world multimodal signals is an essential and challenging task
for intelligent voice assistants (IVAs). Mainstream approaches have achieved
remarkable performance on various downstream tasks of IVAs with pre-trained
audio models and text models. However, these models are pre-trained
independently and usually on tasks different from target domains, resulting in
sub-optimal modality representations for downstream tasks. Moreover, in many
domains, collecting enough language-audio pairs is extremely hard, and
transcribing raw audio also requires high professional skills, making it
difficult or even infeasible to joint pre-training. To address these
painpoints, we propose DSCLAP, a simple and effective framework that enables
language-audio pre-training with only raw audio signal input. Specifically,
DSCLAP converts raw audio signals into text via an ASR system and combines a
contrastive learning objective and a language-audio matching objective to align
the audio and ASR transcriptions. We pre-train DSCLAP on 12,107 hours of
in-vehicle domain audio. Empirical results on two downstream tasks show that
while conceptually simple, DSCLAP significantly outperforms the baseline models
in all metrics, showing great promise for domain-specific IVAs applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M$^{3}$V: A multi-modal multi-view approach for Device-Directed Speech
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Wang, Da Liu, Zhiyu Zhang, Shengqiang Liu, Jie Gao, Yali Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the goal of more natural and human-like interaction with virtual voice
assistants, recent research in the field has focused on full duplex interaction
mode without relying on repeated wake-up words. This requires that in scenes
with complex sound sources, the voice assistant must classify utterances as
device-oriented or non-device-oriented. The dual-encoder structure, which is
jointly modeled by text and speech, has become the paradigm of device-directed
speech detection. However, in practice, these models often produce incorrect
predictions for unaligned input pairs due to the unavoidable errors of
automatic speech recognition (ASR).To address this challenge, we propose
M$^{3}$V, a multi-modal multi-view approach for device-directed speech
detection, which frames we frame the problem as a multi-view learning task that
introduces unimodal views and a text-audio alignment view in the network
besides the multi-modal. Experimental results show that M$^{3}$V significantly
outperforms models trained using only single or multi-modality and surpasses
human judgment performance on ASR error data for the first time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Turbo your multi-modal classification with contrastive learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Zhang, Da Liu, Shengqiang Liu, Anna Wang, Jie Gao, Yali Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has become one of the most impressive approaches for
multi-modal representation learning. However, previous multi-modal works mainly
focused on cross-modal understanding, ignoring in-modal contrastive learning,
which limits the representation of each modality. In this paper, we propose a
novel contrastive learning strategy, called $Turbo$, to promote multi-modal
understanding by joint in-modal and cross-modal contrastive learning.
Specifically, multi-modal data pairs are sent through the forward pass twice
with different hidden dropout masks to get two different representations for
each modality. With these representations, we obtain multiple in-modal and
cross-modal contrastive objectives for training. Finally, we combine the
self-supervised Turbo with the supervised multi-modal classification and
demonstrate its effectiveness on two audio-text classification tasks, where the
state-of-the-art performance is achieved on a speech emotion recognition
benchmark dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SafeEar: Content Privacy-Preserving Audio Deepfake Detection <span class="chip">CCS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinfeng Li, Kai Li, Yifan Zheng, Chen Yan, Xiaoyu Ji, Wenyuan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-Speech (TTS) and Voice Conversion (VC) models have exhibited
remarkable performance in generating realistic and natural audio. However,
their dark side, audio deepfake poses a significant threat to both society and
individuals. Existing countermeasures largely focus on determining the
genuineness of speech based on complete original audio recordings, which
however often contain private content. This oversight may refrain deepfake
detection from many applications, particularly in scenarios involving sensitive
information like business secrets. In this paper, we propose SafeEar, a novel
framework that aims to detect deepfake audios without relying on accessing the
speech content within. Our key idea is to devise a neural audio codec into a
novel decoupling model that well separates the semantic and acoustic
information from audio samples, and only use the acoustic information (e.g.,
prosody and timbre) for deepfake detection. In this way, no semantic content
will be exposed to the detector. To overcome the challenge of identifying
diverse deepfake audio without semantic clues, we enhance our deepfake detector
with real-world codec augmentation. Extensive experiments conducted on four
benchmark datasets demonstrate SafeEar's effectiveness in detecting various
deepfake techniques with an equal error rate (EER) down to 2.02%.
Simultaneously, it shields five-language speech content from being deciphered
by both machine and human auditory analysis, demonstrated by word error rates
(WERs) all above 93.93% and our user study. Furthermore, our benchmark
constructed for anti-deepfake and anti-content recovery evaluation helps
provide a basis for future research in the realms of audio privacy preservation
and deepfake detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM CCS 2024. Please cite this paper as "Xinfeng Li, Kai
  Li, Yifan Zheng, Chen Yan, Xiaoyu Ji, Wenyuan Xu. SafeEar: Content
  Privacy-Preserving Audio Deepfake Detection. In Proceedings of ACM Conference
  on Computer and Communications Security (CCS), 2024."</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating Large Language Models into a Tri-Modal Architecture for
  Automated Depression Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19340v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19340v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santosh V. Patapati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Major Depressive Disorder (MDD) is a pervasive mental health condition that
affects 300 million people worldwide. This work presents a novel, BiLSTM-based
tri-modal model-level fusion architecture for the binary classification of
depression from clinical interview recordings. The proposed architecture
incorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses
a two-shot learning based GPT-4 model to process text data. This is the first
work to incorporate large language models into a multi-modal architecture for
this task. It achieves impressive results on the DAIC-WOZ AVEC 2016 Challenge
cross-validation split and Leave-One-Subject-Out cross-validation split,
surpassing all baseline models and multiple state-of-the-art models. In
Leave-One-Subject-Out testing, it achieves an accuracy of 91.01%, an F1-Score
of 85.95%, a precision of 80%, and a recall of 92.86%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Keywords: Multi-Modal Neural Networks, Deep Learning, Large Language
  Models, Depression Diagnosis, Biomedical Informatics, DAIC-WOZ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HiSC4D: Human-centered interaction and 4D Scene Capture in Large-scale
  Space Using Wearable IMUs and LiDAR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04398v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04398v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yudi Dai, Zhiyong Wang, Xiping Lin, Chenglu Wen, Lan Xu, Siqi Shen, Yuexin Ma, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HiSC4D, a novel Human-centered interaction and 4D Scene Capture
method, aimed at accurately and efficiently creating a dynamic digital world,
containing large-scale indoor-outdoor scenes, diverse human motions, rich
human-human interactions, and human-environment interactions. By utilizing
body-mounted IMUs and a head-mounted LiDAR, HiSC4D can capture egocentric human
motions in unconstrained space without the need for external devices and
pre-built maps. This affords great flexibility and accessibility for
human-centered interaction and 4D scene capturing in various environments.
Taking into account that IMUs can capture human spatially unrestricted poses
but are prone to drifting for long-period using, and while LiDAR is stable for
global localization but rough for local positions and orientations, HiSC4D
employs a joint optimization method, harmonizing all sensors and utilizing
environment cues, yielding promising results for long-term capture in large
scenes. To promote research of egocentric human interaction in large scenes and
facilitate downstream tasks, we also present a dataset, containing 8 sequences
in 4 large scenes (200 to 5,000 $m^2$), providing 36k frames of accurate 4D
human motions with SMPL annotations and dynamic scenes, 31k frames of cropped
human point clouds, and scene mesh of the environment. A variety of scenarios,
such as the basketball gym and commercial street, alongside challenging human
motions, such as daily greeting, one-on-one basketball playing, and tour
guiding, demonstrate the effectiveness and the generalization ability of
HiSC4D. The dataset and code will be publicated on
www.lidarhumanmotion.net/hisc4d available for research purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 10 figures, Jornal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POINTS: Improving Your Vision-language Model with Affordable Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04828v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04828v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Liu, Zhongyin Zhao, Ziyuan Zhuang, Le Tian, Xiao Zhou, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, vision-language models have made significant strides,
excelling in tasks like optical character recognition and geometric
problem-solving. However, several critical issues remain: 1) Proprietary models
often lack transparency about their architectures, while open-source models
need more detailed ablations of their training strategies. 2) Pre-training data
in open-source works is under-explored, with datasets added empirically, making
the process cumbersome. 3) Fine-tuning often focuses on adding datasets,
leading to diminishing returns. To address these issues, we propose the
following contributions: 1) We trained a robust baseline model using the latest
advancements in vision-language models, introducing effective improvements and
conducting comprehensive ablation and validation for each technique. 2)
Inspired by recent work on large language models, we filtered pre-training data
using perplexity, selecting the lowest perplexity data for training. This
approach allowed us to train on a curated 1M dataset, achieving competitive
performance. 3) During visual instruction tuning, we used model soup on
different datasets when adding more datasets yielded marginal improvements.
These innovations resulted in a 9B parameter model that performs competitively
with state-of-the-art models. Our strategies are efficient and lightweight,
making them easily adoptable by the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Multimodal Mis<span class="highlight-title">information</span> Detection with Logic Reasoning <span class="chip">ACL 23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Liu, Wenya Wang, Haoliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal misinformation on online social platforms is becoming a critical
concern due to increasing credibility and easier dissemination brought by
multimedia content, compared to traditional text-only information. While
existing multimodal detection approaches have achieved high performance, the
lack of interpretability hinders these systems' reliability and practical
deployment. Inspired by NeuralSymbolic AI which combines the learning ability
of neural networks with the explainability of symbolic learning, we propose a
novel logic-based neural model for multimodal misinformation detection which
integrates interpretable logic clauses to express the reasoning process of the
target task. To make learning effective, we parameterize symbolic logical
elements using neural representations, which facilitate the automatic
generation and evaluation of meaningful logic clauses. Additionally, to make
our framework generalizable across diverse misinformation sources, we introduce
five meta-predicates that can be instantiated with different correlations.
Results on three public datasets (Twitter, Weibo, and Sarcasm) demonstrate the
feasibility and versatility of our model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Findings of ACL 23. 9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-13T00:00:00Z">2024-09-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">40</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-modal Speech Transformer Decoders: When Do Multiple Modalities
  Improve Accuracy? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Guan, Viet Anh Trinh, Vivek Voleti, Jacob Whitehill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoder-only discrete-token language models have recently achieved
significant success in automatic speech recognition. However, systematic
analyses of how different modalities impact performance in specific scenarios
remain limited. In this paper, we investigate the effects of multiple
modalities on recognition accuracy on both synthetic and real-world datasets.
Our experiments suggest that: (1) Integrating more modalities can increase
accuracy; in particular, our paper is, to our best knowledge, the first to show
the benefit of combining audio, image context, and lip information; (2) Images
as a supplementary modality for speech recognition provide the greatest benefit
at moderate noise levels, moreover, they exhibit a different trend compared to
inherently synchronized modalities like lip movements; (3) Performance improves
on both synthetic and real-world datasets when the most relevant visual
information is filtered as a preprocessing step.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sreyan Ghosh, Sonal Kumar, Chandra Kiran Reddy Evuru, Oriol Nieto, Ramani Duraiswami, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-vocabulary audio-language models, like CLAP, offer a promising approach
for zero-shot audio classification (ZSAC) by enabling classification with any
arbitrary set of categories specified with natural language prompts. In this
paper, we propose a simple but effective method to improve ZSAC with CLAP.
Specifically, we shift from the conventional method of using prompts with
abstract category labels (e.g., Sound of an organ) to prompts that describe
sounds using their inherent descriptive features in a diverse context (e.g.,The
organ's deep and resonant tones filled the cathedral.). To achieve this, we
first propose ReCLAP, a CLAP model trained with rewritten audio captions for
improved understanding of sounds in the wild. These rewritten captions describe
each sound event in the original caption using their unique discriminative
characteristics. ReCLAP outperforms all baselines on both multi-modal
audio-text retrieval and ZSAC. Next, to improve zero-shot audio classification
with ReCLAP, we propose prompt augmentation. In contrast to the traditional
method of employing hand-written template prompts, we generate custom prompts
for each unique label in the dataset. These custom prompts first describe the
sound event in the label and then employ them in diverse scenes. Our proposed
method improves ReCLAP's performance on ZSAC by 1%-18% and outperforms all
baselines by 1% - 55%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and Checkpoints: https://github.com/Sreyan88/ReCLAP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contextual Evaluation of Large Language Models for Classifying Tropical
  and Infectious Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mercy Asiedu, Nenad Tomasev, Chintan Ghate, Tiya Tiyasirichokchai, Awa Dieng, Oluwatosin Akande, Geoffrey Siwo, Steve Adudans, Sylvanus Aitkins, Odianosen Ehiakhamen, Katherine Heller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have shown promise for medical question
answering, there is limited work focused on tropical and infectious
disease-specific exploration. We build on an opensource tropical and infectious
diseases (TRINDs) dataset, expanding it to include demographic and semantic
clinical and consumer augmentations yielding 11000+ prompts. We evaluate LLM
performance on these, comparing generalist and medical LLMs, as well as LLM
outcomes to human experts. We demonstrate through systematic experimentation,
the benefit of contextual information such as demographics, location, gender,
risk factors for optimal LLM response. Finally we develop a prototype of
TRINDs-LM, a research tool that provides a playground to navigate how context
impacts LLM outputs for health.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformer with Controlled Attention for Synchronous Motion Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karim Radouane, Sylvie Ranwez, Julien Lagarde, Andon Tchechmedjiev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address a challenging task, synchronous motion captioning,
that aim to generate a language description synchronized with human motion
sequences. This task pertains to numerous applications, such as aligned sign
language transcription, unsupervised action segmentation and temporal
grounding. Our method introduces mechanisms to control self- and
cross-attention distributions of the Transformer, allowing interpretability and
time-aligned text generation. We achieve this through masking strategies and
structuring losses that push the model to maximize attention only on the most
important frames contributing to the generation of a motion word. These
constraints aim to prevent undesired mixing of information in attention maps
and to provide a monotonic attention distribution across tokens. Thus, the
cross attentions of tokens are used for progressive text generation in
synchronization with human motion sequences. We demonstrate the superior
performance of our approach through evaluation on the two available benchmark
datasets, KIT-ML and HumanML3D. As visual evaluation is essential for this
task, we provide a comprehensive set of animated visual illustrations in the
code repository: https://github.com/rd20karim/Synch-Transformer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Precision Characterization of Communication Disorders using
  Models of Perceived Pragmatic Similarity <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nigel G. Ward, Andres Segura, Georgina Bugarini, Heike Lehnert-LeHouillier, Dancheng Liu, Jinjun Xiong, Olac Fuentes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diagnosis and treatment of individuals with communication disorders
offers many opportunities for the application of speech technology, but
research so far has not adequately considered: the diversity of conditions, the
role of pragmatic deficits, and the challenges of limited data. This paper
explores how a general-purpose model of perceived pragmatic similarity may
overcome these limitations. It explains how it might support several use cases
for clinicians and clients, and presents evidence that a simple model can
provide value, and in particular can capture utterance aspects that are
relevant to diagnoses of autism and specific language impairment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DomURLs_BERT: <span class="highlight-title">Pre-train</span>ed BERT-based Model for Malicious Domains and
  URLs Detection and Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelkader El Mahdaouy, Salima Lamsiyah, Meryem Janati Idrissi, Hamza Alami, Zakaria Yartaoui, Ismail Berrada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting and classifying suspicious or malicious domain names and URLs is
fundamental task in cybersecurity. To leverage such indicators of compromise,
cybersecurity vendors and practitioners often maintain and update blacklists of
known malicious domains and URLs. However, blacklists frequently fail to
identify emerging and obfuscated threats. Over the past few decades, there has
been significant interest in developing machine learning models that
automatically detect malicious domains and URLs, addressing the limitations of
blacklists maintenance and updates. In this paper, we introduce DomURLs_BERT, a
pre-trained BERT-based encoder adapted for detecting and classifying
suspicious/malicious domains and URLs. DomURLs_BERT is pre-trained using the
Masked Language Modeling (MLM) objective on a large multilingual corpus of
URLs, domain names, and Domain Generation Algorithms (DGA) dataset. In order to
assess the performance of DomURLs_BERT, we have conducted experiments on
several binary and multi-class classification tasks involving domain names and
URLs, covering phishing, malware, DGA, and DNS tunneling. The evaluations
results show that the proposed encoder outperforms state-of-the-art
character-based deep learning models and cybersecurity-focused BERT models
across multiple tasks and datasets. The pre-training dataset, the pre-trained
DomURLs_BERT encoder, and the experiments source code are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Fusion with <span class="highlight-title">LLM</span>s for Engagement Prediction in Natural
  Conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Charles Ma, Kevin Hyekang Joo, Alexandria K. Vail, Sunreeta Bhattacharya, Álvaro Fernández García, Kailana Baker-Matsuoka, Sheryl Mathew, Lori L. Holt, Fernando De la Torre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past decade, wearable computing devices (``smart glasses'') have
undergone remarkable advancements in sensor technology, design, and processing
power, ushering in a new era of opportunity for high-density human behavior
data. Equipped with wearable cameras, these glasses offer a unique opportunity
to analyze non-verbal behavior in natural settings as individuals interact. Our
focus lies in predicting engagement in dyadic interactions by scrutinizing
verbal and non-verbal cues, aiming to detect signs of disinterest or confusion.
Leveraging such analyses may revolutionize our understanding of human
communication, foster more effective collaboration in professional
environments, provide better mental health support through empathetic virtual
interactions, and enhance accessibility for those with communication barriers.
  In this work, we collect a dataset featuring 34 participants engaged in
casual dyadic conversations, each providing self-reported engagement ratings at
the end of each conversation. We introduce a novel fusion strategy using Large
Language Models (LLMs) to integrate multiple behavior modalities into a
``multimodal transcript'' that can be processed by an LLM for behavioral
reasoning tasks. Remarkably, this method achieves performance comparable to
established fusion techniques even in its preliminary implementation,
indicating strong potential for further research and optimization. This fusion
method is one of the first to approach ``reasoning'' about real-world human
behavior through a language model. Smart glasses provide us the ability to
unobtrusively gather high-density multimodal data on human behavior, paving the
way for new approaches to understanding and improving human communication with
the potential for important societal benefits. The features and data collected
during the studies will be made publicly available to promote further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, first three authors equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agents in Software Engineering: <span class="highlight-title">Survey</span>, Landscape, and Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanxian Huang, Wanjun Zhong, Ensheng Shi, Min Yang, Jiachi Chen, Hui Li, Yuchi Ma, Qianxiang Wang, Zibin Zheng, Yanlin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Large Language Models (LLMs) have achieved remarkable
success and have been widely used in various downstream tasks, especially in
the tasks of the software engineering (SE) field. We find that many studies
combining LLMs with SE have employed the concept of agents either explicitly or
implicitly. However, there is a lack of an in-depth survey to sort out the
development context of existing works, analyze how existing works combine the
LLM-based agent technologies to optimize various tasks, and clarify the
framework of LLM-based agents in SE. In this paper, we conduct the first survey
of the studies on combining LLM-based agents with SE and present a framework of
LLM-based agents in SE which includes three key modules: perception, memory,
and action. We also summarize the current challenges in combining the two
fields and propose future opportunities in response to existing challenges. We
maintain a GitHub repository of the related papers at:
https://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in <span class="highlight-title">LLM</span>
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Su, Xuhui Zhou, Sanketh Rangreji, Anubha Kabra, Julia Mendelsohn, Faeze Brahman, Maarten Sap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To be safely and successfully deployed, LLMs must simultaneously satisfy
truthfulness and utility goals. Yet, often these two goals compete (e.g., an AI
agent assisting a used car salesman selling a car with flaws), partly due to
ambiguous or misleading user instructions. We propose AI-LieDar, a framework to
study how LLM-based agents navigate scenarios with utility-truthfulness
conflicts in a multi-turn interactive setting. We design a set of realistic
scenarios where language agents are instructed to achieve goals that are in
conflict with being truthful during a multi-turn conversation with simulated
human agents. To evaluate the truthfulness at large scale, we develop a
truthfulness detector inspired by psychological literature to assess the
agents' responses. Our experiment demonstrates that all models are truthful
less than 50% of the time, although truthfulness and goal achievement (utility)
rates vary across models. We further test the steerability of LLMs towards
truthfulness, finding that models follow malicious instructions to deceive, and
even truth-steered models can still lie. These findings reveal the complex
nature of truthfulness in LLMs and underscore the importance of further
research to ensure the safe and reliable deployment of LLMs and AI agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Rare Word Accuracy in Direct Speech Translation with a
  Retrieval-and-Demonstration Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Li, Danni Liu, Jan Niehues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct speech translation (ST) models often struggle with rare words.
Incorrect translation of these words can have severe consequences, impacting
translation quality and user trust. While rare word translation is inherently
challenging for neural models due to sparse learning signals, real-world
scenarios often allow access to translations of past recordings on similar
topics. To leverage these valuable resources, we propose a
retrieval-and-demonstration approach to enhance rare word translation accuracy
in direct ST models. First, we adapt existing ST models to incorporate
retrieved examples for rare word translation, which allows the model to benefit
from prepended examples, similar to in-context learning. We then develop a
cross-modal (speech-to-speech, speech-to-text, text-to-text) retriever to
locate suitable examples. We demonstrate that standard ST models can be
effectively adapted to leverage examples for rare word translation, improving
rare word translation accuracy over the baseline by 17.6% with gold examples
and 8.5% with retrieved examples. Moreover, our speech-to-speech retrieval
approach outperforms other modalities and exhibits higher robustness to unseen
speakers. Our code is publicly available
(https://github.com/SiqiLii/Retrieve-and-Demonstration-ST).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E2MoCase: A <span class="highlight-title">Dataset</span> for Emotional, <span class="highlight-title">Event</span> and Moral Observations in News
  Articles on High-impact Legal Cases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Candida M. Greco, Lorenzo Zangari, Davide Picca, Andrea Tagarelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The way media reports on legal cases can significantly shape public opinion,
often embedding subtle biases that influence societal views on justice and
morality. Analyzing these biases requires a holistic approach that captures the
emotional tone, moral framing, and specific events within the narratives. In
this work we introduce E2MoCase, a novel dataset designed to facilitate the
integrated analysis of emotions, moral values, and events within legal
narratives and media coverage. By leveraging advanced models for emotion
detection, moral value identification, and event extraction, E2MoCase offers a
multi-dimensional perspective on how legal cases are portrayed in news
articles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safeguarding Decentralized Social Media: <span class="highlight-title">LLM</span> Agents for Automating
  Community Rule Compliance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucio La Cava, Andrea Tagarelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring content compliance with community guidelines is crucial for
maintaining healthy online social environments. However, traditional
human-based compliance checking struggles with scaling due to the increasing
volume of user-generated content and a limited number of moderators. Recent
advancements in Natural Language Understanding demonstrated by Large Language
Models unlock new opportunities for automated content compliance verification.
This work evaluates six AI-agents built on Open-LLMs for automated rule
compliance checking in Decentralized Social Networks, a challenging environment
due to heterogeneous community scopes and rules. Analyzing over 50,000 posts
from hundreds of Mastodon servers, we find that AI-agents effectively detect
non-compliant content, grasp linguistic subtleties, and adapt to diverse
community contexts. Most agents also show high inter-rater reliability and
consistency in score justification and suggestions for compliance. Human-based
evaluation with domain experts confirmed the agents' reliability and
usefulness, rendering them promising tools for semi-automated or
human-in-the-loop content moderation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SynSUM -- Synthetic Benchmark with Structured and Unstructured Medical
  Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paloma Rabaey, Henri Arno, Stefan Heytens, Thomas Demeester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the SynSUM benchmark, a synthetic dataset linking unstructured
clinical notes to structured background variables. The dataset consists of
10,000 artificial patient records containing tabular variables (like symptoms,
diagnoses and underlying conditions) and related notes describing the fictional
patient encounter in the domain of respiratory diseases. The tabular portion of
the data is generated through a Bayesian network, where both the causal
structure between the variables and the conditional probabilities are proposed
by an expert based on domain knowledge. We then prompt a large language model
(GPT-4o) to generate a clinical note related to this patient encounter,
describing the patient symptoms and additional context. The SynSUM dataset is
primarily designed to facilitate research on clinical information extraction in
the presence of tabular background variables, which can be linked through
domain knowledge to concepts of interest to be extracted from the text - the
symptoms, in the case of SynSUM. Secondary uses include research on the
automation of clinical reasoning over both tabular data and text, causal effect
estimation in the presence of tabular and/or textual confounders, and
multi-modal synthetic data generation. The dataset can be downloaded from
https://github.com/prabaey/SynSUM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Affective Computing Has Changed: The Foundation Model Disruption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Björn Schuller, Adria Mallol-Ragolta, Alejandro Peña Almansa, Iosif Tsangko, Mostafa M. Amin, Anastasia Semertzidou, Lukas Christ, Shahin Amiriparian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dawn of Foundation Models has on the one hand revolutionised a wide range
of research problems, and, on the other hand, democratised the access and use
of AI-based tools by the general public. We even observe an incursion of these
models into disciplines related to human psychology, such as the Affective
Computing domain, suggesting their affective, emerging capabilities. In this
work, we aim to raise awareness of the power of Foundation Models in the field
of Affective Computing by synthetically generating and analysing multimodal
affective data, focusing on vision, linguistics, and speech (acoustics). We
also discuss some fundamental problems, such as ethical issues and regulatory
aspects, related to the use of Foundation Models in this research area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Language Tracking with Multi-modal Interaction: A Robust
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuchen Li, Shiyu Hu, Xiaokun Feng, Dailing Zhang, Meiqi Wu, Jing Zhang, Kaiqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Language Tracking (VLT) enhances tracking by mitigating the
limitations of relying solely on the visual modality, utilizing high-level
semantic information through language. This integration of the language enables
more advanced human-machine interaction. The essence of interaction is
cognitive alignment, which typically requires multiple information exchanges,
especially in the sequential decision-making process of VLT. However, current
VLT benchmarks do not account for multi-round interactions during tracking.
They provide only an initial text and bounding box (bbox) in the first frame,
with no further interaction as tracking progresses, deviating from the original
motivation of the VLT task. To address these limitations, we propose a novel
and robust benchmark, VLT-MI (Visual Language Tracking with Multi-modal
Interaction), which introduces multi-round interaction into the VLT task for
the first time. (1) We generate diverse, multi-granularity texts for
multi-round, multi-modal interaction based on existing mainstream VLT
benchmarks using DTLLM-VLT, leveraging the world knowledge of LLMs. (2) We
propose a new VLT interaction paradigm that achieves multi-round interaction
through text updates and object recovery. When multiple tracking failures
occur, we provide the tracker with more aligned texts and corrected bboxes
through interaction, thereby expanding the scope of VLT downstream tasks. (3)
We conduct comparative experiments on both traditional VLT benchmarks and
VLT-MI, evaluating and analyzing the accuracy and robustness of trackers under
the interactive paradigm. This work offers new insights and paradigms for the
VLT task, enabling a fine-grained evaluation of multi-modal trackers. We
believe this approach can be extended to additional datasets in the future,
supporting broader evaluations and comparisons of video-language model
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Impact of Data Quantity on ASR in Extremely Low-resource
  Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao-Fei Cheng, Li-Wei Chen, Hung-Shin Lee, Hsin-Min Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the efficacy of data augmentation techniques for
low-resource automatic speech recognition (ASR), focusing on two endangered
Austronesian languages, Amis and Seediq. Recognizing the potential of
self-supervised learning (SSL) in low-resource settings, we explore the impact
of data volume on the continued pre-training of SSL models. We propose a novel
data-selection scheme leveraging a multilingual corpus to augment the limited
target language data. This scheme utilizes a language classifier to extract
utterance embeddings and employs one-class classifiers to identify utterances
phonetically and phonologically proximate to the target languages. Utterances
are ranked and selected based on their decision scores, ensuring the inclusion
of highly relevant data in the SSL-ASR pipeline. Our experimental results
demonstrate the effectiveness of this approach, yielding substantial
improvements in ASR performance for both Amis and Seediq. These findings
underscore the feasibility and promise of data augmentation through
cross-lingual transfer learning for low-resource language ASR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FP-VEC: Fingerprinting Large Language Models via Efficient Vector
  Addition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhua Xu, Wenpeng Xing, Zhebo Wang, Chang Hu, Chen Jie, Meng Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training Large Language Models (LLMs) requires immense computational power
and vast amounts of data. As a result, protecting the intellectual property of
these models through fingerprinting is essential for ownership authentication.
While adding fingerprints to LLMs through fine-tuning has been attempted, it
remains costly and unscalable. In this paper, we introduce FP-VEC, a pilot
study on using fingerprint vectors as an efficient fingerprinting method for
LLMs. Our approach generates a fingerprint vector that represents a
confidential signature embedded in the model, allowing the same fingerprint to
be seamlessly incorporated into an unlimited number of LLMs via vector
addition. Results on several LLMs show that FP-VEC is lightweight by running on
CPU-only devices for fingerprinting, scalable with a single training and
unlimited fingerprinting process, and preserves the model's normal behavior.
The project page is available at https://fingerprintvector.github.io .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIPO: Improving Training Objective for Iterative Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaojie Shen, Xinyao Wang, Yulei Niu, Ying Zhou, Lexin Tang, Libo Zhang, Fan Chen, Longyin Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference Optimization (PO), is gaining popularity as an alternative choice
of Proximal Policy Optimization (PPO) for aligning Large Language Models
(LLMs). Recent research on aligning LLMs iteratively with synthetic or
partially synthetic data shows promising results in scaling up PO training for
both academic settings and proprietary trained models such as Llama3. Despite
its success, our study shows that the length exploitation issue present in PO
is even more severe in Iterative Preference Optimization (IPO) due to the
iterative nature of the process. In this work, we study iterative preference
optimization with synthetic data. We share the findings and analysis along the
way of building the iterative preference optimization pipeline. More
specifically, we discuss the length exploitation issue during iterative
preference optimization and propose our training objective for iterative
preference optimization, namely Agreement-aware Iterative Preference
Optimization (AIPO). To demonstrate the effectiveness of our method, we conduct
comprehensive experiments and achieve state-of-the-art performance on MT-Bench,
AlpacaEval 2.0, and Arena-Hard. Our implementation and model checkpoints will
be made available at https://github.com/bytedance/AIPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Your Weak <span class="highlight-title">LLM</span> is Secretly a Strong Teacher for Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leitian Tao, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The burgeoning capabilities of large language models (LLMs) have underscored
the need for alignment to ensure these models act in accordance with human
values and intentions. Existing alignment frameworks present constraints either
in the form of expensive human effort or high computational costs. This paper
explores a promising middle ground, where we employ a weak LLM that is
significantly less resource-intensive than top-tier models, yet offers more
automation than purely human feedback. We present a systematic study to
evaluate and understand weak LLM's ability to generate feedback for alignment.
Our empirical findings demonstrate that weak LLMs can provide feedback that
rivals or even exceeds that of fully human-annotated data. Our study indicates
a minimized impact of model size on feedback efficacy, shedding light on a
scalable and sustainable alignment strategy. To deepen our understanding of
alignment under weak LLM feedback, we conduct a series of qualitative and
quantitative analyses, offering novel insights into the quality discrepancies
between human feedback vs. weak LLM feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring SSL Discrete Tokens for Multilingual ASR <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Cui, Daxin Tan, Yifan Yang, Dingdong Wang, Huimeng Wang, Xiao Chen, Xie Chen, Xunying Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of Self-supervised Learning (SSL) in speech-related
tasks, there has been growing interest in utilizing discrete tokens generated
by SSL for automatic speech recognition (ASR), as they offer faster processing
techniques. However, previous studies primarily focused on multilingual ASR
with Fbank features or English ASR with discrete tokens, leaving a gap in
adapting discrete tokens for multilingual ASR scenarios. This study presents a
comprehensive comparison of discrete tokens generated by various leading SSL
models across multiple language domains. We aim to explore the performance and
efficiency of speech discrete tokens across multiple language domains for both
monolingual and multilingual ASR scenarios. Experimental results demonstrate
that discrete tokens achieve comparable results against systems trained on
Fbank features in ASR tasks across seven language domains with an average word
error rate (WER) reduction of 0.31% and 1.76% absolute (2.80% and 15.70%
relative) on dev and test sets respectively, with particularly WER reduction of
6.82% absolute (41.48% relative) on the Polish test set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring SSL Discrete Speech Features for Zipformer-based Contextual
  ASR <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Cui, Yifan Yang, Jiajun Deng, Jiawen Kang, Shujie Hu, Tianzi Wang, Zhaoqing Li, Shiliang Zhang, Xie Chen, Xunying Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) based discrete speech representations are
highly compact and domain adaptable. In this paper, SSL discrete speech
features extracted from WavLM models are used as additional cross-utterance
acoustic context features in Zipformer-Transducer ASR systems. The efficacy of
replacing Fbank features with discrete token features for modelling either
cross-utterance contexts (from preceding and future segments), or current
utterance's internal contexts alone, or both at the same time, are demonstrated
thoroughly on the Gigaspeech 1000-hr corpus. The best Zipformer-Transducer
system using discrete tokens based cross-utterance context features outperforms
the baseline using utterance internal context only with statistically
significant word error rate (WER) reductions of 0.32% to 0.41% absolute (2.78%
to 3.54% relative) on the dev and test data. The lowest published WER of 11.15%
and 11.14% were obtained on the dev and test sets. Our work is open-source and
publicly available at
https://github.com/open-creator/icefall/tree/master/egs/gigaspeech/Context\_ASR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Ingredient Substitution Using Large Language Models to
  Enhance Phytochemical Content in Recipes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Rita, Josh Southern, Ivan Laponogov, Kyle Higgins, Kirill Veselkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the emerging field of computational gastronomy, aligning culinary
practices with scientifically supported nutritional goals is increasingly
important. This study explores how large language models (LLMs) can be applied
to optimize ingredient substitutions in recipes, specifically to enhance the
phytochemical content of meals. Phytochemicals are bioactive compounds found in
plants, which, based on preclinical studies, may offer potential health
benefits. We fine-tuned models, including OpenAI's GPT-3.5, DaVinci, and Meta's
TinyLlama, using an ingredient substitution dataset. These models were used to
predict substitutions that enhance phytochemical content and create a
corresponding enriched recipe dataset. Our approach improved Hit@1 accuracy on
ingredient substitution tasks, from the baseline 34.53 plus-minus 0.10% to
38.03 plus-minus 0.28% on the original GISMo dataset, and from 40.24 plus-minus
0.36% to 54.46 plus-minus 0.29% on a refined version of the same dataset. These
substitutions led to the creation of 1,951 phytochemically enriched ingredient
pairings and 1,639 unique recipes. While this approach demonstrates potential
in optimizing ingredient substitutions, caution must be taken when drawing
conclusions about health benefits, as the claims are based on preclinical
evidence. Future work should include clinical validation and broader datasets
to further evaluate the nutritional impact of these substitutions. This
research represents a step forward in using AI to promote healthier eating
practices, providing potential pathways for integrating computational methods
with nutritional science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sign Language Sense Disambiguation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jana Grimm, Miriam Winkler, Oliver Kraus, Tanalp Agustoslu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This project explores methods to enhance sign language translation of German
sign language, specifically focusing on disambiguation of homonyms. Sign
language is ambiguous and understudied which is the basis for our experiments.
We approach the improvement by training transformer-based models on various
bodypart representations to shift the focus on said bodypart. To determine the
impact of, e.g., the hand or mouth representations, we experiment with
different combinations. The results show that focusing on the mouth increases
the performance in small dataset settings while shifting the focus on the hands
retrieves better results in larger dataset settings. Our results contribute to
better accessibility for non-hearing persons by improving the systems powering
digital assistants, enabling a more accurate interaction. The code for this
project can be found on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LIMO2024 @ KONVENS 2024, 8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Journalists, Emotions, and the Introduction of Generative AI Chatbots: A
  Large-Scale Analysis of Tweets Before and After the Launch of ChatGPT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seth C. Lewis, David M. Markowitz, Jon Benedik Bunquin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As part of a broader look at the impact of generative AI, this study
investigated the emotional responses of journalists to the release of ChatGPT
at the time of its launch. By analyzing nearly 1 million Tweets from
journalists at major U.S. news outlets, we tracked changes in emotional tone
and sentiment before and after the introduction of ChatGPT in November 2022.
Using various computational and natural language processing techniques to
measure emotional shifts in response to ChatGPT's release, we found an increase
in positive emotion and a more favorable tone post-launch, suggesting initial
optimism toward AI's potential. This research underscores the pivotal role of
journalists as interpreters of technological innovation and disruption,
highlighting how their emotional reactions may shape public narratives around
emerging technologies. The study contributes to understanding the intersection
of journalism, emotion, and AI, offering insights into the broader societal
impact of generative AI tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Monolingual and Crosslingual Word-in-Context Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki Arase, Tomoyuki Kajiwara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a method that distils representations of word
meaning in context from a pre-trained masked language model in both monolingual
and crosslingual settings. Word representations are the basis for context-aware
lexical semantics and unsupervised semantic textual similarity (STS)
estimation. Different from existing approaches, our method does not require
human-annotated corpora nor updates of the parameters of the pre-trained model.
The latter feature is appealing for practical scenarios where the off-the-shelf
pre-trained model is a common asset among different applications. Specifically,
our method learns to combine the outputs of different hidden layers of the
pre-trained model using self-attention. Our auto-encoder based training only
requires an automatically generated corpus. To evaluate the performance of the
proposed approach, we performed extensive experiments using various benchmark
tasks. The results on the monolingual tasks confirmed that our representations
exhibited a competitive performance compared to that of the previous study for
the context-aware lexical semantic tasks and outperformed it for STS
estimation. The results of the crosslingual tasks revealed that the proposed
method largely improved crosslingual word representations of multilingual
pre-trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Layerwise Change of Knowledge in Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Cheng, Lei Cheng, Zhaoran Peng, Yang Xu, Tian Han, Quanshi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to explain how a deep neural network (DNN) gradually extracts
new knowledge and forgets noisy features through layers in forward propagation.
Up to now, although the definition of knowledge encoded by the DNN has not
reached a consensus, Previous studies have derived a series of mathematical
evidence to take interactions as symbolic primitive inference patterns encoded
by a DNN. We extend the definition of interactions and, for the first time,
extract interactions encoded by intermediate layers. We quantify and track the
newly emerged interactions and the forgotten interactions in each layer during
the forward propagation, which shed new light on the learning behavior of DNNs.
The layer-wise change of interactions also reveals the change of the
generalization capacity and instability of feature representations of a DNN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ L3Cube-IndicQuest: A Benchmark Questing Answering <span class="highlight-title">Dataset</span> for Evaluating
  Knowledge of <span class="highlight-title">LLM</span>s in Indic Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pritika Rohera, Chaitrali Ginimav, Akanksha Salunke, Gayatri Sawant, Raviraj Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have made significant progress in incorporating
Indic languages within multilingual models. However, it is crucial to
quantitatively assess whether these languages perform comparably to globally
dominant ones, such as English. Currently, there is a lack of benchmark
datasets specifically designed to evaluate the regional knowledge of LLMs in
various Indic languages. In this paper, we present the L3Cube-IndicQuest, a
gold-standard question-answering benchmark dataset designed to evaluate how
well multilingual LLMs capture regional knowledge across various Indic
languages. The dataset contains 200 question-answer pairs, each for English and
19 Indic languages, covering five domains specific to the Indic region. We aim
for this dataset to serve as a benchmark, providing ground truth for evaluating
the performance of LLMs in understanding and representing knowledge relevant to
the Indian context. The IndicQuest can be used for both reference-based
evaluation and LLM-as-a-judge evaluation. The dataset is shared publicly at
https://github.com/l3cube-pune/indic-nlp .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NSP: A Neuro-Symbolic Natural Language Navigational Planner <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06859v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06859v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William English, Dominic Simon, Sumit Jha, Rickard Ewetz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Path planners that can interpret free-form natural language instructions hold
promise to automate a wide range of robotics applications. These planners
simplify user interactions and enable intuitive control over complex
semi-autonomous systems. While existing symbolic approaches offer guarantees on
the correctness and efficiency, they struggle to parse free-form natural
language inputs. Conversely, neural approaches based on pre-trained Large
Language Models (LLMs) can manage natural language inputs but lack performance
guarantees. In this paper, we propose a neuro-symbolic framework for path
planning from natural language inputs called NSP. The framework leverages the
neural reasoning abilities of LLMs to i) craft symbolic representations of the
environment and ii) a symbolic path planning algorithm. Next, a solution to the
path planning problem is obtained by executing the algorithm on the environment
representation. The framework uses a feedback loop from the symbolic execution
environment to the neural generation process to self-correct syntax errors and
satisfy execution time constraints. We evaluate our neuro-symbolic approach
using a benchmark suite with 1500 path-planning problems. The experimental
evaluation shows that our neuro-symbolic approach produces 90.1% valid paths
that are on average 19-77% shorter than state-of-the-art neural approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, Preprint of paper accepted at 23rd International Conference
  on Machine Learning and Applications (ICMLA) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UI-JEPA: Towards Active Perception of User Intent through Onscreen User
  Activity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng Fu, Raviteja Anantha, Prabal Vashisht, Jianpeng Cheng, Etai Littwin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating user intent from a sequence of user interface (UI) actions is a
core challenge in comprehensive UI understanding. Recent advancements in
multimodal large language models (MLLMs) have led to substantial progress in
this area, but their demands for extensive model parameters, computing power,
and high latency makes them impractical for scenarios requiring lightweight,
on-device solutions with low latency or heightened privacy. Additionally, the
lack of high-quality datasets has hindered the development of such lightweight
models. To address these challenges, we propose UI-JEPA, a novel framework that
employs masking strategies to learn abstract UI embeddings from unlabeled data
through self-supervised learning, combined with an LLM decoder fine-tuned for
user intent prediction. We also introduce two new UI-grounded multimodal
datasets, "Intent in the Wild" (IIW) and "Intent in the Tame" (IIT), designed
for few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos
across 219 intent categories, while IIT contains 914 videos across 10
categories. We establish the first baselines for these datasets, showing that
representations learned using a JEPA-style objective, combined with an LLM
decoder, can achieve user intent predictions that match the performance of
state-of-the-art large MLLMs, but with significantly reduced annotation and
deployment resources. Measured by intent similarity scores, UI-JEPA outperforms
GPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged
across two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x
reduction in computational cost and a 6.6x improvement in latency in the IIW
dataset. These results underscore the effectiveness of UI-JEPA, highlighting
its potential for lightweight, high-performance UI understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>-based speaker diarization correction: A generalizable approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Efstathiadis, Vijay Yadav, Anzar Abbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speaker diarization is necessary for interpreting conversations transcribed
using automated speech recognition (ASR) tools. Despite significant
developments in diarization methods, diarization accuracy remains an issue.
Here, we investigate the use of large language models (LLMs) for diarization
correction as a post-processing step. LLMs were fine-tuned using the Fisher
corpus, a large dataset of transcribed conversations. The ability of the models
to improve diarization accuracy in a holdout dataset from the Fisher corpus as
well as an independent dataset was measured. We report that fine-tuned LLMs can
markedly improve diarization accuracy. However, model performance is
constrained to transcripts produced using the same ASR tool as the transcripts
used for fine-tuning, limiting generalizability. To address this constraint, an
ensemble model was developed by combining weights from three separate models,
each fine-tuned using transcripts from a different ASR tool. The ensemble model
demonstrated better overall performance than each of the ASR-specific models,
suggesting that a generalizable and ASR-agnostic approach may be achievable. We
have made the weights of these models publicly available on HuggingFace at
https://huggingface.co/bklynhlth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ContextCite: Attributing Model Generation to Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Cohen-Wang, Harshay Shah, Kristian Georgiev, Aleksander Madry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How do language models use information provided as context when generating a
response? Can we infer whether a particular generated statement is actually
grounded in the context, a misinterpretation, or fabricated? To help answer
these questions, we introduce the problem of context attribution: pinpointing
the parts of the context (if any) that led a model to generate a particular
statement. We then present ContextCite, a simple and scalable method for
context attribution that can be applied on top of any existing language model.
Finally, we showcase the utility of ContextCite through three applications: (1)
helping verify generated statements (2) improving response quality by pruning
the context and (3) detecting poisoning attacks. We provide code for
ContextCite at https://github.com/MadryLab/context-cite.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion Capture Analysis of Verb and Adjective Types in Austrian Sign
  Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julia Krebs, Evie Malaia, Ronnie B. Wilbur, Isabella Fessl, Hans-Peter Wiesinger, Hermann Schwameder, Dietmar Roehm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Across a number of sign languages, temporal and spatial characteristics of
dominant hand articulation are used to express semantic and grammatical
features. In this study of Austrian Sign Language (\"Osterreichische
Geb\"ardensprache, or \"OGS), motion capture data of four Deaf signers is used
to quantitatively characterize the kinematic parameters of sign production in
verbs and adjectives. We investigate (1) the difference in production between
verbs involving a natural endpoint (telic verbs; e.g. arrive) and verbs lacking
an endpoint (atelic verbs; e.g. analyze), and (2) adjective signs in
intensified vs. non-intensified (plain) forms. Motion capture data analysis
using linear-mixed effects models (LME) indicates that both the endpoint
marking in verbs, as well as marking of intensification in adjectives, are
expressed by movement modulation in \"OGS. While the semantic distinction
between verb types (telic/atelic) is marked by higher peak velocity and shorter
duration for telic signs compared to atelic ones, the grammatical distinction
(intensification) in adjectives is expressed by longer duration for intensified
compared to non-intensified adjectives. The observed individual differences of
signers might be interpreted as personal signing style.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pseudo-<span class="highlight-title">Prompt</span> Generating in <span class="highlight-title">Pre-train</span>ed Vision-Language Models for
  Multi-Label Medical Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06468v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06468v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaoqin Ye, Junjie Zhang, Hongwei Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of medical image recognition is notably complicated by the presence
of varied and multiple pathological indications, presenting a unique challenge
in multi-label classification with unseen labels. This complexity underlines
the need for computer-aided diagnosis methods employing multi-label zero-shot
learning. Recent advancements in pre-trained vision-language models (VLMs) have
showcased notable zero-shot classification abilities on medical images.
However, these methods have limitations on leveraging extensive pre-trained
knowledge from broader image datasets, and often depend on manual prompt
construction by expert radiologists. By automating the process of prompt
tuning, prompt learning techniques have emerged as an efficient way to adapt
VLMs to downstream tasks. Yet, existing CoOp-based strategies fall short in
performing class-specific prompts on unseen categories, limiting
generalizability in fine-grained scenarios. To overcome these constraints, we
introduce a novel prompt generation approach inspirited by text generation in
natural language processing (NLP). Our method, named Pseudo-Prompt Generating
(PsPG), capitalizes on the priori knowledge of multi-modal features. Featuring
a RNN-based decoder, PsPG autoregressively generates class-tailored embedding
vectors, i.e., pseudo-prompts. Comparative evaluations on various multi-label
chest radiograph datasets affirm the superiority of our approach against
leading medical vision-language and multi-label prompt learning methods. The
source code is available at https://github.com/fallingnight/PsPG
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by PRCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural Language Processing with Commonsense Knowledge: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.04674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.04674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yubo Xie, Zonghui Liu, Zongyang Ma, Fanyuan Meng, Yan Xiao, Fahui Miao, Pearl Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commonsense knowledge is essential for advancing natural language processing
(NLP) by enabling models to engage in human-like reasoning, which requires a
deeper understanding of context and often involves making inferences based on
implicit external knowledge. This paper explores the integration of commonsense
knowledge into various NLP tasks. We begin by reviewing prominent commonsense
knowledge bases and then discuss the benchmarks used to evaluate the
commonsense reasoning capabilities of NLP models, particularly language models.
Furthermore, we highlight key methodologies for incorporating commonsense
knowledge and their applications across different NLP tasks. The paper also
examines the challenges and emerging trends in enhancing NLP systems with
commonsense reasoning. All literature referenced in this survey can be accessed
via our GitHub repository: https://github.com/yuboxie/awesome-commonsense.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 3 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding How Code<span class="highlight-title">LLM</span>s (Mis)Predict Types with Activation Steering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01903v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01903v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Lucchetti, Arjun Guha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CodeLLMs are transforming software development as we know it. This is
especially true for tasks where rule-based approaches fall short, like type
prediction. The type prediction task consists in adding a new type annotation
to a partially typed program, such that the resulting program is closer to
being fully typed. The intractability of rule-based approaches and high cost of
manual annotation make CodeLLMs an attractive solution to the problem. However,
CodeLLMs are still far from being deployed on the large-scale due to doubts
surrounding their reliability.
  To shed some light on how CodeLLMs approach type prediction, we investigate
what happens when a model mispredicts a type. We show that by applying
semantics-preserving edits to code, CodeLLMs are eventually misled into
mispredicting type annotations. However, by leveraging activation steering we
are able to "steer" the model back to the correct prediction, making models
more robust against semantically irrelevant prompt features. We show that
steering achieves comparable performance to fine-tuning directly on the type
prediction task. Furthermore, we find that steering vectors computed from
Python code are effective at correcting TypeScript mispredictions, and vice
versa. To our knowledge, this is the first evidence of its kind to suggest that
CodeLLMs learn task representations that transfer across languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Automatic Quality Metric for Evaluating Simultaneous Interpretation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06650v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06650v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mana Makinae, Katsuhito Sudoh, Mararu Yamada, Satoshi Nakamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous interpretation (SI), the translation of one language to another
in real time, starts translation before the original speech has finished. Its
evaluation needs to consider both latency and quality. This trade-off is
challenging especially for distant word order language pairs such as English
and Japanese. To handle this word order gap, interpreters maintain the word
order of the source language as much as possible to keep up with original
language to minimize its latency while maintaining its quality, whereas in
translation reordering happens to keep fluency in the target language. This
means outputs synchronized with the source language are desirable based on the
real SI situation, and it's a key for further progress in computational SI and
simultaneous machine translation (SiMT). In this work, we propose an automatic
evaluation metric for SI and SiMT focusing on word order synchronization. Our
evaluation metric is based on rank correlation coefficients, leveraging
cross-lingual pre-trained language models. Our experimental results on
NAIST-SIC-Aligned and JNPC showed our metrics' effectiveness to measure word
order synchronization between source and target language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Combining Data Generation and Active Learning for Low-Resource Question
  Answering <span class="chip">ICANN 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Kimmich, Andrea Bartezzaghi, Jasmina Bogojeska, Cristiano Malossi, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural approaches have become very popular in Question Answering (QA),
however, they require a large amount of annotated data. In this work, we
propose a novel approach that combines data augmentation via question-answer
generation with Active Learning to improve performance in low-resource
settings, where the target domains are diverse in terms of difficulty and
similarity to the source domain. We also investigate Active Learning for
question answering in different stages, overall reducing the annotation effort
of humans. For this purpose, we consider target domains in realistic settings,
with an extremely low amount of annotated samples but with many unlabeled
documents, which we assume can be obtained with little effort. Additionally, we
assume a sufficient amount of labeled data from the source domain being
available. We perform extensive experiments to find the best setup for
incorporating domain experts. Our findings show that our novel approach, where
humans are incorporated in a data generation approach, boosts performance in
the low-resource, domain-specific setting, allowing for low-labeling-effort
question answering systems in new, specialized domains. They further
demonstrate how human annotation affects the performance of QA depending on the
stage it is performed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICANN 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting Sentence-Level Factuality of News and Bias of Media Outlets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11850v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11850v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francielle Vargas, Kokil Jaidka, Thiago A. S. Pardo, Fabrício Benevenuto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated news credibility and fact-checking at scale require accurately
predicting news factuality and media bias. This paper introduces a large
sentence-level dataset, titled "FactNews", composed of 6,191 sentences expertly
annotated according to factuality and media bias definitions proposed by
AllSides. We use FactNews to assess the overall reliability of news sources, by
formulating two text classification problems for predicting sentence-level
factuality of news reporting and bias of media outlets. Our experiments
demonstrate that biased sentences present a higher number of words compared to
factual sentences, besides having a predominance of emotions. Hence, the
fine-grained analysis of subjectivity and impartiality of news articles
provided promising results for predicting the reliability of media outlets.
Finally, due to the severity of fake news and political polarization in Brazil,
and the lack of research for Portuguese, both dataset and baseline were
proposed for Brazilian Portuguese.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 14th International Conference on Recent Advances
  in Natural Language Processing (RANLP 2023).
  https://aclanthology.org/2023.ranlp-1.127</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Performance Law of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09895v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09895v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhan Wu, Ruiming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Guided by the belief of the scaling law, large language models (LLMs) have
achieved impressive performance in recent years. However, scaling law only
gives a qualitative estimation of loss, which is influenced by various factors
such as model architectures, data distributions, tokenizers, and computation
precision. Thus, estimating the real performance of LLMs with different
training settings rather than loss may be quite useful in practical
development. In this article, we present an empirical equation named
"Performance Law" to directly predict the MMLU score of an LLM, which is a
widely used metric to indicate the general capability of LLMs in real-world
conversations and applications. Based on only a few key hyperparameters of the
LLM architecture and the size of training data, we obtain a quite accurate MMLU
prediction of various LLMs with diverse sizes and architectures developed by
different organizations in different years. Performance law can be used to
guide the choice of LLM architecture and the effective allocation of
computational resources without extensive experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Personal opinions of the authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Design of Informative Take-Over Requests for Semi-Autonomous
  Cyber-Physical Systems: Combining Spoken Language and Visual Icons in a
  Drone-Controller Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08253v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08253v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwini Gundappa, Emilia Ellsiepen, Lukas Schmitz, Frederik Wiehr, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The question of how cyber-physical systems should interact with human
partners that can take over control or exert oversight is becoming more
pressing, as these systems are deployed for an ever larger range of tasks.
Drawing on the literatures on handing over control during semi-autonomous
driving and human-robot interaction, we propose a design of a take-over request
that combines an abstract pre-alert with an informative TOR: Relevant sensor
information is highlighted on the controller's display, while a spoken message
verbalizes the reason for the TOR. We conduct our study in the context of a
semi-autonomous drone control scenario as our testbed. The goal of our online
study is to assess in more detail what form a language-based TOR should take.
Specifically, we compare a full sentence condition to shorter fragments, and
test whether the visual highlighting should be done synchronously or
asynchronously with the speech. Participants showed a higher accuracy in
choosing the correct solution with our bi-modal TOR and felt that they were
better able to recognize the critical situation. Using only fragments in the
spoken message rather than full sentences did not lead to improved accuracy or
faster reactions. Also, synchronizing the visual highlighting with the spoken
message did not result in better accuracy and response times were even
increased in this condition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contri(e)ve: Context + Retrieve for Scholarly Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanchan Shivashankar, Nadine Steinmetz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scholarly communication is a rapid growing field containing a wealth of
knowledge. However, due to its unstructured and document format, it is
challenging to extract useful information from them through conventional
document retrieval methods. Scholarly knowledge graphs solve this problem, by
representing the documents in a semantic network, providing, hidden insights,
summaries and ease of accessibility through queries. Naturally, question
answering for scholarly graphs expands the accessibility to a wider audience.
But some of the knowledge in this domain is still presented as unstructured
text, thus requiring a hybrid solution for question answering systems. In this
paper, we present a two step solution using open source Large Language
Model(LLM): Llama3.1 for Scholarly-QALD dataset. Firstly, we extract the
context pertaining to the question from different structured and unstructured
data sources: DBLP, SemOpenAlex knowledge graphs and Wikipedia text. Secondly,
we implement prompt engineering to improve the information retrieval
performance of the LLM. Our approach achieved an F1 score of 40% and also
observed some anomalous responses from the LLM, that are discussed in the final
part of the paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Analysis of <span class="highlight-title">Pretrain</span>ed Audio Representations in Music
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan-Martin Tamm, Anna Aljanaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the years, Music Information Retrieval (MIR) has proposed various models
pretrained on large amounts of music data. Transfer learning showcases the
proven effectiveness of pretrained backend models with a broad spectrum of
downstream tasks, including auto-tagging and genre classification. However, MIR
papers generally do not explore the efficiency of pretrained models for Music
Recommender Systems (MRS). In addition, the Recommender Systems community tends
to favour traditional end-to-end neural network learning over these models. Our
research addresses this gap and evaluates the applicability of six pretrained
backend models (MusicFM, Music2Vec, MERT, EncodecMAE, Jukebox, and MusiCNN) in
the context of MRS. We assess their performance using three recommendation
models: K-nearest neighbours (KNN), shallow neural network, and BERT4Rec. Our
findings suggest that pretrained audio representations exhibit significant
performance variability between traditional MIR tasks and MRS, indicating that
valuable aspects of musical information captured by backend models may differ
depending on the task. This study establishes a foundation for further
exploration of pretrained audio representations to enhance music recommendation
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accurate and Fast Estimation of Temporal Motifs using Path Sampling <span class="chip">ICDM'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjie Pan, Omkar Bhalerao, C. Seshadhri, Nishil Talati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counting the number of small subgraphs, called motifs, is a fundamental
problem in social network analysis and graph mining. Many real-world networks
are directed and temporal, where edges have timestamps. Motif counting in
directed, temporal graphs is especially challenging because there are a
plethora of different kinds of patterns. Temporal motif counts reveal much
richer information and there is a need for scalable algorithms for motif
counting.
  A major challenge in counting is that there can be trillions of temporal
motif matches even with a graph with only millions of vertices. Both the motifs
and the input graphs can have multiple edges between two vertices, leading to a
combinatorial explosion problem. Counting temporal motifs involving just four
vertices is not feasible with current state-of-the-art algorithms.
  We design an algorithm, TEACUPS, that addresses this problem using a novel
technique of temporal path sampling. We combine a path sampling method with
carefully designed temporal data structures, to propose an efficient
approximate algorithm for temporal motif counting. TEACUPS is an unbiased
estimator with provable concentration behavior, which can be used to bound the
estimation error. For a Bitcoin graph with hundreds of millions of edges,
TEACUPS runs in less than 1 minute, while the exact counting algorithm takes
more than a day. We empirically demonstrate the accuracy of TEACUPS on large
datasets, showing an average of 30$\times$ speedup (up to 2000$\times$ speedup)
compared to existing GPU-based exact counting methods while preserving high
count estimation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ICDM'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proactive Recommendation in Social Networks: Steering User Interest via
  Neighbor Influence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Pan, Shuxian Bi, Wenjie Wang, Haoxuan Li, Peng Wu, Fuli Feng, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommending items solely catering to users' historical interests narrows
users' horizons. Recent works have considered steering target users beyond
their historical interests by directly adjusting items exposed to them.
However, the recommended items for direct steering might not align perfectly
with users' interests evolution, detrimentally affecting target users'
experience. To avoid this issue, we propose a new task named Proactive
Recommendation in Social Networks (PRSN) that indirectly steers users' interest
by utilizing the influence of social neighbors, i.e., indirect steering by
adjusting the exposure of a target item to target users' neighbors. The key to
PRSN lies in answering an interventional question: what would a target user's
feedback be on a target item if the item is exposed to the user's different
neighbors? To answer this question, we resort to causal inference and formalize
PRSN as: (1) estimating the potential feedback of a user on an item, under the
network interference by the item's exposure to the user's neighbors; and (2)
adjusting the exposure of a target item to target users' neighbors to trade off
steering performance and the damage to the neighbors' experience. To this end,
we propose a Neighbor Interference Recommendation (NIRec) framework with two
key modules: (1)an interference representation-based estimation module for
modeling potential feedback; and (2) a post-learning-based optimization module
for optimizing a target item's exposure to trade off steering performance and
the neighbors' experience by greedy search. We conduct extensive
semi-simulation experiments based on three real-world datasets, validating the
steering effectiveness of NIRec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>-based Weak Supervision Framework for Query Intent Classification in
  Video Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farnoosh Javadi, Phanideep Gampa, Alyssa Woo, Xingxing Geng, Hang Zhang, Jose Sepulveda, Belhassen Bayar, Fei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Streaming services have reshaped how we discover and engage with digital
entertainment. Despite these advancements, effectively understanding the wide
spectrum of user search queries continues to pose a significant challenge. An
accurate query understanding system that can handle a variety of entities that
represent different user intents is essential for delivering an enhanced user
experience. We can build such a system by training a natural language
understanding (NLU) model; however, obtaining high-quality labeled training
data in this specialized domain is a substantial obstacle. Manual annotation is
costly and impractical for capturing users' vast vocabulary variations. To
address this, we introduce a novel approach that leverages large language
models (LLMs) through weak supervision to automatically annotate a vast
collection of user search queries. Using prompt engineering and a diverse set
of LLM personas, we generate training data that matches human annotator
expectations. By incorporating domain knowledge via Chain of Thought and
In-Context Learning, our approach leverages the labeled data to train
low-latency models optimized for real-time inference. Extensive evaluations
demonstrated that our approach outperformed the baseline with an average
relative gain of 113% in recall. Furthermore, our novel prompt engineering
framework yields higher quality LLM-generated data to be used for weak
supervision; we observed 47.60% improvement over baseline in agreement rate
between LLM predictions and human annotations with respect to F1 score,
weighted according to the distribution of occurrences of the search queries.
Our persona selection routing mechanism further adds an additional 3.67%
increase in weighted F1 score on top of our novel prompt engineering framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeSHFS: Neighborhood Search with Heuristic-based Feature Selection for
  Click-Through Rate Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dogukan Aksu, Ismail Hakki Toroslu, Hasan Davulcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-through-rate (CTR) prediction plays an important role in online
advertising and ad recommender systems. In the past decade, maximizing CTR has
been the main focus of model development and solution creation. Therefore,
researchers and practitioners have proposed various models and solutions to
enhance the effectiveness of CTR prediction. Most of the existing literature
focuses on capturing either implicit or explicit feature interactions. Although
implicit interactions are successfully captured in some studies, explicit
interactions present a challenge for achieving high CTR by extracting both
low-order and high-order feature interactions. Unnecessary and irrelevant
features may cause high computational time and low prediction performance.
Furthermore, certain features may perform well with specific predictive models
while underperforming with others. Also, feature distribution may fluctuate due
to traffic variations. Most importantly, in live production environments,
resources are limited, and the time for inference is just as crucial as
training time. Because of all these reasons, feature selection is one of the
most important factors in enhancing CTR prediction model performance. Simple
filter-based feature selection algorithms do not perform well and they are not
sufficient. An effective and efficient feature selection algorithm is needed to
consistently filter the most useful features during live CTR prediction
process. In this paper, we propose a heuristic algorithm named Neighborhood
Search with Heuristic-based Feature Selection (NeSHFS) to enhance CTR
prediction performance while reducing dimensionality and training time costs.
We conduct comprehensive experiments on three public datasets to validate the
efficiency and effectiveness of our proposed solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ATFLRec: A Multimodal Recommender System with Audio-Text Fusion and
  Low-Rank Adaptation via Instruction-Tuned Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezheng Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender Systems (RS) play a pivotal role in boosting user satisfaction by
providing personalized product suggestions in domains such as e-commerce and
entertainment. This study examines the integration of multimodal data text and
audio into large language models (LLMs) with the aim of enhancing
recommendation performance. Traditional text and audio recommenders encounter
limitations such as the cold-start problem, and recent advancements in LLMs,
while promising, are computationally expensive. To address these issues,
Low-Rank Adaptation (LoRA) is introduced, which enhances efficiency without
compromising performance. The ATFLRec framework is proposed to integrate audio
and text modalities into a multimodal recommendation system, utilizing various
LoRA configurations and modality fusion techniques. Results indicate that
ATFLRec outperforms baseline models, including traditional and graph neural
network-based approaches, achieving higher AUC scores. Furthermore, separate
fine-tuning of audio and text data with distinct LoRA modules yields optimal
performance, with different pooling methods and Mel filter bank numbers
significantly impacting performance. This research offers valuable insights
into optimizing multimodal recommender systems and advancing the integration of
diverse data modalities in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring <span class="highlight-title">Information</span> Retrieval Landscapes: An Investigation of a Novel
  Evaluation Techniques and Comparative Document Splitting Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Esmaeil Narimissa, David Raithel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of Retrieval-Augmented Generation (RAG) systems in
information retrieval is significantly influenced by the characteristics of the
documents being processed. In this study, the structured nature of textbooks,
the conciseness of articles, and the narrative complexity of novels are shown
to require distinct retrieval strategies. A comparative evaluation of multiple
document-splitting methods reveals that the Recursive Character Splitter
outperforms the Token-based Splitter in preserving contextual integrity. A
novel evaluation technique is introduced, utilizing an open-source model to
generate a comprehensive dataset of question-and-answer pairs, simulating
realistic retrieval scenarios to enhance testing efficiency and metric
reliability. The evaluation employs weighted scoring metrics, including
SequenceMatcher, BLEU, METEOR, and BERT Score, to assess the system's accuracy
and relevance. This approach establishes a refined standard for evaluating the
precision of RAG systems, with future research focusing on optimizing chunk and
overlap sizes to improve retrieval accuracy and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article is 16 pages long and includes detailed comparisons of
  RAG systems and document splitting techniques</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Transfer Learning Based Cooperative Wideband Spectrum Sensing
  with Model Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jibin Jia, Peihao Dong, Fuhui Zhou, Qihui Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For ultra-wideband and high-rate wireless communication systems, wideband
spectrum sensing (WSS) is critical, since it empowers secondary users (SUs) to
capture the spectrum holes for opportunistic transmission. However, WSS
encounters challenges such as excessive costs of hardware and computation due
to the high sampling rate, as well as robustness issues arising from scenario
mismatch. In this paper, a WSS neural network (WSSNet) is proposed by
exploiting multicoset preprocessing to enable the sub-Nyquist sampling, with
the two dimensional convolution design specifically tailored to work with the
preprocessed samples. A federated transfer learning (FTL) based framework
mobilizing multiple SUs is further developed to achieve a robust model
adaptable to various scenarios, which is paved by the selective weight pruning
for the fast model adaptation and inference. Simulation results demonstrate
that the proposed FTL-WSSNet achieves the fairly good performance in different
target scenarios even without local adaptation samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RePlay: a Recommendation Framework for Experimentation and Production
  Use 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07272v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07272v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Vasilev, Anna Volodkevich, Denis Kulandin, Tatiana Bysheva, Anton Klenitskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using a single tool to build and compare recommender systems significantly
reduces the time to market for new models. In addition, the comparison results
when using such tools look more consistent. This is why many different tools
and libraries for researchers in the field of recommendations have recently
appeared. Unfortunately, most of these frameworks are aimed primarily at
researchers and require modification for use in production due to the inability
to work on large datasets or an inappropriate architecture. In this demo, we
present our open-source toolkit RePlay - a framework containing an end-to-end
pipeline for building recommender systems, which is ready for production use.
RePlay also allows you to use a suitable stack for the pipeline on each stage:
Pandas, Polars, or Spark. This allows the library to scale computations and
deploy to a cluster. Thus, RePlay allows data scientists to easily move from
research mode to production mode using the same interfaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STORE: Streamlining Semantic Tokenization and Generative Recommendation
  with A Single <span class="highlight-title">LLM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07276v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07276v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qijiong Liu, Jieming Zhu, Lu Fan, Zhou Zhao, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional recommendation models often rely on unique item identifiers (IDs)
to distinguish between items, which can hinder their ability to effectively
leverage item content information and generalize to long-tail or cold-start
items. Recently, semantic tokenization has been proposed as a promising
solution that aims to tokenize each item's semantic representation into a
sequence of discrete tokens. In this way, it preserves the item's semantics
within these tokens and ensures that semantically similar items are represented
by similar tokens. These semantic tokens have become fundamental in training
generative recommendation models. However, existing generative recommendation
methods typically involve multiple sub-models for embedding, quantization, and
recommendation, leading to an overly complex system. In this paper, we propose
to streamline the semantic tokenization and generative recommendation process
with a unified framework, dubbed STORE, which leverages a single large language
model (LLM) for both tasks. Specifically, we formulate semantic tokenization as
a text-to-token task and generative recommendation as a token-to-token task,
supplemented by a token-to-text reconstruction task and a text-to-token
auxiliary task. All these tasks are framed in a generative manner and trained
using a single LLM backbone. Extensive experiments have been conducted to
validate the effectiveness of our STORE framework across various recommendation
tasks and datasets. We will release the source code and configurations for
reproducible research.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-modal Speech Transformer Decoders: When Do Multiple Modalities
  Improve Accuracy? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Guan, Viet Anh Trinh, Vivek Voleti, Jacob Whitehill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoder-only discrete-token language models have recently achieved
significant success in automatic speech recognition. However, systematic
analyses of how different modalities impact performance in specific scenarios
remain limited. In this paper, we investigate the effects of multiple
modalities on recognition accuracy on both synthetic and real-world datasets.
Our experiments suggest that: (1) Integrating more modalities can increase
accuracy; in particular, our paper is, to our best knowledge, the first to show
the benefit of combining audio, image context, and lip information; (2) Images
as a supplementary modality for speech recognition provide the greatest benefit
at moderate noise levels, moreover, they exhibit a different trend compared to
inherently synchronized modalities like lip movements; (3) Performance improves
on both synthetic and real-world datasets when the most relevant visual
information is filtered as a preprocessing step.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Computation of BD-Rate over a Set of Videos for Fair Assessment
  of Performance of Learned Video Codecs <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Akin Yilmaz, Onur Keleş, A. Murat Tekalp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Bj{\o}ntegaard Delta (BD) measure is widely employed to evaluate and
quantify the variations in the rate-distortion(RD) performance across different
codecs. Many researchers report the average BD value over multiple videos
within a dataset for different codecs. We claim that the current practice in
the learned video compression community of computing the average BD value over
a dataset based on the average RD curve of multiple videos can lead to
misleading conclusions. We show both by analysis of a simplistic case of linear
RD curves and experimental results with two recent learned video codecs that
averaging RD curves can lead to a single video to disproportionately influence
the average BD value especially when the operating bitrate range of different
codecs do not exactly match. Instead, we advocate for calculating the BD
measure per-video basis, as commonly done by the traditional video compression
community, followed by averaging the individual BD values over videos, to
provide a fair comparison of learned video codecs. Our experimental results
demonstrate that the comparison of two recent learned video codecs is affected
by how we evaluate the average BD measure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rhythmic Foley: A Framework For Seamless Audio-Visual Alignment In
  Video-to-Audio Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqi Huang, Dan Luo, Jun Wang, Huan Liao, Zhiheng Li, Zhiyong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our research introduces an innovative framework for video-to-audio synthesis,
which solves the problems of audio-video desynchronization and semantic loss in
the audio. By incorporating a semantic alignment adapter and a temporal
synchronization adapter, our method significantly improves semantic integrity
and the precision of beat point synchronization, particularly in fast-paced
action sequences. Utilizing a contrastive audio-visual pre-trained encoder, our
model is trained with video and high-quality audio data, improving the quality
of the generated audio. This dual-adapter approach empowers users with enhanced
control over audio semantics and beat effects, allowing the adjustment of the
controller to achieve better results. Extensive experiments substantiate the
effectiveness of our framework in achieving seamless audio-visual alignment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STA-V2A: Video-to-Audio Generation with Semantic and Temporal Alignment <span class="chip">ICASSP2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Ren, Chenxing Li, Manjie Xu, Wei Liang, Yu Gu, Rilin Chen, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual and auditory perception are two crucial ways humans experience the
world. Text-to-video generation has made remarkable progress over the past
year, but the absence of harmonious audio in generated video limits its broader
applications. In this paper, we propose Semantic and Temporal Aligned
Video-to-Audio (STA-V2A), an approach that enhances audio generation from
videos by extracting both local temporal and global semantic video features and
combining these refined video features with text as cross-modal guidance. To
address the issue of information redundancy in videos, we propose an onset
prediction pretext task for local temporal feature extraction and an attentive
pooling module for global semantic feature extraction. To supplement the
insufficient semantic information in videos, we propose a Latent Diffusion
Model with Text-to-Audio priors initialization and cross-modal guidance. We
also introduce Audio-Audio Align, a new metric to assess audio-temporal
alignment. Subjective and objective metrics demonstrate that our method
surpasses existing Video-to-Audio models in generating audio with better
quality, semantic consistency, and temporal alignment. The ablation experiment
validated the effectiveness of each module. Audio samples are available at
https://y-ren16.github.io/STAV2A.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confidence Calibration for Audio Captioning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rehana Mahfuz, Yinyi Guo, Erik Visser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Systems that automatically generate text captions for audio, images and video
lack a confidence indicator of the relevance and correctness of the generated
sequences. To address this, we build on existing methods of confidence
measurement for text by introduce selective pooling of token probabilities,
which aligns better with traditional correctness measures than conventional
pooling does. Further, we propose directly measuring the similarity between
input audio and text in a shared embedding space. To measure self-consistency,
we adapt semantic entropy for audio captioning, and find that these two methods
align even better than pooling-based metrics with the correctness measure that
calculates acoustic similarity between captions. Finally, we explain why
temperature scaling of confidences improves calibration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diverse Neural Audio Embeddings -- Bringing Features back ! <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08751v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08751v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prateek Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advent of modern AI architectures, a shift has happened towards
end-to-end architectures. This pivot has led to neural architectures being
trained without domain-specific biases/knowledge, optimized according to the
task. We in this paper, learn audio embeddings via diverse feature
representations, in this case, domain-specific. For the case of audio
classification over hundreds of categories of sound, we learn robust separate
embeddings for diverse audio properties such as pitch, timbre, and neural
representation, along with also learning it via an end-to-end architecture. We
observe handcrafted embeddings, e.g., pitch and timbre-based, although on their
own, are not able to beat a fully end-to-end representation, yet adding these
together with end-to-end embedding helps us, significantly improve performance.
This work would pave the way to bring some domain expertise with end-to-end
models to learn robust, diverse representations, surpassing the performance of
just training end-to-end models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure, 2 table, Under Review for 50th IEEE ICASSP 2025,
  Hyderabad, India</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SaMoye: Zero-shot Singing Voice Conversion Model Based on Feature
  Disentanglement and Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07728v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07728v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Wang, Le Ma, Yongsheng Feng, Xin Pan, Yuhang Jin, Kejun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Singing voice conversion (SVC) aims to convert a singer's voice to another
singer's from a reference audio while keeping the original semantics. However,
existing SVC methods can hardly perform zero-shot due to incomplete feature
disentanglement or dependence on the speaker look-up table. We propose the
first open-source high-quality zero-shot SVC model SaMoye that can convert
singing to human and non-human timbre. SaMoye disentangles the singing voice's
features into content, timbre, and pitch features, where we combine multiple
ASR models and compress the content features to reduce timbre leaks. Besides,
we enhance the timbre features by unfreezing the speaker encoder and mixing the
speaker embedding with top-3 similar speakers. We also establish an
unparalleled large-scale dataset to guarantee zero-shot performance, which
comprises more than 1,815 hours of pure singing voice and 6,367 speakers. We
conduct objective and subjective experiments to find that SaMoye outperforms
other models in zero-shot SVC tasks even under extreme conditions like
converting singing to animals' timbre. The code and weight of SaMoye are
available on https://github.com/CarlWangChina/SaMoye-SVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deepfake Detection: A Comprehensive <span class="highlight-title">Survey</span> from the Reliability
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10881v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10881v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Wang, Xin Liao, Kam Pui Chow, Xiaodong Lin, Yinglong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The mushroomed Deepfake synthetic materials circulated on the internet have
raised a profound social impact on politicians, celebrities, and individuals
worldwide. In this survey, we provide a thorough review of the existing
Deepfake detection studies from the reliability perspective. We identify three
reliability-oriented research challenges in the current Deepfake detection
domain: transferability, interpretability, and robustness. Moreover, while
solutions have been frequently addressed regarding the three challenges, the
general reliability of a detection model has been barely considered, leading to
the lack of reliable evidence in real-life usages and even for prosecutions on
Deepfake-related cases in court. We, therefore, introduce a model reliability
study metric using statistical random sampling knowledge and the publicly
available benchmark datasets to review the reliability of the existing
detection models on arbitrary Deepfake candidate suspects. Case studies are
further executed to justify the real-life Deepfake cases including different
groups of victims with the help of the reliably qualified detection models as
reviewed in this survey. Reviews and experiments on the existing approaches
provide informative discussions and future research directions for Deepfake
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study
  on Word Error Rate and Fusion Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08353v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08353v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanchao Li, Peter Bell, Catherine Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text data is commonly utilized as a primary input to enhance Speech Emotion
Recognition (SER) performance and reliability. However, the reliance on
human-transcribed text in most studies impedes the development of practical SER
systems, creating a gap between in-lab research and real-world scenarios where
Automatic Speech Recognition (ASR) serves as the text source. Hence, this study
benchmarks SER performance using ASR transcripts with varying Word Error Rates
(WERs) from eleven models on three well-known corpora: IEMOCAP, CMU-MOSI, and
MSP-Podcast. Our evaluation includes both text-only and bimodal SER with six
fusion techniques, aiming for a comprehensive analysis that uncovers novel
findings and challenges faced by current SER research. Additionally, we propose
a unified ASR error-robust framework integrating ASR error correction and
modality-gated fusion, achieving lower WER and higher SER results compared to
the best-performing ASR transcript. These findings provide insights into SER
with ASR assistance, especially for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE SLT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dance-to-Music Generation with Encoder-based Textual Inversion <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17800v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17800v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sifei Li, Weiming Dong, Yuxin Zhang, Fan Tang, Chongyang Ma, Oliver Deussen, Tong-Yee Lee, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The seamless integration of music with dance movements is essential for
communicating the artistic intent of a dance piece. This alignment also
significantly improves the immersive quality of gaming experiences and
animation productions. Although there has been remarkable advancement in
creating high-fidelity music from textual descriptions, current methodologies
mainly focus on modulating overall characteristics such as genre and emotional
tone. They often overlook the nuanced management of temporal rhythm, which is
indispensable in crafting music for dance, since it intricately aligns the
musical beats with the dancers' movements. Recognizing this gap, we propose an
encoder-based textual inversion technique to augment text-to-music models with
visual control, facilitating personalized music generation. Specifically, we
develop dual-path rhythm-genre inversion to effectively integrate the rhythm
and genre of a dance motion sequence into the textual space of a text-to-music
model. Contrary to traditional textual inversion methods, which directly update
text embeddings to reconstruct a single target object, our approach utilizes
separate rhythm and genre encoders to obtain text embeddings for two
pseudo-words, adapting to the varying rhythms and genres. We collect a new
dataset called In-the-wild Dance Videos (InDV) and demonstrate that our
approach outperforms state-of-the-art methods across multiple evaluation
metrics. Furthermore, our method is able to adapt to changes in tempo and
effectively integrates with the inherent text-guided generation capability of
the pre-trained model. Our source code and demo videos are available at
\url{https://github.com/lsfhuihuiff/Dance-to-music_Siggraph_Asia_2024}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, SIGGRAPH ASIA 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-12T00:00:00Z">2024-09-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the challenges of studying bias in Recommender Systems: A UserKNN
  case study <span class="chip">RecSys 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Savvina Daniil, Manel Slokom, Mirjam Cuper, Cynthia C. S. Liem, Jacco van Ossenbruggen, Laura Hollink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Statements on the propagation of bias by recommender systems are often hard
to verify or falsify. Research on bias tends to draw from a small pool of
publicly available datasets and is therefore bound by their specific
properties. Additionally, implementation choices are often not explicitly
described or motivated in research, while they may have an effect on bias
propagation. In this paper, we explore the challenges of measuring and
reporting popularity bias. We showcase the impact of data properties and
algorithm configurations on popularity bias by combining synthetic data with
well known recommender systems frameworks that implement UserKNN. First, we
identify data characteristics that might impact popularity bias, based on the
functionality of UserKNN. Accordingly, we generate various datasets that
combine these characteristics. Second, we locate UserKNN configurations that
vary across implementations in literature. We evaluate popularity bias for five
synthetic datasets and five UserKNN configurations, and offer insights on their
joint effect. We find that, depending on the data characteristics, various
UserKNN configurations can lead to different conclusions regarding the
propagation of popularity bias. These results motivate the need for explicitly
addressing algorithmic configuration and data properties when reporting and
interpreting bias in recommender systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at FAccTRec@RecSys 2024, 11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Evaluation Framework for Attributed <span class="highlight-title">Information</span> Retrieval using Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanane Djeddal, Pierre Erbacher, Raouf Toukal, Laure Soulier, Karen Pinel-Sauvagnat, Sophia Katrenko, Lynda Tamine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing success of Large Language models (LLMs) in
information-seeking scenarios, search engines are now adopting generative
approaches to provide answers along with in-line citations as attribution.
While existing work focuses mainly on attributed question answering, in this
paper, we target information-seeking scenarios which are often more challenging
due to the open-ended nature of the queries and the size of the label space in
terms of the diversity of candidate-attributed answers per query. We propose a
reproducible framework to evaluate and benchmark attributed information
seeking, using any backbone LLM, and different architectural designs: (1)
Generate (2) Retrieve then Generate, and (3) Generate then Retrieve.
Experiments using HAGRID, an attributed information-seeking dataset, show the
impact of different scenarios on both the correctness and attributability of
answers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Cross-Market Recommendation System with Graph Isomorphism
  Networks: A Novel Approach to Personalized User Experience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sümeyye Öztürk, Ahmed Burak Ercan, Resul Tugay, Şule Gündüz Öğüdücü
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's world of globalized commerce, cross-market recommendation systems
(CMRs) are crucial for providing personalized user experiences across diverse
market segments. However, traditional recommendation algorithms have
difficulties dealing with market specificity and data sparsity, especially in
new or emerging markets. In this paper, we propose the CrossGR model, which
utilizes Graph Isomorphism Networks (GINs) to improve CMR systems. It
outperforms existing benchmarks in NDCG@10 and HR@10 metrics, demonstrating its
adaptability and accuracy in handling diverse market segments. The CrossGR
model is adaptable and accurate, making it well-suited for handling the
complexities of cross-market recommendation tasks. Its robustness is
demonstrated by consistent performance across different evaluation timeframes,
indicating its potential to cater to evolving market trends and user
preferences. Our findings suggest that GINs represent a promising direction for
CMRs, paving the way for more sophisticated, personalized, and context-aware
recommendation systems in the dynamic landscape of global e-commerce.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure, 3 tables, 5 equations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDC-FRS: Privacy-preserving Data Contribution for Federated Recommender
  System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoqun Yang, Wei Yuan, Liang Qu, Thanh Tam Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated recommender systems (FedRecs) have emerged as a popular research
direction for protecting users' privacy in on-device recommendations. In
FedRecs, users keep their data locally and only contribute their local
collaborative information by uploading model parameters to a central server.
While this rigid framework protects users' raw data during training, it
severely compromises the recommendation model's performance due to the
following reasons: (1) Due to the power law distribution nature of user
behavior data, individual users have few data points to train a recommendation
model, resulting in uploaded model updates that may be far from optimal; (2) As
each user's uploaded parameters are learned from local data, which lacks global
collaborative information, relying solely on parameter aggregation methods such
as FedAvg to fuse global collaborative information may be suboptimal. To bridge
this performance gap, we propose a novel federated recommendation framework,
PDC-FRS. Specifically, we design a privacy-preserving data contribution
mechanism that allows users to share their data with a differential privacy
guarantee. Based on the shared but perturbed data, an auxiliary model is
trained in parallel with the original federated recommendation process. This
auxiliary model enhances FedRec by augmenting each user's local dataset and
integrating global collaborative information. To demonstrate the effectiveness
of PDC-FRS, we conduct extensive experiments on two widely used recommendation
datasets. The empirical results showcase the superiority of PDC-FRS compared to
baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Harnessing TI Feeds for Exploitation Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kajal Patel, Zubair Shafiq, Mateus Nogueira, Daniel Sadoc Menasché, Enrico Lovat, Taimur Kashif, Ashton Woiwood, Matheus Martins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many organizations rely on Threat Intelligence (TI) feeds to assess the risk
associated with security threats. Due to the volume and heterogeneity of data,
it is prohibitive to manually analyze the threat information available in
different loosely structured TI feeds. Thus, there is a need to develop
automated methods to vet and extract actionable information from TI feeds. To
this end, we present a machine learning pipeline to automatically detect
vulnerability exploitation from TI feeds. We first model threat vocabulary in
loosely structured TI feeds using state-of-the-art embedding techniques
(Doc2Vec and BERT) and then use it to train a supervised machine learning
classifier to detect exploitation of security vulnerabilities. We use our
approach to identify exploitation events in 191 different TI feeds. Our
longitudinal evaluation shows that it is able to accurately identify
exploitation events from TI feeds only using past data for training and even on
TI feeds withheld from training. Our proposed approach is useful for a variety
of downstream tasks such as data-driven vulnerability risk assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper appears at IEEE International Conference on Cyber Security
  and Resilience (IEEE CSR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Q&A Text Retrieval with Ranking Models: Benchmarking,
  fine-tuning and deploying Rerankers for RAG <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel de Souza P. Moreira, Ronay Ak, Benedikt Schifferer, Mengyao Xu, Radek Osmulski, Even Oldridge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ranking models play a crucial role in enhancing overall accuracy of text
retrieval systems. These multi-stage systems typically utilize either dense
embedding models or sparse lexical indices to retrieve relevant passages based
on a given query, followed by ranking models that refine the ordering of the
candidate passages by its relevance to the query.
  This paper benchmarks various publicly available ranking models and examines
their impact on ranking accuracy. We focus on text retrieval for
question-answering tasks, a common use case for Retrieval-Augmented Generation
systems. Our evaluation benchmarks include models some of which are
commercially viable for industrial applications.
  We introduce a state-of-the-art ranking model, NV-RerankQA-Mistral-4B-v3,
which achieves a significant accuracy increase of ~14% compared to pipelines
with other rerankers. We also provide an ablation study comparing the
fine-tuning of ranking models with different sizes, losses and self-attention
mechanisms.
  Finally, we discuss challenges of text retrieval pipelines with ranking
models in real-world industry applications, in particular the trade-offs among
model size, ranking accuracy and system requirements like indexing and serving
latency / throughput.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the 1st Workshop on GenAI and RAG Systems for Enterprise
  @ CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Diffusion Bridges for Unsupervised Musical Audio Timbre Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Mancusi, Yurii Halychanskyi, Kin Wai Cheuk, Chieh-Hsin Lai, Stefan Uhlich, Junghyun Koo, Marco A. Martínez-Ramírez, Wei-Hsiang Liao, Giorgio Fabbro, Yuhki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music timbre transfer is a challenging task that involves modifying the
timbral characteristics of an audio signal while preserving its melodic
structure. In this paper, we propose a novel method based on dual diffusion
bridges, trained using the CocoChorales Dataset, which consists of unpaired
monophonic single-instrument audio data. Each diffusion model is trained on a
specific instrument with a Gaussian prior. During inference, a model is
designated as the source model to map the input audio to its corresponding
Gaussian prior, and another model is designated as the target model to
reconstruct the target audio from this Gaussian prior, thereby facilitating
timbre transfer. We compare our approach against existing unsupervised timbre
transfer models such as VAEGAN and Gaussian Flow Bridges (GFB). Experimental
results demonstrate that our method achieves both better Fr\'echet Audio
Distance (FAD) and melody preservation, as reflected by lower pitch distances
(DPD) compared to VAEGAN and GFB. Additionally, we discover that the noise
level from the Gaussian prior, $\sigma$, can be adjusted to control the degree
of melody preservation and amount of timbre transferred.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GACL: Graph Attention Collaborative Learning for Temporal QoS Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10555v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10555v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengxiang Hu, Guobing Zou, Bofeng Zhang, Shaogang Wu, Shiyi Lin, Yanglan Gan, Yixin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate prediction of temporal QoS is crucial for maintaining service
reliability and enhancing user satisfaction in dynamic service-oriented
environments. However, current methods often neglect high-order latent
collaborative relationships and fail to dynamically adjust feature learning for
specific user-service invocations, which are critical for precise feature
extraction within each time slice. Moreover, the prevalent use of RNNs for
modeling temporal feature evolution patterns is constrained by their inherent
difficulty in managing long-range dependencies, thereby limiting the detection
of long-term QoS trends across multiple time slices. These shortcomings
dramatically degrade the performance of temporal QoS prediction. To address the
two issues, we propose a novel Graph Attention Collaborative Learning (GACL)
framework for temporal QoS prediction. Building on a dynamic user-service
invocation graph to comprehensively model historical interactions, it designs a
target-prompt graph attention network to extract deep latent features of users
and services at each time slice, considering implicit target-neighboring
collaborative relationships and historical QoS values. Additionally, a
multi-layer Transformer encoder is introduced to uncover temporal feature
evolution patterns, enhancing temporal QoS prediction. Extensive experiments on
the WS-DREAM dataset demonstrate that GACL significantly outperforms
state-of-the-art methods for temporal QoS prediction across multiple evaluation
metrics, achieving the improvements of up to 38.80%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking <span class="highlight-title">Prompt</span>ing Strategies for Multi-Label Recognition with Partial
  Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samyak Rawlekar, Shubhang Bhatnagar, Narendra Ahuja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) like CLIP have been adapted for Multi-Label
Recognition (MLR) with partial annotations by leveraging prompt-learning, where
positive and negative prompts are learned for each class to associate their
embeddings with class presence or absence in the shared vision-text feature
space. While this approach improves MLR performance by relying on VLM priors,
we hypothesize that learning negative prompts may be suboptimal, as the
datasets used to train VLMs lack image-caption pairs explicitly focusing on
class absence. To analyze the impact of positive and negative prompt learning
on MLR, we introduce PositiveCoOp and NegativeCoOp, where only one prompt is
learned with VLM guidance while the other is replaced by an embedding vector
learned directly in the shared feature space without relying on the text
encoder. Through empirical analysis, we observe that negative prompts degrade
MLR performance, and learning only positive prompts, combined with learned
negative embeddings (PositiveCoOp), outperforms dual prompt learning
approaches. Moreover, we quantify the performance benefits that prompt-learning
offers over a simple vision-features-only baseline, observing that the baseline
displays strong performance comparable to dual prompt learning approach
(DualCoOp), when the proportion of missing labels is low, while requiring half
the training compute and 16 times fewer parameters
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally <span class="chip">ECCV'2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuhong Shen, Xingyi Yang, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the challenge of accurately segmenting 3D Gaussian
Splatting from 2D masks. Conventional methods often rely on iterative gradient
descent to assign each Gaussian a unique label, leading to lengthy optimization
and sub-optimal solutions. Instead, we propose a straightforward yet globally
optimal solver for 3D-GS segmentation. The core insight of our method is that,
with a reconstructed 3D-GS scene, the rendering of the 2D masks is essentially
a linear function with respect to the labels of each Gaussian. As such, the
optimal label assignment can be solved via linear programming in closed form.
This solution capitalizes on the alpha blending characteristic of the splatting
process for single step optimization. By incorporating the background bias in
our objective function, our method shows superior robustness in 3D segmentation
against noises. Remarkably, our optimization completes within 30 seconds, about
50$\times$ faster than the best existing methods. Extensive experiments
demonstrate the efficiency and robustness of our method in segmenting various
scenes, and its superior performance in downstream tasks such as object removal
and inpainting. Demos and code will be available at
https://github.com/florinshen/FlashSplat.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV'2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Text-guided Object Inpainting with Semantic Pre-inpainting <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifu Chen, Jingwen Chen, Yingwei Pan, Yehao Li, Ting Yao, Zhineng Chen, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed the success of large text-to-image diffusion
models and their remarkable potential to generate high-quality images. The
further pursuit of enhancing the editability of images has sparked significant
interest in the downstream task of inpainting a novel object described by a
text prompt within a designated region in the image. Nevertheless, the problem
is not trivial from two aspects: 1) Solely relying on one single U-Net to align
text prompt and visual object across all the denoising timesteps is
insufficient to generate desired objects; 2) The controllability of object
generation is not guaranteed in the intricate sampling space of diffusion
model. In this paper, we propose to decompose the typical single-stage object
inpainting into two cascaded processes: 1) semantic pre-inpainting that infers
the semantic features of desired objects in a multi-modal feature space; 2)
high-fieldity object generation in diffusion latent space that pivots on such
inpainted semantic features. To achieve this, we cascade a Transformer-based
semantic inpainter and an object inpainting diffusion model, leading to a novel
CAscaded Transformer-Diffusion (CAT-Diffusion) framework for text-guided object
inpainting. Technically, the semantic inpainter is trained to predict the
semantic features of the target object conditioning on unmasked context and
text prompt. The outputs of the semantic inpainter then act as the informative
visual prompts to guide high-fieldity object generation through a reference
adapter layer, leading to controllable object inpainting. Extensive evaluations
on OpenImages-V6 and MSCOCO validate the superiority of CAT-Diffusion against
the state-of-the-art methods. Code is available at
\url{https://github.com/Nnn-s/CATdiffusion}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024. Source code is available at
  https://github.com/Nnn-s/CATdiffusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Virtual Try-On with Garment-focused Diffusion Models <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Wan, Yehao Li, Jingwen Chen, Yingwei Pan, Ting Yao, Yang Cao, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have led to the revolutionizing of generative modeling in
numerous image synthesis tasks. Nevertheless, it is not trivial to directly
apply diffusion models for synthesizing an image of a target person wearing a
given in-shop garment, i.e., image-based virtual try-on (VTON) task. The
difficulty originates from the aspect that the diffusion process should not
only produce holistically high-fidelity photorealistic image of the target
person, but also locally preserve every appearance and texture detail of the
given garment. To address this, we shape a new Diffusion model, namely GarDiff,
which triggers the garment-focused diffusion process with amplified guidance of
both basic visual appearance and detailed textures (i.e., high-frequency
details) derived from the given garment. GarDiff first remoulds a pre-trained
latent diffusion model with additional appearance priors derived from the CLIP
and VAE encodings of the reference garment. Meanwhile, a novel garment-focused
adapter is integrated into the UNet of diffusion model, pursuing local
fine-grained alignment with the visual appearance of reference garment and
human pose. We specifically design an appearance loss over the synthesized
garment to enhance the crucial, high-frequency details. Extensive experiments
on VITON-HD and DressCode datasets demonstrate the superiority of our GarDiff
when compared to state-of-the-art VTON approaches. Code is publicly available
at:
\href{https://github.com/siqi0905/GarDiff/tree/master}{https://github.com/siqi0905/GarDiff/tree/master}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024. Source code is available at
  https://github.com/siqi0905/GarDiff/tree/master</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ComAlign: Compositional Alignment in Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Abdollah, Amirmohammad Izadi, Armin Saghafian, Reza Vahidimajd, Mohammad Mozafari, Amirreza Mirzaei, Mohammadmahdi Samiei, Mahdieh Soleymani Baghshah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) like CLIP have showcased a remarkable ability
to extract transferable features for downstream tasks. Nonetheless, the
training process of these models is usually based on a coarse-grained
contrastive loss between the global embedding of images and texts which may
lose the compositional structure of these modalities. Many recent studies have
shown VLMs lack compositional understandings like attribute binding and
identifying object relationships. Although some recent methods have tried to
achieve finer-level alignments, they either are not based on extracting
meaningful components of proper granularity or don't properly utilize the
modalities' correspondence (especially in image-text pairs with more
ingredients). Addressing these limitations, we introduce Compositional
Alignment (ComAlign), a fine-grained approach to discover more exact
correspondence of text and image components using only the weak supervision in
the form of image-text pairs. Our methodology emphasizes that the compositional
structure (including entities and relations) extracted from the text modality
must also be retained in the image modality. To enforce correspondence of
fine-grained concepts in image and text modalities, we train a lightweight
network lying on top of existing visual and language encoders using a small
dataset. The network is trained to align nodes and edges of the structure
across the modalities. Experimental results on various VLMs and datasets
demonstrate significant improvements in retrieval and compositional benchmarks,
affirming the effectiveness of our plugin model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Discrete and Continuous: A Multimodal Strategy for Complex
  Emotion Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiehui Jia, Huan Zhang, Jinhua Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of human-computer interaction, accurately recognizing and
interpreting human emotions is crucial yet challenging due to the complexity
and subtlety of emotional expressions. This study explores the potential for
detecting a rich and flexible range of emotions through a multimodal approach
which integrates facial expressions, voice tones, and transcript from video
clips. We propose a novel framework that maps variety of emotions in a
three-dimensional Valence-Arousal-Dominance (VAD) space, which could reflect
the fluctuations and positivity/negativity of emotions to enable a more variety
and comprehensive representation of emotional states. We employed K-means
clustering to transit emotions from traditional discrete categorization to a
continuous labeling system and built a classifier for emotion recognition upon
this system. The effectiveness of the proposed model is evaluated using the
MER2024 dataset, which contains culturally consistent video clips from Chinese
movies and TV series, annotated with both discrete and open-vocabulary emotion
labels. Our experiment successfully achieved the transformation between
discrete and continuous models, and the proposed model generated a more diverse
and comprehensive set of emotion vocabulary while maintaining strong accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MSMF: Multi-Scale Multi-Modal Fusion for Enhanced Stock Market
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents MSMF (Multi-Scale Multi-Modal Fusion), a novel approach
for enhanced stock market prediction. MSMF addresses key challenges in
multi-modal stock analysis by integrating a modality completion encoder,
multi-scale feature extraction, and an innovative fusion mechanism. Our model
leverages blank learning and progressive fusion to balance complementarity and
redundancy across modalities, while multi-scale alignment facilitates direct
correlations between heterogeneous data types. We introduce Multi-Granularity
Gates and a specialized architecture to optimize the integration of local and
global information for different tasks. Additionally, a Task-targeted
Prediction layer is employed to preserve both coarse and fine-grained features
during fusion. Experimental results demonstrate that MSMF outperforms existing
methods, achieving significant improvements in accuracy and reducing prediction
errors across various stock market forecasting tasks. This research contributes
valuable insights to the field of multi-modal financial analysis and offers a
robust framework for enhanced market prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 1 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Paintings and Music -- Exploring Emotion based Music Generation
  through Paintings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanisha Hisariya, Huan Zhang, Jinhua Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid advancements in artificial intelligence have significantly enhanced
generative tasks involving music and images, employing both unimodal and
multimodal approaches. This research develops a model capable of generating
music that resonates with the emotions depicted in visual arts, integrating
emotion labeling, image captioning, and language models to transform visual
inputs into musical compositions. Addressing the scarcity of aligned art and
music data, we curated the Emotion Painting Music Dataset, pairing paintings
with corresponding music for effective training and evaluation. Our dual-stage
framework converts images to text descriptions of emotional content and then
transforms these descriptions into music, facilitating efficient learning with
minimal data. Performance is evaluated using metrics such as Fr\'echet Audio
Distance (FAD), Total Harmonic Distortion (THD), Inception Score (IS), and KL
divergence, with audio-emotion text similarity confirmed by the pre-trained
CLAP model to demonstrate high alignment between generated music and text. This
synthesis tool bridges visual art and music, enhancing accessibility for the
visually impaired and opening avenues in educational and therapeutic
applications by providing enriched multi-sensory experiences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SwinGS: Sliding Window Gaussian Splatting for Volumetric Video Streaming
  with Arbitrary Length 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bangya Liu, Suman Banerjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in 3D Gaussian Splatting (3DGS) have garnered significant
attention in computer vision and computer graphics due to its high rendering
speed and remarkable quality. While extant research has endeavored to extend
the application of 3DGS from static to dynamic scenes, such efforts have been
consistently impeded by excessive model sizes, constraints on video duration,
and content deviation. These limitations significantly compromise the
streamability of dynamic 3D Gaussian models, thereby restricting their utility
in downstream applications, including volumetric video, autonomous vehicle, and
immersive technologies such as virtual, augmented, and mixed reality.
  This paper introduces SwinGS, a novel framework for training, delivering, and
rendering volumetric video in a real-time streaming fashion. To address the
aforementioned challenges and enhance streamability, SwinGS integrates
spacetime Gaussian with Markov Chain Monte Carlo (MCMC) to adapt the model to
fit various 3D scenes across frames, in the meantime employing a sliding window
captures Gaussian snapshots for each frame in an accumulative way. We implement
a prototype of SwinGS and demonstrate its streamability across various datasets
and scenes. Additionally, we develop an interactive WebGL viewer enabling
real-time volumetric video playback on most devices with modern browsers,
including smartphones and tablets. Experimental results show that SwinGS
reduces transmission costs by 83.6% compared to previous work with ignorable
compromise in PSNR. Moreover, SwinGS easily scales to long video sequences
without compromising quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TMFNet: Two-Stream Multi-Channels Fusion Networks for Color Image
  Operation Chain Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yakun Niu, Lei Tan, Lei Zhang, Xianyu Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image operation chain detection techniques have gained increasing attention
recently in the field of multimedia forensics. However, existing detection
methods suffer from the generalization problem. Moreover, the channel
correlation of color images that provides additional forensic evidence is often
ignored. To solve these issues, in this article, we propose a novel two-stream
multi-channels fusion networks for color image operation chain detection in
which the spatial artifact stream and the noise residual stream are explored in
a complementary manner. Specifically, we first propose a novel deep residual
architecture without pooling in the spatial artifact stream for learning the
global features representation of multi-channel correlation. Then, a set of
filters is designed to aggregate the correlation information of multi-channels
while capturing the low-level features in the noise residual stream.
Subsequently, the high-level features are extracted by the deep residual model.
Finally, features from the two streams are fed into a fusion module, to
effectively learn richer discriminative representations of the operation chain.
Extensive experiments show that the proposed method achieves state-of-the-art
generalization ability while maintaining robustness to JPEG compression. The
source code used in these experiments will be released at
https://github.com/LeiTan-98/TMFNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Video Context as Interleaved Multimodal Sequences <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Qinghong Lin, Pengchuan Zhang, Difei Gao, Xide Xia, Joya Chen, Ziteng Gao, Jinheng Xie, Xuhong Xiao, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Narrative videos, such as movies, pose significant challenges in video
understanding due to their rich contexts (characters, dialogues, storylines)
and diverse demands (identify who, relationship, and reason). In this paper, we
introduce MovieSeq, a multimodal language model developed to address the wide
range of challenges in understanding video contexts. Our core idea is to
represent videos as interleaved multimodal sequences (including images, plots,
videos, and subtitles), either by linking external knowledge databases or using
offline models (such as whisper for subtitles). Through instruction-tuning,
this approach empowers the language model to interact with videos using
interleaved multimodal instructions. For example, instead of solely relying on
video as input, we jointly provide character photos alongside their names and
dialogues, allowing the model to associate these elements and generate more
comprehensive responses. To demonstrate its effectiveness, we validate
MovieSeq's performance on six datasets (LVU, MAD, Movienet, CMD, TVC, MovieQA)
across five settings (video classification, audio description, video-text
retrieval, video captioning, and video question-answering). The code will be
public at https://github.com/showlab/MovieSeq.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An End-to-End Pipeline Perspective on Video Streaming in Best-Effort
  Networks: A <span class="highlight-title">Survey</span> and Tutorial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05192v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05192v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Peroni, Sergey Gorinsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remaining a dominant force in Internet traffic, video streaming captivates
end users, service providers, and researchers. This paper takes a pragmatic
approach to reviewing recent advances in the field by focusing on the prevalent
streaming paradigm that involves delivering long-form two-dimensional videos
over the best-effort Internet with client-side adaptive bitrate (ABR)
algorithms and assistance from content delivery networks (CDNs). To enhance
accessibility, we supplement the survey with tutorial material. Unlike existing
surveys that offer fragmented views, our work provides a holistic perspective
on the entire end-to-end streaming pipeline, from video capture by a
camera-equipped device to playback by the end user. Our novel perspective
covers the ingestion, processing, and distribution stages of the pipeline and
addresses key challenges such as video compression, upload, transcoding, ABR
algorithms, CDN support, and quality of experience. We review over 200 papers
and classify streaming designs by their problem-solving methodology, whether
based on intuition (simple heuristics), theory (formal optimization), or
machine learning (generalizable data patterns). The survey further refines
these methodology-based categories and characterizes each design by additional
traits such as compatible codecs and use of super resolution. We connect the
reviewed research to real-world applications by discussing the practices of
commercial streaming platforms. Finally, the survey highlights prominent
current trends and outlines future directions in video streaming.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-11T00:00:00Z">2024-09-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging User-Generated <span class="highlight-title">Review</span>s for Recommender Systems with Dynamic
  Headers <span class="chip">ECAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanu Vashishtha, Abhay Kumar, Lalitesh Morishetti, Kaushiki Nag, Kannan Achan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  E-commerce platforms have a vast catalog of items to cater to their
customers' shopping interests. Most of these platforms assist their customers
in the shopping process by offering optimized recommendation carousels,
designed to help customers quickly locate their desired items. Many models have
been proposed in academic literature to generate and enhance the ranking and
recall set of items in these carousels. Conventionally, the accompanying
carousel title text (header) of these carousels remains static. In most
instances, a generic text such as "Items similar to your current viewing" is
utilized. Fixed variations such as the inclusion of specific attributes "Other
items from a similar seller" or "Items from a similar brand" in addition to
"frequently bought together" or "considered together" are observed as well.
This work proposes a novel approach to customize the header generation process
of these carousels. Our work leverages user-generated reviews that lay focus on
specific attributes (aspects) of an item that were favorably perceived by users
during their interaction with the given item. We extract these aspects from
reviews and train a graph neural network-based model under the framework of a
conditional ranking task. We refer to our innovative methodology as Dynamic
Text Snippets (DTS) which generates multiple header texts for an anchor item
and its recall set. Our approach demonstrates the potential of utilizing
user-generated reviews and presents a unique paradigm for exploring
increasingly context-aware recommendation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, PAIS 2024 (ECAI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual <span class="highlight-title">Prompt</span>s in <span class="highlight-title">LLM</span>-Based Recommenders: Performance Across
  Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makbule Gulcin Ozsoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly used in natural language
processing tasks. Recommender systems traditionally use methods such as
collaborative filtering and matrix factorization, as well as advanced
techniques like deep learning and reinforcement learning. Although language
models have been applied in recommendation, the recent trend have focused on
leveraging the generative capabilities of LLMs for more personalized
suggestions. While current research focuses on English due to its resource
richness, this work explores the impact of non-English prompts on
recommendation performance. Using OpenP5, a platform for developing and
evaluating LLM-based recommendations, we expanded its English prompt templates
to include Spanish and Turkish. Evaluation on three real-world datasets, namely
ML1M, LastFM, and Amazon-Beauty, showed that usage of non-English prompts
generally reduce performance, especially in less-resourced languages like
Turkish. We also retrained an LLM-based recommender model with multilingual
prompts to analyze performance variations. Retraining with multilingual prompts
resulted in more balanced performance across languages, but slightly reduced
English performance. This work highlights the need for diverse language support
in LLM-based recommenders and suggests future research on creating evaluation
datasets, using newer models and additional languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dot Product is All You Need: Bridging the Gap Between Item
  Recommendation and Link Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Malitesta, Alberto Carlo Maria Mancino, Pasquale Minervini, Tommaso Di Noia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Item recommendation (the task of predicting if a user may interact with new
items from the catalogue in a recommendation system) and link prediction (the
task of identifying missing links in a knowledge graph) have long been regarded
as distinct problems. In this work, we show that the item recommendation
problem can be seen as an instance of the link prediction problem, where
entities in the graph represent users and items, and the task consists of
predicting missing instances of the relation type <<interactsWith>>. In a
preliminary attempt to demonstrate the assumption, we decide to test three
popular factorisation-based link prediction models on the item recommendation
task, showing that their predictive accuracy is competitive with ten
state-of-the-art recommendation models. The purpose is to show how the former
may be seamlessly and effectively applied to the recommendation task without
any specific modification to their architectures. Finally, while beginning to
unveil the key reasons behind the recommendation performance of the selected
link prediction models, we explore different settings for their hyper-parameter
values, paving the way for future directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Reinforcement Learning for Temporal Abstraction of Listwise
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luo Ji, Gao Liu, Mingyang Yin, Hongxia Yang, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern listwise recommendation systems need to consider both long-term user
perceptions and short-term interest shifts. Reinforcement learning can be
applied on recommendation to study such a problem but is also subject to large
search space, sparse user feedback and long interactive latency. Motivated by
recent progress in hierarchical reinforcement learning, we propose a novel
framework called mccHRL to provide different levels of temporal abstraction on
listwise recommendation. Within the hierarchical framework, the high-level
agent studies the evolution of user perception, while the low-level agent
produces the item selection policy by modeling the process as a sequential
decision-making problem. We argue that such framework has a well-defined
decomposition of the outra-session context and the intra-session context, which
are encoded by the high-level and low-level agents, respectively. To verify
this argument, we implement both a simulator-based environment and an
industrial dataset-based experiment. Results observe significant performance
improvement by our method, compared with several well-known baselines. Data and
codes have been made public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Sequential Music Recommendation with Negative
  Feedback-informed Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavan Seshadri, Shahrzad Shashaani, Peter Knees
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern music streaming services are heavily based on recommendation engines
to serve content to users. Sequential recommendation -- continuously providing
new items within a single session in a contextually coherent manner -- has been
an emerging topic in current literature. User feedback -- a positive or
negative response to the item presented -- is used to drive content
recommendations by learning user preferences. We extend this idea to
session-based recommendation to provide context-coherent music recommendations
by modelling negative user feedback, i.e., skips, in the loss function. We
propose a sequence-aware contrastive sub-task to structure item embeddings in
session-based music recommendation, such that true next-positive items
(ignoring skipped items) are structured closer in the session embedding space,
while skipped tracks are structured farther away from all items in the session.
This directly affects item rankings using a K-nearest-neighbors search for
next-item recommendations, while also promoting the rank of the true next item.
Experiments incorporating this task into SoTA methods for sequential item
recommendation show consistent performance gains in terms of next-item hit
rate, item ranking, and skip down-ranking on three music recommendation
datasets, strongly benefiting from the increasing presence of user feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To-appear at 18th ACM Conference on Recommendation Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diff-VPS: Video Polyp Segmentation via a Multi-task Diffusion Network
  with Adversarial Temporal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingling Lu, Yijun Yang, Zhaohu Xing, Qiong Wang, Lei Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Probabilistic Models have recently attracted significant attention
in the community of computer vision due to their outstanding performance.
However, while a substantial amount of diffusion-based research has focused on
generative tasks, no work introduces diffusion models to advance the results of
polyp segmentation in videos, which is frequently challenged by polyps' high
camouflage and redundant temporal cues.In this paper, we present a novel
diffusion-based network for video polyp segmentation task, dubbed as Diff-VPS.
We incorporate multi-task supervision into diffusion models to promote the
discrimination of diffusion models on pixel-by-pixel segmentation. This
integrates the contextual high-level information achieved by the joint
classification and detection tasks. To explore the temporal dependency,
Temporal Reasoning Module (TRM) is devised via reasoning and reconstructing the
target frame from the previous frames. We further equip TRM with a generative
adversarial self-supervised strategy to produce more realistic frames and thus
capture better dynamic cues. Extensive experiments are conducted on SUN-SEG,
and the results indicate that our proposed Diff-VPS significantly achieves
state-of-the-art performance. Code is available at
https://github.com/lydia-yllu/Diff-VPS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Negative Sampling in Recommendation: A <span class="highlight-title">Survey</span> and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07237v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07237v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haokai Ma, Ruobing Xie, Lei Meng, Fuli Feng, Xiaoyu Du, Xingwu Sun, Zhanhui Kang, Xiangxu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems aim to capture users' personalized preferences from the
cast amount of user behaviors, making them pivotal in the era of information
explosion. However, the presence of the dynamic preference, the "information
cocoons", and the inherent feedback loops in recommendation make users interact
with a limited number of items. Conventional recommendation algorithms
typically focus on the positive historical behaviors, while neglecting the
essential role of negative feedback in user interest understanding. As a
promising but easy-to-ignored area, negative sampling is proficients in
revealing the genuine negative aspect inherent in user behaviors, emerging as
an inescapable procedure in recommendation. In this survey, we first discuss
the role of negative sampling in recommendation and thoroughly analyze
challenges that consistently impede its progress. Then, we conduct an extensive
literature review on the existing negative sampling strategies in
recommendation and classify them into five categories with their discrepant
techniques. Finally, we detail the insights of the tailored negative sampling
strategies in diverse recommendation scenarios and outline an overview of the
prospective research directions toward which the community may engage and
benefit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 9 figures; Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ E-commerce Webpage Recommendation Scheme Base on Semantic Mining and
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenchao Zhao, Xiaoyi Liu, Ruilin Xu, Lingxi Xiao, Muqing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In e-commerce websites, web mining web page recommendation technology has
been widely used. However, recommendation solutions often cannot meet the
actual application needs of online shopping users. To address this problem,
this paper proposes an e-commerce web page recommendation solution that
combines semantic web mining and BP neural networks. First, the web logs of
user searches are processed, and 5 features are extracted: content priority,
time consumption priority, online shopping users' explicit/implicit feedback on
the website, recommendation semantics and input deviation amount. Then, these
features are used as input features of the BP neural network to classify and
identify the priority of the final output web page. Finally, the web pages are
sorted according to priority and recommended to users. This project uses book
sales webpages as samples for experiments. The results show that this solution
can quickly and accurately identify the webpages required by users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2409.01137</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smart E-commerce Recommendations with Semantic AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01137v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01137v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Badouch, M. Boutaounte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In e-commerce, web mining for page recommendations is widely used but often
fails to meet user needs. To address this, we propose a novel solution
combining semantic web mining with BP neural networks. We process user search
logs to extract five key features: content priority, time spent, user feedback,
recommendation semantics, and input deviation. These features are then fed into
a BP neural network to classify and prioritize web pages. The prioritized pages
are recommended to users. Using book sales pages for testing, our results
demonstrate that this solution can quickly and accurately identify the pages
users need. Our approach ensures that recommendations are more relevant and
tailored to individual preferences, enhancing the online shopping experience.
By leveraging advanced semantic analysis and neural network techniques, we
bridge the gap between user expectations and actual recommendations. This
innovative method not only improves accuracy but also speeds up the
recommendation process, making it a valuable tool for e-commerce platforms
aiming to boost user satisfaction and engagement. Additionally, our system
ability to handle large datasets and provide real-time recommendations makes it
a scalable and efficient solution for modern e-commerce challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>My paper contain some errors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explainable Identification of Hate Speech towards Islam using Graph
  Neural Networks <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04916v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04916v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azmine Toushik Wasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Islamophobic language on online platforms fosters intolerance, making
detection and elimination crucial for promoting harmony. Traditional hate
speech detection models rely on NLP techniques like tokenization,
part-of-speech tagging, and encoder-decoder models. However, Graph Neural
Networks (GNNs), with their ability to utilize relationships between data
points, offer more effective detection and greater explainability. In this
work, we represent speeches as nodes and connect them with edges based on their
context and similarity to develop the graph. This study introduces a novel
paradigm using GNNs to identify and explain hate speech towards Islam. Our
model leverages GNNs to understand the context and patterns of hate speech by
connecting texts via pretrained NLP-generated word embeddings, achieving
state-of-the-art performance and enhancing detection accuracy while providing
valuable explanations. This highlights the potential of GNNs in combating
online hate speech and fostering a safer, more inclusive online environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in: (i) NeurIPS 2023 : Muslims in ML Workshop (non-archival)
  (https://www.musiml.org/schedule/#:~:text=Azmine%20Toushik%20Wasi) (ii) EMNLP
  2024 : NLP for Positive Impact Workshop (archival)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Empirical Study of Training ID-Agnostic Multi-modal Sequential
  Recommenders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17372v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17372v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youhua Li, Hanwen Du, Yongxin Ni, Yuanqi He, Junchen Fu, Xiangyan Liu, Qi Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommendation (SR) aims to predict future user-item interactions
based on historical interactions. While many SR approaches concentrate on user
IDs and item IDs, the human perception of the world through multi-modal
signals, like text and images, has inspired researchers to delve into
constructing SR from multi-modal information without using IDs. However, the
complexity of multi-modal learning manifests in diverse feature extractors,
fusion methods, and pre-trained models. Consequently, designing a simple and
universal \textbf{M}ulti-\textbf{M}odal \textbf{S}equential
\textbf{R}ecommendation (\textbf{MMSR}) framework remains a formidable
challenge. We systematically summarize the existing multi-modal related SR
methods and distill the essence into four core components: visual encoder, text
encoder, multimodal fusion module, and sequential architecture. Along these
dimensions, we dissect the model designs, and answer the following
sub-questions: First, we explore how to construct MMSR from scratch, ensuring
its performance either on par with or exceeds existing SR methods without
complex techniques. Second, we examine if MMSR can benefit from existing
multi-modal pre-training paradigms. Third, we assess MMSR's capability in
tackling common challenges like cold start and domain transferring. Our
experiment results across four real-world recommendation scenarios demonstrate
the great potential ID-agnostic multi-modal sequential recommendation. Our
framework can be found at: https://github.com/MMSR23/MMSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We are requesting to withdraw the paper due to a significant
  methodological error discovered in the experimental setup, specifically in
  Section 4.3. This error affects the validity of the results and conclusions
  drawn from the study. We intend to address these issues and submit a
  corrected version in the future</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rs4rs: Semantically Find Recent Publications from Top Recommendation
  System-Related Venues <span class="chip">RecSys 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05570v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05570v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tri Kurniawan Wijaya, Edoardo D'Amico, Gabor Fodor, Manuel V. Loureiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rs4rs is a web application designed to perform semantic search on recent
papers from top conferences and journals related to Recommender Systems.
Current scholarly search engine tools like Google Scholar, Semantic Scholar,
and ResearchGate often yield broad results that fail to target the most
relevant high-quality publications. Moreover, manually visiting individual
conference and journal websites is a time-consuming process that primarily
supports only syntactic searches. Rs4rs addresses these issues by providing a
user-friendly platform where researchers can input their topic of interest and
receive a list of recent, relevant papers from top Recommender Systems venues.
Utilizing semantic search techniques, Rs4rs ensures that the search results are
not only precise and relevant but also comprehensive, capturing papers
regardless of variations in wording. This tool significantly enhances research
efficiency and accuracy, thereby benefitting the research community and public
by facilitating access to high-quality, pertinent academic resources in the
field of Recommender Systems. Rs4rs is available at https://rs4rs.com.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACM RecSys 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Normative Framework for Benchmarking Consumer Fairness in Large
  Language Model Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yashar Deldjoo, Fatemeh Nazary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid adoption of large language models (LLMs) in recommender systems
(RS) presents new challenges in understanding and evaluating their biases,
which can result in unfairness or the amplification of stereotypes. Traditional
fairness evaluations in RS primarily focus on collaborative filtering (CF)
settings, which may not fully capture the complexities of LLMs, as these models
often inherit biases from large, unregulated data. This paper proposes a
normative framework to benchmark consumer fairness in LLM-powered recommender
systems (RecLLMs).
  We critically examine how fairness norms in classical RS fall short in
addressing the challenges posed by LLMs. We argue that this gap can lead to
arbitrary conclusions about fairness, and we propose a more structured, formal
approach to evaluate fairness in such systems. Our experiments on the MovieLens
dataset on consumer fairness, using in-context learning (zero-shot vs.
few-shot) reveal fairness deviations in age-based recommendations, particularly
when additional contextual examples are introduced (ICL-2). Statistical
significance tests confirm that these deviations are not random, highlighting
the need for robust evaluation methods. While this work offers a preliminary
discussion on a proposed normative framework, our hope is that it could provide
a formal, principled approach for auditing and mitigating bias in RecLLMs. The
code and dataset used for this work will be shared at "gihub-anonymized".
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CF-KAN: Kolmogorov-Arnold Network-based Collaborative Filtering to
  Mitigate Catastrophic Forgetting in Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin-Duk Park, Kyung-Min Kim, Won-Yong Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering (CF) remains essential in recommender systems,
leveraging user--item interactions to provide personalized recommendations.
Meanwhile, a number of CF techniques have evolved into sophisticated model
architectures based on multi-layer perceptrons (MLPs). However, MLPs often
suffer from catastrophic forgetting, and thus lose previously acquired
knowledge when new information is learned, particularly in dynamic environments
requiring continual learning. To tackle this problem, we propose CF-KAN, a new
CF method utilizing Kolmogorov-Arnold networks (KANs). By learning nonlinear
functions on the edge level, KANs are more robust to the catastrophic
forgetting problem than MLPs. Built upon a KAN-based autoencoder, CF-KAN is
designed in the sense of effectively capturing the intricacies of sparse
user--item interactions and retaining information from previous data instances.
Despite its simplicity, our extensive experiments demonstrate 1) CF-KAN's
superiority over state-of-the-art methods in recommendation accuracy, 2)
CF-KAN's resilience to catastrophic forgetting, underscoring its effectiveness
in both static and dynamic recommendation scenarios, and 3) CF-KAN's edge-level
interpretation facilitating the explainability of recommendations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Multimodal Composite Editing and Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05405v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05405v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suyan Li, Fuxiang Huang, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the real world, where information is abundant and diverse across different
modalities, understanding and utilizing various data types to improve retrieval
systems is a key focus of research. Multimodal composite retrieval integrates
diverse modalities such as text, image and audio, etc. to provide more
accurate, personalized, and contextually relevant results. To facilitate a
deeper understanding of this promising direction, this survey explores
multimodal composite editing and retrieval in depth, covering image-text
composite editing, image-text composite retrieval, and other multimodal
composite retrieval. In this survey, we systematically organize the application
scenarios, methods, benchmarks, experiments, and future directions. Multimodal
learning is a hot topic in large model era, and have also witnessed some
surveys in multimodal learning and vision-language models with transformers
published in the PAMI journal. To the best of our knowledge, this survey is the
first comprehensive review of the literature on multimodal composite retrieval,
which is a timely complement of multimodal fusion to existing reviews. To help
readers' quickly track this field, we build the project page for this survey,
which can be found at
https://github.com/fuxianghuang1/Multimodal-Composite-Editing-and-Retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 3 figures, and 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Dataset</span> Regeneration for Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17795v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17795v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Suojuan Zhang, Sirui Zhao, Defu Lian, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sequential recommender (SR) system is a crucial component of modern
recommender systems, as it aims to capture the evolving preferences of users.
Significant efforts have been made to enhance the capabilities of SR systems.
These methods typically follow the model-centric paradigm, which involves
developing effective models based on fixed datasets. However, this approach
often overlooks potential quality issues and flaws inherent in the data. Driven
by the potential of data-centric AI, we propose a novel data-centric paradigm
for developing an ideal training dataset using a model-agnostic dataset
regeneration framework called DR4SR. This framework enables the regeneration of
a dataset with exceptional cross-architecture generalizability. Additionally,
we introduce the DR4SR+ framework, which incorporates a model-aware dataset
personalizer to tailor the regenerated dataset specifically for a target model.
To demonstrate the effectiveness of the data-centric paradigm, we integrate our
framework with various model-centric methods and observe significant
performance improvements across four widely adopted datasets. Furthermore, we
conduct in-depth analyses to explore the potential of the data-centric paradigm
and provide valuable insights. The code can be found at
https://github.com/USTC-StarTeam/DR4SR.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamMesh: Jointly Manipulating and Texturing Triangle Meshes for
  Text-to-3D Generation <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibo Yang, Yang Chen, Yingwei Pan, Ting Yao, Zhineng Chen, Zuxuan Wu, Yu-Gang Jiang, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning radiance fields (NeRF) with powerful 2D diffusion models has
garnered popularity for text-to-3D generation. Nevertheless, the implicit 3D
representations of NeRF lack explicit modeling of meshes and textures over
surfaces, and such surface-undefined way may suffer from the issues, e.g.,
noisy surfaces with ambiguous texture details or cross-view inconsistency. To
alleviate this, we present DreamMesh, a novel text-to-3D architecture that
pivots on well-defined surfaces (triangle meshes) to generate high-fidelity
explicit 3D model. Technically, DreamMesh capitalizes on a distinctive
coarse-to-fine scheme. In the coarse stage, the mesh is first deformed by
text-guided Jacobians and then DreamMesh textures the mesh with an interlaced
use of 2D diffusion models in a tuning free manner from multiple viewpoints. In
the fine stage, DreamMesh jointly manipulates the mesh and refines the texture
map, leading to high-quality triangle meshes with high-fidelity textured
materials. Extensive experiments demonstrate that DreamMesh significantly
outperforms state-of-the-art text-to-3D methods in faithfully generating 3D
content with richer textual details and enhanced geometry. Our project page is
available at https://dreammesh.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024. Project page is available at
  \url{https://dreammesh.github.io}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haibo Yang, Yang Chen, Yingwei Pan, Ting Yao, Zhineng Chen, Chong-Wah Ngo, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite having tremendous progress in image-to-3D generation, existing
methods still struggle to produce multi-view consistent images with
high-resolution textures in detail, especially in the paradigm of 2D diffusion
that lacks 3D awareness. In this work, we present High-resolution Image-to-3D
model (Hi3D), a new video diffusion based paradigm that redefines a single
image to multi-view images as 3D-aware sequential image generation (i.e.,
orbital video generation). This methodology delves into the underlying temporal
consistency knowledge in video diffusion model that generalizes well to
geometry consistency across multiple views in 3D generation. Technically, Hi3D
first empowers the pre-trained video diffusion model with 3D-aware prior
(camera pose condition), yielding multi-view images with low-resolution texture
details. A 3D-aware video-to-video refiner is learnt to further scale up the
multi-view images with high-resolution texture details. Such high-resolution
multi-view images are further augmented with novel views through 3D Gaussian
Splatting, which are finally leveraged to obtain high-fidelity meshes via 3D
reconstruction. Extensive experiments on both novel view synthesis and single
view reconstruction demonstrate that our Hi3D manages to produce superior
multi-view consistency images with highly-detailed textures. Source code and
data are available at \url{https://github.com/yanghb22-fdu/Hi3D-Official}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Multimedia 2024. Source code is available at
  \url{https://github.com/yanghb22-fdu/Hi3D-Official}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreeEnhance: Tuning-Free Image Enhancement via Content-Consistent
  Noising-and-Denoising Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Luo, Yiheng Zhang, Zhaofan Qiu, Ting Yao, Zhineng Chen, Yu-Gang Jiang, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of text-to-image generation models has led to the recognition
that image enhancement, performed as post-processing, would significantly
improve the visual quality of the generated images. Exploring diffusion models
to enhance the generated images nevertheless is not trivial and necessitates to
delicately enrich plentiful details while preserving the visual appearance of
key content in the original image. In this paper, we propose a novel framework,
namely FreeEnhance, for content-consistent image enhancement using the
off-the-shelf image diffusion models. Technically, FreeEnhance is a two-stage
process that firstly adds random noise to the input image and then capitalizes
on a pre-trained image diffusion model (i.e., Latent Diffusion Models) to
denoise and enhance the image details. In the noising stage, FreeEnhance is
devised to add lighter noise to the region with higher frequency to preserve
the high-frequent patterns (e.g., edge, corner) in the original image. In the
denoising stage, we present three target properties as constraints to
regularize the predicted noise, enhancing images with high acutance and high
visual quality. Extensive experiments conducted on the HPDv2 dataset
demonstrate that our FreeEnhance outperforms the state-of-the-art image
enhancement models in terms of quantitative metrics and human preference. More
remarkably, FreeEnhance also shows higher human preference compared to the
commercial image enhancement solution of Magnific AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Multimedia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VMAS: Video-to-Music Generation via Semantic Alignment in Web Music
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan-Bo Lin, Yu Tian, Linjie Yang, Gedas Bertasius, Heng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework for learning to generate background music from video
inputs. Unlike existing works that rely on symbolic musical annotations, which
are limited in quantity and diversity, our method leverages large-scale web
videos accompanied by background music. This enables our model to learn to
generate realistic and diverse music. To accomplish this goal, we develop a
generative video-music Transformer with a novel semantic video-music alignment
scheme. Our model uses a joint autoregressive and contrastive learning
objective, which encourages the generation of music aligned with high-level
video content. We also introduce a novel video-beat alignment scheme to match
the generated music beats with the low-level motions in the video. Lastly, to
capture fine-grained visual cues in a video needed for realistic background
music generation, we introduce a new temporal video encoder architecture,
allowing us to efficiently process videos consisting of many densely sampled
frames. We train our framework on our newly curated DISCO-MV dataset,
consisting of 2.2M video-music samples, which is orders of magnitude larger
than any prior datasets used for video music generation. Our method outperforms
existing approaches on the DISCO-MV and MusicCaps datasets according to various
music generation evaluation metrics, including human evaluation. Results are
available at https://genjib.github.io/project_page/VMAs/index.html
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://genjib.github.io/project_page/VMAs/index.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CustomContrast: A Multilevel Contrastive Perspective For Subject-Driven
  Text-to-Image Customization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Chen, Mengqi Huang, Zhuowei Chen, Yang Zheng, Lei Zhang, Zhendong Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subject-driven text-to-image (T2I) customization has drawn significant
interest in academia and industry. This task enables pre-trained models to
generate novel images based on unique subjects. Existing studies adopt a
self-reconstructive perspective, focusing on capturing all details of a single
image, which will misconstrue the specific image's irrelevant attributes (e.g.,
view, pose, and background) as the subject intrinsic attributes. This
misconstruction leads to both overfitting or underfitting of irrelevant and
intrinsic attributes of the subject, i.e., these attributes are
over-represented or under-represented simultaneously, causing a trade-off
between similarity and controllability. In this study, we argue an ideal
subject representation can be achieved by a cross-differential perspective,
i.e., decoupling subject intrinsic attributes from irrelevant attributes via
contrastive learning, which allows the model to focus more on intrinsic
attributes through intra-consistency (features of the same subject are
spatially closer) and inter-distinctiveness (features of different subjects
have distinguished differences). Specifically, we propose CustomContrast, a
novel framework, which includes a Multilevel Contrastive Learning (MCL)
paradigm and a Multimodal Feature Injection (MFI) Encoder. The MCL paradigm is
used to extract intrinsic features of subjects from high-level semantics to
low-level appearance through crossmodal semantic contrastive learning and
multiscale appearance contrastive learning. To facilitate contrastive learning,
we introduce the MFI encoder to capture cross-modal representations. Extensive
experiments show the effectiveness of CustomContrast in subject similarity and
text controllability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attack on Scene Flow using Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13621v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13621v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haniyeh Ehsani Oskouie, Mohammad-Shahram Moin, Shohreh Kasaei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have made significant advancements in accurately
estimating scene flow using point clouds, which is vital for many applications
like video analysis, action recognition, and navigation. The robustness of
these techniques, however, remains a concern, particularly in the face of
adversarial attacks that have been proven to deceive state-of-the-art deep
neural networks in many domains. Surprisingly, the robustness of scene flow
networks against such attacks has not been thoroughly investigated. To address
this problem, the proposed approach aims to bridge this gap by introducing
adversarial white-box attacks specifically tailored for scene flow networks.
Experimental results show that the generated adversarial examples obtain up to
33.7 relative degradation in average end-point error on the KITTI and
FlyingThings3D datasets. The study also reveals the significant impact that
attacks targeting point clouds in only one dimension or color channel have on
average end-point error. Analyzing the success and failure of these attacks on
the scene flow networks and their 2D optical flow network variants shows a
higher vulnerability for the optical flow networks. Code is available at
https://github.com/aheldis/Attack-on-Scene-Flow-using-Point-Clouds.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Looking Backward: Streaming Video-to-Video Translation with Feature
  Banks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Liang, Akio Kodaira, Chenfeng Xu, Masayoshi Tomizuka, Kurt Keutzer, Diana Marculescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces StreamV2V, a diffusion model that achieves real-time
streaming video-to-video (V2V) translation with user prompts. Unlike prior V2V
methods using batches to process limited frames, we opt to process frames in a
streaming fashion, to support unlimited frames. At the heart of StreamV2V lies
a backward-looking principle that relates the present to the past. This is
realized by maintaining a feature bank, which archives information from past
frames. For incoming frames, StreamV2V extends self-attention to include banked
keys and values and directly fuses similar past features into the output. The
feature bank is continually updated by merging stored and new features, making
it compact but informative. StreamV2V stands out for its adaptability and
efficiency, seamlessly integrating with image diffusion models without
fine-tuning. It can run 20 FPS on one A100 GPU, being 15x, 46x, 108x, and 158x
faster than FlowVid, CoDeF, Rerender, and TokenFlow, respectively. Quantitative
metrics and user studies confirm StreamV2V's exceptional ability to maintain
temporal consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://jeff-liangf.github.io/projects/streamv2v</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Multimodal Composite Editing and Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05405v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05405v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suyan Li, Fuxiang Huang, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the real world, where information is abundant and diverse across different
modalities, understanding and utilizing various data types to improve retrieval
systems is a key focus of research. Multimodal composite retrieval integrates
diverse modalities such as text, image and audio, etc. to provide more
accurate, personalized, and contextually relevant results. To facilitate a
deeper understanding of this promising direction, this survey explores
multimodal composite editing and retrieval in depth, covering image-text
composite editing, image-text composite retrieval, and other multimodal
composite retrieval. In this survey, we systematically organize the application
scenarios, methods, benchmarks, experiments, and future directions. Multimodal
learning is a hot topic in large model era, and have also witnessed some
surveys in multimodal learning and vision-language models with transformers
published in the PAMI journal. To the best of our knowledge, this survey is the
first comprehensive review of the literature on multimodal composite retrieval,
which is a timely complement of multimodal fusion to existing reviews. To help
readers' quickly track this field, we build the project page for this survey,
which can be found at
https://github.com/fuxianghuang1/Multimodal-Composite-Editing-and-Retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 3 figures, and 11 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-10T00:00:00Z">2024-09-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive Counterfactual Exploration of Algorithmic Harms in
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongsu Ahn, Quinn K Wolter, Jonilyn Dick, Janet Dick, Yu-Ru Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have become integral to digital experiences, shaping user
interactions and preferences across various platforms. Despite their widespread
use, these systems often suffer from algorithmic biases that can lead to unfair
and unsatisfactory user experiences. This study introduces an interactive tool
designed to help users comprehend and explore the impacts of algorithmic harms
in recommender systems. By leveraging visualizations, counterfactual
explanations, and interactive modules, the tool allows users to investigate how
biases such as miscalibration, stereotypes, and filter bubbles affect their
recommendations. Informed by in-depth user interviews, this tool benefits both
general users and researchers by increasing transparency and offering
personalized impact assessments, ultimately fostering a better understanding of
algorithmic biases and contributing to more equitable recommendation outcomes.
This work provides valuable insights for future research and practical
applications in mitigating bias and enhancing fairness in machine learning
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Attacks to Multi-Modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Dou, Xin Hu, Haibo Yang, Zhuqing Liu, Minghong Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal models have gained significant attention due to their powerful
capabilities. These models effectively align embeddings across diverse data
modalities, showcasing superior performance in downstream tasks compared to
their unimodal counterparts. Recent study showed that the attacker can
manipulate an image or audio file by altering it in such a way that its
embedding matches that of an attacker-chosen targeted input, thereby deceiving
downstream models. However, this method often underperforms due to inherent
disparities in data from different modalities. In this paper, we introduce
CrossFire, an innovative approach to attack multi-modal models. CrossFire
begins by transforming the targeted input chosen by the attacker into a format
that matches the modality of the original image or audio file. We then
formulate our attack as an optimization problem, aiming to minimize the angular
deviation between the embeddings of the transformed input and the modified
image or audio file. Solving this problem determines the perturbations to be
added to the original media. Our extensive experiments on six real-world
benchmark datasets reveal that CrossFire can significantly manipulate
downstream tasks, surpassing existing attacks. Additionally, we evaluate six
defensive strategies against CrossFire, finding that current defenses are
insufficient to counteract our CrossFire.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the ACM Workshop on Large AI Systems and Models with
  Privacy and Safety Analysis 2024 (LAMPS '24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Critical Features Tracking on Triangulated Irregular Networks by a
  Scale-Space Method <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoan Feng, Yunting Song, Leila De Floriani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scale-space method is a well-established framework that constructs a
hierarchical representation of an input signal and facilitates coarse-to-fine
visual reasoning. Considering the terrain elevation function as the input
signal, the scale-space method can identify and track significant topographic
features across different scales. The number of scales a feature persists,
called its life span, indicates the importance of that feature. In this way,
important topographic features of a landscape can be selected, which are useful
for many applications, including cartography, nautical charting, and land-use
planning. The scale-space methods developed for terrain data use gridded
Digital Elevation Models (DEMs) to represent the terrain. However, gridded DEMs
lack the flexibility to adapt to the irregular distribution of input data and
the varied topological complexity of different regions. Instead, Triangulated
Irregular Networks (TINs) can be directly generated from irregularly
distributed point clouds and accurately preserve important features. In this
work, we introduce a novel scale-space analysis pipeline for TINs, addressing
the multiple challenges in extending grid-based scale-space methods to TINs.
Our pipeline can efficiently identify and track topologically important
features on TINs. Moreover, it is capable of analyzing terrains with irregular
boundaries, which poses challenges for grid-based methods. Comprehensive
experiments show that, compared to grid-based methods, our TIN-based pipeline
is more efficient, accurate, and has better resolution robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13pages, ACM SIGSPATIAL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DV-FSR: A Dual-View Target Attack Framework for Federated Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qitao Qin, Yucong Luo, Mingyue Cheng, Qingyang Mao, Chenyi Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated recommendation (FedRec) preserves user privacy by enabling
decentralized training of personalized models, but this architecture is
inherently vulnerable to adversarial attacks. Significant research has been
conducted on targeted attacks in FedRec systems, motivated by commercial and
social influence considerations. However, much of this work has largely
overlooked the differential robustness of recommendation models. Moreover, our
empirical findings indicate that existing targeted attack methods achieve only
limited effectiveness in Federated Sequential Recommendation (FSR) tasks.
Driven by these observations, we focus on investigating targeted attacks in FSR
and propose a novel dualview attack framework, named DV-FSR. This attack method
uniquely combines a sampling-based explicit strategy with a contrastive
learning-based implicit gradient strategy to orchestrate a coordinated attack.
Additionally, we introduce a specific defense mechanism tailored for targeted
attacks in FSR, aiming to evaluate the mitigation effects of the attack method
we proposed. Extensive experiments validate the effectiveness of our proposed
approach on representative sequential models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Operational Advice for Dense and Sparse Retrievers: HNSW, Flat, or
  Inverted Indexes? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Practitioners working on dense retrieval today face a bewildering number of
choices. Beyond selecting the embedding model, another consequential choice is
the actual implementation of nearest-neighbor vector search. While best
practices recommend HNSW indexes, flat vector indexes with brute-force search
represent another viable option, particularly for smaller corpora and for rapid
prototyping. In this paper, we provide experimental results on the BEIR dataset
using the open-source Lucene search library that explicate the tradeoffs
between HNSW and flat indexes (including quantized variants) from the
perspectives of indexing time, query evaluation performance, and retrieval
quality. With additional comparisons between dense and sparse retrievers, our
results provide guidance for today's search practitioner in understanding the
design space of dense and sparse retrievers. To our knowledge, we are the first
to provide operational advice supported by empirical experiments in this
regard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Sequential Recommendations through Multi-Perspective
  Reflections and Iteration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weicong Qin, Yi Xu, Weijie Yu, Chenglei Shen, Xiao Zhang, Ming He, Jianping Fan, Jun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequence recommendation (SeqRec) aims to predict the next item a user will
interact with by understanding user intentions and leveraging collaborative
filtering information. Large language models (LLMs) have shown great promise in
recommendation tasks through prompt-based, fixed reflection libraries, and
fine-tuning techniques. However, these methods face challenges, including lack
of supervision, inability to optimize reflection sources, inflexibility to
diverse user needs, and high computational costs. Despite promising results,
current studies primarily focus on reflections of users' explicit preferences
(e.g., item titles) while neglecting implicit preferences (e.g., brands) and
collaborative filtering information. This oversight hinders the capture of
preference shifts and dynamic user behaviors. Additionally, existing approaches
lack mechanisms for reflection evaluation and iteration, often leading to
suboptimal recommendations. To address these issues, we propose the Mixture of
REflectors (MoRE) framework, designed to model and learn dynamic user
preferences in SeqRec. Specifically, MoRE introduces three reflectors for
generating LLM-based reflections on explicit preferences, implicit preferences,
and collaborative signals. Each reflector incorporates a self-improving
strategy, termed refining-and-iteration, to evaluate and iteratively update
reflections. Furthermore, a meta-reflector employs a contextual bandit
algorithm to select the most suitable expert and corresponding reflections for
each user's recommendation, effectively capturing dynamic preferences.
Extensive experiments on three real-world datasets demonstrate that MoRE
consistently outperforms state-of-the-art methods, requiring less training time
and GPU memory compared to other LLM-based approaches in SeqRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First 3 authors contributes equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ User Preferences for Large Language Model versus Template-Based
  Explanations of Movie Recommendations: A Pilot Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Albert, Martin Balfroid, Miriam Doh, Jeremie Bogaert, Luca La Fisca, Liesbet De Vos, Bryan Renard, Vincent Stragier, Emmanuel Jean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have become integral to our digital experiences, from
online shopping to streaming platforms. Still, the rationale behind their
suggestions often remains opaque to users. While some systems employ a
graph-based approach, offering inherent explainability through paths
associating recommended items and seed items, non-experts could not easily
understand these explanations. A popular alternative is to convert graph-based
explanations into textual ones using a template and an algorithm, which we
denote here as ''template-based'' explanations. Yet, these can sometimes come
across as impersonal or uninspiring. A novel method would be to employ large
language models (LLMs) for this purpose, which we denote as ''LLM-based''. To
assess the effectiveness of LLMs in generating more resonant explanations, we
conducted a pilot study with 25 participants. They were presented with three
explanations: (1) traditional template-based, (2) LLM-based rephrasing of the
template output, and (3) purely LLM-based explanations derived from the
graph-based explanations. Although subject to high variance, preliminary
findings suggest that LLM-based explanations may provide a richer and more
engaging user experience, further aligning with user expectations. This study
sheds light on the potential limitations of current explanation methods and
offers promising directions for leveraging large language models to improve
user satisfaction and trust in recommender systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented to the Dutch-Belgian Workshop on Recommender Systems 2023
  (14-15 December, 2023 - Antwerp, Belgium)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NLP-Powered Repository and Search Engine for Academic Papers: A Case
  Study on Cyber Risk Literature with CyLit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linfeng Zhang, Changyue Hu, Zhiyu Quan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the body of academic literature continues to grow, researchers face
increasing difficulties in effectively searching for relevant resources.
Existing databases and search engines often fall short of providing a
comprehensive and contextually relevant collection of academic literature. To
address this issue, we propose a novel framework that leverages Natural
Language Processing (NLP) techniques. This framework automates the retrieval,
summarization, and clustering of academic literature within a specific research
domain. To demonstrate the effectiveness of our approach, we introduce CyLit,
an NLP-powered repository specifically designed for the cyber risk literature.
CyLit empowers researchers by providing access to context-specific resources
and enabling the tracking of trends in the dynamic and rapidly evolving field
of cyber risk. Through the automatic processing of large volumes of data, our
NLP-powered solution significantly enhances the efficiency and specificity of
academic literature searches. We compare the literature categorization results
of CyLit to those presented in survey papers or generated by ChatGPT,
highlighting the distinctive insights this tool provides into cyber risk
research literature. Using NLP techniques, we aim to revolutionize the way
researchers discover, analyze, and utilize academic resources, ultimately
fostering advancements in various domains of knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hier<span class="highlight-title">LLM</span>: Hierarchical Large Language Model for Question Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Liu, Haipeng Liu, Ting Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question recommendation is a task that sequentially recommends questions for
students to enhance their learning efficiency. That is, given the learning
history and learning target of a student, a question recommender is supposed to
select the question that will bring the most improvement for students. Previous
methods typically model the question recommendation as a sequential
decision-making problem, estimating students' learning state with the learning
history, and feeding the learning state with the learning target to a neural
network to select the recommended question from a question set. However,
previous methods are faced with two challenges: (1) learning history is
unavailable in the cold start scenario, which makes the recommender generate
inappropriate recommendations; (2) the size of the question set is much large,
which makes it difficult for the recommender to select the best question
precisely. To address the challenges, we propose a method called hierarchical
large language model for question recommendation (HierLLM), which is a
LLM-based hierarchical structure. The LLM-based structure enables HierLLM to
tackle the cold start issue with the strong reasoning abilities of LLM. The
hierarchical structure takes advantage of the fact that the number of concepts
is significantly smaller than the number of questions, narrowing the range of
selectable questions by first identifying the relevant concept for the
to-recommend question, and then selecting the recommended question based on
that concept. This hierarchical structure reduces the difficulty of the
recommendation.To investigate the performance of HierLLM, we conduct extensive
experiments, and the results demonstrate the outstanding performance of
HierLLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What makes a good concept anyway ? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naren Khatwani, James Geller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A good medical ontology is expected to cover its domain completely and
correctly. On the other hand, large ontologies are hard to build, hard to
understand, and hard to maintain. Thus, adding new concepts (often multi-word
concepts) to an existing ontology must be done judiciously. Only "good"
concepts should be added; however, it is difficult to define what makes a
concept good. In this research, we propose a metric to measure the goodness of
a concept. We identified factors that appear to influence goodness judgments of
medical experts and combined them into a single metric. These factors include
concept name length (in words), concept occurrence frequency in the medical
literature, and syntactic categories of component words. As an added factor, we
used the simplicity of a term after mapping it into a specific foreign
language. We performed Bayesian optimization of factor weights to achieve
maximum agreement between the metric and three medical experts. The results
showed that our metric had a 50.67% overall agreement with the experts, as
measured by Krippendorff's alpha.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QueryBuilder: Human-in-the-Loop Query Development for <span class="highlight-title">Information</span>
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hemanth Kandula, Damianos Karakos, Haoling Qiu, Benjamin Rozonoyer, Ian Soboroff, Lee Tarlin, Bonan Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frequently, users of an Information Retrieval (IR) system start with an
overarching information need (a.k.a., an analytic task) and proceed to define
finer-grained queries covering various important aspects (i.e., sub-topics) of
that analytic task. We present a novel, interactive system called
$\textit{QueryBuilder}$, which allows a novice, English-speaking user to create
queries with a small amount of effort, through efficient exploration of an
English development corpus in order to rapidly develop cross-lingual
information retrieval queries corresponding to the user's information needs.
QueryBuilder performs near real-time retrieval of documents based on
user-entered search terms; the user looks through the retrieved documents and
marks sentences as relevant to the information needed. The marked sentences are
used by the system as additional information in query formation and refinement:
query terms (and, optionally, event features, which capture event $'triggers'$
(indicator terms) and agent/patient roles) are appropriately weighted, and a
neural-based system, which better captures textual meaning, retrieves other
relevant content. The process of retrieval and marking is repeated as many
times as desired, giving rise to increasingly refined queries in each
iteration. The final product is a fine-grained query used in Cross-Lingual
Information Retrieval (CLIR). Our experiments using analytic tasks and requests
from the IARPA BETTER IR datasets show that with a small amount of effort (at
most 10 minutes per sub-topic), novice users can form $\textit{useful}$
fine-grained queries including in languages they don't understand. QueryBuilder
also provides beneficial capabilities to the traditional corpus exploration and
query formation process. A demonstration video is released at
https://vimeo.com/734795835
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RBoard: A Unified Platform for Reproducible and Reusable Recommender
  System Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05526v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05526v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyang Shao, Edoardo D'Amico, Gabor Fodor, Tri Kurniawan Wijaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems research lacks standardized benchmarks for
reproducibility and algorithm comparisons. We introduce RBoard, a novel
framework addressing these challenges by providing a comprehensive platform for
benchmarking diverse recommendation tasks, including CTR prediction, Top-N
recommendation, and others. RBoard's primary objective is to enable fully
reproducible and reusable experiments across these scenarios. The framework
evaluates algorithms across multiple datasets within each task, aggregating
results for a holistic performance assessment. It implements standardized
evaluation protocols, ensuring consistency and comparability. To facilitate
reproducibility, all user-provided code can be easily downloaded and executed,
allowing researchers to reliably replicate studies and build upon previous
work. By offering a unified platform for rigorous, reproducible evaluation
across various recommendation scenarios, RBoard aims to accelerate progress in
the field and establish a new standard for recommender systems benchmarking in
both academia and industry. The platform is available at https://rboard.org and
the demo video can be found at https://bit.ly/rboard-demo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Retrieval-Augmented Generation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08921v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08921v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, Siliang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable
success in addressing the challenges of Large Language Models (LLMs) without
necessitating retraining. By referencing an external knowledge base, RAG
refines LLM outputs, effectively mitigating issues such as ``hallucination'',
lack of domain-specific knowledge, and outdated information. However, the
complex structure of relationships among different entities in databases
presents challenges for RAG systems. In response, GraphRAG leverages structural
information across entities to enable more precise and comprehensive retrieval,
capturing relational knowledge and facilitating more accurate, context-aware
responses. Given the novelty and potential of GraphRAG, a systematic review of
current technologies is imperative. This paper provides the first comprehensive
overview of GraphRAG methodologies. We formalize the GraphRAG workflow,
encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced
Generation. We then outline the core technologies and training methods at each
stage. Additionally, we examine downstream tasks, application domains,
evaluation methodologies, and industrial use cases of GraphRAG. Finally, we
explore future research directions to inspire further inquiries and advance
progress in the field. In order to track recent progress in this field, we set
up a repository at \url{https://github.com/pengboci/GraphRAG-Survey}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work. Compared to the first version, several references have
  been added and a GitHub repository link has been provided</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Margin Cosine Loss: Proposal and Application in Recommender
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04614v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04614v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Makbule Gulcin Ozsoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems guide users through vast amounts of information by
suggesting items based on their predicted preferences. Collaborative
filtering-based deep learning techniques have regained popularity due to their
straightforward nature, relying only on user-item interactions. Typically,
these systems consist of three main components: an interaction module, a loss
function, and a negative sampling strategy. Initially, researchers focused on
enhancing performance by developing complex interaction modules. However, there
has been a recent shift toward refining loss functions and negative sampling
strategies. This shift has led to an increased interest in contrastive
learning, which pulls similar pairs closer while pushing dissimilar ones apart.
Contrastive learning may bring challenges like high memory demands and
under-utilization of some negative samples. The proposed Multi-Margin Cosine
Loss (MMCL) addresses these challenges by introducing multiple margins and
varying weights for negative samples. It efficiently utilizes not only the
hardest negatives but also other non-trivial negatives, offers a simpler yet
effective loss function that outperforms more complex methods, especially when
resources are limited. Experiments on two well-known datasets demonstrated that
MMCL achieved up to a 20\% performance improvement compared to a baseline loss
function when fewer number of negative samples are used.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-shot Audio Topic Reranking using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengjie Qian, Rao Ma, Adian Liusie, Erfan Loweimi, Kate M. Knill, Mark J. F. Gales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Video Search by Examples (MVSE) investigates using video clips as
the query term for information retrieval, rather than the more traditional text
query. This enables far richer search modalities such as images, speaker,
content, topic, and emotion. A key element for this process is highly rapid and
flexible search to support large archives, which in MVSE is facilitated by
representing video attributes with embeddings. This work aims to compensate for
any performance loss from this rapid archive search by examining reranking
approaches. In particular, zero-shot reranking methods using large language
models (LLMs) are investigated as these are applicable to any video archive
audio content. Performance is evaluated for topic-based retrieval on a publicly
available video archive, the BBC Rewind corpus. Results demonstrate that
reranking significantly improves retrieval ranking without requiring any
task-specific in-domain training data. Furthermore, three sources of
information (ASR transcriptions, automatic summaries and synopses) as input for
LLM reranking were compared. To gain a deeper understanding and further
insights into the performance differences and limitations of these text
sources, we employ a fact-checking approach to analyse the information
consistency among them.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Counterfactual Explanation Framework for Retrieval Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhavik Chandna, Procheta Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainability has become a crucial concern in today's world, aiming to
enhance transparency in machine learning and deep learning models. Information
retrieval is no exception to this trend. In existing literature on
explainability of information retrieval, the emphasis has predominantly been on
illustrating the concept of relevance concerning a retrieval model. The
questions addressed include why a document is relevant to a query, why one
document exhibits higher relevance than another, or why a specific set of
documents is deemed relevant for a query.
  However, limited attention has been given to understanding why a particular
document is considered non-relevant to a query with respect to a retrieval
model. In an effort to address this gap, our work focus on the question of what
terms need to be added within a document to improve its ranking. This in turn
answers the question of which words played a role in not being favored by a
retrieval model for a particular query. We use an optimization framework to
solve the above-mentioned research problem. % To the best of our knowledge, we
mark the first attempt to tackle this specific counterfactual problem. Our
experiments show the effectiveness of our proposed approach in predicting
counterfactuals for both statistical (e.g. BM25) and deep-learning-based models
(e.g. DRMM, DSSM, ColBERT).
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Sub-Genre Classification For Mainstage Dance Music <span class="chip">ICASSP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhi Shu, Xinglin Li, Hongyu Jiang, Minghao Fu, Xinyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music classification, with a wide range of applications, is one of the most
prominent tasks in music information retrieval. To address the absence of
comprehensive datasets and high-performing methods in the classification of
mainstage dance music, this work introduces a novel benchmark comprising a new
dataset and a baseline. Our dataset extends the number of sub-genres to cover
most recent mainstage live sets by top DJs worldwide in music festivals. A
continuous soft labeling approach is employed to account for tracks that span
multiple sub-genres, preserving the inherent sophistication. For the baseline,
we developed deep learning models that outperform current state-of-the-art
multimodel language models, which struggle to identify house music sub-genres,
emphasizing the need for specialized models trained on fine-grained datasets.
Our benchmark is applicable to serve for application scenarios such as music
recommendation, DJ set curation, and interactive multimedia, where we also
provide video demos. Our code is on
\url{https://anonymous.4open.science/r/Mainstage-EDM-Benchmark/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Generative-Discriminative Representations for Very
  Low-Resolution Face Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junzheng Zhang, Weijia Guo, Bochao Liu, Ruixin Shi, Yong Li, Shiming Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Very low-resolution face recognition is challenging due to the serious loss
of informative facial details in resolution degradation. In this paper, we
propose a generative-discriminative representation distillation approach that
combines generative representation with cross-resolution aligned knowledge
distillation. This approach facilitates very low-resolution face recognition by
jointly distilling generative and discriminative models via two distillation
modules. Firstly, the generative representation distillation takes the encoder
of a diffusion model pretrained for face super-resolution as the generative
teacher to supervise the learning of the student backbone via feature
regression, and then freezes the student backbone. After that, the
discriminative representation distillation further considers a pretrained face
recognizer as the discriminative teacher to supervise the learning of the
student head via cross-resolution relational contrastive distillation. In this
way, the general backbone representation can be transformed into discriminative
head representation, leading to a robust and discriminative student model for
very low-resolution face recognition. Our approach improves the recovery of the
missing details in very low-resolution faces and achieves better knowledge
transfer. Extensive experiments on face datasets demonstrate that our approach
enhances the recognition accuracy of very low-resolution faces, showcasing its
effectiveness and adaptability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIP-GAF: A M<span class="highlight-title">LLM</span>-annotated Benchmark for Most Important Person
  Localization and Group Context Understanding <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06224v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06224v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Surbhi Madan, Shreya Ghosh, Lownish Rai Sookha, M. A. Ganaie, Ramanathan Subramanian, Abhinav Dhall, Tom Gedeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the Most Important Person (MIP) in any social event setup is a
challenging problem mainly due to contextual complexity and scarcity of labeled
data. Moreover, the causality aspects of MIP estimation are quite subjective
and diverse. To this end, we aim to address the problem by annotating a
large-scale `in-the-wild' dataset for identifying human perceptions about the
`Most Important Person (MIP)' in an image. The paper provides a thorough
description of our proposed Multimodal Large Language Model (MLLM) based data
annotation strategy, and a thorough data quality analysis. Further, we perform
a comprehensive benchmarking of the proposed dataset utilizing state-of-the-art
MIP localization methods, indicating a significant drop in performance compared
to existing datasets. The performance drop shows that the existing MIP
localization algorithms must be more robust with respect to `in-the-wild'
situations. We believe the proposed dataset will play a vital role in building
the next-generation social situation understanding methods. The code and data
is available at https://github.com/surbhimadan92/MIP-GAF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design and Implementation of Online Live Streaming System Using A 3D
  Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aizierjiang Aiersilan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing demand for live video streaming, there is an increasing need
for low-latency and high-quality transmission, especially with the advent of 5G
networks. While 5G offers hardware-level improvements, effective software
solutions for minimizing latency remain essential. Current methods, such as
multi-channel streaming, fail to address latency issues fundamentally, often
only adding new channels without optimizing overall performance. This thesis
proposes a novel approach using a 3D engine (e.g., Unity 3D) to stream
multi-input video data through a single channel with reduced latency. By
leveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D
Canvases, and Webcam Textures, the proposed system consolidates video streams
from multiple external cameras into a unified, low-latency output. The
affiliated project of this thesis demonstrates the implementation of a
low-latency multi-channel live video streaming system. It employs the RTSP
protocol and examines video encoding techniques, alongside a client-side
application based on Unity 3D. The system architecture includes a WebSocket
server for persistent connections, an HTTP server for communication, a MySQL
database for storage, Redis for caching, and Nginx for load balancing. Each
module operates independently, ensuring flexibility and scalability in the
system's design. A key innovation of this system is its use of a 3D scene to
map multiple video inputs onto a virtual canvas, recorded by an in-engine
camera for transmission. This design minimizes redundant data, enabling an
efficient and director-guided live streaming network. The thesis concludes by
discussing challenges encountered during the project and provides solutions for
future improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Draw an Audio: Leveraging Multi-Instruction for Video-to-Audio Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Yang, Binjie Mao, Zili Wang, Xing Nie, Pengfei Gao, Ying Guo, Cheng Zhen, Pengfei Yan, Shiming Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foley is a term commonly used in filmmaking, referring to the addition of
daily sound effects to silent films or videos to enhance the auditory
experience. Video-to-Audio (V2A), as a particular type of automatic foley task,
presents inherent challenges related to audio-visual synchronization. These
challenges encompass maintaining the content consistency between the input
video and the generated audio, as well as the alignment of temporal and
loudness properties within the video. To address these issues, we construct a
controllable video-to-audio synthesis model, termed Draw an Audio, which
supports multiple input instructions through drawn masks and loudness signals.
To ensure content consistency between the synthesized audio and target video,
we introduce the Mask-Attention Module (MAM), which employs masked video
instruction to enable the model to focus on regions of interest. Additionally,
we implement the Time-Loudness Module (TLM), which uses an auxiliary loudness
signal to ensure the synthesis of sound that aligns with the video in both
loudness and temporal dimensions. Furthermore, we have extended a large-scale
V2A dataset, named VGGSound-Caption, by annotating caption prompts. Extensive
experiments on challenging benchmarks across two large-scale V2A datasets
verify Draw an Audio achieves the state-of-the-art. Project page:
https://yannqi.github.io/Draw-an-Audio/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do <span class="highlight-title">LLM</span>s Understand Visual Anomalies? Uncovering <span class="highlight-title">LLM</span>'s Capabilities in
  Zero-shot Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.09654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.09654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Zhu, Shaofeng Cai, Fang Deng, Beng Chin Ooi, Junran Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) are markedly proficient in deriving
visual representations guided by natural language. Recent explorations have
utilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by
pairing images with textual descriptions indicative of normal and abnormal
conditions, referred to as anomaly prompts. However, existing approaches depend
on static anomaly prompts that are prone to cross-semantic ambiguity, and
prioritize global image-level representations over crucial local pixel-level
image-to-text alignment that is necessary for accurate anomaly localization. In
this paper, we present ALFA, a training-free approach designed to address these
challenges via a unified model. We propose a run-time prompt adaptation
strategy, which first generates informative anomaly prompts to leverage the
capabilities of a large language model (LLM). This strategy is enhanced by a
contextual scoring mechanism for per-image anomaly prompt adaptation and
cross-semantic ambiguity mitigation. We further introduce a novel fine-grained
aligner to fuse local pixel-level semantics for precise anomaly localization,
by projecting the image-text alignment from global to local semantic spaces.
Extensive evaluations on MVTec and VisA datasets confirm ALFA's effectiveness
in harnessing the language potential for zero-shot VAD, achieving significant
PRO improvements of 12.1% on MVTec and 8.9% on VisA compared to
state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MM'24 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Question-Answering Dense Video <span class="highlight-title">Event</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04388v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04388v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hangyu Qin, Junbin Xiao, Angela Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have shown excellent performance in
question-answering of single-event videos. In this paper, we present
question-answering dense video events, a novel task that requires answering and
grounding the dense-event questions in long videos, thus challenging MLLMs to
faithfully comprehend and reason about multiple events occurring over extended
time periods. To facilitate the study, we construct DeVE-QA - a dataset
featuring 78K questions about 26K events on 10.6K long videos. We then
benchmark and show that existing MLLMs excelling at single-event QA struggle to
perform well in DeVE-QA. For improvement, we propose DeVi, a novel
training-free MLLM approach that highlights a hierarchical captioning module, a
temporal event memory module, and a self-consistency checking module to
respectively detect, contextualize and memorize, and ground dense-events in
long videos for question answering. Extensive experiments show that DeVi is
superior at answering dense-event questions and grounding relevant video
moments. Compared with existing MLLMs, it achieves a remarkable increase of 4.1
percent and 3.7 percent for G(round)QA accuracy on DeVE-QA and NExT-GQA
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FrameCorr: Adaptive, Autoencoder-based Neural Compression for Video
  Reconstruction in Resource and Timing Constrained Network Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Li, Shehab Sarar Ahmed, Deepak Nair
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the growing adoption of video processing via Internet of Things (IoT)
devices due to their cost-effectiveness, transmitting captured data to nearby
servers poses challenges due to varying timing constraints and scarcity of
network bandwidth. Existing video compression methods face difficulties in
recovering compressed data when incomplete data is provided. Here, we introduce
FrameCorr, a deep-learning based solution that utilizes previously received
data to predict the missing segments of a frame, enabling the reconstruction of
a frame from partially received data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning-Driven Open-Source Framework for Assessing QoE in
  Multimedia Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08564v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08564v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parsa Hassani Shariat Panahi, Amir Hossein Jalilvand, Abolfazl Diyanat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Internet is integral to modern life, influencing communication, business,
and lifestyles globally. As dependence on Internet services grows, the demand
for high-quality service delivery increases. Service providers must maintain
high standards of quality of service and quality of experience (QoE) to ensure
user satisfaction. QoE, which reflects user satisfaction with service quality,
is a key metric for multimedia services, yet it is challenging to measure due
to its subjective nature and the complexities of real-time feedback. This paper
introduces a machine learning-based framework for objectively assessing QoE in
multimedia networks. The open-source framework complies with the ITU-T P.1203
standard. It automates data collection and user satisfaction prediction using
key network parameters such as delay, jitter, packet loss, bitrate, and
throughput. Using a dataset of over 20,000 records from various network
conditions, the Random Forest model predicts the mean opinion score with 95.8%
accuracy. Our framework addresses the limitations of existing QoE models by
integrating real-time data collection, machine learning predictions, and
adherence to international standards. This approach enhances QoE evaluation
accuracy and allows dynamic network resource management, optimizing performance
and cost-efficiency. Its open-source nature encourages adaptation and extension
for various multimedia services. The findings significantly affect the
telecommunications industry in managing and optimizing multimedia services. The
network centric QoE prediction of the framework offers a scalable solution to
improve user satisfaction without the need for content-specific data. Future
enhancements could include advanced machine learning models and broader
applicability to digital services. This research contributes a practical,
standardized tool for QoE assessment across diverse networks and platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-09T00:00:00Z">2024-09-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">15</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Chinese Knowledge Rectification in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhe Lu, Jizhan Fang, Yunzhi Yao, Xin Xu, Ningyu Zhang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) exhibit remarkable generative
capabilities, they are not without flaws, particularly in the form of
hallucinations. This issue is even more pronounced when LLMs are applied to
specific languages and domains. For example, LLMs may generate nonsense
information when handling Chinese ancient poetry, proverbs, or idioms, owing to
the lack of specific knowledge. To this end, this paper introduces a benchmark
for rectifying Chinese knowledge in LLMs via knowledge editing. Specifically,
we introduce a new Chinese dataset, CKnowEdit, by collecting seven type of
knowledge from various sources, including classical texts, idioms, and content
from Baidu Tieba Ruozhiba, thereby accounting for the unique polyphony,
antithesis, and logical constructs inherent in the Chinese language. Through
the analysis of this dataset, we uncover the challenges faced by current LLMs
in mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge
editing techniques on this dataset unveil the substantial scope for advancement
in the rectification of Chinese knowledge. Code and dataset are available at
https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work; code and dataset are available at
  https://github.com/zjunlp/EasyEdit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OneEdit: A Neural-Symbolic Collaboratively Knowledge Editing System <span class="chip">VLDB2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ningyu Zhang, Zekun Xi, Yujie Luo, Peng Wang, Bozhong Tian, Yunzhi Yao, Jintian Zhang, Shumin Deng, Mengshu Sun, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge representation has been a central aim of AI since its inception.
Symbolic Knowledge Graphs (KGs) and neural Large Language Models (LLMs) can
both represent knowledge. KGs provide highly accurate and explicit knowledge
representation, but face scalability issue; while LLMs offer expansive coverage
of knowledge, but incur significant training costs and struggle with precise
and reliable knowledge manipulation. To this end, we introduce OneEdit, a
neural-symbolic prototype system for collaborative knowledge editing using
natural language, which facilitates easy-to-use knowledge management with KG
and LLM. OneEdit consists of three modules: 1) The Interpreter serves for user
interaction with natural language; 2) The Controller manages editing requests
from various users, leveraging the KG with rollbacks to handle knowledge
conflicts and prevent toxic knowledge attacks; 3) The Editor utilizes the
knowledge from the Controller to edit KG and LLM. We conduct experiments on two
new datasets with KGs which demonstrate that OneEdit can achieve superior
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LLM+KG@VLDB2024, code is available at
  https://github.com/zjunlp/OneEdit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting the U.S. building types from OpenStreetMap data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henrique F. de Arruda, Sandro M. Reia, Shiyang Ruan, Kuldip S. Atwal, Hamdi Kavak, Taylor Anderson, Dieter Pfoser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building type information is crucial for population estimation, traffic
planning, urban planning, and emergency response applications. Although
essential, such data is often not readily available. To alleviate this problem,
this work creates a comprehensive dataset by providing
residential/non-residential building classification covering the entire United
States. We propose and utilize an unsupervised machine learning method to
classify building types based on building footprints and available
OpenStreetMap information. The classification result is validated using
authoritative ground truth data for select counties in the U.S. The validation
shows a high precision for non-residential building classification and a high
recall for residential buildings. We identified various approaches to improving
the quality of the classification, such as removing sheds and garages from the
dataset. Furthermore, analyzing the misclassifications revealed that they are
mainly due to missing and scarce metadata in OSM. A major result of this work
is the resulting dataset of classifying 67,705,475 buildings. We hope that this
data is of value to the scientific community, including urban and
transportation planners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RegNLP in Action: Facilitating Compliance Through Automated <span class="highlight-title">Information</span>
  Retrieval and Answer Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuba Gokhan, Kexin Wang, Iryna Gurevych, Ted Briscoe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regulatory documents, issued by governmental regulatory bodies, establish
rules, guidelines, and standards that organizations must adhere to for legal
compliance. These documents, characterized by their length, complexity and
frequent updates, are challenging to interpret, requiring significant
allocation of time and expertise on the part of organizations to ensure ongoing
compliance.Regulatory Natural Language Processing (RegNLP) is a
multidisciplinary subfield aimed at simplifying access to and interpretation of
regulatory rules and obligations. We define an Automated Question-Passage
Generation task for RegNLP, create the ObliQA dataset containing 27,869
questions derived from the Abu Dhabi Global Markets (ADGM) financial regulation
document collection, design a baseline Regulatory Information Retrieval and
Answer Generation system, and evaluate it with RePASs, a novel evaluation
metric that tests whether generated answers accurately capture all relevant
obligations and avoid contradictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Graph Contrastive Learning with Reliable and Informative
  Augmentation for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Zheng, Junjie Zhang, Hongyu Lu, Yu Chen, Ming Chen, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural network (GNN) has been a powerful approach in collaborative
filtering (CF) due to its ability to model high-order user-item relationships.
Recently, to alleviate the data sparsity and enhance representation learning,
many efforts have been conducted to integrate contrastive learning (CL) with
GNNs. Despite the promising improvements, the contrastive view generation based
on structure and representation perturbations in existing methods potentially
disrupts the collaborative information in contrastive views, resulting in
limited effectiveness of positive alignment. To overcome this issue, we propose
CoGCL, a novel framework that aims to enhance graph contrastive learning by
constructing contrastive views with stronger collaborative information via
discrete codes. The core idea is to map users and items into discrete codes
rich in collaborative information for reliable and informative contrastive view
generation. To this end, we initially introduce a multi-level vector quantizer
in an end-to-end manner to quantize user and item representations into discrete
codes. Based on these discrete codes, we enhance the collaborative information
of contrastive views by considering neighborhood structure and semantic
relevance respectively. For neighborhood structure, we propose virtual neighbor
augmentation by treating discrete codes as virtual neighbors, which expands an
observed user-item interaction into multiple edges involving discrete codes.
Regarding semantic relevance, we identify similar users/items based on shared
discrete codes and interaction targets to generate the semantically relevant
view. Through these strategies, we construct contrastive views with stronger
collaborative information and develop a triple-view graph contrastive learning
approach. Extensive experiments on four public datasets demonstrate the
effectiveness of our proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Learnable Item Tokenization for Generative Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enze Liu, Bowen Zheng, Cheng Ling, Lantao Hu, Han Li, Wayne Xin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, generative recommendation has emerged as a promising new paradigm
that directly generates item identifiers for recommendation. However, a key
challenge lies in how to effectively construct item identifiers that are
suitable for recommender systems. Existing methods typically decouple item
tokenization from subsequent generative recommendation training, likely
resulting in suboptimal performance. To address this limitation, we propose
ETEGRec, a novel End-To-End Generative Recommender by seamlessly integrating
item tokenization and generative recommendation. Our framework is developed
based on the dual encoder-decoder architecture, which consists of an item
tokenizer and a generative recommender. In order to achieve mutual enhancement
between the two components, we propose a recommendation-oriented alignment
approach by devising two specific optimization objectives: sequence-item
alignment and preference-semantic alignment. These two alignment objectives can
effectively couple the learning of item tokenizer and generative recommender,
thereby fostering the mutual enhancement between the two components. Finally,
we further devise an alternating optimization method, to facilitate stable and
effective end-to-end learning of the entire framework. Extensive experiments
demonstrate the effectiveness of our proposed framework compared to a series of
traditional sequential recommendation models and generative recommendation
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DatAasee -- A Metadata-Lake as Metadata Catalog for a Virtual Data-Lake 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Himpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metadata management for distributed data sources is a long-standing but
ever-growing problem. To counter this challenge in a research-data and
library-oriented setting, this work constructs a data architecture, derived
from the data-lake: the metadata-lake. A proof-of-concept implementation of
this proposed metadata system is presented and evaluated as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recommender Systems Algorithm Selection for Ranking Prediction on
  Implicit Feedback <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Wegmeth, Tobias Vente, Joeran Beel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recommender systems algorithm selection problem for ranking prediction on
implicit feedback datasets is under-explored. Traditional approaches in
recommender systems algorithm selection focus predominantly on rating
prediction on explicit feedback datasets, leaving a research gap for ranking
prediction on implicit feedback datasets. Algorithm selection is a critical
challenge for nearly every practitioner in recommender systems. In this work,
we take the first steps toward addressing this research gap. We evaluate the
NDCG@10 of 24 recommender systems algorithms, each with two hyperparameter
configurations, on 72 recommender systems datasets. We train four optimized
machine-learning meta-models and one automated machine-learning meta-model with
three different settings on the resulting meta-dataset. Our results show that
the predictions of all tested meta-models exhibit a median Spearman correlation
ranging from 0.857 to 0.918 with the ground truth. We show that the median
Spearman correlation between meta-model predictions and the ground truth
increases by an average of 0.124 when the meta-model is optimized to predict
the ranking of algorithms instead of their performance. Furthermore, in terms
of predicting the best algorithm for an unknown dataset, we demonstrate that
the best optimized traditional meta-model, e.g., XGBoost, achieves a recall of
48.6%, outperforming the best tested automated machine learning meta-model,
e.g., AutoGluon, which achieves a recall of 47.2%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at the 18th ACM Conference on Recommender
  Systems in the Late-Breaking Results Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing SPARQL capabilities of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lars-Peter Meyer, Johannes Frey, Felix Brei, Natanael Arndt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs)
offers significant synergistic potential for knowledge-driven applications. One
possible integration is the interpretation and generation of formal languages,
such as those used in the Semantic Web, with SPARQL being a core technology for
accessing KGs. In this paper, we focus on measuring out-of-the box capabilities
of LLMs to work with SPARQL and more specifically with SPARQL SELECT queries
applying a quantitative approach.
  We implemented various benchmarking tasks in the LLM-KG-Bench framework for
automated execution and evaluation with several LLMs. The tasks assess
capabilities along the dimensions of syntax, semantic read, semantic create,
and the role of knowledge graph prompt inclusion.
  With this new benchmarking tasks, we evaluated a selection of GPT, Gemini,
and Claude models. Our findings indicate that working with SPARQL SELECT
queries is still challenging for LLMs and heavily depends on the specific LLM
as well as the complexity of the task. While fixing basic syntax errors seems
to pose no problems for the best of the current LLMs evaluated, creating
semantically correct SPARQL SELECT queries is difficult in several cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>peer reviewed publication at NLP4KGc @ Semantics 2024, see
  https://sites.google.com/view/3rdnlp4kgc</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Replicability Measures for Longitudinal <span class="highlight-title">Information</span> Retrieval Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jüri Keller, Timo Breuer, Philipp Schaer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information Retrieval (IR) systems are exposed to constant changes in most
components. Documents are created, updated, or deleted, the information needs
are changing, and even relevance might not be static. While it is generally
expected that the IR systems retain a consistent utility for the users, test
collection evaluations rely on a fixed experimental setup. Based on the
LongEval shared task and test collection, this work explores how the
effectiveness measured in evolving experiments can be assessed. Specifically,
the persistency of effectiveness is investigated as a replicability task. It is
observed how the effectiveness progressively deteriorates over time compared to
the initial measurement. Employing adapted replicability measures provides
further insight into the persistence of effectiveness. The ranking of systems
varies across retrieval measures and time. In conclusion, it was found that the
most effective systems are not necessarily the ones with the most persistent
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Experimental IR Meets Multilinguality, Multimodality, and Interaction
  - 15th International Conference of the CLEF Association, CLEF 2024, Grenoble,
  France, September 9-12, 2024, Proceedings. arXiv admin note: text overlap
  with arXiv:2308.10549</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NLLB-E5: A Scalable Multilingual Retrieval Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arkadeep Acharya, Rudra Murthy, Vishwajeet Kumar, Jaydeep Sen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant progress in multilingual information retrieval, the lack
of models capable of effectively supporting multiple languages, particularly
low-resource like Indic languages, remains a critical challenge. This paper
presents NLLB-E5: A Scalable Multilingual Retrieval Model. NLLB-E5 leverages
the in-built multilingual capabilities in the NLLB encoder for translation
tasks. It proposes a distillation approach from multilingual retriever E5 to
provide a zero-shot retrieval approach handling multiple languages, including
all major Indic languages, without requiring multilingual training data. We
evaluate the model on a comprehensive suite of existing benchmarks, including
Hindi-BEIR, highlighting its robust performance across diverse languages and
tasks. Our findings uncover task and domain-specific challenges, providing
valuable insights into the retrieval performance, especially for low-resource
languages. NLLB-E5 addresses the urgent need for an inclusive, scalable, and
language-agnostic text retrieval model, advancing the field of multilingual
information access and promoting digital inclusivity for millions of users
globally.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhishLang: A Lightweight, Client-Side Phishing Detection Framework using
  MobileBERT for Real-Time, Explainable Threat Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayak Saha Roy, Shirin Nilizadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce PhishLang, an open-source, lightweight language
model specifically designed for phishing website detection through contextual
analysis of the website. Unlike traditional heuristic or machine learning
models that rely on static features and struggle to adapt to new threats, and
deep learning models that are computationally intensive, our model leverages
MobileBERT, a fast and memory-efficient variant of the BERT architecture, to
learn granular features characteristic of phishing attacks. PhishLang operates
with minimal data preprocessing and offers performance comparable to leading
deep learning anti-phishing tools, while being significantly faster and less
resource-intensive. Over a 3.5-month testing period, PhishLang successfully
identified 25,796 phishing URLs, many of which were undetected by popular
antiphishing blocklists, thus demonstrating its potential to enhance current
detection measures. Capitalizing on PhishLang's resource efficiency, we release
the first open-source fully client-side Chromium browser extension that
provides inference locally without requiring to consult an online blocklist and
can be run on low-end systems with no impact on inference times. Our
implementation not only outperforms prevalent (server-side) phishing tools, but
is significantly more effective than the limited commercial client-side
measures available. Furthermore, we study how PhishLang can be integrated with
GPT-3.5 Turbo to create explainable blocklisting -- which, upon detection of a
website, provides users with detailed contextual information about the features
that led to a website being marked as phishing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03753v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03753v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuntian Deng, Wenting Zhao, Jack Hessel, Xiang Ren, Claire Cardie, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing availability of real-world conversation data offers exciting
opportunities for researchers to study user-chatbot interactions. However, the
sheer volume of this data makes manually examining individual conversations
impractical. To overcome this challenge, we introduce WildVis, an interactive
tool that enables fast, versatile, and large-scale conversation analysis.
WildVis provides search and visualization capabilities in the text and
embedding spaces based on a list of criteria. To manage million-scale datasets,
we implemented optimizations including search index construction, embedding
precomputation and compression, and caching to ensure responsive user
interactions within seconds. We demonstrate WildVis' utility through three case
studies: facilitating chatbot misuse research, visualizing and comparing topic
distributions across datasets, and characterizing user-specific conversation
patterns. WildVis is open-source and designed to be extendable, supporting
additional datasets and customized search and visualization functionalities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Fairness in Recommender Systems: A Healthcare Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03893v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03893v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Veronica Kecki, Alan Said
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness in AI-driven decision-making systems has become a critical concern,
especially when these systems directly affect human lives. This paper explores
the public's comprehension of fairness in healthcare recommendations. We
conducted a survey where participants selected from four fairness metrics --
Demographic Parity, Equal Accuracy, Equalized Odds, and Positive Predictive
Value -- across different healthcare scenarios to assess their understanding of
these concepts. Our findings reveal that fairness is a complex and often
misunderstood concept, with a generally low level of public understanding
regarding fairness metrics in recommender systems. This study highlights the
need for enhanced information and education on algorithmic fairness to support
informed decision-making in using these systems. Furthermore, the results
suggest that a one-size-fits-all approach to fairness may be insufficient,
pointing to the importance of context-sensitive designs in developing equitable
AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 18th ACM Conference on Recommender Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatQA 2: Bridging the Gap to Proprietary <span class="highlight-title">LLM</span>s in Long Context and RAG
  Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Xu, Wei Ping, Xianchao Wu, Chejian Xu, Zihan Liu, Mohammad Shoeybi, Bryan Catanzaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K
context window, designed to bridge the gap between open-source LLMs and leading
proprietary models (e.g., GPT-4-Turbo) in long-context understanding and
retrieval-augmented generation (RAG) capabilities. These two capabilities are
essential for LLMs to process large volumes of information that cannot fit into
a single prompt and are complementary to each other, depending on the
downstream tasks and computational budgets. We present a detailed continued
training recipe to extend the context window of Llama3-70B-base from 8K to 128K
tokens, along with a three-stage instruction tuning process to enhance the
model's instruction-following, RAG performance, and long-context understanding
capabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model
outperforms most existing state-of-the-art models, including
GPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-Instruct, on
ultra-long tasks beyond 100K tokens, as well as on the RAG benchmark using only
a 4K context window, showing the strong long context capability across varying
sequence lengths. We further provide extensive comparisons between direct
long-context and RAG solutions using the same state-of-the-art long-context
LLMs. Interestingly, we find that the performance of strong long-context LLMs
using RAG improves when retrieving a larger number of chunks. With a large set
of top-k chunks, RAG consistently outperforms direct long-context solution
using the same state-of-the-art long-context models (e.g., Llama3-ChatQA-2-70B
and Qwen2-72B-Instruct) on both 32K benchmarks and real-world 128K tasks. To
advance research in this field, we open-sourced the model weights, training
data, and the evaluation setup for the for the community:
https://chatqa2-project.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: major update with significantly improved results</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REVISION: A Roadmap on Adaptive Video Streaming Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farzad Tashtarian, Christian Timmerer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the soaring popularity of video applications and the consequent rise
in video traffic on the Internet, technologies like HTTP Adaptive Streaming
(HAS) are crucial for delivering high Quality of Experience (QoE) to consumers.
HAS technology enables video players on consumer devices to enhance viewer
engagement by dynamically adapting video content quality based on network
conditions. This is especially relevant for consumer electronics as it ensures
an optimized viewing experience across a variety of devices, from smartphones
to smart TVs. This paper introduces REVISION, an efficient roadmap designed to
enhance adaptive video streaming, a core feature of modern consumer
electronics. The REVISION optimization triangle highlights three essential
aspects for improving streaming: Objective, Input Space, and Action Domain.
Additionally, REVISION proposes a novel layer-based architecture tailored to
refine video streaming systems, comprising Application, Control and Management,
and Resource layers. Each layer is designed to optimize different components of
the streaming process, which is directly linked to the performance and
efficiency of consumer devices. By adopting the principles of the REVISION,
manufacturers and developers can significantly improve the streaming
capabilities of consumer electronics, thereby enriching the consumer's
multimedia experience and accommodating the increasing demand for high-quality,
real-time video content. This approach addresses the complexities of today's
diverse video streaming ecosystem and paves the way for future advancements in
consumer technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A CLIP-based siamese approach for meme classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Huertas-Tato, Christos Koutlis, Symeon Papadopoulos, David Camacho, Ioannis Kompatsiaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Memes are an increasingly prevalent element of online discourse in social
networks, especially among young audiences. They carry ideas and messages that
range from humorous to hateful, and are widely consumed. Their potentially high
impact requires adequate means of control to moderate their use in large scale.
In this work, we propose SimCLIP a deep learning-based architecture for
cross-modal understanding of memes, leveraging a pre-trained CLIP encoder to
produce context-aware embeddings and a Siamese fusion technique to capture the
interactions between text and image. We perform an extensive experimentation on
seven meme classification tasks across six datasets. We establish a new state
of the art in Memotion7k with a 7.25% relative F1-score improvement, and
achieve super-human performance on Harm-P with 13.73% F1-Score improvement. Our
approach demonstrates the potential for compact meme classification models,
enabling accurate and efficient meme monitoring. We share our code at
https://github.com/jahuerta92/meme-classification-simclip
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Toolkit for Joint Speaker Diarization and Identification with
  Application to Speaker-Attributed ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Morrone, Enrico Zovato, Fabio Brugnara, Enrico Sartori, Leonardo Badino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a modular toolkit to perform joint speaker diarization and speaker
identification. The toolkit can leverage on multiple models and algorithms
which are defined in a configuration file. Such flexibility allows our system
to work properly in various conditions (e.g., multiple registered speakers'
sets, acoustic conditions and languages) and across application domains (e.g.
media monitoring, institutional, speech analytics). In this demonstration we
show a practical use-case in which speaker-related information is used jointly
with automatic speech recognition engines to generate speaker-attributed
transcriptions. To achieve that, we employ a user-friendly web-based interface
to process audio and video inputs with the chosen configuration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Show and Tell paper. Presented at Interspeech 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-Visual Speaker Diarization: Current Databases, Approaches and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victoria Mingote, Alfonso Ortega, Antonio Miguel, Eduardo Lleida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, the large amount of audio-visual content available has fostered the
need to develop new robust automatic speaker diarization systems to analyse and
characterise it. This kind of system helps to reduce the cost of doing this
process manually and allows the use of the speaker information for different
applications, as a huge quantity of information is present, for example, images
of faces, or audio recordings. Therefore, this paper aims to address a critical
area in the field of speaker diarization systems, the integration of
audio-visual content of different domains. This paper seeks to push beyond
current state-of-the-art practices by developing a robust audio-visual speaker
diarization framework adaptable to various data domains, including TV
scenarios, meetings, and daily activities. Unlike most of the existing
audio-visual speaker diarization systems, this framework will also include the
proposal of an approach to lead the precise assignment of specific identities
in TV scenarios where celebrities appear. In addition, in this work, we have
conducted an extensive compilation of the current state-of-the-art approaches
and the existing databases for developing audio-visual speaker diarization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Rich Subjective Quality <span class="highlight-title">Information</span> for Image Quality
  Assessment in the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiongkuo Min, Yixuan Gao, Yuqin Cao, Guangtao Zhai, Wenjun Zhang, Huifang Sun, Chang Wen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional in the wild image quality assessment (IQA) models are generally
trained with the quality labels of mean opinion score (MOS), while missing the
rich subjective quality information contained in the quality ratings, for
example, the standard deviation of opinion scores (SOS) or even distribution of
opinion scores (DOS). In this paper, we propose a novel IQA method named
RichIQA to explore the rich subjective rating information beyond MOS to predict
image quality in the wild. RichIQA is characterized by two key novel designs:
(1) a three-stage image quality prediction network which exploits the powerful
feature representation capability of the Convolutional vision Transformer (CvT)
and mimics the short-term and long-term memory mechanisms of human brain; (2) a
multi-label training strategy in which rich subjective quality information like
MOS, SOS and DOS are concurrently used to train the quality prediction network.
Powered by these two novel designs, RichIQA is able to predict the image
quality in terms of a distribution, from which the mean image quality can be
subsequently obtained. Extensive experimental results verify that the
three-stage network is tailored to predict rich quality information, while the
multi-label training strategy can fully exploit the potentials within
subjective quality rating and enhance the prediction performance and
generalizability of the network. RichIQA outperforms state-of-the-art
competitors on multiple large-scale in the wild IQA databases with rich
subjective rating labels. The code of RichIQA will be made publicly available
on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Educational Virtual Field Trips based on Social VR and 360° Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Surya Kalvakolu, Heinrich Söbke, Jannicke Baalsrud Hauge, Eckhard Kraft
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual field trips (VFTs) have proven to be valuable learning tools. Such
applications are mostly based on 360{\deg} technology and are to be
characterized as single-user applications in technological terms. In contrast,
Social VR applications are characterized by multi-user capability and
user-specific avatars. From a learning perspective, the concepts of
collaborative learning and embodiment have long been proposed as conducive to
learning. Both concepts might be supported using Social VR. However, little is
currently known about the use of Social VR for VFTs. Accordingly, the research
questions are to what extent VFTs can be implemented in Social VR environments
and how these Social VR-based VFTs are perceived by learners. This article
presents an evaluation study on the development and evaluation of a VFT
environment using the Social VR platform Mozilla Hubs. It describes the design
decisions to create the environment and evaluation results from a mixed-method
study (N=16) using a questionnaire and focus group discussions. The study
highlighted the opportunities offered by Social VR-based VFTs but also revealed
several challenges that need to be addressed to embrace the potential of Social
VR-based VFTs to be utilized regularly in education.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures, 1 table, submitted to Games and Learning Alliance
  Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Look One and More: Distilling Hybrid Order Relational Knowledge for
  Cross-Resolution Image Recognition <span class="chip">AAAI 2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiming Ge, Kangkai Zhang, Haolin Liu, Yingying Hua, Shengwei Zhao, Xin Jin, Hao Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In spite of great success in many image recognition tasks achieved by recent
deep models, directly applying them to recognize low-resolution images may
suffer from low accuracy due to the missing of informative details during
resolution degradation. However, these images are still recognizable for
subjects who are familiar with the corresponding high-resolution ones. Inspired
by that, we propose a teacher-student learning approach to facilitate
low-resolution image recognition via hybrid order relational knowledge
distillation. The approach refers to three streams: the teacher stream is
pretrained to recognize high-resolution images in high accuracy, the student
stream is learned to identify low-resolution images by mimicking the teacher's
behaviors, and the extra assistant stream is introduced as bridge to help
knowledge transfer across the teacher to the student. To extract sufficient
knowledge for reducing the loss in accuracy, the learning of student is
supervised with multiple losses, which preserves the similarities in various
order relational structures. In this way, the capability of recovering missing
details of familiar low-resolution images can be effectively enhanced, leading
to a better knowledge transfer. Extensive experiments on metric learning,
low-resolution image classification and low-resolution face recognition tasks
show the effectiveness of our approach, while taking reduced models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KAN-Based Fusion of Dual-Domain for Audio-Driven Facial Landmarks
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoang-Son Vo-Thanh, Quang-Vinh Nguyen, Soo-Hyung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven talking face generation is a widely researched topic due to its
high applicability. Reconstructing a talking face using audio significantly
contributes to fields such as education, healthcare, online conversations,
virtual assistants, and virtual reality. Early studies often focused solely on
changing the mouth movements, which resulted in outcomes with limited practical
applications. Recently, researchers have proposed a new approach of
constructing the entire face, including face pose, neck, and shoulders. To
achieve this, they need to generate through landmarks. However, creating stable
landmarks that align well with the audio is a challenge. In this paper, we
propose the KFusion of Dual-Domain model, a robust model that generates
landmarks from audio. We separate the audio into two distinct domains to learn
emotional information and facial context, then use a fusion mechanism based on
the KAN model. Our model demonstrates high efficiency compared to recent
models. This will lay the groundwork for the development of the audio-driven
talking face generation problem in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Offloading and Enhancement for Low-Light Video Analytics on
  Mobile Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanyi He, Peng Yang, Tian Qin, Jiawei Hou, Ning Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore adaptive offloading and enhancement strategies for
video analytics tasks on computing-constrained mobile devices in low-light
conditions. We observe that the accuracy of low-light video analytics varies
from different enhancement algorithms. The root cause could be the disparities
in the effectiveness of enhancement algorithms for feature extraction in
analytic models. Specifically, the difference in class activation maps (CAMs)
between enhanced and low-light frames demonstrates a positive correlation with
video analytics accuracy. Motivated by such observations, a novel enhancement
quality assessment method is proposed on CAMs to evaluate the effectiveness of
different enhancement algorithms for low-light videos. Then, we design a
multi-edge system, which adaptively offloads and enhances low-light video
analytics tasks from mobile devices. To achieve the trade-off between the
enhancement quality and the latency for all system-served mobile devices, we
propose a genetic-based scheduling algorithm, which can find a near-optimal
solution in a reasonable time to meet the latency requirement. Thereby, the
offloading strategies and the enhancement algorithms are properly selected
under the condition of limited end-edge bandwidth and edge computation
resources. Simulation experiments demonstrate the superiority of the proposed
system, improving accuracy up to 20.83\% compared to existing benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Auto-ACD: A Large-scale <span class="highlight-title">Dataset</span> for Audio-Language Representation
  Learning <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11500v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11500v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luoyi Sun, Xuenan Xu, Mengyue Wu, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the AI community has made significant strides in developing
powerful foundation models, driven by large-scale multimodal datasets. However,
for audio representation learning, existing datasets suffer from limitations in
the following aspects: insufficient volume, simplistic content, and arduous
collection procedures. To establish an audio dataset with high-quality
captions, we propose an innovative, automatic approach leveraging multimodal
inputs, such as video frames, audio streams. Specifically, we construct a
large-scale, high-quality, audio-language dataset, named as Auto-ACD,
comprising over 1.5M audio-text pairs. We exploit a series of pre-trained
models or APIs, to determine audio-visual synchronisation, generate image
captions, object detection, or audio tags for specific videos. Subsequently, we
employ LLM to paraphrase a congruent caption for each audio, guided by the
extracted multi-modality clues. To demonstrate the effectiveness of the
proposed dataset, we train widely used models on our dataset and show
performance improvement on various downstream tasks, for example,
audio-language retrieval, audio captioning, zero-shot classification. In
addition, we establish a novel benchmark with environmental information and
provide a benchmark for audio-text tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concept Conductor: Orchestrating Multiple Personalized Concepts in
  Text-to-Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.03632v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.03632v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zebin Yao, Fangxiang Feng, Ruifan Li, Xiaojie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The customization of text-to-image models has seen significant advancements,
yet generating multiple personalized concepts remains a challenging task.
Current methods struggle with attribute leakage and layout confusion when
handling multiple concepts, leading to reduced concept fidelity and semantic
consistency. In this work, we introduce a novel training-free framework,
Concept Conductor, designed to ensure visual fidelity and correct layout in
multi-concept customization. Concept Conductor isolates the sampling processes
of multiple custom models to prevent attribute leakage between different
concepts and corrects erroneous layouts through self-attention-based spatial
guidance. Additionally, we present a concept injection technique that employs
shape-aware masks to specify the generation area for each concept. This
technique injects the structure and appearance of personalized concepts through
feature fusion in the attention layers, ensuring harmony in the final image.
Extensive qualitative and quantitative experiments demonstrate that Concept
Conductor can consistently generate composite images with accurate layouts
while preserving the visual details of each concept. Compared to existing
baselines, Concept Conductor shows significant performance improvements. Our
method supports the combination of any number of concepts and maintains high
fidelity even when dealing with visually similar concepts. The code and models
are available at https://github.com/Nihukat/Concept-Conductor.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github Page: https://github.com/Nihukat/Concept-Conductor</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 360VFI: A <span class="highlight-title">Dataset</span> and Benchmark for Omnidirectional Video Frame
  Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14066v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14066v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Lu, Mengshun Hu, Yansheng Qiu, Liang Liao, Zheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Head-mounted 360{\deg} displays and portable 360{\deg} cameras have
significantly progressed, providing viewers a realistic and immersive
experience. However, many omnidirectional videos have low frame rates that can
lead to visual fatigue, and the prevailing plane frame interpolation
methodologies are unsuitable for omnidirectional video interpolation because
they are designed solely for traditional videos. This paper introduces the
benchmark dataset, 360VFI, for Omnidirectional Video Frame Interpolation. We
present a practical implementation that introduces a distortion prior from
omnidirectional video into the network to modulate distortions. Specifically,
we propose a pyramid distortion-sensitive feature extractor that uses the
unique characteristics of equirectangular projection (ERP) format as prior
information. Moreover, we devise a decoder that uses an affine transformation
to further facilitate the synthesis of intermediate frames. 360VFI is the first
dataset and benchmark that explores the challenge of Omnidirectional Video
Frame Interpolation. Through our benchmark analysis, we present four different
distortion condition scenes in the proposed 360VFI dataset to evaluate the
challenges triggered by distortion during interpolation. Besides, experimental
results demonstrate that Omnidirectional Video Interpolation can be effectively
improved by modeling for omnidirectional distortion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint version</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-08T00:00:00Z">2024-09-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OneGen: Efficient One-Pass Unified Generation and Retrieval for <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintian Zhang, Cheng Peng, Mengshu Sun, Xiang Chen, Lei Liang, Zhiqiang Zhang, Jun Zhou, Huajun Chen, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent advancements in Large Language Models (LLMs), which have
significantly enhanced the generative capabilities for various NLP tasks, LLMs
still face limitations in directly handling retrieval tasks. However, many
practical applications demand the seamless integration of both retrieval and
generation. This paper introduces a novel and efficient One-pass Generation and
retrieval framework (OneGen), designed to improve LLMs' performance on tasks
that require both generation and retrieval. The proposed framework bridges the
traditionally separate training approaches for generation and retrieval by
incorporating retrieval tokens generated autoregressively. This enables a
single LLM to handle both tasks simultaneously in a unified forward pass. We
conduct experiments on two distinct types of composite tasks, RAG and Entity
Linking, to validate the pluggability, effectiveness, and efficiency of OneGen
in training and inference. Furthermore, our results show that integrating
generation and retrieval within the same context preserves the generative
capabilities of LLMs while improving retrieval performance. To the best of our
knowledge, OneGen is the first to enable LLMs to conduct vector retrieval
during the generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress; code is available at
  https://github.com/zjunlp/OneGen</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential Recommendation via Adaptive Robust Attention with
  Multi-dimensional Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linsey Pang, Amir Hossein Raffiee, Wei Liu, Keld Lundgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation models have achieved state-of-the-art performance
using self-attention mechanism. It has since been found that moving beyond only
using item ID and positional embeddings leads to a significant accuracy boost
when predicting the next item. In recent literature, it was reported that a
multi-dimensional kernel embedding with temporal contextual kernels to capture
users' diverse behavioral patterns results in a substantial performance
improvement. In this study, we further improve the sequential recommender
model's robustness and generalization by introducing a mix-attention mechanism
with a layer-wise noise injection (LNI) regularization. We refer to our
proposed model as adaptive robust sequential recommendation framework (ADRRec),
and demonstrate through extensive experiments that our model outperforms
existing self-attention architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Keyword-driven Retrieval-Augmented Large Language Models for Cold-start
  User Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19612v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19612v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai-Dang Kieu, Minh Duc Nguyen, Thanh-Son Nguyen, Dung D. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have shown significant
potential in enhancing recommender systems. However, addressing the cold-start
recommendation problem, where users lack historical data, remains a
considerable challenge. In this paper, we introduce KALM4Rec (Keyword-driven
Retrieval-Augmented Large Language Models for Cold-start User Recommendations),
a novel framework specifically designed to tackle this problem by requiring
only a few input keywords from users in a practical scenario of cold-start user
restaurant recommendations. KALM4Rec operates in two main stages: candidates
retrieval and LLM-based candidates re-ranking. In the first stage,
keyword-driven retrieval models are used to identify potential candidates,
addressing LLMs' limitations in processing extensive tokens and reducing the
risk of generating misleading information. In the second stage, we employ LLMs
with various prompting strategies, including zero-shot and few-shot techniques,
to re-rank these candidates by integrating multiple examples directly into the
LLM prompts. Our evaluation, using a Yelp restaurant dataset with user reviews
from three English-speaking cities, shows that our proposed framework
significantly improves recommendation quality. Specifically, the integration of
in-context instructions with LLMs for re-ranking markedly enhances the
performance of the cold-start user recommender system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 10 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DREAM: A Dual Representation Learning Model for Multimodal
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangning Zhang, Yingjie Qin, Jiarui Jin, Yifan Liu, Ruilong Su, Weinan Zhang, Yong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal recommendation focuses primarily on effectively exploiting both
behavioral and multimodal information for the recommendation task. However,
most existing models suffer from the following issues when fusing information
from two different domains: (1) Previous works do not pay attention to the
sufficient utilization of modal information by only using direct concatenation,
addition, or simple linear layers for modal information extraction. (2)
Previous works treat modal features as learnable embeddings, which causes the
modal embeddings to gradually deviate from the original modal features during
learning. We refer to this issue as Modal Information Forgetting. (3) Previous
approaches fail to account for the significant differences in the distribution
between behavior and modality, leading to the issue of representation
misalignment. To address these challenges, this paper proposes a novel Dual
REpresentAtion learning model for Multimodal Recommendation called DREAM. For
sufficient information extraction, we introduce separate dual lines, including
Behavior Line and Modal Line, in which the Modal-specific Encoder is applied to
empower modal representations. To address the issue of Modal Information
Forgetting, we introduce the Similarity Supervised Signal to constrain the
modal representations. Additionally, we design a Behavior-Modal Alignment
module to fuse the dual representations through Intra-Alignment and
Inter-Alignment. Extensive experiments on three public datasets demonstrate
that the proposed DREAM method achieves state-of-the-art (SOTA) results. The
source code will be available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visual Grounding with Multi-modal Conditional Adaptation <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruilin Yao, Shengwu Xiong, Yichen Zhao, Yi Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual grounding is the task of locating objects specified by natural
language expressions. Existing methods extend generic object detection
frameworks to tackle this task. They typically extract visual and textual
features separately using independent visual and textual encoders, then fuse
these features in a multi-modal decoder for final prediction. However, visual
grounding presents unique challenges. It often involves locating objects with
different text descriptions within the same image. Existing methods struggle
with this task because the independent visual encoder produces identical visual
features for the same image, limiting detection performance. Some recently
approaches propose various language-guided visual encoders to address this
issue, but they mostly rely solely on textual information and require
sophisticated designs. In this paper, we introduce Multi-modal Conditional
Adaptation (MMCA), which enables the visual encoder to adaptively update
weights, directing its focus towards text-relevant regions. Specifically, we
first integrate information from different modalities to obtain multi-modal
embeddings. Then we utilize a set of weighting coefficients, which generated
from the multimodal embeddings, to reorganize the weight update matrices and
apply them to the visual encoder of the visual grounding model. Extensive
experiments on four widely used datasets demonstrate that MMCA achieves
significant improvements and state-of-the-art results. Ablation experiments
further demonstrate the lightweight and efficiency of our method. Our source
code is available at: https://github.com/Mr-Bigworth/MMCA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2024 [Oral]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating Indoor Scene Depth Maps from Ultrasonic Echoes <span class="chip">ICIP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junpei Honma, Akisato Kimura, Go Irie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measuring 3D geometric structures of indoor scenes requires dedicated depth
sensors, which are not always available. Echo-based depth estimation has
recently been studied as a promising alternative solution. All previous studies
have assumed the use of echoes in the audible range. However, one major problem
is that audible echoes cannot be used in quiet spaces or other situations where
producing audible sounds is prohibited. In this paper, we consider echo-based
depth estimation using inaudible ultrasonic echoes. While ultrasonic waves
provide high measurement accuracy in theory, the actual depth estimation
accuracy when ultrasonic echoes are used has remained unclear, due to its
disadvantage of being sensitive to noise and susceptible to attenuation. We
first investigate the depth estimation accuracy when the frequency of the sound
source is restricted to the high-frequency band, and found that the accuracy
decreased when the frequency was limited to ultrasonic ranges. Based on this
observation, we propose a novel deep learning method to improve the accuracy of
ultrasonic echo-based depth estimation by using audible echoes as auxiliary
data only during training. Experimental results with a public dataset
demonstrate that our method improves the estimation accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FakeBench: Probing Explainable Fake Image Detection via Large Multimodal
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13306v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13306v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Li, Xuelin Liu, Xiaoyang Wang, Bu Sung Lee, Shiqi Wang, Anderson Rocha, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to distinguish whether an image is generated by artificial
intelligence (AI) is a crucial ingredient in human intelligence, usually
accompanied by a complex and dialectical forensic and reasoning process.
However, current fake image detection models and databases focus on binary
classification without understandable explanations for the general populace.
This weakens the credibility of authenticity judgment and may conceal potential
model biases. Meanwhile, large multimodal models (LMMs) have exhibited immense
visual-text capabilities on various tasks, bringing the potential for
explainable fake image detection. Therefore, we pioneer the probe of LMMs for
explainable fake image detection by presenting a multimodal database
encompassing textual authenticity descriptions, the FakeBench. For
construction, we first introduce a fine-grained taxonomy of generative visual
forgery concerning human perception, based on which we collect forgery
descriptions in human natural language with a human-in-the-loop strategy.
FakeBench examines LMMs with four evaluation criteria: detection, reasoning,
interpretation and fine-grained forgery analysis, to obtain deeper insights
into image authenticity-relevant capabilities. Experiments on various LMMs
confirm their merits and demerits in different aspects of fake image detection
tasks. This research presents a paradigm shift towards transparency for the
fake image detection area and reveals the need for greater emphasis on forensic
elements in visual-language research and AI risk control. FakeBench will be
available at https://github.com/Yixuan423/FakeBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DREAM: A Dual Representation Learning Model for Multimodal
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangning Zhang, Yingjie Qin, Jiarui Jin, Yifan Liu, Ruilong Su, Weinan Zhang, Yong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal recommendation focuses primarily on effectively exploiting both
behavioral and multimodal information for the recommendation task. However,
most existing models suffer from the following issues when fusing information
from two different domains: (1) Previous works do not pay attention to the
sufficient utilization of modal information by only using direct concatenation,
addition, or simple linear layers for modal information extraction. (2)
Previous works treat modal features as learnable embeddings, which causes the
modal embeddings to gradually deviate from the original modal features during
learning. We refer to this issue as Modal Information Forgetting. (3) Previous
approaches fail to account for the significant differences in the distribution
between behavior and modality, leading to the issue of representation
misalignment. To address these challenges, this paper proposes a novel Dual
REpresentAtion learning model for Multimodal Recommendation called DREAM. For
sufficient information extraction, we introduce separate dual lines, including
Behavior Line and Modal Line, in which the Modal-specific Encoder is applied to
empower modal representations. To address the issue of Modal Information
Forgetting, we introduce the Similarity Supervised Signal to constrain the
modal representations. Additionally, we design a Behavior-Modal Alignment
module to fuse the dual representations through Intra-Alignment and
Inter-Alignment. Extensive experiments on three public datasets demonstrate
that the proposed DREAM method achieves state-of-the-art (SOTA) results. The
source code will be available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-07T00:00:00Z">2024-09-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporate <span class="highlight-title">LLM</span>s with Influential Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingze Wang, Shuxian Bi, Wenjie Wang, Chongming Gao, Yangyang Li, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have achieved increasing accuracy over the years.
However, this precision often leads users to narrow their interests, resulting
in issues such as limited diversity and the creation of echo chambers. Current
research addresses these challenges through proactive recommender systems by
recommending a sequence of items (called influence path) to guide user interest
in the target item. However, existing methods struggle to construct a coherent
influence path that builds up with items the user is likely to enjoy. In this
paper, we leverage the Large Language Model's (LLMs) exceptional ability for
path planning and instruction following, introducing a novel approach named
LLM-based Influence Path Planning (LLM-IPP). Our approach maintains coherence
between consecutive recommendations and enhances user acceptability of the
recommended items. To evaluate LLM-IPP, we implement various user simulators
and metrics to measure user acceptability and path coherence. Experimental
results demonstrate that LLM-IPP significantly outperforms traditional
proactive recommender systems. This study pioneers the integration of LLMs into
proactive recommender systems, offering a reliable and user-engaging
methodology for future recommendation technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debias Can be Unreliable: Mitigating Bias Issue in Evaluating Debiasing
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengbing Wang, Wentao Shi, Jizhi Zhang, Wenjie Wang, Hang Pan, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has improved recommendation models remarkably by equipping them
with debiasing methods. Due to the unavailability of fully-exposed datasets,
most existing approaches resort to randomly-exposed datasets as a proxy for
evaluating debiased models, employing traditional evaluation scheme to
represent the recommendation performance. However, in this study, we reveal
that traditional evaluation scheme is not suitable for randomly-exposed
datasets, leading to inconsistency between the Recall performance obtained
using randomly-exposed datasets and that obtained using fully-exposed datasets.
Such inconsistency indicates the potential unreliability of experiment
conclusions on previous debiasing techniques and calls for unbiased Recall
evaluation using randomly-exposed datasets. To bridge the gap, we propose the
Unbiased Recall Evaluation (URE) scheme, which adjusts the utilization of
randomly-exposed datasets to unbiasedly estimate the true Recall performance on
fully-exposed datasets. We provide theoretical evidence to demonstrate the
rationality of URE and perform extensive experiments on real-world datasets to
validate its soundness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Günther, Isabelle Mohr, Bo Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many use cases require retrieving smaller portions of text, and dense
vector-based retrieval systems often perform better with shorter text segments,
as the semantics are less likely to be "over-compressed" in the embeddings.
Consequently, practitioners often split text documents into smaller chunks and
encode them separately. However, chunk embeddings created in this way can lose
contextual information from surrounding chunks, resulting in suboptimal
representations. In this paper, we introduce a novel method called "late
chunking," which leverages long context embedding models to first embed all
tokens of the long text, with chunking applied after the transformer model and
just before mean pooling. The resulting chunk embeddings capture the full
contextual information, leading to superior results across various retrieval
tasks without the need for additional training. Moreover, our method is generic
enough to be applied to any long-context embedding model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, early draft</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoST: Contrastive Quantization based Semantic Tokenization for
  Generative Recommendation <span class="chip">RecSys'2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieming Zhu, Mengqun Jin, Qijiong Liu, Zexuan Qiu, Zhenhua Dong, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embedding-based retrieval serves as a dominant approach to candidate item
matching for industrial recommender systems. With the success of generative AI,
generative retrieval has recently emerged as a new retrieval paradigm for
recommendation, which casts item retrieval as a generation problem. Its model
consists of two stages: semantic tokenization and autoregressive generation.
The first stage involves item tokenization that constructs discrete semantic
tokens to index items, while the second stage autoregressively generates
semantic tokens of candidate items. Therefore, semantic tokenization serves as
a crucial preliminary step for training generative recommendation models.
Existing research usually employs a vector quantizier with reconstruction loss
(e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to
capture the essential neighborhood relationships that are vital for effective
item modeling in recommender systems. In this paper, we propose a contrastive
quantization-based semantic tokenization approach, named CoST, which harnesses
both item relationships and semantic information to learn semantic tokens. Our
experimental results highlight the significant impact of semantic tokenization
on generative recommendation performance, with CoST achieving up to a 43%
improvement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over
previous baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by RecSys'2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implementing Streaming algorithm and k-means clusters to RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21300v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21300v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Kang, Yuzhou Zhu, Yukun Zhong, Ke Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has achieved significant success in
information retrieval to assist large language models LLMs because it builds an
external knowledge database. However, it also has many problems, it consumes a
lot of memory because of the enormous database, and it cannot update the
established index database in time when confronted with massive streaming data.
To reduce the memory required for building the database and maintain accuracy
simultaneously, we proposed a new approach integrating a streaming algorithm
with k-means clustering into RAG. Our approach applied a streaming algorithm to
update the index dynamically and reduce memory consumption. Additionally, the
k-means algorithm clusters highly similar documents, and the query time would
be shortened. We conducted comparative experiments on four methods, and the
results indicated that RAG with streaming algorithm and k-means clusters
outperforms traditional RAG in accuracy and memory, particularly when dealing
with large-scale streaming data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Editing for Video Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Zhu, Kevin Flanagan, Adriano Fragomeni, Michael Wray, Dima Damen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though pre-training vision-language models have demonstrated significant
benefits in boosting video-text retrieval performance from large-scale web
videos, fine-tuning still plays a critical role with manually annotated clips
with start and end times, which requires considerable human effort. To address
this issue, we explore an alternative cheaper source of annotations, single
timestamps, for video-text retrieval. We initialise clips from timestamps in a
heuristic way to warm up a retrieval model. Then a video clip editing method is
proposed to refine the initial rough boundaries to improve retrieval
performance. A student-teacher network is introduced for video clip editing.
The teacher model is employed to edit the clips in the training set whereas the
student model trains on the edited clips. The teacher weights are updated from
the student's after the student's performance increases. Our method is model
agnostic and applicable to any retrieval models. We conduct experiments based
on three state-of-the-art retrieval models, COOT, VideoCLIP and CLIP4Clip.
Experiments conducted on three video retrieval datasets, YouCook2, DiDeMo and
ActivityNet-Captions show that our edited clips consistently improve retrieval
performance over initial clips across all the three retrieval models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reformulating Conversational Recommender Systems as Tri-Phase Offline
  Policy Learning <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06809v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06809v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gangyi Zhang, Chongming Gao, Hang Pan, Runzhe Teng, Ruizhe Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Conversational Recommender Systems (CRS) predominantly utilize user
simulators for training and evaluating recommendation policies. These
simulators often oversimplify the complexity of user interactions by focusing
solely on static item attributes, neglecting the rich, evolving preferences
that characterize real-world user behavior. This limitation frequently leads to
models that perform well in simulated environments but falter in actual
deployment. Addressing these challenges, this paper introduces the Tri-Phase
Offline Policy Learning-based Conversational Recommender System (TCRS), which
significantly reduces dependency on real-time interactions and mitigates
overfitting issues prevalent in traditional approaches. TCRS integrates a
model-based offline learning strategy with a controllable user simulation that
dynamically aligns with both personalized and evolving user preferences.
Through comprehensive experiments, TCRS demonstrates enhanced robustness,
adaptability, and accuracy in recommendations, outperforming traditional CRS
models in diverse user scenarios. This approach not only provides a more
realistic evaluation environment but also facilitates a deeper understanding of
user behavior dynamics, thereby refining the recommendation process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Tree-based Retrieval for Efficient Recommendation: Theory and
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11345v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11345v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ze Liu, Jin Zhang, Chao Feng, Defu Lian, Jie Wang, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of deep learning techniques, deep recommendation models
also achieve remarkable improvements in terms of recommendation accuracy.
However, due to the large number of candidate items in practice and the high
cost of preference computation, these methods also suffer from low efficiency
of recommendation. The recently proposed tree-based deep recommendation models
alleviate the problem by directly learning tree structure and representations
under the guidance of recommendation objectives. However, such models have
shortcomings. The max-heap assumption in the hierarchical tree, in which the
preference for a parent node should be the maximum between the preferences for
its children, is difficult to satisfy in their binary classification
objectives. To this end, we propose Tree-based Deep Retrieval (TDR for short)
for efficient recommendation. In TDR, all the trees generated during the
training process are retained to form the forest. When learning the node
representation of each tree, we have to satisfy the max-heap assumption as much
as possible and mimic beam search behavior over the tree in the training stage.
This is achieved by TDR to regard the training task as multi-classification
over tree nodes at the same level. However, the number of tree nodes grows
exponentially with levels, making us train the preference model with the
guidance of the sampled-softmax technique. The experiments are conducted on
real-world datasets, validating the effectiveness of the proposed preference
model learning method and tree learning method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ YouTube Videos for Public Health Literacy? A Machine Learning Pipeline
  to Curate Covid-19 Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09425v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09425v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yawen Guo, Xiao Liu, Anjana Susarla, Rema Padman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The COVID-19 pandemic has highlighted the dire necessity to improve public
health literacy for societal resilience. YouTube, the largest video-sharing
social media platform, provides a vast repository of user-generated health
information in a multi-media-rich format which may be easier for the public to
understand and use if major concerns about content quality and accuracy are
addressed. This study develops an automated solution to identify, retrieve and
shortlist medically relevant and understandable YouTube videos that domain
experts can subsequently review and recommend for disseminating and educating
the public on the COVID-19 pandemic and similar public health outbreaks. Our
approach leverages domain knowledge from human experts and machine learning and
natural language processing methods to provide a scalable, replicable, and
generalizable approach that can also be applied to enhance the management of
many health conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Studies in health technology and informatics(MedInfo) 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Speech Enhancement Using Burst Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.03275v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.03275v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsin Raza, Leandro A. Passos, Ahmed Khubaib, Ahsan Adeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes the MBURST, a novel multimodal solution for audio-visual
speech enhancements that consider the most recent neurological discoveries
regarding pyramidal cells of the prefrontal cortex and other brain regions. The
so-called burst propagation implements several criteria to address the credit
assignment problem in a more biologically plausible manner: steering the sign
and magnitude of plasticity through feedback, multiplexing the feedback and
feedforward information across layers through different weight connections,
approximating feedback and feedforward connections, and linearizing the
feedback signals. MBURST benefits from such capabilities to learn correlations
between the noisy signal and the visual stimuli, thus attributing meaning to
the speech by amplifying relevant information and suppressing noise.
Experiments conducted over a Grid Corpus and CHiME3-based dataset show that
MBURST can reproduce similar mask reconstructions to the multimodal
backpropagation-based baseline while demonstrating outstanding energy
efficiency management, reducing the neuron firing rates to values up to
\textbf{$70\%$} lower. Such a feature implies more sustainable implementations,
suitable and desirable for hearing aids or any other similar embedded systems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-06T00:00:00Z">2024-09-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">14</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preserving Individuality while Following the Crowd: Understanding the
  Role of User Taste and Crowd Wisdom in Online Product Rating Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Wang, Shubham Jain, Yingtong Dou, Junpeng Wang, Chin-Chia Michael Yeh, Yujie Fan, Prince Aboagye, Yan Zheng, Xin Dai, Zhongfang Zhuang, Uday Singh Saini, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous algorithms have been developed for online product rating prediction,
but the specific influence of user and product information in determining the
final prediction score remains largely unexplored. Existing research often
relies on narrowly defined data settings, which overlooks real-world challenges
such as the cold-start problem, cross-category information utilization, and
scalability and deployment issues. To delve deeper into these aspects, and
particularly to uncover the roles of individual user taste and collective
wisdom, we propose a unique and practical approach that emphasizes historical
ratings at both the user and product levels, encapsulated using a continuously
updated dynamic tree representation. This representation effectively captures
the temporal dynamics of users and products, leverages user information across
product categories, and provides a natural solution to the cold-start problem.
Furthermore, we have developed an efficient data processing strategy that makes
this approach highly scalable and easily deployable. Comprehensive experiments
in real industry settings demonstrate the effectiveness of our approach.
Notably, our findings reveal that individual taste dominates over collective
wisdom in online product rating prediction, a perspective that contrasts with
the commonly observed wisdom of the crowd phenomenon in other domains. This
dominance of individual user taste is consistent across various model types,
including the boosting tree model, recurrent neural network (RNN), and
transformer-based architectures. This observation holds true across the overall
population, within individual product categories, and in cold-start scenarios.
Our findings underscore the significance of individual user tastes in the
context of online product rating prediction and the robustness of our approach
across different model architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Framework for Cross-Domain Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangxia Cao, Shen Wang, Gaode Chen, Rui Huang, Shuang Yang, Zhaojie Liu, Guorui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In addressing the persistent challenges of data-sparsity and cold-start
issues in domain-expert recommender systems, Cross-Domain Recommendation (CDR)
emerges as a promising methodology. CDR aims at enhancing prediction
performance in the target domain by leveraging interaction knowledge from
related source domains, particularly through users or items that span across
multiple domains (e.g., Short-Video and Living-Room). For academic research
purposes, there are a number of distinct aspects to guide CDR method designing,
including the auxiliary domain number, domain-overlapped element, user-item
interaction types, and downstream tasks. With so many different CDR combination
scenario settings, the proposed scenario-expert approaches are tailored to
address a specific vertical CDR scenario, and often lack the capacity to adapt
to multiple horizontal scenarios. In an effect to coherently adapt to various
scenarios, and drawing inspiration from the concept of domain-invariant
transfer learning, we extend the former SOTA model UniCDR in five different
aspects, named as UniCDR+. Our work was successfully deployed on the Kuaishou
Living-Room RecSys.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Knowledge Organization Systems of Research Fields: Resources
  and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angelo Salatino, Tanay Aggarwal, Andrea Mannocci, Francesco Osborne, Enrico Motta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Organization Systems (KOSs), such as term lists, thesauri,
taxonomies, and ontologies, play a fundamental role in categorising, managing,
and retrieving information. In the academic domain, KOSs are often adopted for
representing research areas and their relationships, primarily aiming to
classify research articles, academic courses, patents, books, scientific
venues, domain experts, grants, software, experiment materials, and several
other relevant products and agents. These structured representations of
research areas, widely embraced by many academic fields, have proven effective
in empowering AI-based systems to i) enhance retrievability of relevant
documents, ii) enable advanced analytic solutions to quantify the impact of
academic research, and iii) analyse and forecast research dynamics. This paper
aims to present a comprehensive survey of the current KOS for academic
disciplines. We analysed and compared 45 KOSs according to five main
dimensions: scope, structure, curation, usage, and links to other KOSs. Our
results reveal a very heterogeneous scenario in terms of scope, scale, quality,
and usage, highlighting the need for more integrated solutions for representing
research knowledge across academic fields. We conclude by discussing the main
challenges and the most promising future directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Fair is Your Diffusion Recommender Model? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Malitesta, Giacomo Medda, Erasmo Purificato, Ludovico Boratto, Fragkiskos D. Malliaros, Mirko Marras, Ernesto William De Luca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based recommender systems have recently proven to outperform
traditional generative recommendation approaches, such as variational
autoencoders and generative adversarial networks. Nevertheless, the machine
learning literature has raised several concerns regarding the possibility that
diffusion models, while learning the distribution of data samples, may
inadvertently carry information bias and lead to unfair outcomes. In light of
this aspect, and considering the relevance that fairness has held in
recommendations over the last few decades, we conduct one of the first fairness
investigations in the literature on DiffRec, a pioneer approach in
diffusion-based recommendation. First, we propose an experimental setting
involving DiffRec (and its variant L-DiffRec) along with nine state-of-the-art
recommendation models, two popular recommendation datasets from the
fairness-aware literature, and six metrics accounting for accuracy and
consumer/provider fairness. Then, we perform a twofold analysis, one assessing
models' performance under accuracy and recommendation fairness separately, and
the other identifying if and to what extent such metrics can strike a
performance trade-off. Experimental results from both studies confirm the
initial unfairness warnings but pave the way for how to address them in future
research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Sequential Music Recommendation with Personalized Popularity
  Awareness <span class="chip">RecSys'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Abbattista, Vito Walter Anelli, Tommaso Di Noia, Craig Macdonald, Aleksandr Vladimirovich Petrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of music recommendation, sequential recommender systems have
shown promise in capturing the dynamic nature of music consumption.
Nevertheless, traditional Transformer-based models, such as SASRec and
BERT4Rec, while effective, encounter challenges due to the unique
characteristics of music listening habits. In fact, existing models struggle to
create a coherent listening experience due to rapidly evolving preferences.
Moreover, music consumption is characterized by a prevalence of repeated
listening, i.e., users frequently return to their favourite tracks, an
important signal that could be framed as individual or personalized popularity.
  This paper addresses these challenges by introducing a novel approach that
incorporates personalized popularity information into sequential
recommendation. By combining user-item popularity scores with model-generated
scores, our method effectively balances the exploration of new music with the
satisfaction of user preferences. Experimental results demonstrate that a
Personalized Most Popular recommender, a method solely based on user-specific
popularity, outperforms existing state-of-the-art models. Furthermore,
augmenting Transformer-based models with personalized popularity awareness
yields superior performance, showing improvements ranging from 25.2% to 69.8%.
The code for this paper is available at
https://github.com/sisinflab/personalized-popularity-awareness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by RecSys'24 as an LBR paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WarpAdam: A new Adam optimizer based on Meta-Learning approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengxi Pan, Junshang Chen, Jingrui Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal selection of optimization algorithms is crucial for training deep
learning models. The Adam optimizer has gained significant attention due to its
efficiency and wide applicability. However, to enhance the adaptability of
optimizers across diverse datasets, we propose an innovative optimization
strategy by integrating the 'warped gradient descend'concept from Meta Learning
into the Adam optimizer. In the conventional Adam optimizer, gradients are
utilized to compute estimates of gradient mean and variance, subsequently
updating model parameters. Our approach introduces a learnable distortion
matrix, denoted as P, which is employed for linearly transforming gradients.
This transformation slightly adjusts gradients during each iteration, enabling
the optimizer to better adapt to distinct dataset characteristics. By learning
an appropriate distortion matrix P, our method aims to adaptively adjust
gradient information across different data distributions, thereby enhancing
optimization performance. Our research showcases the potential of this novel
approach through theoretical insights and empirical evaluations. Experimental
results across various tasks and datasets validate the superiority of our
optimizer that integrates the 'warped gradient descend' concept in terms of
adaptability. Furthermore, we explore effective strategies for training the
adaptation matrix P and identify scenarios where this method can yield optimal
results. In summary, this study introduces an innovative approach that merges
the 'warped gradient descend' concept from Meta Learning with the Adam
optimizer. By introducing a learnable distortion matrix P within the optimizer,
we aim to enhance the model's generalization capability across diverse data
distributions, thus opening up new possibilities in the field of deep learning
optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Refining Wikidata Taxonomy using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Peng, Thomas Bonald, Mehwish Alam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to its collaborative nature, Wikidata is known to have a complex
taxonomy, with recurrent issues like the ambiguity between instances and
classes, the inaccuracy of some taxonomic paths, the presence of cycles, and
the high level of redundancy across classes. Manual efforts to clean up this
taxonomy are time-consuming and prone to errors or subjective decisions. We
present WiKC, a new version of Wikidata taxonomy cleaned automatically using a
combination of Large Language Models (LLMs) and graph mining techniques.
Operations on the taxonomy, such as cutting links or merging classes, are
performed with the help of zero-shot prompting on an open-source LLM. The
quality of the refined taxonomy is evaluated from both intrinsic and extrinsic
perspectives, on a task of entity typing for the latter, showing the practical
interest of WiKC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM International Conference on Information and Knowledge Management,
  Oct 2024, Boise, Idaho, United States</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FiNER-ORD: Financial Named <span class="highlight-title">Entity</span> Recognition Open Research <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agam Shah, Abhinav Gullapalli, Ruchit Vithani, Michael Galarnyk, Sudheer Chava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last two decades, the development of the CoNLL-2003 named entity
recognition (NER) dataset has helped enhance the capabilities of deep learning
and natural language processing (NLP). The finance domain, characterized by its
unique semantic and lexical variations for the same entities, presents specific
challenges to the NER task; thus, a domain-specific customized dataset is
crucial for advancing research in this field. In our work, we develop the first
high-quality English Financial NER Open Research Dataset (FiNER-ORD). We
benchmark multiple pre-trained language models (PLMs) and large-language models
(LLMs) on FiNER-ORD. We believe our proposed FiNER-ORD dataset will open future
opportunities to use FiNER-ORD as a benchmark for financial domain-specific NER
and NLP tasks. Our dataset, models, and code are publicly available on GitHub
and Hugging Face under CC BY-NC 4.0 license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GraphEx: A Graph-based <span class="highlight-title">Extraction</span> Method for Advertiser Keyphrase
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03140v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03140v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashirbad Mishra, Soumik Dey, Marshall Wu, Jinyu Zhao, He Yu, Kaichen Ni, Binbin Li, Kamesh Madduri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online sellers and advertisers are recommended keyphrases for their listed
products, which they bid on to enhance their sales. One popular paradigm that
generates such recommendations is Extreme Multi-Label Classification (XMC),
which involves tagging/mapping keyphrases to items. We outline the limitations
of using traditional item-query based tagging or mapping techniques for
keyphrase recommendations on E-Commerce platforms. We introduce GraphEx, an
innovative graph-based approach that recommends keyphrases to sellers using
extraction of token permutations from item titles. Additionally, we demonstrate
that relying on traditional metrics such as precision/recall can be misleading
in practical applications, thereby necessitating a combination of metrics to
evaluate performance in real-world scenarios. These metrics are designed to
assess the relevance of keyphrases to items and the potential for buyer
outreach. GraphEx outperforms production models at eBay, achieving the
objectives mentioned above. It supports near real-time inferencing in
resource-constrained production environments and scales effectively for
billions of items.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Topic Classification of Column Headers: Leveraging <span class="highlight-title">LLM</span>s for
  Metadata Enrichment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00884v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00884v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Margherita Martorana, Tobias Kuhn, Lise Stork, Jacco van Ossenbruggen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional dataset retrieval systems rely on metadata for indexing, rather
than on the underlying data values. However, high-quality metadata creation and
enrichment often require manual annotations, which is a labour-intensive and
challenging process to automate. In this study, we propose a method to support
metadata enrichment using topic annotations generated by three Large Language
Models (LLMs): ChatGPT-3.5, GoogleBard, and GoogleGemini. Our analysis focuses
on classifying column headers based on domain-specific topics from the
Consortium of European Social Science Data Archives (CESSDA), a Linked Data
controlled vocabulary. Our approach operates in a zero-shot setting,
integrating the controlled topic vocabulary directly within the input prompt.
This integration serves as a Large Context Windows approach, with the aim of
improving the results of the topic classification task.
  We evaluated the performance of the LLMs in terms of internal consistency,
inter-machine alignment, and agreement with human classification. Additionally,
we investigate the impact of contextual information (i.e., dataset description)
on the classification outcomes. Our findings suggest that ChatGPT and
GoogleGemini outperform GoogleBard in terms of internal consistency as well as
LLM-human-agreement. Interestingly, we found that contextual information had no
significant impact on LLM performance.
  This work proposes a novel approach that leverages LLMs for topic
classification of column headers using a controlled vocabulary, presenting a
practical application of LLMs and Large Context Windows within the Semantic Web
domain. This approach has the potential to facilitate automated metadata
enrichment, thereby enhancing dataset retrieval and the Findability,
Accessibility, Interoperability, and Reusability (FAIR) of research data on the
Web.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAG based Question-Answering for Contextual Response Prediction System <span class="chip">CIKM'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sriram Veturi, Saurabh Vaichal, Reshma Lal Jagadheesh, Nafis Irtiza Tripto, Nian Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown versatility in various Natural
Language Processing (NLP) tasks, including their potential as effective
question-answering systems. However, to provide precise and relevant
information in response to specific customer queries in industry settings, LLMs
require access to a comprehensive knowledge base to avoid hallucinations.
Retrieval Augmented Generation (RAG) emerges as a promising technique to
address this challenge. Yet, developing an accurate question-answering
framework for real-world applications using RAG entails several challenges: 1)
data availability issues, 2) evaluating the quality of generated content, and
3) the costly nature of human evaluation. In this paper, we introduce an
end-to-end framework that employs LLMs with RAG capabilities for industry use
cases. Given a customer query, the proposed system retrieves relevant knowledge
documents and leverages them, along with previous chat history, to generate
response suggestions for customer service agents in the contact centers of a
major retail company. Through comprehensive automated and human evaluations, we
show that this solution outperforms the current BERT-based algorithms in
accuracy and relevance. Our findings suggest that RAG-based LLMs can be an
excellent support to human customer service representatives by lightening their
workload.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 1st Workshop on GenAI and RAG Systems for Enterprise,
  CIKM'24. 6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid Semantic Search: Unveiling User Intent Beyond Keywords 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09236v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09236v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Ahluwalia, Bishwajit Sutradhar, Karishma Ghosh, Indrapal Yadav, Arpan Sheetal, Prashant Patil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the limitations of traditional keyword-based search in
understanding user intent and introduces a novel hybrid search approach that
leverages the strengths of non-semantic search engines, Large Language Models
(LLMs), and embedding models. The proposed system integrates keyword matching,
semantic vector embeddings, and LLM-generated structured queries to deliver
highly relevant and contextually appropriate search results. By combining these
complementary methods, the hybrid approach effectively captures both explicit
and implicit user intent.The paper further explores techniques to optimize
query execution for faster response times and demonstrates the effectiveness of
this hybrid search model in producing comprehensive and accurate search
outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hi-Gen: Generative Retrieval For Large-Scale Personalized E-commerce
  Search <span class="chip">ICDM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjing Wu, Yinfu Feng, Jian Wang, Wenji Zhou, Yunan Ye, Rong Xiao, Jun Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging generative retrieval (GR) techniques to enhance search systems is
an emerging methodology that has shown promising results in recent years. In
GR, a text-to-text model maps string queries directly to relevant document
identifiers (docIDs), dramatically simplifying the retrieval process. However,
when applying most GR models in large-scale E-commerce for personalized item
search, we must face two key problems in encoding and decoding. (1) Existing
docID generation methods ignore the encoding of efficiency information, which
is critical in E-commerce. (2) The positional information is important in
decoding docIDs, while prior studies have not adequately discriminated the
significance of positional information or well exploited the inherent
interrelation among these positions. To overcome these problems, we introduce
an efficient Hierarchical encoding-decoding Generative retrieval method
(Hi-Gen) for large-scale personalized E-commerce search systems. Specifically,
we first design a representation learning model using metric learning to learn
discriminative feature representations of items to capture semantic relevance
and efficiency information. Then, we propose a category-guided hierarchical
clustering scheme that makes full use of the semantic and efficiency
information of items to facilitate docID generation. Finally, we design a
position-aware loss to discriminate the importance of positions and mine the
inherent interrelation between different tokens at the same position. This loss
boosts the performance of the language model used in the decoding stage.
Besides, we propose two variants of Hi-Gen (Hi-Gen-I2I and Hi-Gen-Cluster) to
support online real-time large-scale recall in the online serving process.
Hi-Gen gets 3.30% and 4.62% improvements over SOTA for Recall@1 on the public
and industry datasets, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICDM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Med<span class="highlight-title">Prompt</span>Extract (Medical Data <span class="highlight-title">Extraction</span> Tool): Anonymization and
  Hi-fidelity Automated data <span class="highlight-title">extraction</span> using NLP and <span class="highlight-title">prompt</span> engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02664v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02664v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roomani Srivastava, Suraj Prasad, Lipika Bhat, Sarvesh Deshpande, Barnali Das, Kshitij Jadhav
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Introduction: The labour-intensive nature of data extraction from sources
like discharge summaries (DS) poses significant obstacles to the digitisation
of medical records particularly for low- and middle-income countries (LMICs).
In this paper we present a completely automated method MedPromptExtract to
efficiently extract data from DS while maintaining confidentiality. Methods:
The source of data was Discharge Summaries (DS) from Kokilaben Dhirubhai Ambani
Hospital (KDAH) of patients having Acute Kidney Injury (AKI). A pre-existing
tool EIGEN which leverages semi-supervised learning techniques for
high-fidelity information extraction was used to anonymize the DS, Natural
Language Processing (NLP) was used to extract data from regular fields. We used
Prompt Engineering and Large Language Model(LLM) to extract custom clinical
information from free flowing text describing the patients stay in the
hospital. Twelve features associated with occurrence of AKI were extracted. The
LLM responses were validated against clinicians annotations. Results: The
MedPromptExtracttool first subjected DS to the anonymization pipeline which
took three seconds per summary. Successful anonymization was verified by
clinicians, thereafter NLP pipeline extracted structured text from the
anonymized pdfs at the rate of 0.2 seconds per summary with 100%
accuracy.Finally DS were analysed by the LLM pipeline using Gemini Pro for the
twelve features. Accuracy metrics were calculated by comparing model responses
to clinicians annotations with seven features achieving AUCs above 0.9,
indicating high fidelity of the extraction process. Conclusion:
MedPromptExtract serves as an automated adaptable tool for efficient data
extraction from medical records with a dynamic user interface. Keywords:
Digitizing Medical Records, Automated Anonymisation, Information Retrieval,
Large Language Models, Prompt Engineering
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-GP-LMVIC: Learning-based Multi-View Image Coding with 3D Gaussian
  Geometric Priors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujun Huang, Bin Chen, Niu Lian, Baoyi An, Shu-Tao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view image compression is vital for 3D-related applications. To
effectively model correlations between views, existing methods typically
predict disparity between two views on a 2D plane, which works well for small
disparities, such as in stereo images, but struggles with larger disparities
caused by significant view changes. To address this, we propose a novel
approach: learning-based multi-view image coding with 3D Gaussian geometric
priors (3D-GP-LMVIC). Our method leverages 3D Gaussian Splatting to derive
geometric priors of the 3D scene, enabling more accurate disparity estimation
across views within the compression model. Additionally, we introduce a depth
map compression model to reduce redundancy in geometric information between
views. A multi-view sequence ordering method is also proposed to enhance
correlations between adjacent views. Experimental results demonstrate that
3D-GP-LMVIC surpasses both traditional and learning-based methods in
performance, while maintaining fast encoding and decoding speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19pages, 8 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MSLIQA: Enhancing Learning Representations for Image Quality Assessment
  through Multi-Scale Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nasim Jamshidi Avanaki, Abhijay Ghildyal, Nabajeet Barman, Saman Zadtootaghaj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  No-Reference Image Quality Assessment (NR-IQA) remains a challenging task due
to the diversity of distortions and the lack of large annotated datasets. Many
studies have attempted to tackle these challenges by developing more accurate
NR-IQA models, often employing complex and computationally expensive networks,
or by bridging the domain gap between various distortions to enhance
performance on test datasets. In our work, we improve the performance of a
generic lightweight NR-IQA model by introducing a novel augmentation strategy
that boosts its performance by almost 28\%. This augmentation strategy enables
the network to better discriminate between different distortions in various
parts of the image by zooming in and out. Additionally, the inclusion of
test-time augmentation further enhances performance, making our lightweight
network's results comparable to the current state-of-the-art models, simply
through the use of augmentations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LAR-IQA: A Lightweight, Accurate, and Robust No-Reference Image Quality
  Assessment Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nasim Jamshidi Avanaki, Abhijay Ghildyal, Nabajeet Barman, Saman Zadtootaghaj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in the field of No-Reference Image Quality Assessment
(NR-IQA) using deep learning techniques demonstrate high performance across
multiple open-source datasets. However, such models are typically very large
and complex making them not so suitable for real-world deployment, especially
on resource- and battery-constrained mobile devices. To address this
limitation, we propose a compact, lightweight NR-IQA model that achieves
state-of-the-art (SOTA) performance on ECCV AIM UHD-IQA challenge validation
and test datasets while being also nearly 5.7 times faster than the fastest
SOTA model. Our model features a dual-branch architecture, with each branch
separately trained on synthetically and authentically distorted images which
enhances the model's generalizability across different distortion types. To
improve robustness under diverse real-world visual conditions, we additionally
incorporate multiple color spaces during the training process. We also
demonstrate the higher accuracy of recently proposed Kolmogorov-Arnold Networks
(KANs) for final quality regression as compared to the conventional Multi-Layer
Perceptrons (MLPs). Our evaluation considering various open-source datasets
highlights the practical, high-accuracy, and robust performance of our proposed
lightweight model. Code: https://github.com/nasimjamshidi/LAR-IQA.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-05T00:00:00Z">2024-09-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RETAIN: Interactive Tool for Regression Testing Guided <span class="highlight-title">LLM</span> Migration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03928v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03928v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tanay Dixit, Daniel Lee, Sally Fang, Sai Sree Harsha, Anirudh Sureshan, Akash Maharaj, Yunyao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly integrated into diverse
applications. The rapid evolution of LLMs presents opportunities for developers
to enhance applications continuously. However, this constant adaptation can
also lead to performance regressions during model migrations. While several
interactive tools have been proposed to streamline the complexity of prompt
engineering, few address the specific requirements of regression testing for
LLM Migrations. To bridge this gap, we introduce RETAIN (REgression Testing
guided LLM migrAtIoN), a tool designed explicitly for regression testing in LLM
Migrations. RETAIN comprises two key components: an interactive interface
tailored to regression testing needs during LLM migrations, and an error
discovery module that facilitates understanding of differences in model
behaviors. The error discovery module generates textual descriptions of various
errors or differences between model outputs, providing actionable insights for
prompt refinement. Our automatic evaluation and empirical user studies
demonstrate that RETAIN, when compared to manual evaluation, enabled
participants to identify twice as many errors, facilitated experimentation with
75% more prompts, and achieves 12% higher metric scores in a given time frame.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HGAMN: Heterogeneous Graph Attention Matching Network for Multilingual
  POI Retrieval at Baidu Maps <span class="chip">KDD'21</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jizhou Huang, Haifeng Wang, Yibo Sun, Miao Fan, Zhengjie Huang, Chunyuan Yuan, Yawen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing interest in international travel has raised the demand of
retrieving point of interests in multiple languages. This is even superior to
find local venues such as restaurants and scenic spots in unfamiliar languages
when traveling abroad. Multilingual POI retrieval, enabling users to find
desired POIs in a demanded language using queries in numerous languages, has
become an indispensable feature of today's global map applications such as
Baidu Maps. This task is non-trivial because of two key challenges: (1)
visiting sparsity and (2) multilingual query-POI matching. To this end, we
propose a Heterogeneous Graph Attention Matching Network (HGAMN) to
concurrently address both challenges. Specifically, we construct a
heterogeneous graph that contains two types of nodes: POI node and query node
using the search logs of Baidu Maps. To alleviate challenge \#1, we construct
edges between different POI nodes to link the low-frequency POIs with the
high-frequency ones, which enables the transfer of knowledge from the latter to
the former. To mitigate challenge \#2, we construct edges between POI and query
nodes based on the co-occurrences between queries and POIs, where queries in
different languages and formulations can be aggregated for individual POIs.
Moreover, we develop an attention-based network to jointly learn node
representations of the heterogeneous graph and further design a cross-attention
module to fuse the representations of both types of nodes for query-POI
relevance scoring. Extensive experiments conducted on large-scale real-world
datasets from Baidu Maps demonstrate the superiority and effectiveness of
HGAMN. In addition, HGAMN has already been deployed in production at Baidu
Maps, and it successfully keeps serving hundreds of millions of requests every
day.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD'21</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOBIUS: Towards the Next Generation of Query-Ad Matching in Baidu's
  Sponsored Search <span class="chip">KDD'19</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miao Fan, Jiacheng Guo, Shuai Zhu, Shuo Miao, Mingming Sun, Ping Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Baidu runs the largest commercial web search engine in China, serving
hundreds of millions of online users every day in response to a great variety
of queries. In order to build a high-efficiency sponsored search engine, we
used to adopt a three-layer funnel-shaped structure to screen and sort hundreds
of ads from billions of ad candidates subject to the requirement of low
response latency and the restraints of computing resources. Given a user query,
the top matching layer is responsible for providing semantically relevant ad
candidates to the next layer, while the ranking layer at the bottom concerns
more about business indicators (e.g., CPM, ROI, etc.) of those ads. The clear
separation between the matching and ranking objectives results in a lower
commercial return. The Mobius project has been established to address this
serious issue. It is our first attempt to train the matching layer to consider
CPM as an additional optimization objective besides the query-ad relevance, via
directly predicting CTR (click-through rate) from billions of query-ad pairs.
Specifically, this paper will elaborate on how we adopt active learning to
overcome the insufficiency of click history at the matching layer when training
our neural click networks offline, and how we use the SOTA ANN search technique
for retrieving ads more efficiently (Here ``ANN'' stands for approximate
nearest neighbor search). We contribute the solutions to Mobius-V1 as the first
version of our next generation query-ad matching system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD'19</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Prototype-based Contrastive Learning for Privacy-Preserving
  Cross-domain Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Wang, Quangui Zhang, Lei Sang, Qiang Wu, Min Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-domain recommendation (CDR) aims to improve recommendation accuracy in
sparse domains by transferring knowledge from data-rich domains. However,
existing CDR methods often assume the availability of user-item interaction
data across domains, overlooking user privacy concerns. Furthermore, these
methods suffer from performance degradation in scenarios with sparse
overlapping users, as they typically depend on a large number of fully shared
users for effective knowledge transfer. To address these challenges, we propose
a Federated Prototype-based Contrastive Learning (CL) method for
Privacy-Preserving CDR, named FedPCL-CDR. This approach utilizes
non-overlapping user information and prototypes to improve multi-domain
performance while protecting user privacy. FedPCL-CDR comprises two modules:
local domain (client) learning and global server aggregation. In the local
domain, FedPCL-CDR clusters all user data to learn representative prototypes,
effectively utilizing non-overlapping user information and addressing the
sparse overlapping user issue. It then facilitates knowledge transfer by
employing both local and global prototypes returned from the server in a CL
manner. Simultaneously, the global server aggregates representative prototypes
from local domains to learn both local and global prototypes. The combination
of prototypes and federated learning (FL) ensures that sensitive user data
remains decentralized, with only prototypes being shared across domains,
thereby protecting user privacy. Extensive experiments on four CDR tasks using
two real-world datasets demonstrate that FedPCL-CDR outperforms the
state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ iText2KG: Incremental Knowledge Graphs Construction Using Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yassir Lairgi, Ludovic Moncla, Rémy Cazabet, Khalid Benabdeslem, Pierre Cléau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most available data is unstructured, making it challenging to access valuable
information. Automatically building Knowledge Graphs (KGs) is crucial for
structuring data and making it accessible, allowing users to search for
information effectively. KGs also facilitate insights, inference, and
reasoning. Traditional NLP methods, such as named entity recognition and
relation extraction, are key in information retrieval but face limitations,
including the use of predefined entity types and the need for supervised
learning. Current research leverages large language models' capabilities, such
as zero- or few-shot learning. However, unresolved and semantically duplicated
entities and relations still pose challenges, leading to inconsistent graphs
and requiring extensive post-processing. Additionally, most approaches are
topic-dependent. In this paper, we propose iText2KG, a method for incremental,
topic-independent KG construction without post-processing. This plug-and-play,
zero-shot method is applicable across a wide range of KG construction scenarios
and comprises four modules: Document Distiller, Incremental Entity Extractor,
Incremental Relation Extractor, and Graph Integrator and Visualization. Our
method demonstrates superior performance compared to baseline methods across
three scenarios: converting scientific papers to graphs, websites to graphs,
and CVs to graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at The International Web Information Systems Engineering
  conference (the WISE conference) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Behavior-Dependent Linear Recurrent Units for Efficient Sequential
  Recommendation <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12580v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12580v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengkai Liu, Jianghao Lin, Hanzhou Liu, Jianling Wang, James Caverlee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems aims to predict the users' next interaction
through user behavior modeling with various operators like RNNs and attentions.
However, existing models generally fail to achieve the three golden principles
for sequential recommendation simultaneously, i.e., training efficiency,
low-cost inference, and strong performance. To this end, we propose RecBLR, an
Efficient Sequential Recommendation Model based on Behavior-Dependent Linear
Recurrent Units to accomplish the impossible triangle of the three principles.
By incorporating gating mechanisms and behavior-dependent designs into linear
recurrent units, our model significantly enhances user behavior modeling and
recommendation performance. Furthermore, we unlock the parallelizable training
as well as inference efficiency for our model by designing a hardware-aware
scanning acceleration algorithm with a customized CUDA kernel. Extensive
experiments on real-world datasets with varying lengths of user behavior
sequences demonstrate RecBLR's remarkable effectiveness in simultaneously
achieving all three golden principles - strong recommendation performance,
training efficiency, and low-cost inference, while exhibiting excellent
scalability to datasets with long user interaction histories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pooling And Attention: What Are Effective Designs For <span class="highlight-title">LLM</span>-Based
  Embedding Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Tang, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The significant advancements of Large Language Models (LLMs) in generative
tasks have led to a growing body of work exploring LLM-based embedding models.
While these models, employing different pooling and attention strategies, have
achieved state-of-the-art performance on public embedding benchmarks, questions
still arise about what constitutes an effective design for LLM-based embedding
models. However, these models are often trained on different datasets, using
different LLM base models or training settings. Moreover, evaluations on public
embedding benchmarks often fail to report statistical significance, making it
difficult to determine which designs truly contribute to final performance.
This complicates the process for practitioners seeking optimal training recipes
for LLM-based embedding models. In this study, we conduct a large-scale
experiment by training a series of LLM-based embedding models using the same
training data and base model but differing in their pooling and attention
strategies. The results show that there is no one-size-fits-all solution: while
bidirectional attention and an additional trainable pooling layer outperform in
text similarity and information retrieval tasks, they do not significantly
surpass simpler designs like EOS-last token pooling and default causal
attention in clustering and classification tasks. Furthermore, we propose a new
pooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs
of all hidden layers, rather than just the last layer, using a cross-attention
network. This method proves to be statistically superior in text similarity and
retrieval tasks compared to existing pooling methods. Overall, this paper sheds
light on effective training strategies for LLM-based embedding models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/yixuantt/PoolingAndAttn</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WaterMAS: Sharpness-Aware Maximization for Neural Network Watermarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carl De Sousa Trias, Mihai Mitrea, Attilio Fiandrotti, Marco Cagnazzo, Sumanta Chaudhuri, Enzo Tartaglione
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, deep neural networks are used for solving complex tasks in several
critical applications and protecting both their integrity and intellectual
property rights (IPR) has become of utmost importance. To this end, we advance
WaterMAS, a substitutive, white-box neural network watermarking method that
improves the trade-off among robustness, imperceptibility, and computational
complexity, while making provisions for increased data payload and security.
WasterMAS insertion keeps unchanged the watermarked weights while sharpening
their underlying gradient space. The robustness is thus ensured by limiting the
attack's strength: even small alterations of the watermarked weights would
impact the model's performance. The imperceptibility is ensured by inserting
the watermark during the training process. The relationship among the WaterMAS
data payload, imperceptibility, and robustness properties is discussed. The
secret key is represented by the positions of the weights conveying the
watermark, randomly chosen through multiple layers of the model. The security
is evaluated by investigating the case in which an attacker would intercept the
key. The experimental validations consider 5 models and 2 tasks (VGG16,
ResNet18, MobileNetV3, SwinT for CIFAR10 image classification, and DeepLabV3
for Cityscapes image segmentation) as well as 4 types of attacks (Gaussian
noise addition, pruning, fine-tuning, and quantization). The code will be
released open-source upon acceptance of the article.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaBGM: Dynamic Soundtrack Transformation For Continuous Multi-Scene
  Experiences With Ambient Awareness And Personalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxuan Liu, Zihao Wang, Haorong Hong, Youwei Feng, Jiaxin Yu, Han Diao, Yunfei Xu, Kejun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces MetaBGM, a groundbreaking framework for generating
background music that adapts to dynamic scenes and real-time user interactions.
We define multi-scene as variations in environmental contexts, such as
transitions in game settings or movie scenes. To tackle the challenge of
converting backend data into music description texts for audio generation
models, MetaBGM employs a novel two-stage generation approach that transforms
continuous scene and user state data into these texts, which are then fed into
an audio generation model for real-time soundtrack creation. Experimental
results demonstrate that MetaBGM effectively generates contextually relevant
and dynamic background music for interactive applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SegTalker: Segmentation-based Talking Face Generation with Mask-guided
  Local Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingyu Xiong, Xize Cheng, Jintao Tan, Xianjia Wu, Xiandong Li, Lei Zhu, Fei Ma, Minglei Li, Huang Xu, Zhihu Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-driven talking face generation aims to synthesize video with lip
movements synchronized to input audio. However, current generative techniques
face challenges in preserving intricate regional textures (skin, teeth). To
address the aforementioned challenges, we propose a novel framework called
SegTalker to decouple lip movements and image textures by introducing
segmentation as intermediate representation. Specifically, given the mask of
image employed by a parsing network, we first leverage the speech to drive the
mask and generate talking segmentation. Then we disentangle semantic regions of
image into style codes using a mask-guided encoder. Ultimately, we inject the
previously generated talking segmentation and style codes into a mask-guided
StyleGAN to synthesize video frame. In this way, most of textures are fully
preserved. Moreover, our approach can inherently achieve background separation
and facilitate mask-guided facial local editing. In particular, by editing the
mask and swapping the region textures from a given reference image (e.g. hair,
lip, eyebrows), our approach enables facial editing seamlessly when generating
talking face video. Experiments demonstrate that our proposed approach can
effectively preserve texture details and generate temporally consistent video
while remaining competitive in lip synchronization. Quantitative and
qualitative results on the HDTF and MEAD datasets illustrate the superior
performance of our method over existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Make Graph-based Referring Expression Comprehension Great Again through
  Expression-guided Dynamic Gating and Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingcheng Ke, Dele Wang, Jun-Cheng Chen, I-Hong Jhuo, Chia-Wen Lin, Yen-Yu Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One common belief is that with complex models and pre-training on large-scale
datasets, transformer-based methods for referring expression comprehension
(REC) perform much better than existing graph-based methods. We observe that
since most graph-based methods adopt an off-the-shelf detector to locate
candidate objects (i.e., regions detected by the object detector), they face
two challenges that result in subpar performance: (1) the presence of
significant noise caused by numerous irrelevant objects during reasoning, and
(2) inaccurate localization outcomes attributed to the provided detector. To
address these issues, we introduce a plug-and-adapt module guided by
sub-expressions, called dynamic gate constraint (DGC), which can adaptively
disable irrelevant proposals and their connections in graphs during reasoning.
We further introduce an expression-guided regression strategy (EGR) to refine
location prediction. Extensive experimental results on the RefCOCO, RefCOCO+,
RefCOCOg, Flickr30K, RefClef, and Ref-reasoning datasets demonstrate the
effectiveness of the DGC module and the EGR strategy in consistently boosting
the performances of various graph-based REC methods. Without any pretaining,
the proposed graph-based method achieves better performance than the
state-of-the-art (SOTA) transformer-based methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages to appear in IEEE Transactions on Multimedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Character Identification and Speaker Prediction in Comics via
  Iterative Multimodal Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.13993v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.13993v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingxuan Li, Ryota Hinami, Kiyoharu Aizawa, Yusuke Matsui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing characters and predicting speakers of dialogue are critical for
comic processing tasks, such as voice generation or translation. However,
because characters vary by comic title, supervised learning approaches like
training character classifiers which require specific annotations for each
comic title are infeasible. This motivates us to propose a novel zero-shot
approach, allowing machines to identify characters and predict speaker names
based solely on unannotated comic images. In spite of their importance in
real-world applications, these task have largely remained unexplored due to
challenges in story comprehension and multimodal integration. Recent large
language models (LLMs) have shown great capability for text understanding and
reasoning, while their application to multimodal content analysis is still an
open problem. To address this problem, we propose an iterative multimodal
framework, the first to employ multimodal information for both character
identification and speaker prediction tasks. Our experiments demonstrate the
effectiveness of the proposed framework, establishing a robust baseline for
these tasks. Furthermore, since our method requires no training data or
annotations, it can be used as-is on any comic series.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Multimedia 2024. Project page:
  https://liyingxuan1012.github.io/zeroshot-speaker-prediction ; Github repo:
  https://github.com/liyingxuan1012/zeroshot-speaker-prediction</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-04T00:00:00Z">2024-09-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bioinformatics Retrieval Augmentation Data (BRAD) Digital Assistant 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Pickard, Marc Andrew Choi, Natalie Oliven, Cooper Stansbury, Jillian Cwycyshyn, Nicholas Galioto, Alex Gorodetsky, Alvaro Velasquez, Indika Rajapakse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a prototype for a Bioinformatics Retrieval Augmentation Data
(BRAD) digital assistant. BRAD integrates a suite of tools to handle a wide
range of bioinformatics tasks, from code execution to online search. We
demonstrate BRAD's capabilities through (1) improved question-and-answering
with retrieval augmented generation (RAG), (2) BRAD's ability to run and write
complex software pipelines, and (3) BRAD's ability to organize and distribute
tasks across individual and teams of agents. We use BRAD for automation of
bioinformatics workflows, performing tasks ranging from gene enrichment and
searching the archive to automatic code generation and running biomarker
identification pipelines. BRAD is a step toward the ultimate goal to develop a
digital twin of laboratories driven by self-contained loops for hypothesis
generation and testing of digital biology experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building a Scalable, Effective, and Steerable Search and Ranking
  Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marjan Celikik, Jacek Wasilewski, Ana Peleteiro Ramallo, Alexey Kurennoy, Evgeny Labzin, Danilo Ascione, Tural Gurbanov, Géraud Le Falher, Andrii Dzhoha, Ian Harris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern e-commerce platforms offer vast product selections, making it
difficult for customers to find items that they like and that are relevant to
their current session intent. This is why it is key for e-commerce platforms to
have near real-time scalable and adaptable personalized ranking and search
systems. While numerous methods exist in the scientific literature for building
such systems, many are unsuitable for large-scale industrial use due to
complexity and performance limitations. Consequently, industrial ranking
systems often resort to computationally efficient yet simplistic retrieval or
candidate generation approaches, which overlook near real-time and
heterogeneous customer signals, which results in a less personalized and
relevant experience. Moreover, related customer experiences are served by
completely different systems, which increases complexity, maintenance, and
inconsistent experiences.
  In this paper, we present a personalized, adaptable near real-time ranking
platform that is reusable across various use cases, such as browsing and
search, and that is able to cater to millions of items and customers under
heavy load (thousands of requests per second). We employ transformer-based
models through different ranking layers which can learn complex behavior
patterns directly from customer action sequences while being able to
incorporate temporal (e.g. in-session) and contextual information. We validate
our system through a series of comprehensive offline and online real-world
experiments at a large online e-commerce platform, and we demonstrate its
superiority when compared to existing systems, both in terms of customer
experience as well as in net revenue. Finally, we share the lessons learned
from building a comprehensive, modern ranking platform for use in a large-scale
e-commerce environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RouterRetriever: Exploring the Benefits of Routing over Multiple Expert
  Embedding Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunji Lee, Luca Soldaini, Arman Cohan, Minjoon Seo, Kyle Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval methods often rely on a single embedding model trained
on large, general-domain datasets like MSMARCO. While this approach can produce
a retriever with reasonable overall performance, models trained on
domain-specific data often yield better results within their respective
domains. While prior work in information retrieval has tackled this through
multi-task training, the topic of combining multiple domain-specific expert
retrievers remains unexplored, despite its popularity in language model
generation. In this work, we introduce RouterRetriever, a retrieval model that
leverages multiple domain-specific experts along with a routing mechanism to
select the most appropriate expert for each query. It is lightweight and allows
easy addition or removal of experts without additional training. Evaluation on
the BEIR benchmark demonstrates that RouterRetriever outperforms both
MSMARCO-trained (+2.1 absolute nDCG@10) and multi-task trained (+3.2) models.
This is achieved by employing our routing mechanism, which surpasses other
routing techniques (+1.8 on average) commonly used in language modeling.
Furthermore, the benefit generalizes well to other datasets, even in the
absence of a specific expert on the dataset. To our knowledge, RouterRetriever
is the first work to demonstrate the advantages of using multiple
domain-specific expert embedding models with effective routing over a single,
general-purpose embedding model in retrieval tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Fashion Item Recommendation Model in Hyperbolic Space <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryotaro Shimizu, Yu Wang, Masanari Kimura, Yuki Hirakawa, Takashi Wada, Yuki Saito, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a fashion item recommendation model that
incorporates hyperbolic geometry into user and item representations. Using
hyperbolic space, our model aims to capture implicit hierarchies among items
based on their visual data and users' purchase history. During training, we
apply a multi-task learning framework that considers both hyperbolic and
Euclidean distances in the loss function. Our experiments on three data sets
show that our model performs better than previous models trained in Euclidean
space only, confirming the effectiveness of our model. Our ablation studies
show that multi-task learning plays a key role, and removing the Euclidean loss
substantially deteriorates the model performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work was presented at the CVFAD Workshop at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlignGroup: Learning and Aligning Group Consensus with Member
  Preferences for Group Recommendation <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinfeng Xu, Zheyu Chen, Jinze Li, Shuo Yang, Hewei Wang, Edith C. -H. Ngai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group activities are important behaviors in human society, providing
personalized recommendations for groups is referred to as the group
recommendation task. Existing methods can usually be categorized into two
strategies to infer group preferences: 1) determining group preferences by
aggregating members' personalized preferences, and 2) inferring group consensus
by capturing group members' coherent decisions after common compromises.
However, the former would suffer from the lack of group-level considerations,
and the latter overlooks the fine-grained preferences of individual users. To
this end, we propose a novel group recommendation method AlignGroup, which
focuses on both group consensus and individual preferences of group members to
infer the group decision-making. Specifically, AlignGroup explores group
consensus through a well-designed hypergraph neural network that efficiently
learns intra- and inter-group relationships. Moreover, AlignGroup innovatively
utilizes a self-supervised alignment task to capture fine-grained group
decision-making by aligning the group consensus with members' common
preferences. Extensive experiments on two real-world datasets validate that our
AlignGroup outperforms the state-of-the-art on both the group recommendation
task and the user recommendation task, as well as outperforms the efficiency of
most baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, accepted by CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ iRangeGraph: Improvising Range-dedicated Graphs for Range-filtering
  Nearest Neighbor Search <span class="chip">SIGMOD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuexuan Xu, Jianyang Gao, Yutong Gou, Cheng Long, Christian S. Jensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Range-filtering approximate nearest neighbor (RFANN) search is attracting
increasing attention in academia and industry. Given a set of data objects,
each being a pair of a high-dimensional vector and a numeric value, an RFANN
query with a vector and a numeric range as parameters returns the data object
whose numeric value is in the query range and whose vector is nearest to the
query vector. To process this query, a recent study proposes to build $O(n^2)$
dedicated graph-based indexes for all possible query ranges to enable efficient
processing on a database of $n$ objects. As storing all these indexes is
prohibitively expensive, the study constructs compressed indexes instead, which
reduces the memory consumption considerably. However, this incurs suboptimal
performance because the compression is lossy. In this study, instead of
materializing a compressed index for every possible query range in preparation
for querying, we materialize graph-based indexes, called elemental graphs, for
a moderate number of ranges. We then provide an effective and efficient
algorithm that during querying can construct an index for any query range using
the elemental graphs. We prove that the time needed to construct such an index
is low. We also cover an experimental study on real-world datasets that
provides evidence that the materialized elemental graphs only consume moderate
space and that the proposed method is capable of superior and stable query
performance across different query workloads.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted by SIGMOD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Effective Tag Assignment Approach for Billboard Advertisement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dildar Ali, Harishchandra Kumar, Suman Banerjee, Yamuna Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Billboard Advertisement has gained popularity due to its significant outrage
in return on investment. To make this advertisement approach more effective,
the relevant information about the product needs to be reached to the relevant
set of people. This can be achieved if the relevant set of tags can be mapped
to the correct slots. Formally, we call this problem the Tag Assignment Problem
in Billboard Advertisement. Given trajectory, billboard database, and a set of
selected billboard slots and tags, this problem asks to output a mapping of
selected tags to the selected slots so that the influence is maximized. We
model this as a variant of traditional bipartite matching called One-To-Many
Bipartite Matching (OMBM). Unlike traditional bipartite matching, a tag can be
assigned to only one slot; in the OMBM, a tag can be assigned to multiple slots
while the vice versa can not happen. We propose an iterative solution approach
that incrementally allocates the tags to the slots. The proposed methodology
has been explained with an illustrated example. A complexity analysis of the
proposed solution approach has also been conducted. The experimental results on
real-world trajectory and billboard datasets prove our claim on the
effectiveness and efficiency of the proposed solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This Paper has been accepted at The 25th International Web
  Information Systems Engineering Conference (WISE-2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Adaptive Interest Network: Personalized Recommendation with
  Context-Aware Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuaishuai Huang, Haowei Yang, You Yao, Xueting Lin, Yuming Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In personalized recommendation systems, accurately capturing users' evolving
interests and combining them with contextual information is a critical research
area. This paper proposes a novel model called the Deep Adaptive Interest
Network (DAIN), which dynamically models users' interests while incorporating
context-aware learning mechanisms to achieve precise and adaptive personalized
recommendations. DAIN leverages deep learning techniques to build an adaptive
interest network structure that can capture users' interest changes in
real-time while further optimizing recommendation results by integrating
contextual information. Experiments conducted on several public datasets
demonstrate that DAIN excels in both recommendation performance and
computational efficiency. This research not only provides a new solution for
personalized recommendation systems but also offers fresh insights into the
application of context-aware learning in recommendation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do We Trust What They Say or What They Do? A Multimodal User Embedding
  Provides Personalized Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicheng Ren, Zhiping Xiao, Yizhou Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of social media, the importance of analyzing
social network user data has also been put on the agenda. User representation
learning in social media is a critical area of research, based on which we can
conduct personalized content delivery, or detect malicious actors. Being more
complicated than many other types of data, social network user data has
inherent multimodal nature. Various multimodal approaches have been proposed to
harness both text (i.e. post content) and relation (i.e. inter-user
interaction) information to learn user embeddings of higher quality. The advent
of Graph Neural Network models enables more end-to-end integration of user text
embeddings and user interaction graphs in social networks. However, most of
those approaches do not adequately elucidate which aspects of the data - text
or graph structure information - are more helpful for predicting each specific
user under a particular task, putting some burden on personalized downstream
analysis and untrustworthy information filtering. We propose a simple yet
effective framework called Contribution-Aware Multimodal User Embedding (CAMUE)
for social networks. We have demonstrated with empirical evidence, that our
approach can provide personalized explainable predictions, automatically
mitigating the impact of unreliable information. We also conducted case studies
to show how reasonable our results are. We observe that for most users, graph
structure information is more trustworthy than text information, but there are
some reasonable cases where text helps more. Our work paves the way for more
explainable, reliable, and effective social media user embedding which allows
for better personalized content delivery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NUDGE: Lightweight Non-Parametric Fine-Tuning of Embeddings for
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sepanta Zeighami, Zac Wellmer, Aditya Parameswaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $k$-Nearest Neighbor search on dense vector embeddings ($k$-NN retrieval)
from pre-trained embedding models is the predominant retrieval method for text
and images, as well as Retrieval-Augmented Generation (RAG) pipelines. In
practice, application developers often fine-tune the embeddings to improve
their accuracy on the dataset and query workload in hand. Existing approaches
either fine-tune the pre-trained model itself or, more efficiently, but at the
cost of accuracy, train adaptor models to transform the output of the
pre-trained model. We present NUDGE, a family of novel non-parametric embedding
fine-tuning approaches that are significantly more accurate and efficient than
both sets of existing approaches. NUDGE directly modifies the embeddings of
data records to maximize the accuracy of $k$-NN retrieval. We present a
thorough theoretical and experimental study of NUDGE's non-parametric approach.
We show that even though the underlying problem is NP-Hard, constrained
variations can be solved efficiently. These constraints additionally ensure
that the changes to the embeddings are modest, avoiding large distortions to
the semantics learned during pre-training. In experiments across five
pre-trained models and nine standard text and image retrieval datasets, NUDGE
runs in minutes and often improves NDCG@10 by more than 10% over existing
fine-tuning methods. On average, NUDGE provides 3.3x and 4.3x higher increase
in accuracy and runs 200x and 3x faster, respectively, over fine-tuning the
pre-trained model and training adaptors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Design of an <span class="highlight-title">LLM</span>-powered Unstructured Analytics System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Anderson, Jonathan Fritz, Austin Lee, Bohou Li, Mark Lindblad, Henry Lindeman, Alex Meyer, Parth Parmar, Tanvi Ranade, Mehul A. Shah, Benjamin Sowell, Dan Tecuci, Vinayak Thapliyal, Matt Welsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs demonstrate an uncanny ability to process unstructured data, and as
such, have the potential to go beyond search and run complex, semantic analyses
at scale. We describe the design of an unstructured analytics system, Aryn, and
the tenets and use cases that motivate its design. With Aryn, users can specify
queries in natural language and the system automatically determines a semantic
plan and executes it to compute an answer from a large collection of
unstructured documents using LLMs. At the core of Aryn is Sycamore, a
declarative document processing engine, built using Ray, that provides a
reliable distributed abstraction called DocSets. Sycamore allows users to
analyze, enrich, and transform complex documents at scale. Aryn also comprises
Luna, a query planner that translates natural language queries to Sycamore
scripts, and the Aryn Partitioner, which takes raw PDFs and document images,
and converts them to DocSets for downstream processing. Using Aryn, we
demonstrate a real world use case for analyzing accident reports from the
National Transportation Safety Board (NTSB), and discuss some of the major
challenges we encountered in deploying Aryn in the wild.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, fixed typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Recommender Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qidong Liu, Jiaxi Hu, Yutian Xiao, Xiangyu Zhao, Jingtong Gao, Wanyu Wang, Qing Li, Jiliang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recommender system (RS) has been an integral toolkit of online services.
They are equipped with various deep learning techniques to model user
preference based on identifier and attribute information. With the emergence of
multimedia services, such as short videos, news and etc., understanding these
contents while recommending becomes critical. Besides, multimodal features are
also helpful in alleviating the problem of data sparsity in RS. Thus,
Multimodal Recommender System (MRS) has attracted much attention from both
academia and industry recently. In this paper, we will give a comprehensive
survey of the MRS models, mainly from technical views. First, we conclude the
general procedures and major challenges for MRS. Then, we introduce the
existing MRS models according to four categories, i.e., Modality Encoder,
Feature Interaction, Feature Enhancement and Model Optimization. Besides, to
make it convenient for those who want to research this field, we also summarize
the dataset and code resources. Finally, we discuss some promising future
directions of MRS and conclude this paper. To access more details of the
surveyed papers, such as implementation code, we open source a repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CSUR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MARS: Matching Attribute-aware Representations for Text-based Sequential
  Recommendation <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunsoo Kim, Junyoung Kim, Minjin Choi, Sunkyung Lee, Jongwuk Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation aims to predict the next item a user is likely to
prefer based on their sequential interaction history. Recently, text-based
sequential recommendation has emerged as a promising paradigm that uses
pre-trained language models to exploit textual item features to enhance
performance and facilitate knowledge transfer to unseen datasets. However,
existing text-based recommender models still struggle with two key challenges:
(i) representing users and items with multiple attributes, and (ii) matching
items with complex user interests. To address these challenges, we propose a
novel model, Matching Attribute-aware Representations for Text-based Sequential
Recommendation (MARS). MARS extracts detailed user and item representations
through attribute-aware text encoding, capturing diverse user intents with
multiple attribute-aware representations. It then computes user-item scores via
attribute-wise interaction matching, effectively capturing attribute-level user
preferences. Our extensive experiments demonstrate that MARS significantly
outperforms existing sequential models, achieving improvements of up to 24.43%
and 29.26% in Recall@10 and NDCG@10 across five benchmark datasets. Code is
available at https://github.com/junieberry/MARS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HIRO: Hierarchical <span class="highlight-title">Information</span> Retrieval Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09979v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09979v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krish Goel, Mahek Chandak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) has revolutionized natural language
processing by dynamically integrating external knowledge into Large Language
Models (LLMs), addressing their limitation of static training datasets. Recent
implementations of RAG leverage hierarchical data structures, which organize
documents at various levels of summarization and information density. This
complexity, however, can cause LLMs to "choke" on information overload,
necessitating more sophisticated querying mechanisms. In this context, we
introduce Hierarchical Information Retrieval Optimization (HIRO), a novel
querying approach that employs a Depth-First Search (DFS)-based recursive
similarity score calculation and branch pruning. This method uniquely minimizes
the context delivered to the LLM without informational loss, effectively
managing the challenge of excessive data. HIRO's refined approach is validated
by a 10.85% improvement in performance on the NarrativeQA dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models for <span class="highlight-title">Information</span> Retrieval: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07107v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07107v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Haonan Chen, Zheng Liu, Zhicheng Dou, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a primary means of information acquisition, information retrieval (IR)
systems, such as search engines, have integrated themselves into our daily
lives. These systems also serve as components of dialogue, question-answering,
and recommender systems. The trajectory of IR has evolved dynamically from its
origins in term-based methods to its integration with advanced neural models.
While the neural models excel at capturing complex contextual signals and
semantic nuances, thereby reshaping the IR landscape, they still face
challenges such as data scarcity, interpretability, and the generation of
contextually plausible yet potentially inaccurate responses. This evolution
requires a combination of both traditional methods (such as term-based sparse
retrieval methods with rapid response) and modern neural architectures (such as
language models with powerful language understanding capacity). Meanwhile, the
emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has
revolutionized natural language processing due to their remarkable language
understanding, generation, generalization, and reasoning abilities.
Consequently, recent research has sought to leverage LLMs to improve IR
systems. Given the rapid evolution of this research trajectory, it is necessary
to consolidate existing methodologies and provide nuanced insights through a
comprehensive overview. In this survey, we delve into the confluence of LLMs
and IR systems, including crucial aspects such as query rewriters, retrievers,
rerankers, and readers. Additionally, we explore promising directions, such as
search agents, within this expanding field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>updated to version 3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NFARec: A Negative Feedback-Aware Recommender Model <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06900v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06900v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinfeng Wang, Fumiyo Fukumoto, Jin Cui, Yoshimi Suzuki, Dongjin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural network (GNN)-based models have been extensively studied for
recommendations, as they can extract high-order collaborative signals
accurately which is required for high-quality recommender systems. However,
they neglect the valuable information gained through negative feedback in two
aspects: (1) different users might hold opposite feedback on the same item,
which hampers optimal information propagation in GNNs, and (2) even when an
item vastly deviates from users' preferences, they might still choose it and
provide a negative rating. In this paper, we propose a negative feedback-aware
recommender model (NFARec) that maximizes the leverage of negative feedback. To
transfer information to multi-hop neighbors along an optimal path effectively,
NFARec adopts a feedback-aware correlation that guides hypergraph convolutions
(HGCs) to learn users' structural representations. Moreover, NFARec
incorporates an auxiliary task - predicting the feedback sentiment polarity
(i.e., positive or negative) of the next interaction - based on the Transformer
Hawkes Process. The task is beneficial for understanding users by learning the
sentiment expressed in their previous sequential feedback patterns and
predicting future interactions. Extensive experiments demonstrate that NFARec
outperforms competitive baselines. Our source code and data are released at
https://github.com/WangXFng/NFARec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CaDRec: Contextualized and Debiased Recommender Model <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06895v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06895v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinfeng Wang, Fumiyo Fukumoto, Jin Cui, Yoshimi Suzuki, Jiyi Li, Dongjin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender models aimed at mining users' behavioral patterns have raised
great attention as one of the essential applications in daily life. Recent work
on graph neural networks (GNNs) or debiasing methods has attained remarkable
gains. However, they still suffer from (1) over-smoothing node embeddings
caused by recursive convolutions with GNNs, and (2) the skewed distribution of
interactions due to popularity and user-individual biases. This paper proposes
a contextualized and debiased recommender model (CaDRec). To overcome the
over-smoothing issue, we explore a novel hypergraph convolution operator that
can select effective neighbors during convolution by introducing both
structural context and sequential context. To tackle the skewed distribution,
we propose two strategies for disentangling interactions: (1) modeling
individual biases to learn unbiased item embeddings, and (2) incorporating item
popularity with positional encoding. Moreover, we mathematically show that the
imbalance of the gradients to update item embeddings exacerbates the popularity
bias, thus adopting regularization and weighting schemes as solutions.
Extensive experiments on four datasets demonstrate the superiority of the
CaDRec against state-of-the-art (SOTA) methods. Our source code and data are
released at https://github.com/WangXFng/CaDRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Named <span class="highlight-title">Entity</span> Recognition Using Few-Shot <span class="highlight-title">Prompt</span>ing with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15796v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15796v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hédi Zeghidi, Ludovic Moncla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper evaluates Few-Shot Prompting with Large Language Models for Named
Entity Recognition (NER). Traditional NER systems rely on extensive labeled
datasets, which are costly and time-consuming to obtain. Few-Shot Prompting or
in-context learning enables models to recognize entities with minimal examples.
We assess state-of-the-art models like GPT-4 in NER tasks, comparing their
few-shot performance to fully supervised benchmarks. Results show that while
there is a performance gap, large models excel in adapting to new entity types
and domains with very limited data. We also explore the effects of prompt
engineering, guided output format and context length on performance. This study
underscores Few-Shot Learning's potential to reduce the need for large labeled
datasets, enhancing NER scalability and accessibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github repo: https://github.com/GEODE-project/ner-llm</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongLLaVA: Scaling Multi-modal <span class="highlight-title">LLM</span>s to 1000 Images Efficiently via
  Hybrid Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Expanding the long-context capabilities of Multi-modal Large Language
Models~(MLLMs) is crucial for video understanding, high-resolution image
understanding, and multi-modal agents. This involves a series of systematic
optimizations, including model architecture, data construction and training
strategy, particularly addressing challenges such as \textit{degraded
performance with more images} and \textit{high computational costs}. In this
paper, we adapt the model architecture to a hybrid of Mamba and Transformer
blocks, approach data construction with both temporal and spatial dependencies
among multiple images and employ a progressive training strategy. The released
model \textbf{LongLLaVA}~(\textbf{Long}-Context \textbf{L}arge
\textbf{L}anguage \textbf{a}nd \textbf{V}ision \textbf{A}ssistant) is the first
hybrid MLLM, which achieved a better balance between efficiency and
effectiveness. LongLLaVA not only achieves competitive results across various
benchmarks, but also maintains high throughput and low memory consumption.
Especially, it could process nearly a thousand images on a single A100 80GB
GPU, showing promising application prospects for a wide range of tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 7 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Track MusicLDM: Towards Versatile Music Generation with Latent
  Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tornike Karchkhadze, Mohammad Rasool Izadi, Ke Chen, Gerard Assayag, Shlomo Dubnov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have shown promising results in cross-modal generation tasks
involving audio and music, such as text-to-sound and text-to-music generation.
These text-controlled music generation models typically focus on generating
music by capturing global musical attributes like genre and mood. However,
music composition is a complex, multilayered task that often involves musical
arrangement as an integral part of the process. This process involves composing
each instrument to align with existing ones in terms of beat, dynamics,
harmony, and melody, requiring greater precision and control over tracks than
text prompts usually provide. In this work, we address these challenges by
extending the MusicLDM, a latent diffusion model for music, into a multi-track
generative model. By learning the joint probability of tracks sharing a
context, our model is capable of generating music across several tracks that
correspond well to each other, either conditionally or unconditionally.
Additionally, our model is capable of arrangement generation, where the model
can generate any subset of tracks given the others (e.g., generating a piano
track complementing given bass and drum tracks). We compared our model with an
existing multi-track generative model and demonstrated that our model achieves
considerable improvements across objective metrics for both total and
arrangement generation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exp<span class="highlight-title">LLM</span>: Towards Chain of Thought for Facial Expression Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xing Lan, Jian Xue, Ji Qi, Dongmei Jiang, Ke Lu, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial expression recognition (FER) is a critical task in multimedia with
significant implications across various domains. However, analyzing the causes
of facial expressions is essential for accurately recognizing them. Current
approaches, such as those based on facial action units (AUs), typically provide
AU names and intensities but lack insight into the interactions and
relationships between AUs and the overall expression. In this paper, we propose
a novel method called ExpLLM, which leverages large language models to generate
an accurate chain of thought (CoT) for facial expression recognition.
Specifically, we have designed the CoT mechanism from three key perspectives:
key observations, overall emotional interpretation, and conclusion. The key
observations describe the AU's name, intensity, and associated emotions. The
overall emotional interpretation provides an analysis based on multiple AUs and
their interactions, identifying the dominant emotions and their relationships.
Finally, the conclusion presents the final expression label derived from the
preceding analysis. Furthermore, we also introduce the Exp-CoT Engine, designed
to construct this expression CoT and generate instruction-description data for
training our ExpLLM. Extensive experiments on the RAF-DB and AffectNet datasets
demonstrate that ExpLLM outperforms current state-of-the-art FER methods.
ExpLLM also surpasses the latest GPT-4o in expression CoT generation,
particularly in recognizing micro-expressions where GPT-4o frequently fails.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://starhiking.github.io/ExpLLM_Page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for
  One-Shot Talking Head Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Ling, Yiwen Wang, Han Xue, Rong Xie, Li Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While previous audio-driven talking head generation (THG) methods generate
head poses from driving audio, the generated poses or lips cannot match the
audio well or are not editable. In this study, we propose \textbf{PoseTalk}, a
THG system that can freely generate lip-synchronized talking head videos with
free head poses conditioned on text prompts and audio. The core insight of our
method is using head pose to connect visual, linguistic, and audio signals.
First, we propose to generate poses from both audio and text prompts, where the
audio offers short-term variations and rhythm correspondence of the head
movements and the text prompts describe the long-term semantics of head
motions. To achieve this goal, we devise a Pose Latent Diffusion (PLD) model to
generate motion latent from text prompts and audio cues in a pose latent space.
Second, we observe a loss-imbalance problem: the loss for the lip region
contributes less than 4\% of the total reconstruction loss caused by both pose
and lip, making optimization lean towards head movements rather than lip
shapes. To address this issue, we propose a refinement-based learning strategy
to synthesize natural talking videos using two cascaded networks, i.e.,
CoarseNet, and RefineNet. The CoarseNet estimates coarse motions to produce
animated images in novel poses and the RefineNet focuses on learning finer lip
motions by progressively estimating lip motions from low-to-high resolutions,
yielding improved lip-synchronization performance. Experiments demonstrate our
pose prediction strategy achieves better pose diversity and realness compared
to text-only or audio-only, and our video generator model outperforms
state-of-the-art methods in synthesizing talking videos with natural head
motions. Project: https://junleen.github.io/projects/posetalk.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7+5 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Resolution Object Recognition with Cross-Resolution Relational
  Contrastive Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangkai Zhang, Shiming Ge, Ruixin Shi, Dan Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing objects in low-resolution images is a challenging task due to the
lack of informative details. Recent studies have shown that knowledge
distillation approaches can effectively transfer knowledge from a
high-resolution teacher model to a low-resolution student model by aligning
cross-resolution representations. However, these approaches still face
limitations in adapting to the situation where the recognized objects exhibit
significant representation discrepancies between training and testing images.
In this study, we propose a cross-resolution relational contrastive
distillation approach to facilitate low-resolution object recognition. Our
approach enables the student model to mimic the behavior of a well-trained
teacher model which delivers high accuracy in identifying high-resolution
objects. To extract sufficient knowledge, the student learning is supervised
with contrastive relational distillation loss, which preserves the similarities
in various relational structures in contrastive representation space. In this
manner, the capability of recovering missing details of familiar low-resolution
objects can be effectively enhanced, leading to a better knowledge transfer.
Extensive experiments on low-resolution object classification and
low-resolution face recognition clearly demonstrate the effectiveness and
adaptability of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by IEEE Transactions on Circuits and Systems
  for Video Technology (TCSVT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coral Model Generation from Single Images for Virtual Reality
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Fu, Shun Fu, Mick Grierson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of VR technology, the demand for high-quality 3D
models is increasing. Traditional methods struggle with efficiency and quality
in large-scale customization. This paper introduces a deep-learning framework
that generates high-precision 3D coral models from a single image. Using the
Coral dataset, the framework extracts geometric and texture features, performs
3D reconstruction, and optimizes design and material blending. Advanced
optimization and polygon count control ensure shape accuracy, detail retention,
and flexible output for various complexities, catering to high-quality
rendering and real-time interaction needs.The project incorporates Explainable
AI (XAI) to transform AI-generated models into interactive "artworks," best
viewed in VR and XR. This enhances model interpretability and human-machine
collaboration. Real-time feedback in VR interactions displays information like
coral species and habitat, enriching user experience. The generated models
surpass traditional methods in detail, visual quality, and efficiency. This
research offers an intelligent approach to 3D content creation for VR, lowering
production barriers, and promoting widespread VR applications. Additionally,
integrating XAI provides new insights into AI-generated visual content and
advances research in 3D vision interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of Explainable AI for the Arts Workshop 2024 (XAIxArts
  2024) arXiv:2406.14485</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hand1000: Generating Realistic Hands from Text with Only 1,000 Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15461v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15461v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haozhuo Zhang, Bin Zhu, Yu Cao, Yanbin Hao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image generation models have achieved remarkable advancements in
recent years, aiming to produce realistic images from textual descriptions.
However, these models often struggle with generating anatomically accurate
representations of human hands. The resulting images frequently exhibit issues
such as incorrect numbers of fingers, unnatural twisting or interlacing of
fingers, or blurred and indistinct hands. These issues stem from the inherent
complexity of hand structures and the difficulty in aligning textual
descriptions with precise visual depictions of hands. To address these
challenges, we propose a novel approach named Hand1000 that enables the
generation of realistic hand images with target gesture using only 1,000
training samples. The training of Hand1000 is divided into three stages with
the first stage aiming to enhance the model's understanding of hand anatomy by
using a pre-trained hand gesture recognition model to extract gesture
representation. The second stage further optimizes text embedding by
incorporating the extracted hand gesture representation, to improve alignment
between the textual descriptions and the generated hand images. The third stage
utilizes the optimized embedding to fine-tune the Stable Diffusion model to
generate realistic hand images. In addition, we construct the first publicly
available dataset specifically designed for text-to-hand image generation.
Based on the existing hand gesture recognition dataset, we adopt advanced image
captioning models and LLaMA3 to generate high-quality textual descriptions
enriched with detailed gesture information. Extensive experiments demonstrate
that Hand1000 significantly outperforms existing models in producing
anatomically correct hand images while faithfully representing other details in
the text, such as faces, clothing, and colors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page https://haozhuo-zhang.github.io/Hand1000-project-page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MCDubber: Multimodal Context-Aware Expressive Video Dubbing <span class="chip">SC2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11593v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11593v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zhao, Zhenqi Jia, Rui Liu, De Hu, Feilong Bao, Guanglai Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Video Dubbing (AVD) aims to take the given script and generate
speech that aligns with lip motion and prosody expressiveness. Current AVD
models mainly utilize visual information of the current sentence to enhance the
prosody of synthesized speech. However, it is crucial to consider whether the
prosody of the generated dubbing aligns with the multimodal context, as the
dubbing will be combined with the original context in the final video. This
aspect has been overlooked in previous studies. To address this issue, we
propose a Multimodal Context-aware video Dubbing model, termed
\textbf{MCDubber}, to convert the modeling object from a single sentence to a
longer sequence with context information to ensure the consistency of the
global context prosody. MCDubber comprises three main components: (1) A context
duration aligner aims to learn the context-aware alignment between the text and
lip frames; (2) A context prosody predictor seeks to read the global context
visual sequence and predict the context-aware global energy and pitch; (3) A
context acoustic decoder ultimately predicts the global context mel-spectrogram
with the assistance of adjacent ground-truth mel-spectrograms of the target
sentence. Through this process, MCDubber fully considers the influence of
multimodal context on the prosody expressiveness of the current sentence when
dubbing. The extracted mel-spectrogram belonging to the target sentence from
the output context mel-spectrograms is the final required dubbing audio.
Extensive experiments on the Chem benchmark dataset demonstrate that our
MCDubber significantly improves dubbing expressiveness compared to all advanced
baselines. The code and demos are available at
https://github.com/XiaoYuanJun-zy/MCDubber.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NCMMSC2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-03T00:00:00Z">2024-09-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpannerLib: Embedding Declarative <span class="highlight-title">Information</span> <span class="highlight-title">Extraction</span> in an
  Imperative Workflow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dean Light, Ahmad Aiashy, Mahmoud Diab, Daniel Nachmias, Stijn Vansummeren, Benny Kimelfeld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document spanners have been proposed as a formal framework for declarative
Information Extraction (IE) from text, following IE products from the industry
and academia. Over the past decade, the framework has been studied thoroughly
in terms of expressive power, complexity, and the ability to naturally combine
text analysis with relational querying. This demonstration presents SpannerLib
a library for embedding document spanners in Python code. SpannerLib
facilitates the development of IE programs by providing an implementation of
Spannerlog (Datalog-based documentspanners) that interacts with the Python code
in two directions: rules can be embedded inside Python, and they can invoke
custom Python code (e.g., calls to ML-based NLP models) via user-defined
functions. The demonstration scenarios showcase IE programs, with increasing
levels of complexity, within Jupyter Notebook.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Laser: Parameter-Efficient <span class="highlight-title">LLM</span> Bi-Tuning for Sequential Recommendation
  with Collaborative <span class="highlight-title">Information</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Zhang, Linmei Hu, Luhao Zhang, Dandan Song, Heyan Huang, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems are essential for discerning user preferences
from historical interactions and facilitating targeted recommendations. Recent
innovations employing Large Language Models (LLMs) have advanced the field by
encoding item semantics, yet they often necessitate substantial parameter
tuning and are resource-demanding. Moreover, these works fails to consider the
diverse characteristics of different types of users and thus diminishes the
recommendation accuracy. In this paper, we propose a parameter-efficient Large
Language Model Bi-Tuning framework for sequential recommendation with
collaborative information (Laser). Specifically, Bi-Tuning works by inserting
trainable virtual tokens at both the prefix and suffix of the input sequence
and freezing the LLM parameters, thus optimizing the LLM for the sequential
recommendation. In our Laser, the prefix is utilized to incorporate user-item
collaborative information and adapt the LLM to the recommendation task, while
the suffix converts the output embeddings of the LLM from the language space to
the recommendation space for the follow-up item recommendation. Furthermore, to
capture the characteristics of different types of users when integrating the
collaborative information via the prefix, we introduce M-Former, a lightweight
MoE-based querying transformer that uses a set of query experts to integrate
diverse user-specific collaborative information encoded by frozen ID-based
sequential recommender systems, significantly improving the accuracy of
recommendations. Extensive experiments on real-world datasets demonstrate that
Laser can parameter-efficiently adapt LLMs to effective recommender systems,
significantly outperforming state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Blockchain-based Federated Recommendation with Incentive Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhai Chen, Yanlin Wu, Dazhong Rong, Guoyao Yu, Lingqi Jiang, Zhenguang Liu, Peng Zhou, Rui Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, federated recommendation technology is rapidly evolving to help
multiple organisations share data and train models while meeting user privacy,
data security and government regulatory requirements. However, federated
recommendation increases customer system costs such as power, computational and
communication resources. Besides, federated recommendation systems are also
susceptible to model attacks and data poisoning by participating malicious
clients. Therefore, most customers are unwilling to participate in federated
recommendation without any incentive. To address these problems, we propose a
blockchain-based federated recommendation system with incentive mechanism to
promote more trustworthy, secure, and efficient federated recommendation
service. First, we construct a federated recommendation system based on NeuMF
and FedAvg. Then we introduce a reverse auction mechanism to select optimal
clients that can maximize the social surplus. Finally, we employ blockchain for
on-chain evidence storage of models to ensure the safety of the federated
recommendation system. The experimental results show that our proposed
incentive mechanism can attract clients with superior training data to engage
in the federal recommendation at a lower cost, which can increase the economic
benefit of federal recommendation by 54.9\% while improve the recommendation
performance. Thus our work provides theoretical and technological support for
the construction of a harmonious and healthy ecological environment for the
application of federal recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted on 2024 Blockchain and Web3 Technology
  Innovation and Application Exchange Conference (BWTAC 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ rerankers: A Lightweight Python Library to Unify Ranking Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Clavié
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents rerankers, a Python library which provides an easy-to-use
interface to the most commonly used re-ranking approaches. Re-ranking is an
integral component of many retrieval pipelines; however, there exist numerous
approaches to it, relying on different implementation methods. rerankers
unifies these methods into a single user-friendly interface, allowing
practitioners and researchers alike to explore different methods while only
changing a single line of Python code. Moreover ,rerankers ensures that its
implementations are done with the fewest dependencies possible, and re-uses the
original implementation whenever possible, guaranteeing that our simplified
interface results in no performance degradation compared to more complex ones.
The full source code and list of supported models are updated regularly and
available at https://github.com/answerdotai/rerankers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Impedance vs. Power Side-channel Vulnerabilities: A Comparative Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Sadik Awal, Buddhipriya Gayanath, Md Tauhidur Rahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent times, impedance side-channel analysis has emerged as a potent
strategy for adversaries seeking to extract sensitive information from
computing systems. It leverages variations in the intrinsic impedance of a
chip's internal structure across different logic states. In this study, we
conduct a comparative analysis between the newly explored impedance side
channel and the well-established power side channel. Through experimental
evaluation, we investigate the efficacy of these two side channels in
extracting the cryptographic key from the Advanced Encryption Standard (AES)
and analyze their performance. Our results indicate that impedance analysis
demonstrates a higher potential for cryptographic key extraction compared to
power side-channel analysis. Moreover, we identify scenarios where power
side-channel analysis does not yield satisfactory results, whereas impedance
analysis proves to be more robust and effective. This work not only underscores
the significance of impedance side-channel analysis in enhancing cryptographic
security but also emphasizes the necessity for a deeper understanding of its
mechanisms and implications.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LSTMSE-Net: Long Short Term Speech Enhancement Network for Audio-visual
  Speech Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnav Jain, Jasmer Singh Sanjotra, Harshvardhan Choudhary, Krish Agrawal, Rupal Shah, Rohan Jha, M. Sajid, Amir Hussain, M. Tanveer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose long short term memory speech enhancement network
(LSTMSE-Net), an audio-visual speech enhancement (AVSE) method. This innovative
method leverages the complementary nature of visual and audio information to
boost the quality of speech signals. Visual features are extracted with
VisualFeatNet (VFN), and audio features are processed through an encoder and
decoder. The system scales and concatenates visual and audio features, then
processes them through a separator network for optimized speech enhancement.
The architecture highlights advancements in leveraging multi-modal data and
interpolation techniques for robust AVSE challenge systems. The performance of
LSTMSE-Net surpasses that of the baseline model from the COG-MHEAR AVSE
Challenge 2024 by a margin of 0.06 in scale-invariant signal-to-distortion
ratio (SISDR), $0.03$ in short-time objective intelligibility (STOI), and
$1.32$ in perceptual evaluation of speech quality (PESQ). The source code of
the proposed LSTMSE-Net is available at
\url{https://github.com/mtanveer1/AVSEC-3-Challenge}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Deep Shadows: A <span class="highlight-title">Survey</span> on Image and Video Shadow Detection,
  Removal, and Generation in the Era of Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaowei Hu, Zhenghao Xing, Tianyu Wang, Chi-Wing Fu, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shadows are formed when light encounters obstacles, leading to areas of
diminished illumination. In computer vision, shadow detection, removal, and
generation are crucial for enhancing scene understanding, refining image
quality, ensuring visual consistency in video editing, and improving virtual
environments. This paper presents a comprehensive survey of shadow detection,
removal, and generation in images and videos within the deep learning landscape
over the past decade, covering tasks, deep models, datasets, and evaluation
metrics. Our key contributions include a comprehensive survey of shadow
analysis, standardization of experimental comparisons, exploration of the
relationships among model size, speed, and performance, a cross-dataset
generalization study, identification of open issues and future directions, and
provision of publicly available resources to support further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Publicly available results, trained models, and evaluation metrics at
  https://github.com/xw-hu/Unveiling-Deep-Shadows</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Real-World Adverse Weather Image Restoration: Enhancing
  Clearness and Semantics with Vision-Language Models <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Xu, Mengyang Wu, Xiaowei Hu, Chi-Wing Fu, Qi Dou, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the limitations of adverse weather image restoration
approaches trained on synthetic data when applied to real-world scenarios. We
formulate a semi-supervised learning framework employing vision-language models
to enhance restoration performance across diverse adverse weather conditions in
real-world settings. Our approach involves assessing image clearness and
providing semantics using vision-language models on real data, serving as
supervision signals for training restoration models. For clearness enhancement,
we use real-world data, utilizing a dual-step strategy with pseudo-labels
assessed by vision-language models and weather prompt learning. For semantic
enhancement, we integrate real-world data by adjusting weather conditions in
vision-language model descriptions while preserving semantic meaning.
Additionally, we introduce an effective training strategy to bootstrap
restoration performance. Our approach achieves superior results in real-world
adverse weather image restoration, demonstrated through qualitative and
quantitative comparisons with state-of-the-art works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Resolution Face Recognition via Adaptable Instance-Relation
  Distillation <span class="chip">IJCNN 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02049v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02049v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixin Shi, Weijia Guo, Shiming Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-resolution face recognition is a challenging task due to the missing of
informative details. Recent approaches based on knowledge distillation have
proven that high-resolution clues can well guide low-resolution face
recognition via proper knowledge transfer. However, due to the distribution
difference between training and testing faces, the learned models often suffer
from poor adaptability. To address that, we split the knowledge transfer
process into distillation and adaptation steps, and propose an adaptable
instance-relation distillation approach to facilitate low-resolution face
recognition. In the approach, the student distills knowledge from
high-resolution teacher in both instance level and relation level, providing
sufficient cross-resolution knowledge transfer. Then, the learned student can
be adaptable to recognize low-resolution faces with adaptive batch
normalization in inference. In this manner, the capability of recovering
missing details of familiar low-resolution faces can be effectively enhanced,
leading to a better knowledge transfer. Extensive experiments on low-resolution
face recognition clearly demonstrate the effectiveness and adaptability of our
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCNN 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRoGS: Progressive Rendering of Gaussian Splats 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brent Zoomers, Maarten Wijnants, Ivan Molenaers, Joni Vanherck, Jeroen Put, Lode Jorissen, Nick Michiels
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past year, 3D Gaussian Splatting (3DGS) has received significant
attention for its ability to represent 3D scenes in a perceptually accurate
manner. However, it can require a substantial amount of storage since each
splat's individual data must be stored. While compression techniques offer a
potential solution by reducing the memory footprint, they still necessitate
retrieving the entire scene before any part of it can be rendered. In this
work, we introduce a novel approach for progressively rendering such scenes,
aiming to display visible content that closely approximates the final scene as
early as possible without loading the entire scene into memory. This approach
benefits both on-device rendering applications limited by memory constraints
and streaming applications where minimal bandwidth usage is preferred. To
achieve this, we approximate the contribution of each Gaussian to the final
scene and construct an order of prioritization on their inclusion in the
rendering process. Additionally, we demonstrate that our approach can be
combined with existing compression methods to progressively render (and stream)
3DGS scenes, optimizing bandwidth usage by focusing on the most important
splats within a scene. Overall, our work establishes a foundation for making
remotely hosted 3DGS content more quickly accessible to end-users in
over-the-top consumption scenarios, with our results showing significant
improvements in quality across all metrics compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving Multimedia Mobile Cloud Computing Using Protective
  Perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongze Tang, Mengmei Ye, Yao Liu, Sheng Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile cloud computing has been adopted in many multimedia applications,
where the resource-constrained mobile device sends multimedia data (e.g.,
images) to remote cloud servers to request computation-intensive multimedia
services (e.g., image recognition). While significantly improving the
performance of the mobile applications, the cloud-based mechanism often causes
privacy concerns as the multimedia data and services are offloaded from the
trusted user device to untrusted cloud servers. Several recent studies have
proposed perturbation-based privacy preserving mechanisms, which obfuscate the
offloaded multimedia data to eliminate privacy exposures without affecting the
functionality of the remote multimedia services. However, the existing privacy
protection approaches require the deployment of computation-intensive
perturbation generation on the resource-constrained mobile devices. Also, the
obfuscated images are typically not compliant with the standard image
compression algorithms and suffer from significant bandwidth consumption. In
this paper, we develop a novel privacy-preserving multimedia mobile cloud
computing framework, namely $PMC^2$, to address the resource and bandwidth
challenges. $PMC^2$ employs secure confidential computing in the cloud to
deploy the perturbation generator, which addresses the resource challenge while
maintaining the privacy. Furthermore, we develop a neural compressor
specifically trained to compress the perturbed images in order to address the
bandwidth challenge. We implement $PMC^2$ in an end-to-end mobile cloud
computing system, based on which our evaluations demonstrate superior latency,
power efficiency, and bandwidth consumption achieved by $PMC^2$ while
maintaining high accuracy in the target multimedia service.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Think Twice Before Recognizing: Large Multimodal Models for General
  Fine-grained Traffic Sign Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaozong Gan, Guang Li, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new strategy called think twice before recognizing to improve
fine-grained traffic sign recognition (TSR). Fine-grained TSR in the wild is
difficult due to the complex road conditions, and existing approaches
particularly struggle with cross-country TSR when data is lacking. Our strategy
achieves effective fine-grained TSR by stimulating the multiple-thinking
capability of large multimodal models (LMM). We introduce context,
characteristic, and differential descriptions to design multiple thinking
processes for the LMM. The context descriptions with center coordinate prompt
optimization help the LMM to locate the target traffic sign in the original
road images containing multiple traffic signs and filter irrelevant answers
through the proposed prior traffic sign hypothesis. The characteristic
description is based on few-shot in-context learning of template traffic signs,
which decreases the cross-domain difference and enhances the fine-grained
recognition capability of the LMM. The differential descriptions of similar
traffic signs optimize the multimodal thinking capability of the LMM. The
proposed method is independent of training data and requires only simple and
uniform instructions. We conducted extensive experiments on three benchmark
datasets and two real-world datasets from different countries, and the proposed
method achieves state-of-the-art TSR results on all five datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IDNet: A Novel <span class="highlight-title">Dataset</span> for Id<span class="highlight-title">entity</span> Document Analysis and Fraud
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01690v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01690v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Guan, Yancheng Wang, Lulu Xie, Soham Nag, Rajeev Goel, Niranjan Erappa Narayana Swamy, Yingzhen Yang, Chaowei Xiao, Jonathan Prisby, Ross Maciejewski, Jia Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective fraud detection and analysis of government-issued identity
documents, such as passports, driver's licenses, and identity cards, are
essential in thwarting identity theft and bolstering security on online
platforms. The training of accurate fraud detection and analysis tools depends
on the availability of extensive identity document datasets. However, current
publicly available benchmark datasets for identity document analysis, including
MIDV-500, MIDV-2020, and FMIDV, fall short in several respects: they offer a
limited number of samples, cover insufficient varieties of fraud patterns, and
seldom include alterations in critical personal identifying fields like
portrait images, limiting their utility in training models capable of detecting
realistic frauds while preserving privacy.
  In response to these shortcomings, our research introduces a new benchmark
dataset, IDNet, designed to advance privacy-preserving fraud detection efforts.
The IDNet dataset comprises 837,060 images of synthetically generated identity
documents, totaling approximately 490 gigabytes, categorized into 20 types from
$10$ U.S. states and 10 European countries. We evaluate the utility and present
use cases of the dataset, illustrating how it can aid in training
privacy-preserving fraud detection methods, facilitating the generation of
camera and video capturing of identity documents, and testing schema
unification and other identity document management functionalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TALDS-Net: Task-Aware Adaptive Local Descriptors Selection for Few-shot
  Image Classification <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Qiao, Yu Xie, Ziyin Zeng, Fanzhang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot image classification aims to classify images from unseen novel
classes with few samples. Recent works demonstrate that deep local descriptors
exhibit enhanced representational capabilities compared to image-level
features. However, most existing methods solely rely on either employing all
local descriptors or directly utilizing partial descriptors, potentially
resulting in the loss of crucial information. Moreover, these methods primarily
emphasize the selection of query descriptors while overlooking support
descriptors. In this paper, we propose a novel Task-Aware Adaptive Local
Descriptors Selection Network (TALDS-Net), which exhibits the capacity for
adaptive selection of task-aware support descriptors and query descriptors.
Specifically, we compare the similarity of each local support descriptor with
other local support descriptors to obtain the optimal support descriptor subset
and then compare the query descriptors with the optimal support subset to
obtain discriminative query descriptors. Extensive experiments demonstrate that
our TALDS-Net outperforms state-of-the-art methods on both general and
fine-grained datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figures, is accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-02T00:00:00Z">2024-09-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sync from the Sea: Retrieving Alignable Videos from Large-Scale <span class="highlight-title">Dataset</span>s <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ishan Rajendrakumar Dave, Fabian Caba Heilbron, Mubarak Shah, Simon Jenni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal video alignment aims to synchronize the key events like object
interactions or action phase transitions in two videos. Such methods could
benefit various video editing, processing, and understanding tasks. However,
existing approaches operate under the restrictive assumption that a suitable
video pair for alignment is given, significantly limiting their broader
applicability. To address this, we re-pose temporal alignment as a search
problem and introduce the task of Alignable Video Retrieval (AVR). Given a
query video, our approach can identify well-alignable videos from a large
collection of clips and temporally synchronize them to the query. To achieve
this, we make three key contributions: 1) we introduce DRAQ, a video
alignability indicator to identify and re-rank the best alignable video from a
set of candidates; 2) we propose an effective and generalizable frame-level
video feature design to improve the alignment performance of several
off-the-shelf feature representations, and 3) we propose a novel benchmark and
evaluation protocol for AVR using cycle-consistency metrics. Our experiments on
3 datasets, including large-scale Kinetics700, demonstrate the effectiveness of
our approach in identifying alignable video pairs from diverse datasets.
Project Page: https://daveishan.github.io/avr-webpage/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECCV 2024 Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Know When to Fuse: Investigating Non-English Hybrid Retrieval in the
  Legal Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Louis, Gijs van Dijck, Gerasimos Spanakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid search has emerged as an effective strategy to offset the limitations
of different matching paradigms, especially in out-of-domain contexts where
notable improvements in retrieval quality have been observed. However, existing
research predominantly focuses on a limited set of retrieval methods, evaluated
in pairs on domain-general datasets exclusively in English. In this work, we
study the efficacy of hybrid search across a variety of prominent retrieval
models within the unexplored field of law in the French language, assessing
both zero-shot and in-domain scenarios. Our findings reveal that in a zero-shot
context, fusing different domain-general models consistently enhances
performance compared to using a standalone model, regardless of the fusion
method. Surprisingly, when models are trained in-domain, we find that fusion
generally diminishes performance relative to using the best single system,
unless fusing scores with carefully tuned weights. These novel insights, among
others, expand the applicability of prior findings across a new field and
language, and contribute to a deeper understanding of hybrid search in
non-English specialized domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSD4Rec: A Structured State Space Duality Model for Efficient Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohao Qu, Yifeng Zhang, Liangbo Ning, Wenqi Fan, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation methods are crucial in modern recommender systems
for their remarkable capability to understand a user's changing interests based
on past interactions. However, a significant challenge faced by current methods
(e.g., RNN- or Transformer-based models) is to effectively and efficiently
capture users' preferences by modeling long behavior sequences, which impedes
their various applications like short video platforms where user interactions
are numerous. Recently, an emerging architecture named Mamba, built on state
space models (SSM) with efficient hardware-aware designs, has showcased the
tremendous potential for sequence modeling, presenting a compelling avenue for
addressing the challenge effectively. Inspired by this, we propose a novel
generic and efficient sequential recommendation backbone, SSD4Rec, which
explores the seamless adaptation of Mamba for sequential recommendations.
Specifically, SSD4Rec marks the variable- and long-length item sequences with
sequence registers and processes the item representations with bidirectional
Structured State Space Duality (SSD) blocks. This not only allows for
hardware-aware matrix multiplication but also empowers outstanding capabilities
in variable-length and long-range sequence modeling. Extensive evaluations on
four benchmark datasets demonstrate that the proposed model achieves
state-of-the-art performance while maintaining near-linear scalability with
user sequence length. Our code is publicly available at
https://github.com/ZhangYifeng1995/SSD4Rec.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real World Conversational <span class="highlight-title">Entity</span> Linking Requires More Than Zeroshots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohanna Hoveyda, Arjen P. de Vries, Maarten de Rijke, Faegheh Hasibi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity linking (EL) in conversations faces notable challenges in practical
applications, primarily due to the scarcity of entity-annotated conversational
datasets and sparse knowledge bases (KB) containing domain-specific, long-tail
entities. We designed targeted evaluation scenarios to measure the efficacy of
EL models under resource constraints. Our evaluation employs two KBs: Fandom,
exemplifying real-world EL complexities, and the widely used Wikipedia. First,
we assess EL models' ability to generalize to a new unfamiliar KB using Fandom
and a novel zero-shot conversational entity linking dataset that we curated
based on Reddit discussions on Fandom entities. We then evaluate the
adaptability of EL models to conversational settings without prior training.
Our results indicate that current zero-shot EL models falter when introduced to
new, domain-specific KBs without prior training, significantly dropping in
performance. Our findings reveal that previous evaluation approaches fall short
of capturing real-world complexities for zero-shot EL, highlighting the
necessity for new approaches to design and assess conversational EL models to
adapt to limited resources. The evaluation setup and the dataset proposed in
this research are made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>-PQA: <span class="highlight-title">LLM</span>-enhanced Prediction Query Answering <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Li, Wenjie Zhao, Asterios Katsifodimos, Rihan Hai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models (LLMs) provides an opportunity to change
the way queries are processed, moving beyond the constraints of conventional
SQL-based database systems. However, using an LLM to answer a prediction query
is still challenging, since an external ML model has to be employed and
inference has to be performed in order to provide an answer. This paper
introduces LLM-PQA, a novel tool that addresses prediction queries formulated
in natural language. LLM-PQA is the first to combine the capabilities of LLMs
and retrieval-augmented mechanism for the needs of prediction queries by
integrating data lakes and model zoos. This integration provides users with
access to a vast spectrum of heterogeneous data and diverse ML models,
facilitating dynamic prediction query answering. In addition, LLM-PQA can
dynamically train models on demand, based on specific query requirements,
ensuring reliable and relevant results even when no pre-trained model in a
model zoo, available for the task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted as a demo at CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evidential Transformers for Improved Image Retrieval <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danilo Dordevic, Suryansh Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Evidential Transformer, an uncertainty-driven transformer
model for improved and robust image retrieval. In this paper, we make several
contributions to content-based image retrieval (CBIR). We incorporate
probabilistic methods into image retrieval, achieving robust and reliable
results, with evidential classification surpassing traditional training based
on multiclass classification as a baseline for deep metric learning.
Furthermore, we improve the state-of-the-art retrieval results on several
datasets by leveraging the Global Context Vision Transformer (GC ViT)
architecture. Our experimental results consistently demonstrate the reliability
of our approach, setting a new benchmark in CBIR in all test settings on the
Stanford Online Products (SOP) and CUB-200-2011 datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, To be presented at the 3rd Workshop on
  Uncertainty Quantification for Computer Vision, at the ECCV 2024 conference
  in Milan, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Diversity-Promoting Collaborative Metric Learning for
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilong Bao, Qianqian Xu, Zhiyong Yang, Yuan He, Xiaochun Cao, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative Metric Learning (CML) has recently emerged as a popular method
in recommendation systems (RS), closing the gap between metric learning and
collaborative filtering. Following the convention of RS, existing practices
exploit unique user representation in their model design. This paper focuses on
a challenging scenario where a user has multiple categories of interests. Under
this setting, the unique user representation might induce preference bias,
especially when the item category distribution is imbalanced. To address this
issue, we propose a novel method called \textit{Diversity-Promoting
Collaborative Metric Learning} (DPCML), with the hope of considering the
commonly ignored minority interest of the user. The key idea behind DPCML is to
introduce a set of multiple representations for each user in the system where
users' preference toward an item is aggregated by taking the minimum item-user
distance among their embedding set. Specifically, we instantiate two effective
assignment strategies to explore a proper quantity of vectors for each user.
Meanwhile, a \textit{Diversity Control Regularization Scheme} (DCRS) is
developed to accommodate the multi-vector representation strategy better.
Theoretically, we show that DPCML could induce a smaller generalization error
than traditional CML. Furthermore, we notice that CML-based approaches usually
require \textit{negative sampling} to reduce the heavy computational burden
caused by the pairwise objective therein. In this paper, we reveal the
fundamental limitation of the widely adopted hard-aware sampling from the
One-Way Partial AUC (OPAUC) perspective and then develop an effective sampling
alternative for the CML-based paradigm. Finally, comprehensive experiments over
a range of benchmark datasets speak to the efficacy of DPCML. Code are
available at \url{https://github.com/statusrank/LibCML}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2209.15292</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Investigating Biases in Spoken Conversational Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sachin Pathiyan Cherumanal, Falk Scholer, Johanne R. Trippas, Damiano Spina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice-based systems like Amazon Alexa, Google Assistant, and Apple Siri,
along with the growing popularity of OpenAI's ChatGPT and Microsoft's Copilot,
serve diverse populations, including visually impaired and low-literacy
communities. This reflects a shift in user expectations from traditional search
to more interactive question-answering models. However, presenting information
effectively in voice-only channels remains challenging due to their linear
nature. This limitation can impact the presentation of complex queries
involving controversial topics with multiple perspectives. Failing to present
diverse viewpoints may perpetuate or introduce biases and affect user
attitudes. Balancing information load and addressing biases is crucial in
designing a fair and effective voice-based system. To address this, we (i)
review how biases and user attitude changes have been studied in screen-based
web search, (ii) address challenges in studying these changes in voice-based
settings like SCS, (iii) outline research questions, and (iv) propose an
experimental setup with variables, data, and instruments to explore biases in a
voice-based setting like Spoken Conversational Search.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted Late-Breaking Results at ACM ICMI Companion 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Manipulating Large Language Models to Increase Product Visibility 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.07981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.07981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aounon Kumar, Himabindu Lakkaraju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly being integrated into search
engines to provide natural language responses tailored to user queries.
Customers and end-users are also becoming more dependent on these models for
quick and easy purchase decisions. In this work, we investigate whether
recommendations from LLMs can be manipulated to enhance a product's visibility.
We demonstrate that adding a strategic text sequence (STS) -- a carefully
crafted message -- to a product's information page can significantly increase
its likelihood of being listed as the LLM's top recommendation. To understand
the impact of STS, we use a catalog of fictitious coffee machines and analyze
its effect on two target products: one that seldom appears in the LLM's
recommendations and another that usually ranks second. We observe that the
strategic text sequence significantly enhances the visibility of both products
by increasing their chances of appearing as the top recommendation. This
ability to manipulate LLM-generated search responses provides vendors with a
considerable competitive advantage and has the potential to disrupt fair market
competition. Just as search engine optimization (SEO) revolutionized how
webpages are customized to rank higher in search engine results, influencing
LLM recommendations could profoundly impact content optimization for AI-driven
search services. Code for our experiments is available at
https://github.com/aounon/llm-rank-optimizer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A multi-language toolkit for supporting automated checking of research
  outputs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02935v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02935v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard J. Preen, Maha Albashir, Simon Davy, Jim Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents the automatic checking of research outputs package
acro, which assists researchers and data governance teams by automatically
applying best-practice principles-based statistical disclosure control (SDC)
techniques on-the-fly as researchers conduct their analyses. acro distinguishes
between: research output that is safe to publish; output that requires further
analysis; and output that cannot be published because it creates substantial
risk of disclosing private data. This is achieved through the use of a
lightweight Python wrapper that sits over well-known analysis tools that
produce outputs such as tables, plots, and statistical models. This adds
functionality to (i) identify potentially disclosive outputs against a range of
commonly used disclosure tests; (ii) apply disclosure mitigation strategies
where required; (iii) report reasons for applying SDC; and (iv) produce simple
summary documents trusted research environment staff can use to streamline
their workflow. The major analytical programming languages used by researchers
are supported: Python, R, and Stata. The acro code and documentation are
available under an MIT license at https://github.com/AI-SDC/ACRO
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VM-Rec: A Variational Mapping Approach for Cold-start User
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01304v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01304v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linan Zheng, Jiale Chen, Pengsheng Liu, Guangfa Zhang, Jinyun Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The cold-start problem is a common challenge for most recommender systems.
The practical application of most cold-start methods is hindered by the
deficiency in auxiliary content information for users. Moreover, most methods
necessitate simultaneous updates to the extensive parameters of recommender
models, leading to significant training costs, particularly in large-scale
industrial scenarios. We observe that the model can generate expressive
embeddings for warm users with relatively more interactions. Initially, these
users were cold-start users, and after transitioning to warm users, they
exhibit clustering patterns in their embeddings with consistent initial
interactions. Based on this motivation, we propose a Variational Mapping
approach for cold-start user Recommendation (VM-Rec), mapping from few initial
interactions to expressive embeddings for cold-start users. Specifically, we
encode the initial interactions into a latent representation, where each
dimension disentangledly signifies the degree of association with each warm
user. Subsequently, we utilize this latent representation as the parameters for
the mapping function, mapping (decoding) it into an expressive embedding, which
can be integrated into a pre-trained recommender model directly. Our method is
evaluated on three datasets using the same base model, demonstrating superior
performance compared to other popular cold-start methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05141v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05141v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Yuan, Chengwu Liu, Jingyang Yuan, Gongbo Sun, Siqi Li, Ming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is a framework enabling large language
models (LLMs) to enhance their accuracy and reduce hallucinations by
integrating external knowledge bases. In this paper, we introduce a hybrid RAG
system enhanced through a comprehensive suite of optimizations that
significantly improve retrieval quality, augment reasoning capabilities, and
refine numerical computation ability. We refined the text chunks and tables in
web pages, added attribute predictors to reduce hallucinations, conducted LLM
Knowledge Extractor and Knowledge Graph Extractor, and finally built a
reasoning strategy with all the references. We evaluated our system on the CRAG
dataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and
online evaluations demonstrate that our system significantly enhances complex
reasoning capabilities. In local evaluations, we have significantly improved
accuracy and reduced error rates compared to the baseline model, achieving a
notable increase in scores. In the meanwhile, we have attained outstanding
results in online assessments, demonstrating the performance and generalization
capabilities of the proposed system. The source code for our system is released
in \url{https://gitlab.aicrowd.com/shizueyy/crag-new}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report for 3rd prize in Task 1 of Meta CRAG KDD Cup 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PEPT: Expert Finding Meets Personalized <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12162v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12162v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyao Peng, Hongyan Xu, Yinghui Wang, Hongtao Liu, Cuiying Huo, Wenjun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding experts is essential in Community Question Answering (CQA) platforms
as it enables the effective routing of questions to potential users who can
provide relevant answers. The key is to personalized learning expert
representations based on their historical answered questions, and accurately
matching them with target questions. There have been some preliminary works
exploring the usability of PLMs in expert finding, such as pre-training expert
or question representations. However, these models usually learn pure text
representations of experts from histories, disregarding personalized and
fine-grained expert modeling. For alleviating this, we present a personalized
pre-training and fine-tuning paradigm, which could effectively learn expert
interest and expertise simultaneously. Specifically, in our pre-training
framework, we integrate historical answered questions of one expert with one
target question, and regard it as a candidate aware expert-level input unit.
Then, we fuse expert IDs into the pre-training for guiding the model to model
personalized expert representations, which can help capture the unique
characteristics and expertise of each individual expert. Additionally, in our
pre-training task, we design: 1) a question-level masked language model task to
learn the relatedness between histories, enabling the modeling of
question-level expert interest; 2) a vote-oriented task to capture
question-level expert expertise by predicting the vote score the expert would
receive. Through our pre-training framework and tasks, our approach could
holistically learn expert representations including interests and expertise.
Our method has been extensively evaluated on six real-world CQA datasets, and
the experimental results consistently demonstrate the superiority of our
approach over competitive baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectron: Target Speaker <span class="highlight-title">Extraction</span> using Conditional Transformer with
  Adversarial Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tathagata Bandyopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, attention-based transformers have become a de facto standard in
many deep learning applications including natural language processing, computer
vision, signal processing, etc.. In this paper, we propose a transformer-based
end-to-end model to extract a target speaker's speech from a monaural
multi-speaker mixed audio signal. Unlike existing speaker extraction methods,
we introduce two additional objectives to impose speaker embedding consistency
and waveform encoder invertibility and jointly train both speaker encoder and
speech separator to better capture the speaker conditional embedding.
Furthermore, we leverage a multi-scale discriminator to refine the perceptual
quality of the extracted speech. Our experiments show that the use of a dual
path transformer in the separator backbone along with proposed training
paradigm improves the CNN baseline by $3.12$ dB points. Finally, we compare our
approach with recent state-of-the-arts and show that our model outperforms
existing methods by $4.1$ dB points on an average without creating additional
data dependency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Reference Generative Face Video Compression with Contrastive
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Goluck Konuko, Giuseppe Valenzise
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative face video coding (GFVC) has been demonstrated as a potential
approach to low-latency, low bitrate video conferencing. GFVC frameworks
achieve an extreme gain in coding efficiency with over 70% bitrate savings when
compared to conventional codecs at bitrates below 10kbps. In recent MPEG/JVET
standardization efforts, all the information required to reconstruct video
sequences using GFVC frameworks are adopted as part of the supplemental
enhancement information (SEI) in existing compression pipelines. In light of
this development, we aim to address a challenge that has been weakly addressed
in prior GFVC frameworks, i.e., reconstruction drift as the distance between
the reference and target frames increases. This challenge creates the need to
update the reference buffer more frequently by transmitting more Intra-refresh
frames, which are the most expensive element of the GFVC bitstream. To overcome
this problem, we propose instead multiple reference animation as a robust
approach to minimizing reconstruction drift, especially when used in a
bi-directional prediction mode. Further, we propose a contrastive learning
formulation for multi-reference animation. We observe that using a contrastive
learning framework enhances the representation capabilities of the animation
generator. The resulting framework, MRDAC (Multi-Reference Deep Animation
Codec) can therefore be used to compress longer sequences with fewer reference
frames or achieve a significant gain in reconstruction accuracy at comparable
bitrates to previous frameworks. Quantitative and qualitative results show
significant coding and reconstruction quality gains compared to previous GFVC
methods, and more accurate animation quality in presence of large pose and
facial expression changes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Convolutional SyncNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungjoon Park, Jaesub Yun, Donggeon Lee, Minsik Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Because videos in the wild can be out of sync for various reasons, a sync-net
is used to bring the video back into sync for tasks that require synchronized
videos. Previous state-of-the-art (SOTA) sync-nets use InfoNCE loss, rely on
the transformer architecture, or both. Unfortunately, the former makes the
model's output difficult to interpret, and the latter is unfriendly with large
images, thus limiting the usefulness of sync-nets. In this work, we train a
convolutional sync-net using the balanced BCE loss (BBCE), a loss inspired by
the binary cross entropy (BCE) and the InfoNCE losses. In contrast to the
InfoNCE loss, the BBCE loss does not require complicated sampling schemes. Our
model can better handle larger images, and its output can be given a
probabilistic interpretation. The probabilistic interpretation allows us to
define metrics such as probability at offset and offscreen ratio to evaluate
the sync quality of audio-visual (AV) speech datasets. Furthermore, our model
achieves SOTA accuracy of $96.5\%$ on the LRS2 dataset and $93.8\%$ on the LRS3
dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8+5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inter-Frame Compression for Dynamic Point Cloud Geometry Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anique Akhtar, Zhu Li, Geert Van der Auwera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient point cloud compression is essential for applications like virtual
and mixed reality, autonomous driving, and cultural heritage. This paper
proposes a deep learning-based inter-frame encoding scheme for dynamic point
cloud geometry compression. We propose a lossy geometry compression scheme that
predicts the latent representation of the current frame using the previous
frame by employing a novel feature space inter-prediction network. The proposed
network utilizes sparse convolutions with hierarchical multiscale 3D feature
learning to encode the current frame using the previous frame. The proposed
method introduces a novel predictor network for motion compensation in the
feature domain to map the latent representation of the previous frame to the
coordinates of the current frame to predict the current frame's feature
embedding. The framework transmits the residual of the predicted features and
the actual features by compressing them using a learned probabilistic
factorized entropy model. At the receiver, the decoder hierarchically
reconstructs the current frame by progressively rescaling the feature
embedding. The proposed framework is compared to the state-of-the-art
Video-based Point Cloud Compression (V-PCC) and Geometry-based Point Cloud
Compression (G-PCC) schemes standardized by the Moving Picture Experts Group
(MPEG). The proposed method achieves more than 88% BD-Rate (Bjontegaard Delta
Rate) reduction against G-PCCv20 Octree, more than 56% BD-Rate savings against
G-PCCv20 Trisoup, more than 62% BD-Rate reduction against V-PCC intra-frame
encoding mode, and more than 52% BD-Rate savings against V-PCC P-frame-based
inter-frame encoding mode using HEVC. These significant performance gains are
cross-checked and verified in the MPEG working group.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Show Me the World in My Language: Establishing the First Baseline for
  Scene-Text to Scene-Text Translation <span class="chip">ICPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03024v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03024v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyas Vaidya, Arvind Kumar Sharma, Prajwal Gatti, Anand Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study the task of ``visually'' translating scene text from a
source language (e.g., Hindi) to a target language (e.g., English). Visual
translation involves not just the recognition and translation of scene text but
also the generation of the translated image that preserves visual features of
the source scene text, such as font, size, and background. There are several
challenges associated with this task, such as translation with limited context,
deciding between translation and transliteration, accommodating varying text
lengths within fixed spatial boundaries, and preserving the font and background
styles of the source scene text in the target language. To address this
problem, we make the following contributions: (i) We study visual translation
as a standalone problem for the first time in the literature. (ii) We present a
cascaded framework for visual translation that combines state-of-the-art
modules for scene text recognition, machine translation, and scene text
synthesis as a baseline for the task. (iii) We propose a set of task-specific
design enhancements to design a variant of the baseline to obtain performance
improvements. (iv) Currently, the existing related literature lacks any
comprehensive performance evaluation for this novel task. To fill this gap, we
introduce several automatic and user-assisted evaluation metrics designed
explicitly for evaluating visual translation. Further, we evaluate presented
baselines for translating scene text between Hindi and English. Our experiments
demonstrate that although we can effectively perform visual translation over a
large collection of scene text images, the presented baseline only partially
addresses challenges posed by visual translation tasks. We firmly believe that
this new task and the limitations of existing models, as reported in this
paper, should encourage further research in visual translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICPR 2024, Project Website:
  https://vl2g.github.io/projects/visTrans/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-09-01T00:00:00Z">2024-09-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dissecting Temporal Understanding in Text-to-Audio Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreea-Maria Oncescu, João F. Henriques, A. Sophia Koepke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in machine learning have fueled research on multimodal
tasks, such as for instance text-to-video and text-to-audio retrieval. These
tasks require models to understand the semantic content of video and audio
data, including objects, and characters. The models also need to learn spatial
arrangements and temporal relationships. In this work, we analyse the temporal
ordering of sounds, which is an understudied problem in the context of
text-to-audio retrieval. In particular, we dissect the temporal understanding
capabilities of a state-of-the-art model for text-to-audio retrieval on the
AudioCaps and Clotho datasets. Additionally, we introduce a synthetic
text-audio dataset that provides a controlled setting for evaluating temporal
capabilities of recent models. Lastly, we present a loss function that
encourages text-audio models to focus on the temporal ordering of events. Code
and data are available at
https://www.robots.ox.ac.uk/~vgg/research/audio-retrieval/dtu/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, ACM Multimedia 2024,
  https://www.robots.ox.ac.uk/~vgg/research/audio-retrieval/dtu/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building FKG.in: a Knowledge Graph for Indian Food 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saransh Kumar Gupta, Lipika Dey, Partha Pratim Das, Ramesh Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an ontology design along with knowledge engineering, and
multilingual semantic reasoning techniques to build an automated system for
assimilating culinary information for Indian food in the form of a knowledge
graph. The main focus is on designing intelligent methods to derive ontology
designs and capture all-encompassing knowledge about food, recipes,
ingredients, cooking characteristics, and most importantly, nutrition, at
scale. We present our ongoing work in this workshop paper, describe in some
detail the relevant challenges in curating knowledge of Indian food, and
propose our high-level ontology design. We also present a novel workflow that
uses AI, LLM, and language technology to curate information from recipe blog
sites in the public domain to build knowledge graphs for Indian food. The
methods for knowledge curation proposed in this paper are generic and can be
replicated for any domain. The design is application-agnostic and can be used
for AI-driven smart analysis, building recommendation systems for Personalized
Digital Health, and complementing the knowledge graph for Indian food with
contextual information such as user information, food biochemistry, geographic
information, agricultural information, etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures, 25 references, Formal Ontology in Information
  Systems Conference 2024 - Integrated Food Ontology Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hound: Hunting Supervision Signals for Few and Zero Shot Node
  Classification on Text-attributed Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Wang, Xiao Yan, Shiyu Jin, Quanqing Xu, Chuanhui Yang, Yuanyuan Zhu, Chuang Hu, Bo Du, Jiawei Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-attributed graph (TAG) is an important type of graph structured data
with text descriptions for each node. Few- and zero-shot node classification on
TAGs have many applications in fields such as academia and social networks.
However, the two tasks are challenging due to the lack of supervision signals,
and existing methods only use the contrastive loss to align graph-based node
embedding and language-based text embedding. In this paper, we propose Hound to
improve accuracy by introducing more supervision signals, and the core idea is
to go beyond the node-text pairs that come with data. Specifically, we design
three augmentation techniques, i.e., node perturbation, text matching, and
semantics negation to provide more reference nodes for each text and vice
versa. Node perturbation adds/drops edges to produce diversified node
embeddings that can be matched with a text. Text matching retrieves texts with
similar embeddings to match with a node. Semantics negation uses a negative
prompt to construct a negative text with the opposite semantics, which is
contrasted with the original node and text. We evaluate Hound on 5 datasets and
compare with 13 state-of-the-art baselines. The results show that Hound
consistently outperforms all baselines, and its accuracy improvements over the
best-performing baseline are usually over 5%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair Reciprocal Recommendation in Matching Markets <span class="chip">RecSys2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoji Tomita, Tomohiki Yokoyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems play an increasingly crucial role in shaping people's
opportunities, particularly in online dating platforms. It is essential from
the user's perspective to increase the probability of matching with a suitable
partner while ensuring an appropriate level of fairness in the matching
opportunities. We investigate reciprocal recommendation in two-sided matching
markets between agents divided into two sides. In our model, a match is
considered successful only when both individuals express interest in each
other. Additionally, we assume that agents prefer to appear prominently in the
recommendation lists presented to those on the other side. We define each
agent's opportunity to be recommended and introduce its fairness criterion,
envy-freeness, from the perspective of fair division theory. The
recommendations that approximately maximize the expected number of matches,
empirically obtained by heuristic algorithms, are likely to result in
significant unfairness of opportunity. Therefore, there can be a trade-off
between maximizing the expected matches and ensuring fairness of opportunity.
To address this challenge, we propose a method to find a policy that is close
to being envy-free by leveraging the Nash social welfare function. Experiments
on synthetic and real-world datasets demonstrate the effectiveness of our
approach in achieving both relatively high expected matches and fairness for
opportunities of both sides in reciprocal recommender systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at RecSys2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Learnable Agent Collaboration Network Framework for Personalized
  Multimodal AI Search Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxiao Shi, Min Xu, Haimin Zhang, Xing Zi, Qiang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) and retrieval-augmented generation (RAG)
techniques have revolutionized traditional information access, enabling AI
agent to search and summarize information on behalf of users during dynamic
dialogues. Despite their potential, current AI search engines exhibit
considerable room for improvement in several critical areas. These areas
include the support for multimodal information, the delivery of personalized
responses, the capability to logically answer complex questions, and the
facilitation of more flexible interactions. This paper proposes a novel AI
Search Engine framework called the Agent Collaboration Network (ACN). The ACN
framework consists of multiple specialized agents working collaboratively, each
with distinct roles such as Account Manager, Solution Strategist, Information
Manager, and Content Creator. This framework integrates mechanisms for picture
content understanding, user profile tracking, and online evolution, enhancing
the AI search engine's response quality, personalization, and interactivity. A
highlight of the ACN is the introduction of a Reflective Forward Optimization
method (RFO), which supports the online synergistic adjustment among agents.
This feature endows the ACN with online learning capabilities, ensuring that
the system has strong interactive flexibility and can promptly adapt to user
feedback. This learning method may also serve as an optimization approach for
agent-based systems, potentially influencing other domains of agent
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACMMM 2024 MMGR WORKSHOP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Boundary Time Warping for Sub-sequence Matching with Few
  Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.14464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.14464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Łukasz Borchmann, Dawid Jurkiewicz, Filip Graliński, Tomasz Górecki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper presents a novel method of finding a fragment in a long temporal
sequence similar to the set of shorter sequences. We are the first to propose
an algorithm for such a search that does not rely on computing the average
sequence from query examples. Instead, we use query examples as is, utilizing
all of them simultaneously. The introduced method based on the Dynamic Time
Warping (DTW) technique is suited explicitly for few-shot query-by-example
retrieval tasks. We evaluate it on two different few-shot problems from the
field of Natural Language Processing. The results show it either outperforms
baselines and previous approaches or achieves comparable results when a low
number of examples is available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forecasting Live Chat Intent from Browsing History <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04668v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04668v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Se-eun Yoon, Ahmad Bin Rabiah, Zaid Alibadi, Surya Kallumadi, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customers reach out to online live chat agents with various intents, such as
asking about product details or requesting a return. In this paper, we propose
the problem of predicting user intent from browsing history and address it
through a two-stage approach. The first stage classifies a user's browsing
history into high-level intent categories. Here, we represent each browsing
history as a text sequence of page attributes and use the ground-truth class
labels to fine-tune pretrained Transformers. The second stage provides a large
language model (LLM) with the browsing history and predicted intent class to
generate fine-grained intents. For automatic evaluation, we use a separate LLM
to judge the similarity between generated and ground-truth intents, which
closely aligns with human judgments. Our two-stage approach yields significant
performance gains compared to generating intents without the classification
stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparsity-regularized coded ptychography for robust and efficient
  lensless microscopy on a chip 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ninghe Liu, Qianhao Zhao, Guoan Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coded ptychography has emerged as a powerful technique for high-throughput,
high-resolution lensless imaging. However, the trade-off between acquisition
speed and image quality remains a significant challenge. To address this, we
introduce a novel sparsity-regularized approach to coded ptychography that
dramatically reduces the number of required measurements while maintaining high
reconstruction quality. The reported approach, termed the ptychographic
proximal total-variation (PPTV) solver, formulates the reconstruction task as a
total variation regularized optimization problem. Unlike previous
implementations that rely on specialized hardware or illumination schemes, PPTV
integrates seamlessly into existing coded ptychography setups. Through
comprehensive numerical simulations, we demonstrate that PPTV-driven coded
ptychography can produce accurate reconstructions with as few as eight
intensity measurements, a significant reduction compared to conventional
methods. Convergence analysis confirms the robustness and stability of the PPTV
algorithm. Experimental results from our optical prototype, featuring a
disorder-engineered surface for wavefront modulation, validate PPTV's ability
to achieve high-throughput, high-resolution imaging with a substantially
reduced measurement burden. By enabling high-quality reconstructions from fewer
measurements, PPTV paves the way for more compact, efficient, and
cost-effective lensless microscopy systems on a chip, with potential
applications in digital pathology, endoscopy, point-of-care diagnostics, and
high-content screening.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vague Preference Policy Learning for Conversational Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04487v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04487v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gangyi Zhang, Chongming Gao, Wenqiang Lei, Xiaojie Guo, Shijun Li, Hongshen Chen, Zhuozhi Ding, Sulong Xu, Lingfei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational recommendation systems (CRS) commonly assume users have clear
preferences, leading to potential over-filtering of relevant alternatives.
However, users often exhibit vague, non-binary preferences. We introduce the
Vague Preference Multi-round Conversational Recommendation (VPMCR) scenario,
employing a soft estimation mechanism to accommodate users' vague and dynamic
preferences while mitigating over-filtering. In VPMCR, we propose Vague
Preference Policy Learning (VPPL), consisting of Ambiguity-aware Soft
Estimation (ASE) and Dynamism-aware Policy Learning (DPL). ASE captures
preference vagueness by estimating scores for clicked and non-clicked options,
using a choice-based approach and time-aware preference decay. DPL leverages
ASE's preference distribution to guide the conversation and adapt to preference
changes for recommendations or attribute queries. Extensive experiments
demonstrate VPPL's effectiveness within VPMCR, outperforming existing methods
and setting a new benchmark. Our work advances CRS by accommodating users'
inherent ambiguity and relative decision-making processes, improving real-world
applicability.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaDigiHuman: Haptic Interfaces for Digital Humans in Metaverse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Senthil Kumar Jagatheesaperumal, Praveen Sathikumar, Harikrishnan Rajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The way we engage with digital spaces and the digital world has undergone
rapid changes in recent years, largely due to the emergence of the Metaverse.
As technology continues to advance, the demand for sophisticated and immersive
interfaces to interact with the Metaverse has become increasingly crucial.
Haptic interfaces have been developed to meet this need and provide users with
tactile feedback and realistic touch sensations. These interfaces play a vital
role in creating a more authentic and immersive experience within the
Metaverse. This article introduces the concept of MetaDigiHuman, a
groundbreaking framework that combines blended digital humans and haptic
interfaces. By harnessing cutting-edge technologies, MetaDigiHuman enables
seamless and immersive interaction within the Metaverse. Through this
framework, users can simulate the sensation of touching, feeling, and
interacting with digital beings as if they were physically present in the
environments, offering a more compelling and immersive experience within the
Metaverse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Multi-turn Conversation Stance Detection: A Challenge <span class="highlight-title">Dataset</span>
  and Effective Model <span class="chip">ACM MM2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuqiang Niu, Zebang Cheng, Xianghua Fu, Xiaojiang Peng, Genan Dai, Yin Chen, Hu Huang, Bowen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stance detection, which aims to identify public opinion towards specific
targets using social media data, is an important yet challenging task. With the
proliferation of diverse multimodal social media content including text, and
images multimodal stance detection (MSD) has become a crucial research area.
However, existing MSD studies have focused on modeling stance within individual
text-image pairs, overlooking the multi-party conversational contexts that
naturally occur on social media. This limitation stems from a lack of datasets
that authentically capture such conversational scenarios, hindering progress in
conversational MSD. To address this, we introduce a new multimodal multi-turn
conversational stance detection dataset (called MmMtCSD). To derive stances
from this challenging dataset, we propose a novel multimodal large language
model stance detection framework (MLLM-SD), that learns joint stance
representations from textual and visual modalities. Experiments on MmMtCSD show
state-of-the-art performance of our proposed MLLM-SD approach for multimodal
stance detection. We believe that MmMtCSD will contribute to advancing
real-world applications of stance detection research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM MM2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harmonizing Attention: Training-free Texture-aware Geometry Transfer <span class="chip">WACV2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10846v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10846v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eito Ikuta, Yohan Lee, Akihiro Iohara, Yu Saito, Toshiyuki Tanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting geometry features from photographic images independently of
surface texture and transferring them onto different materials remains a
complex challenge. In this study, we introduce Harmonizing Attention, a novel
training-free approach that leverages diffusion models for texture-aware
geometry transfer. Our method employs a simple yet effective modification of
self-attention layers, allowing the model to query information from multiple
reference images within these layers. This mechanism is seamlessly integrated
into the inversion process as Texture-aligning Attention and into the
generation process as Geometry-aligning Attention. This dual-attention approach
ensures the effective capture and transfer of material-independent geometry
features while maintaining material-specific textural continuity, all without
the need for model fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-31T00:00:00Z">2024-08-31</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PSLF: A PID Controller-incorporated Second-order Latent Factor Analysis
  Model for Recommender System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialiang Wang, Yan Xia, Ye Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A second-order-based latent factor (SLF) analysis model demonstrates superior
performance in graph representation learning, particularly for high-dimensional
and incomplete (HDI) interaction data, by incorporating the curvature
information of the loss landscape. However, its objective function is commonly
bi-linear and non-convex, causing the SLF model to suffer from a low
convergence rate. To address this issue, this paper proposes a PID
controller-incorporated SLF (PSLF) model, leveraging two key strategies: a)
refining learning error estimation by incorporating the PID controller
principles, and b) acquiring second-order information insights through
Hessian-vector products. Experimental results on multiple HDI datasets indicate
that the proposed PSLF model outperforms four state-of-the-art latent factor
models based on advanced optimizers regarding convergence rates and
generalization performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Enhanced Batch Query Architecture in Real-time Recommendation <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Zhang, Zhipeng Teng, Disheng Wu, Jiayin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In industrial recommendation systems on websites and apps, it is essential to
recall and predict top-n results relevant to user interests from a content pool
of billions within milliseconds. To cope with continuous data growth and
improve real-time recommendation performance, we have designed and implemented
a high-performance batch query architecture for real-time recommendation
systems. Our contributions include optimizing hash structures with a
cacheline-aware probing method to enhance coalesced hashing, as well as the
implementation of a hybrid storage key-value service built upon it. Our
experiments indicate this approach significantly surpasses conventional hash
tables in batch query throughput, achieving up to 90% of the query throughput
of random memory access when incorporating parallel optimization. The support
for NVMe, integrating two-tier storage for hot and cold data, notably reduces
resource consumption. Additionally, the system facilitates dynamic updates,
automated sharding of attributes and feature embedding tables, and introduces
innovative protocols for consistency in batch queries, thereby enhancing the
effectiveness of real-time incremental learning updates. This architecture has
been deployed and in use in the bilibili recommendation system for over a year,
a video content community with hundreds of millions of users, supporting 10x
increase in model computation with minimal resource growth, improving outcomes
while preserving the system's real-time performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures, CIKM 2024 Applied Research Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoding Knowledge Claims: The Evaluation of Scientific Publication
  Contributions through Semantic Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18646v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18646v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca D'Aniello, Nicolas Robinson-Garcia, Massimo Aria, Corrado Cuccurullo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The surge in scientific publications challenges the use of publication counts
as a measure of scientific progress, requiring alternative metrics that
emphasize the quality and novelty of scientific contributions rather than sheer
quantity. This paper proposes the use of Relaxed Word Mover's Distance (RWMD),
a semantic text similarity measure, to evaluate the novelty of scientific
papers. We hypothesize that RWMD can more effectively gauge the growth of
scientific knowledge. To test such an assumption, we apply RWMD to evaluate
seminal papers, with Hirsch's H-Index paper as a primary case study. We compare
RWMD results across three groups: 1) H-Index-related papers, 2) scientometric
studies, and 3) unrelated papers, aiming to discern redundant literature and
hype from genuine innovations. Findings suggest that emphasizing knowledge
claims offers a deeper insight into scientific contributions, marking RWMD as a
promising alternative method to traditional citation metrics, thus better
tracking significant scientific breakthroughs.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Analysis of Modality Fusion Approaches for Audio-Visual
  Person Identification and Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aref Farhadipour, Masoumeh Chapariniya, Teodora Vukovic, Volker Dellwo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning involves integrating information from various modalities
to enhance learning and comprehension. We compare three modality fusion
strategies in person identification and verification by processing two
modalities: voice and face. In this paper, a one-dimensional convolutional
neural network is employed for x-vector extraction from voice, while the
pre-trained VGGFace2 network and transfer learning are utilized for face
modality. In addition, gammatonegram is used as speech representation in
engagement with the Darknet19 pre-trained network. The proposed systems are
evaluated using the K-fold cross-validation technique on the 118 speakers of
the test set of the VoxCeleb2 dataset. The comparative evaluations are done for
single-modality and three proposed multimodal strategies in equal situations.
Results demonstrate that the feature fusion strategy of gammatonegram and
facial features achieves the highest performance, with an accuracy of 98.37% in
the person identification task. However, concatenating facial features with the
x-vector reaches 0.62% for EER in verification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been submitted to a conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digit Recognition using Multimodal Spiking Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Bjorndahl, Jack Easton, Austin Modoff, Eric C. Larson, Joseph Camp, Prasanna Rangarajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNNs) are the third generation of neural networks
that are biologically inspired to process data in a fashion that emulates the
exchange of signals in the brain. Within the Computer Vision community SNNs
have garnered significant attention due in large part to the availability of
event-based sensors that produce a spatially resolved spike train in response
to changes in scene radiance. SNNs are used to process event-based data due to
their neuromorphic nature. The proposed work examines the neuromorphic
advantage of fusing multiple sensory inputs in classification tasks.
Specifically we study the performance of a SNN in digit classification by
passing in a visual modality branch (Neuromorphic-MNIST [N-MNIST]) and an
auditory modality branch (Spiking Heidelberg Digits [SHD]) from datasets that
were created using event-based sensors to generate a series of time-dependent
events. It is observed that multi-modal SNNs outperform unimodal visual and
unimodal auditory SNNs. Furthermore, it is observed that the process of sensory
fusion is insensitive to the depth at which the visual and auditory branches
are combined. This work achieves a 98.43% accuracy on the combined N-MNIST and
SHD dataset using a multimodal SNN that concatenates the visual and auditory
branches at a late depth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures, submitted to 2025 IEEE International Conference
  on Acoustics, Speech, and Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-scale Multi-instance Visual Sound Localization and Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shentong Mo, Haofan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual sound localization is a typical and challenging problem that predicts
the location of objects corresponding to the sound source in a video. Previous
methods mainly used the audio-visual association between global audio and
one-scale visual features to localize sounding objects in each image. Despite
their promising performance, they omitted multi-scale visual features of the
corresponding image, and they cannot learn discriminative regions compared to
ground truths. To address this issue, we propose a novel multi-scale
multi-instance visual sound localization framework, namely M2VSL, that can
directly learn multi-scale semantic features associated with sound sources from
the input image to localize sounding objects. Specifically, our M2VSL leverages
learnable multi-scale visual features to align audio-visual representations at
multi-level locations of the corresponding image. We also introduce a novel
multi-scale multi-instance transformer to dynamically aggregate multi-scale
cross-modal representations for visual sound localization. We conduct extensive
experiments on VGGSound-Instruments, VGG-Sound Sources, and AVSBench
benchmarks. The results demonstrate that the proposed M2VSL can achieve
state-of-the-art performance on sounding object localization and segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Palantir: Towards Efficient Super Resolution for Ultra-high-definition
  Live Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06152v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06152v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinqi Jin, Zhui Zhu, Xikai Sun, Fan Dang, Jiangchuan Liu, Jingao Xu, Kebin Liu, Xinlei Chen, Yunhao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural enhancement through super-resolution (SR) deep neural networks (DNNs)
opens up new possibilities for ultra-high-definition (UHD) live streaming over
existing encoding and networking infrastructure. Yet, the heavy SR DNN
inference overhead leads to severe deployment challenges. To reduce the
overhead, existing systems propose to apply DNN-based SR only on carefully
selected anchor frames while upscaling non-anchor frames via the lightweight
reusing-based SR approach. However, frame-level scheduling is coarse-grained
and fails to deliver optimal efficiency. In this work, we propose Palantir, the
first neural-enhanced UHD live streaming system with fine-grained patch-level
scheduling. Two novel techniques are incorporated into Palantir to select the
most beneficial anchor patches and support latency-sensitive UHD live streaming
applications. Firstly, under the guidance of our pioneering and theoretical
analysis, Palantir constructs a directed acyclic graph (DAG) for lightweight
yet accurate SR quality estimation under any possible anchor patch set.
Secondly, to further optimize the scheduling latency, Palantir improves
parallelizability by refactoring the computation subprocedure of the estimation
process into a sparse matrix-matrix multiplication operation.
  The evaluation results suggest that Palantir incurs a negligible scheduling
latency accounting for less than 5.7% of the end-to-end latency requirement.
When compared to the naive method of applying DNN-based SR on all the frames,
Palantir can reduce the SR DNN inference overhead by 20 times (or 60 times)
while preserving 54.0-82.6% (or 32.8-64.0%) of the quality gain. When compared
to the state-of-the-art real-time frame-level scheduling strategy, Palantir can
reduce the SR DNN inference overhead by 80.1% at most (and 38.4% on average)
without sacrificing the video quality.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-30T00:00:00Z">2024-08-30</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facilitating phenotyping from clinical texts: the medkit library 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Neuraz, Ghislain Vaillant, Camila Arias, Olivier Birot, Kim-Tam Huynh, Thibaut Fabacher, Alice Rogier, Nicolas Garcelon, Ivan Lerner, Bastien Rance, Adrien Coulet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phenotyping consists in applying algorithms to identify individuals
associated with a specific, potentially complex, trait or condition, typically
out of a collection of Electronic Health Records (EHRs). Because a lot of the
clinical information of EHRs are lying in texts, phenotyping from text takes an
important role in studies that rely on the secondary use of EHRs. However, the
heterogeneity and highly specialized aspect of both the content and form of
clinical texts makes this task particularly tedious, and is the source of time
and cost constraints in observational studies. To facilitate the development,
evaluation and reproductibility of phenotyping pipelines, we developed an
open-source Python library named medkit. It enables composing data processing
pipelines made of easy-to-reuse software bricks, named medkit operations. In
addition to the core of the library, we share the operations and pipelines we
already developed and invite the phenotyping community for their reuse and
enrichment. medkit is available at https://github.com/medkit-lib/medkit
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Videos Become Outdated: Short-Video Recommendation by Learning
  to Deconfound Release Interval Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17332v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17332v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lulu Dong, Guoxiu He, Aixin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Short-video recommender systems often exhibit a biased preference to recently
released videos. However, not all videos become outdated; certain classic
videos can still attract user's attention. Such bias along temporal dimension
can be further aggravated by the matching model between users and videos,
because the model learns from preexisting interactions. From real data, we
observe that different videos have varying sensitivities to recency in
attracting users' attention. Our analysis, based on a causal graph modeling
short-video recommendation, suggests that the release interval serves as a
confounder, establishing a backdoor path between users and videos. To address
this confounding effect, we propose a model-agnostic causal architecture called
Learning to Deconfound the Release Interval Bias (LDRI). LDRI enables jointly
learning of the matching model and the video recency sensitivity perceptron. In
the inference stage, we apply a backdoor adjustment, effectively blocking the
backdoor path by intervening on each video. Extensive experiments on two
benchmarks demonstrate that LDRI consistently outperforms backbone models and
exhibits superior performance against state-of-the-art models. Additional
comprehensive analyses confirm the deconfounding capability of LDRI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metadata practices for simulation workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Villamar, Matthias Kelbling, Heather L. More, Michael Denker, Tom Tetzlaff, Johanna Senk, Stephan Thober
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer simulations are an essential pillar of knowledge generation in
science. Understanding, reproducing, and exploring the results of simulations
relies on tracking and organizing metadata describing numerical experiments.
However, the models used to understand real-world systems, and the
computational machinery required to simulate them, are typically complex, and
produce large amounts of heterogeneous metadata. Here, we present general
practices for acquiring and handling metadata that are agnostic to software and
hardware, and highly flexible for the user. These consist of two steps: 1)
recording and storing raw metadata, and 2) selecting and structuring metadata.
As a proof of concept, we develop the Archivist, a Python tool to help with the
second step, and use it to apply our practices to distinct high-performance
computing use cases from neuroscience and hydrology. Our practices and the
Archivist can readily be applied to existing workflows without the need for
substantial restructuring. They support sustainable numerical workflows,
facilitating reproducibility and data reuse in generic simulation-based
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Multi-task <span class="highlight-title">Prompt</span> Tuning for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting Bai, Le Huang, Yue Yu, Cheng Yang, Cheng Hou, Zhe Zhao, Chuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the expansion of business scenarios, real recommender systems are facing
challenges in dealing with the constantly emerging new tasks in multi-task
learning frameworks. In this paper, we attempt to improve the generalization
ability of multi-task recommendations when dealing with new tasks. We find that
joint training will enhance the performance of the new task but always
negatively impact existing tasks in most multi-task learning methods. Besides,
such a re-training mechanism with new tasks increases the training costs,
limiting the generalization ability of multi-task recommendation models. Based
on this consideration, we aim to design a suitable sharing mechanism among
different tasks while maintaining joint optimization efficiency in new task
learning. A novel two-stage prompt-tuning MTL framework (MPT-Rec) is proposed
to address task irrelevance and training efficiency problems in multi-task
recommender systems. Specifically, we disentangle the task-specific and
task-sharing information in the multi-task pre-training stage, then use
task-aware prompts to transfer knowledge from other tasks to the new task
effectively. By freezing parameters in the pre-training tasks, MPT-Rec solves
the negative impacts that may be brought by the new task and greatly reduces
the training costs. Extensive experiments on three real-world datasets show the
effectiveness of our proposed multi-task learning framework. MPT-Rec achieves
the best performance compared to the SOTA multi-task learning method. Besides,
it maintains comparable model performance but vastly improves the training
efficiency (i.e., with up to 10% parameters in the full training way) in the
new task learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying and Clustering Counter Relationships of Team Compositions in
  PvP Games for Efficient Balance Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiu-Chou Lin, Yu-Wei Shih, Kuei-Ting Kuo, Yu-Cheng Chen, Chien-Hua Chen, Wei-Chen Chiu, I-Chen Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can balance be quantified in game settings? This question is crucial for
game designers, especially in player-versus-player (PvP) games, where analyzing
the strength relations among predefined team compositions-such as hero
combinations in multiplayer online battle arena (MOBA) games or decks in card
games-is essential for enhancing gameplay and achieving balance. We have
developed two advanced measures that extend beyond the simplistic win rate to
quantify balance in zero-sum competitive scenarios. These measures are derived
from win value estimations, which employ strength rating approximations via the
Bradley-Terry model and counter relationship approximations via vector
quantization, significantly reducing the computational complexity associated
with traditional win value estimations. Throughout the learning process of
these models, we identify useful categories of compositions and pinpoint their
counter relationships, aligning with the experiences of human players without
requiring specific game knowledge. Our methodology hinges on a simple technique
to enhance codebook utilization in discrete representation with a deterministic
vector quantization process for an extremely small state space. Our framework
has been validated in popular online games, including Age of Empires II,
Hearthstone, Brawl Stars, and League of Legends. The accuracy of the observed
strength relations in these games is comparable to traditional pairwise win
value predictions, while also offering a more manageable complexity for
analysis. Ultimately, our findings contribute to a deeper understanding of PvP
game dynamics and present a methodology that significantly improves game
balance evaluation and design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TMLR 09/2024 https://openreview.net/forum?id=2D36otXvBE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the User: An Intent-Based Ranking <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhijit Anand, Jurek Leonhardt, V Venktesh, Avishek Anand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As information retrieval systems continue to evolve, accurate evaluation and
benchmarking of these systems become pivotal. Web search datasets, such as MS
MARCO, primarily provide short keyword queries without accompanying intent or
descriptions, posing a challenge in comprehending the underlying information
need. This paper proposes an approach to augmenting such datasets to annotate
informative query descriptions, with a focus on two prominent benchmark
datasets: TREC-DL-21 and TREC-DL-22. Our methodology involves utilizing
state-of-the-art LLMs to analyze and comprehend the implicit intent within
individual queries from benchmark datasets. By extracting key semantic
elements, we construct detailed and contextually rich descriptions for these
queries. To validate the generated query descriptions, we employ crowdsourcing
as a reliable means of obtaining diverse human perspectives on the accuracy and
informativeness of the descriptions. This information can be used as an
evaluation set for tasks such as ranking, query rewriting, or others.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Table Representations to Answer Questions from Tables in
  Documents : A Case Study using 3GPP Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sujoy Roychowdhury, Sumit Soman, HG Ranjani, Avantika Sharma, Neeraj Gunda, Sai Krishna Bala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the ubiquitous use of document corpora for question answering, one
important aspect which is especially relevant for technical documents is the
ability to extract information from tables which are interspersed with text.
The major challenge in this is that unlike free-flow text or isolated set of
tables, the representation of a table in terms of what is a relevant chunk is
not obvious. We conduct a series of experiments examining various
representations of tabular data interspersed with text to understand the
relative benefits of different representations. We choose a corpus of $3^{rd}$
Generation Partnership Project (3GPP) documents since they are heavily
interspersed with tables. We create expert curated dataset of question answers
to evaluate our approach. We conclude that row level representations with
corresponding table header information being included in every cell improves
the performance of the retrieval, thus leveraging the structural information
present in the tabular data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond One-Size-Fits-All: Multi-Domain, Multi-Task Framework for
  Embedding Model Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Khetan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This position paper proposes a systematic approach towards developing a
framework to help select the most effective embedding models for natural
language processing (NLP) tasks, addressing the challenge posed by the
proliferation of both proprietary and open-source encoder models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>It was an initial idea - we plan to work on a detailed version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein A. Rahmani, Xi Wang, Emine Yilmaz, Nick Craswell, Bhaskar Mitra, Paul Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale test collections play a crucial role in Information Retrieval
(IR) research. However, according to the Cranfield paradigm and the research
into publicly available datasets, the existing information retrieval research
studies are commonly developed on small-scale datasets that rely on human
assessors for relevance judgments - a time-intensive and expensive process.
Recent studies have shown the strong capability of Large Language Models (LLMs)
in producing reliable relevance judgments with human accuracy but at a greatly
reduced cost. In this paper, to address the missing large-scale ad-hoc document
retrieval dataset, we extend the TREC Deep Learning Track (DL) test collection
via additional language model synthetic labels to enable researchers to test
and evaluate their search systems at a large scale. Specifically, such a test
collection includes more than 1,900 test queries from the previous years of
tracks. We compare system evaluation with past human labels from past years and
find that our synthetically created large-scale test collection can lead to
highly correlated system rankings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, resource paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Importance of Cognitive Biases in the Recommendation Ecosystem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus Schedl, Oleg Lesota, Stefan Brandl, Mohammad Lotfi, Gustavo Junior Escobedo Ticona, Shahed Masoudian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cognitive biases have been studied in psychology, sociology, and behavioral
economics for decades. Traditionally, they have been considered a negative
human trait that leads to inferior decision-making, reinforcement of
stereotypes, or can be exploited to manipulate consumers, respectively. We
argue that cognitive biases also manifest in different parts of the
recommendation ecosystem and at different stages of the recommendation process.
More importantly, we contest this traditional detrimental perspective on
cognitive biases and claim that certain cognitive biases can be beneficial when
accounted for by recommender systems. Concretely, we provide empirical evidence
that biases such as feature-positive effect, Ikea effect, and cultural
homophily can be observed in various components of the recommendation pipeline,
including input data (such as ratings or side information), recommendation
algorithm or model (and consequently recommended items), and user interactions
with the system. In three small experiments covering recruitment and
entertainment domains, we study the pervasiveness of the aforementioned biases.
We ultimately advocate for a prejudice-free consideration of cognitive biases
to improve user and item models as well as recommendation algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceptual Similarity for Measuring Decision-Making Style and Policy
  Diversity in Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiu-Chou Lin, Wei-Chen Chiu, I-Chen Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Defining and measuring decision-making styles, also known as playstyles, is
crucial in gaming, where these styles reflect a broad spectrum of individuality
and diversity. However, finding a universally applicable measure for these
styles poses a challenge. Building on Playstyle Distance, the first
unsupervised metric to measure playstyle similarity based on game screens and
raw actions, we introduce three enhancements to increase accuracy: multiscale
analysis with varied state granularity, a perceptual kernel rooted in
psychology, and the utilization of the intersection-over-union method for
efficient evaluation. These innovations not only advance measurement precision
but also offer insights into human cognition of similarity. Across two racing
games and seven Atari games, our techniques significantly improve the precision
of zero-shot playstyle classification, achieving an accuracy exceeding 90
percent with fewer than 512 observation-action pairs, which is less than half
an episode of these games. Furthermore, our experiments with 2048 and Go
demonstrate the potential of discrete playstyle measures in puzzle and board
games. We also develop an algorithm for assessing decision-making diversity
using these measures. Our findings improve the measurement of end-to-end game
analysis and the evolution of artificial intelligence for diverse playstyles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TMLR 08/2024 https://openreview.net/forum?id=30C9AWBW49</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video to Music Moment Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijie Xin, Minquan Wang, Ye Ma, Bo Wang, Quan Chen, Peng Jiang, Xirong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adding proper background music helps complete a short video to be shared.
Towards automating the task, previous research focuses on video-to-music
retrieval (VMR), aiming to find amidst a collection of music the one best
matching the content of a given video. Since music tracks are typically much
longer than short videos, meaning the returned music has to be cut to a shorter
moment, there is a clear gap between the practical need and VMR. In order to
bridge the gap, we propose in this paper video to music moment retrieval (VMMR)
as a new task. To tackle the new task, we build a comprehensive dataset
Ad-Moment which contains 50K short videos annotated with music moments and
develop a two-stage approach. In particular, given a test video, the most
similar music is retrieved from a given collection. Then, a Transformer based
music moment localization is performed. We term this approach Retrieval and
Localization (ReaL). Extensive experiments on real-world datasets verify the
effectiveness of the proposed method for VMMR.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-29T00:00:00Z">2024-08-29</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Prototype Model of Zero-Trust Architecture Blockchain with
  EigenTrust-Based Practical Byzantine Fault Tolerance Protocol to Manage
  Decentralized Clinical Trials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashok Kumar Peepliwall, Hari Mohan Pandey, Surya Prakash, Anand A Mahajan, Sudhinder Singh Chowhan, Vinesh Kumar, Rahul Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The COVID-19 pandemic necessitated the emergence of decentralized Clinical
Trials (DCTs) due to patient retention, accelerate trials, improve data
accessibility, enable virtual care, and facilitate seamless communication
through integrated systems. However, integrating systems in DCTs exposes
clinical data to potential security threats, making them susceptible to theft
at any stage, a high risk of protocol deviations, and monitoring issues. To
mitigate these challenges, blockchain technology serves as a secure framework,
acting as a decentralized ledger, creating an immutable environment by
establishing a zero-trust architecture, where data are deemed untrusted until
verified. In combination with Internet of Things (IoT)-enabled wearable
devices, blockchain secures the transfer of clinical trial data on private
blockchains during DCT automation and operations. This paper proposes a
prototype model of the Zero-Trust Architecture Blockchain (z-TAB) to integrate
patient-generated clinical trial data during DCT operation management. The
EigenTrust-based Practical Byzantine Fault Tolerance (T-PBFT) algorithm has
been incorporated as a consensus protocol, leveraging Hyperledger Fabric.
Furthermore, the Internet of Things (IoT) has been integrated to streamline
data processing among stakeholders within the blockchain platforms. Rigorous
evaluation has been done to evaluate the quality of the system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Longitudinal Modularity, a Modularity for Link Streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16877v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16877v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Brabant, Yasaman Asgari, Pierre Borgnat, Angela Bonifati, Remy Cazabet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal networks are commonly used to model real-life phenomena. When these
phenomena represent interactions and are captured at a fine-grained temporal
resolution, they are modeled as link streams. Community detection is an
essential network analysis task. Although many methods exist for static
networks, and some methods have been developed for temporal networks
represented as sequences of snapshots, few works can handle link streams. This
article introduces the first adaptation of the well-known Modularity quality
function to link streams. Unlike existing methods, it is independent of the
time scale of analysis. After introducing the quality function, and its
relation to existing static and dynamic definitions of Modularity, we show
experimentally its relevance for dynamic community evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation
  System for AI Legal and Policy Applications <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishi Kalra, Zekun Wu, Ayesha Gulley, Airlie Hilliard, Xin Guan, Adriano Koshiyama, Philip Treleaven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) excel in text generation and
question-answering, their effectiveness in AI legal and policy is limited by
outdated knowledge, hallucinations, and inadequate reasoning in complex
contexts. Retrieval-Augmented Generation (RAG) systems improve response
accuracy by integrating external knowledge but struggle with retrieval errors,
poor context integration, and high costs, particularly in interpreting
qualitative and quantitative AI legal texts. This paper introduces a Hybrid
Parameter-Adaptive RAG (HyPA-RAG) system tailored for AI legal and policy,
exemplified by NYC Local Law 144 (LL144). HyPA-RAG uses a query complexity
classifier for adaptive parameter tuning, a hybrid retrieval strategy combining
dense, sparse, and knowledge graph methods, and an evaluation framework with
specific question types and metrics. By dynamically adjusting parameters,
HyPA-RAG significantly improves retrieval accuracy and response fidelity.
Testing on LL144 shows enhanced correctness, faithfulness, and contextual
precision, addressing the need for adaptable NLP systems in complex,
high-stakes AI legal and policy applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review for the EMNLP 2024 Workshop on Customizable NLP:
  Progress and Challenges in Customizing NLP for a Domain, Application, Group,
  or Individual</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformers Meet ACT-R: Repeat-Aware and Sequential Listening Session
  Recommendation <span class="chip">RecSys'2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viet-Anh Tran, Guillaume Salha-Galvan, Bruno Sguerra, Romain Hennequin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music streaming services often leverage sequential recommender systems to
predict the best music to showcase to users based on past sequences of
listening sessions. Nonetheless, most sequential recommendation methods ignore
or insufficiently account for repetitive behaviors. This is a crucial
limitation for music recommendation, as repeatedly listening to the same song
over time is a common phenomenon that can even change the way users perceive
this song. In this paper, we introduce PISA (Psychology-Informed Session
embedding using ACT-R), a session-level sequential recommender system that
overcomes this limitation. PISA employs a Transformer architecture learning
embedding representations of listening sessions and users using attention
mechanisms inspired by Anderson's ACT-R (Adaptive Control of Thought-Rational),
a cognitive architecture modeling human information access and memory dynamics.
This approach enables us to capture dynamic and repetitive patterns from user
behaviors, allowing us to effectively predict the songs they will listen to in
subsequent sessions, whether they are repeated or new ones. We demonstrate the
empirical relevance of PISA using both publicly available listening data from
Last.fm and proprietary data from Deezer, a global music streaming service,
confirming the critical importance of repetition modeling for sequential
listening session recommendation. Along with this paper, we publicly release
our proprietary dataset to foster future research in this field, as well as the
source code of PISA to facilitate its future use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages. Accepted by RecSys'2024, full paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is text normalization relevant for classifying medieval charters? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Atzenhofer-Baumgartner, Tamás Kovács
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines the impact of historical text normalization on the
classification of medieval charters, specifically focusing on document dating
and locating. Using a data set of Middle High German charters from a digital
archive, we evaluate various classifiers, including traditional and
transformer-based models, with and without normalization. Our results indicate
that the given normalization minimally improves locating tasks but reduces
accuracy for dating, implying that original texts contain crucial features that
normalization may obscure. We find that support vector machines and gradient
boosting outperform other models, questioning the efficiency of transformers
for this use case. Results suggest a selective approach to historical text
normalization, emphasizing the significance of preserving some textual
characteristics that are critical for classification tasks in document
analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review or any post-submission
  improvements or corrections</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Recommender Systems Promote Local Music? A Reproducibility Study
  Using Music Streaming Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kristina Matrosova, Lilian Marey, Guillaume Salha-Galvan, Thomas Louail, Olivier Bodini, Manuel Moussallam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper examines the influence of recommender systems on local music
representation, discussing prior findings from an empirical study on the LFM-2b
public dataset. This prior study argued that different recommender systems
exhibit algorithmic biases shifting music consumption either towards or against
local content. However, LFM-2b users do not reflect the diverse audience of
music streaming services. To assess the robustness of this study's conclusions,
we conduct a comparative analysis using proprietary listening data from a
global music streaming service, which we publicly release alongside this paper.
We observe significant differences in local music consumption patterns between
our dataset and LFM-2b, suggesting that caution should be exercised when
drawing conclusions on local music based solely on LFM-2b. Moreover, we show
that the algorithmic biases exhibited in the original work vary in our dataset,
and that several unexplored model parameters can significantly influence these
biases and affect the study's conclusion on both datasets. Finally, we discuss
the complexity of accurately labeling local music, emphasizing the risk of
misleading conclusions due to unreliable, biased, or incomplete labels. To
encourage further research and ensure reproducibility, we have publicly shared
our dataset and code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Sparse Lexical Representations for Image Retrieval in the Age
  of Rising Multi-Modal Large Language Models <span class="chip">ECCV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kengo Nakata, Daisuke Miyashita, Youyang Ng, Yasuto Hoshi, Jun Deguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we rethink sparse lexical representations for image retrieval.
By utilizing multi-modal large language models (M-LLMs) that support visual
prompting, we can extract image features and convert them into textual data,
enabling us to utilize efficient sparse retrieval algorithms employed in
natural language processing for image retrieval tasks. To assist the LLM in
extracting image features, we apply data augmentation techniques for key
expansion and analyze the impact with a metric for relevance between images and
textual data. We empirically show the superior precision and recall performance
of our image retrieval method compared to conventional vision-language
model-based methods on the MS-COCO, PASCAL VOC, and NUS-WIDE datasets in a
keyword-based image retrieval scenario, where keywords serve as search queries.
We also demonstrate that the retrieval performance can be improved by
iteratively incorporating keywords into search queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ECCV 2024 Workshops: 2nd Workshop on Traditional Computer
  Vision in the Age of Deep Learning (TradiCV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Transfer Learning Framework for Cross-Domain Click-Through
  Rate Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Liu, Xingyuan Tang, Jianqiang Huang, Xiangqian Yu, Haoran Jin, Jin Chen, Yuanhao Pu, Defu Lian, Tan Qu, Zhe Wang, Jia Cheng, Jun Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural content and advertisement coexist in industrial recommendation
systems but differ in data distribution. Concretely, traffic related to the
advertisement is considerably sparser compared to that of natural content,
which motivates the development of transferring knowledge from the richer
source natural content domain to the sparser advertising domain. The challenges
include the inefficiencies arising from the management of extensive source data
and the problem of 'catastrophic forgetting' that results from the CTR model's
daily updating. To this end, we propose a novel tri-level asynchronous
framework, i.e., Efficient Transfer Learning Framework for Cross-Domain
Click-Through Rate Prediction (E-CDCTR), to transfer comprehensive knowledge of
natural content to advertisement CTR models. This framework consists of three
key components: Tiny Pre-training Model ((TPM), which trains a tiny CTR model
with several basic features on long-term natural data; Complete Pre-training
Model (CPM), which trains a CTR model holding network structure and input
features the same as target advertisement on short-term natural data;
Advertisement CTR model (A-CTR), which derives its parameter initialization
from CPM together with multiple historical embeddings from TPM as extra feature
and then fine-tunes on advertisement data. TPM provides richer representations
of user and item for both the CPM and A-CTR, effectively alleviating the
forgetting problem inherent in the daily updates. CPM further enhances the
advertisement model by providing knowledgeable initialization, thereby
alleviating the data sparsity challenges typically encountered by advertising
CTR models. Such a tri-level cross-domain transfer learning framework offers an
efficient solution to address both data sparsity and `catastrophic forgetting',
yielding remarkable improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Use of a Structured Knowledge Base Enhances Metadata Curation by Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05893v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05893v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sowmya S. Sundaram, Benjamin Solomon, Avani Khatri, Anisha Laumas, Purvesh Khatri, Mark A. Musen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metadata play a crucial role in ensuring the findability, accessibility,
interoperability, and reusability of datasets. This paper investigates the
potential of large language models (LLMs), specifically GPT-4, to improve
adherence to metadata standards. We conducted experiments on 200 random data
records describing human samples relating to lung cancer from the NCBI
BioSample repository, evaluating GPT-4's ability to suggest edits for adherence
to metadata standards. We computed the adherence accuracy of field name-field
value pairs through a peer review process, and we observed a marginal average
improvement in adherence to the standard data dictionary from 79% to 80%
(p<0.5). We then prompted GPT-4 with domain information in the form of the
textual descriptions of CEDAR templates and recorded a significant improvement
to 97% from 79% (p<0.01). These results indicate that, while LLMs may not be
able to correct legacy metadata to ensure satisfactory adherence to standards
when unaided, they do show promise for use in automated metadata curation when
integrated with a structured knowledge base
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smart Multi-Modal Search: Contextual Sparse and Dense Embedding
  Integration in Adobe Express <span class="chip">CIKM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cherag Aroraa, Tracy Holloway King, Jayant Kumar, Yi Lu, Sanat Sharma, Arvind Srikantan, David Uvalle, Josep Valls-Vargas, Harsha Vardhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As user content and queries become increasingly multi-modal, the need for
effective multi-modal search systems has grown. Traditional search systems
often rely on textual and metadata annotations for indexed images, while
multi-modal embeddings like CLIP enable direct search using text and image
embeddings. However, embedding-based approaches face challenges in integrating
contextual features such as user locale and recency. Building a scalable
multi-modal search system requires fine-tuning several components. This paper
presents a multi-modal search architecture and a series of AB tests that
optimize embeddings and multi-modal technologies in Adobe Express template
search. We address considerations such as embedding model selection, the roles
of embeddings in matching and ranking, and the balance between dense and sparse
embeddings. Our iterative approach demonstrates how utilizing sparse, dense,
and contextual features enhances short and long query search, significantly
reduces null rates (over 70\%), and increases click-through rates (CTR). Our
findings provide insights into developing robust multi-modal search systems,
thereby enhancing relevance for complex queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CIKM 2024 (International Conference on Information and Knowledge
  Management), Multimodal Search and Recommendations Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenRec: Generative Sequential Recommendation with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panfeng Cao, Pietro Lio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation is a task to capture hidden user preferences from
historical user item interaction data and recommend next items for the user.
Significant progress has been made in this domain by leveraging classification
based learning methods. Inspired by the recent paradigm of 'pretrain, prompt
and predict' in NLP, we consider sequential recommendation as a sequence to
sequence generation task and propose a novel model named Generative
Recommendation (GenRec). Unlike classification based models that learn explicit
user and item representations, GenRec utilizes the sequence modeling capability
of Transformer and adopts the masked item prediction objective to effectively
learn the hidden bidirectional sequential patterns. Different from existing
generative sequential recommendation models, GenRec does not rely on manually
designed hard prompts. The input to GenRec is textual user item sequence and
the output is top ranked next items. Moreover, GenRec is lightweight and
requires only a few hours to train effectively in low-resource settings, making
it highly applicable to real-world scenarios and helping to democratize large
language models in the sequential recommendation domain. Our extensive
experiments have demonstrated that GenRec generalizes on various public
real-world datasets and achieves state-of-the-art results. Our experiments also
validate the effectiveness of the the proposed masked item prediction objective
that improves the model performance by a large margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Summaries, Highlights, and Action items: Design, implementation and
  evaluation of an <span class="highlight-title">LLM</span>-powered meeting recap system <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumit Asthana, Sagih Hilleli, Pengcheng He, Aaron Halfaker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meetings play a critical infrastructural role in the coordination of work. In
recent years, due to shift to hybrid and remote work, more meetings are moving
to online Computer Mediated Spaces. This has led to new problems (e.g. more
time spent in less engaging meetings) and new opportunities (e.g. automated
transcription/captioning and recap support). Recent advances in large language
models (LLMs) for dialog summarization have the potential to improve the
experience of meetings by reducing individuals' meeting load and increasing the
clarity and alignment of meeting outputs. Despite this potential, they face
technological limitation due to long transcripts and inability to capture
diverse recap needs based on user's context. To address these gaps, we design,
implement and evaluate in-context a meeting recap system. We first
conceptualize two salient recap representations -- important highlights, and a
structured, hierarchical minutes view. We develop a system to operationalize
the representations with dialogue summarization as its building blocks.
Finally, we evaluate the effectiveness of the system with seven users in the
context of their work meetings. Our findings show promise in using LLM-based
dialogue summarization for meeting recap and the need for both representations
in different contexts. However, we find that LLM-based recap still lacks an
understanding of whats personally relevant to participants, can miss important
details, and mis-attributions can be detrimental to group dynamics. We identify
collaboration opportunities such as a shared recap document that a high quality
recap enables. We report on implications for designing AI systems to partner
with users to learn and improve from natural interactions to overcome the
limitations related to personal relevance and summarization quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in review for CSCW 24</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ See or Guess: Counterfactually Regularized Image Captioning <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Cao, Xu Chen, Ruihua Song, Xiting Wang, Xinting Huang, Yuchen Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning, which generates natural language descriptions of the visual
information in an image, is a crucial task in vision-language research.
Previous models have typically addressed this task by aligning the generative
capabilities of machines with human intelligence through statistical fitting of
existing datasets. While effective for normal images, they may struggle to
accurately describe those where certain parts of the image are obscured or
edited, unlike humans who excel in such cases. These weaknesses they exhibit,
including hallucinations and limited interpretability, often hinder performance
in scenarios with shifted association patterns. In this paper, we present a
generic image captioning framework that employs causal inference to make
existing models more capable of interventional tasks, and counterfactually
explainable. Our approach includes two variants leveraging either total effect
or natural direct effect. Integrating them into the training process enables
models to handle counterfactual scenarios, increasing their generalizability.
Extensive experiments on various datasets show that our method effectively
reduces hallucinations and improves the model's faithfulness to images,
demonstrating high portability across both small-scale and large-scale
image-to-text models. The code is available at
https://github.com/Aman-4-Real/See-or-Guess.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiMediate'24: Multi-Domain Engagement Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Müller, Michal Balazia, Tobias Baur, Michael Dietz, Alexander Heimerl, Anna Penzkofer, Dominik Schiller, François Brémond, Jan Alexandersson, Elisabeth André, Andreas Bulling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the momentary level of participant's engagement is an important
prerequisite for assistive systems that support human interactions. Previous
work has addressed this task in within-domain evaluation scenarios, i.e.
training and testing on the same dataset. This is in contrast to real-life
scenarios where domain shifts between training and testing data frequently
occur. With MultiMediate'24, we present the first challenge addressing
multi-domain engagement estimation. As training data, we utilise the NOXI
database of dyadic novice-expert interactions. In addition to within-domain
test data, we add two new test domains. First, we introduce recordings
following the NOXI protocol but covering languages that are not present in the
NOXI training data. Second, we collected novel engagement annotations on the
MPIIGroupInteraction dataset which consists of group discussions between three
to four people. In this way, MultiMediate'24 evaluates the ability of
approaches to generalise across factors such as language and cultural
background, group size, task, and screen-mediated vs. face-to-face interaction.
This paper describes the MultiMediate'24 challenge and presents baseline
results. In addition, we discuss selected challenge solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2308.08256</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Inspired Audio-Visual Speech Recognition: Spike Activity, Cueing
  Interaction and Causal Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianhui Liu, Jiadong Wang, Yang Wang, Xin Yang, Gang Pan, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans naturally perform audiovisual speech recognition (AVSR), enhancing the
accuracy and robustness by integrating auditory and visual information. Spiking
neural networks (SNNs), which mimic the brain's information-processing
mechanisms, are well-suited for emulating the human capability of AVSR. Despite
their potential, research on SNNs for AVSR is scarce, with most existing
audio-visual multimodal methods focused on object or digit recognition. These
models simply integrate features from both modalities, neglecting their unique
characteristics and interactions. Additionally, they often rely on future
information for current processing, which increases recognition latency and
limits real-time applicability. Inspired by human speech perception, this paper
proposes a novel human-inspired SNN named HI-AVSNN for AVSR, incorporating
three key characteristics: cueing interaction, causal processing and spike
activity. For cueing interaction, we propose a visual-cued auditory attention
module (VCA2M) that leverages visual cues to guide attention to auditory
features. We achieve causal processing by aligning the SNN's temporal dimension
with that of visual and auditory features and applying temporal masking to
utilize only past and current information. To implement spike activity, in
addition to using SNNs, we leverage the event camera to capture lip movement as
spikes, mimicking the human retina and providing efficient visual data. We
evaluate HI-AVSNN on an audiovisual speech recognition dataset combining the
DVS-Lip dataset with its corresponding audio samples. Experimental results
demonstrate the superiority of our proposed fusion method, outperforming
existing audio-visual SNN fusion methods and achieving a 2.27% improvement in
accuracy over the only existing SNN-based AVSR method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio
  Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengpeng Ji, Ziyue Jiang, Xize Cheng, Yifu Chen, Minghui Fang, Jialong Zuo, Qian Yang, Ruiqi Li, Ziang Zhang, Xiaoda Yang, Rongjie Huang, Yidi Jiang, Qian Chen, Siqi Zheng, Wen Wang, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have been effectively applied to modeling natural signals,
such as images, video, speech, and audio. A crucial component of these models
is the codec tokenizer, which compresses high-dimensional natural signals into
lower-dimensional discrete tokens. In this paper, we introduce WavTokenizer,
which offers several advantages over previous SOTA acoustic codec models in the
audio domain: 1)extreme compression. By compressing the layers of quantizers
and the temporal dimension of the discrete codec, one-second audio of 24kHz
sampling rate requires only a single quantizer with 40 or 75 tokens. 2)improved
subjective quality. Despite the reduced number of tokens, WavTokenizer achieves
state-of-the-art reconstruction quality with outstanding UTMOS scores and
inherently contains richer semantic information. Specifically, we achieve these
results by designing a broader VQ space, extended contextual windows, and
improved attention networks, as well as introducing a powerful multi-scale
discriminator and an inverse Fourier transform structure. We conducted
extensive reconstruction experiments in the domains of speech, audio, and
music. WavTokenizer exhibited strong performance across various objective and
subjective metrics compared to state-of-the-art models. We also tested semantic
information, VQ utilization, and adaptability to generative models.
Comprehensive ablation studies confirm the necessity of each module in
WavTokenizer. The related code, demos, and pre-trained models are available at
https://github.com/jishengpeng/WavTokenizer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in progress. arXiv admin note: text overlap with
  arXiv:2402.12208</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-28T00:00:00Z">2024-08-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling and Analyzing the Influence of Non-Item Pages on Sequential
  Next-Item Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elisabeth Fischer, Daniel Schlör, Albin Zehe, Andreas Hotho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analyzing the sequence of historical interactions between users and items,
sequential recommendation models learn user intent and make predictions about
the next item of interest. Next to these item interactions, most systems also
have interactions with pages not related to specific items, for example
navigation pages, account pages, and pages for a specific category, which may
provide additional insights into the user's interests. However, while there are
several approaches to integrate additional information about items and users,
the topic of integrating non-item pages has been less explored. We use the
hypotheses testing framework HypTrails to show that there is indeed a
relationship between these non-item pages and the items of interest and fill
this gap by proposing various approaches of representing non-item pages (e.g,
based on their content) to use them as an additional information source for the
task of sequential next-item prediction.
  We create a synthetic dataset with non-item pages highly related to the
subsequent item to show that the models are generally capable of learning from
these interactions, and subsequently evaluate the improvements gained by
including non-item pages in two real-world datasets.
  We adapt eight popular sequential recommender models, covering CNN-, RNN- and
transformer-based architectures, to integrate non-item pages and investigate
the capabilities of these models to leverage their information for next item
prediction. We also analyze their behavior on noisy data and compare different
item representation strategies.
  Our results show that non-item pages are a valuable source of information,
but representing such a page well is the key to successfully leverage them. The
inclusion of non-item pages can increase the performance for next-item
prediction in all examined model architectures with a varying degree.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 19 figures; Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient $k$-NN Search in IoT Data: Overlap Optimization in Tree-Based
  Indexing Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ala-Eddine Benrazek, Zineddine Kouahla, Brahim Farou, Hamid Seridi, Ibtissem Kemouguette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of interconnected devices in the Internet of Things (IoT)
has led to an exponential increase in data, commonly known as Big IoT Data.
Efficient retrieval of this heterogeneous data demands a robust indexing
mechanism for effective organization. However, a significant challenge remains:
the overlap in data space partitions during index construction. This overlap
increases node access during search and retrieval, resulting in higher resource
consumption, performance bottlenecks, and impedes system scalability. To
address this issue, we propose three innovative heuristics designed to quantify
and strategically reduce data space partition overlap. The volume-based method
(VBM) offers a detailed assessment by calculating the intersection volume
between partitions, providing deeper insights into spatial relationships. The
distance-based method (DBM) enhances efficiency by using the distance between
partition centers and radii to evaluate overlap, offering a streamlined yet
accurate approach. Finally, the object-based method (OBM) provides a practical
solution by counting objects across multiple partitions, delivering an
intuitive understanding of data space dynamics. Experimental results
demonstrate the effectiveness of these methods in reducing search time,
underscoring their potential to improve data space partitioning and enhance
overall system performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 21 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Navigator: <span class="highlight-title">LLM</span>-guided Browsing Framework for Exploratory
  Search in Scientific Literature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uri Katz, Mosh Levy, Yoav Goldberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of scientific literature necessitates advanced tools
for effective knowledge exploration. We present Knowledge Navigator, a system
designed to enhance exploratory search abilities by organizing and structuring
the retrieved documents from broad topical queries into a navigable, two-level
hierarchy of named and descriptive scientific topics and subtopics. This
structured organization provides an overall view of the research themes in a
domain, while also enabling iterative search and deeper knowledge discovery
within specific subtopics by allowing users to refine their focus and retrieve
additional relevant documents. Knowledge Navigator combines LLM capabilities
with cluster-based methods to enable an effective browsing method. We
demonstrate our approach's effectiveness through automatic and manual
evaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code,
prompts, and benchmarks are made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Data Creator to Data Reuser: Distance Matters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christine L. Borgman, Paul T. Groth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sharing research data is necessary, but not sufficient, for data reuse. Open
science policies focus more heavily on data sharing than on reuse, yet both are
complex, labor-intensive, expensive, and require infrastructure investments by
multiple stakeholders. The value of data reuse lies in relationships between
creators and reusers. By addressing knowledge exchange, rather than mere
transactions between stakeholders, investments in data management and knowledge
infrastructures can be made more wisely. Drawing upon empirical studies of data
sharing and reuse, we develop the theoretical construct of distance between
data creator and data reuser, identifying six distance dimensions that
influence the ability to transfer knowledge effectively: domain, methods,
collaboration, curation, purposes, and time and temporality. We address the
social and socio-technical aspects of these dimensions, exploring ways in which
they may decrease -- or increase -- distances between creators and reusers. Our
theoretical framing of the distance between data creators and prospective
reusers leads to recommendations to four categories of stakeholders on how to
make data sharing and reuse more effective: data creators, data reusers, data
archivists, and funding agencies. 'It takes a village' to share research data
-- and a village to reuse data. Our aim is to provoke new research questions,
new research, and new investments in effective and efficient circulation of
research data; and to identify criteria for investments at each stage of data
and research life cycles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>74 pages, double-spaced, consisting of Table of Contents, Abstract,
  45 page narrative, 1 box, 1 figure, 1 table, 27 pages references. Original
  work</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SVDD 2024: The Inaugural Singing Voice Deepfake Detection Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        You Zhang, Yongyi Zang, Jiatong Shi, Ryuichi Yamamoto, Tomoki Toda, Zhiyao Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancements in singing voice generation and the growing presence of
AI singers on media platforms, the inaugural Singing Voice Deepfake Detection
(SVDD) Challenge aims to advance research in identifying AI-generated singing
voices from authentic singers. This challenge features two tracks: a controlled
setting track (CtrSVDD) and an in-the-wild scenario track (WildSVDD). The
CtrSVDD track utilizes publicly available singing vocal data to generate
deepfakes using state-of-the-art singing voice synthesis and conversion
systems. Meanwhile, the WildSVDD track expands upon the existing SingFake
dataset, which includes data sourced from popular user-generated content
websites. For the CtrSVDD track, we received submissions from 47 teams, with 37
surpassing our baselines and the top team achieving a 1.65% equal error rate.
For the WildSVDD track, we benchmarked the baselines. This paper reviews these
results, discusses key findings, and outlines future directions for SVDD
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kangaroo: A Powerful Video-Language Model Supporting Long-context Video
  Input 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, Jie Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid advancements have been made in extending Large Language Models (LLMs)
to Large Multi-modal Models (LMMs). However, extending input modality of LLMs
to video data remains a challenging endeavor, especially for long videos. Due
to insufficient access to large-scale high-quality video data and the excessive
compression of visual features, current methods exhibit limitations in
effectively processing long videos. In this paper, we introduce Kangaroo, a
powerful Video LMM aimed at addressing these challenges. Confronted with issue
of inadequate training data, we develop a data curation system to build a
large-scale dataset with high-quality annotations for vision-language
pre-training and instruction tuning. In addition, we design a curriculum
training pipeline with gradually increasing resolution and number of input
frames to accommodate long videos. Evaluation results demonstrate that, with 8B
parameters, Kangaroo achieves state-of-the-art performance across a variety of
video understanding benchmarks while exhibiting competitive results on others.
Particularly, on benchmarks specialized for long videos, Kangaroo excels some
larger models with over 10B parameters and proprietary models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flexible Control in Symbolic Music Generation via Musical Metadata 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangjun Han, Jiwon Ham, Chaeeun Lee, Heejin Kim, Soojong Do, Sihyuk Yi, Jun Seo, Seoyoon Kim, Yountae Jung, Woohyung Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce the demonstration of symbolic music generation,
focusing on providing short musical motifs that serve as the central theme of
the narrative. For the generation, we adopt an autoregressive model which takes
musical metadata as inputs and generates 4 bars of multitrack MIDI sequences.
During training, we randomly drop tokens from the musical metadata to guarantee
flexible control. It provides users with the freedom to select input types
while maintaining generative performance, enabling greater flexibility in music
composition. We validate the effectiveness of the strategy through experiments
in terms of model capacity, musical fidelity, diversity, and controllability.
Additionally, we scale up the model and compare it with other music generation
model through a subjective test. Our results indicate its superiority in both
control and music quality. We provide a URL link
https://www.youtube.com/watch?v=-0drPrFJdMQ to our demonstration video.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple Baseline with Single-encoder for Referring Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonghoon Yu, Ilchae Jung, Byeongju Han, Taeoh Kim, Yunho Kim, Dongyoon Wee, Jeany Son
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring image segmentation (RIS) requires dense vision-language
interactions between visual pixels and textual words to segment objects based
on a given description. However, commonly adapted dual-encoders in RIS, e.g.,
Swin transformer and BERT (uni-modal encoders) or CLIP (a multi-modal
dual-encoder), lack dense multi-modal interactions during pre-training, leading
to a gap with a pixel-level RIS task. To bridge this gap, existing RIS methods
often rely on multi-modal fusion modules that interact two encoders, but this
approach leads to high computational costs. In this paper, we present a novel
RIS method with a single-encoder, i.e., BEiT-3, maximizing the potential of
shared self-attention across all framework components. This enables seamless
interactions of two modalities from input to final prediction, producing
granularly aligned multi-modal features. Furthermore, we propose lightweight
yet effective decoder modules, a Shared FPN and a Shared Mask Decoder, which
contribute to the high efficiency of our model. Our simple baseline with a
single encoder achieves outstanding performances on the RIS benchmark datasets
while maintaining computational efficiency, compared to the most recent SoTA
methods based on dual-encoders.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ArXiv pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Proceedings of The second international workshop on eXplainable AI for
  the Arts (XAIxArts) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14485v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14485v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Bryan-Kinns, Corey Ford, Shuoyang Zheng, Helen Kennedy, Alan Chamberlain, Makayla Lewis, Drew Hemment, Zijin Li, Qiong Wu, Lanxi Xiao, Gus Xia, Jeba Rezwana, Michael Clemens, Gabriel Vigliensoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This second international workshop on explainable AI for the Arts (XAIxArts)
brought together a community of researchers in HCI, Interaction Design, AI,
explainable AI (XAI), and digital arts to explore the role of XAI for the Arts.
Workshop held at the 16th ACM Conference on Creativity and Cognition (C&C
2024), Chicago, USA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of The second international workshop on eXplainable AI
  for the Arts (XAIxArts)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MambaGesture: Enhancing Co-Speech Gesture Generation with Mamba and
  Disentangled Multi-Modality Fusion <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19976v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19976v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chencan Fu, Yabiao Wang, Jiangning Zhang, Zhengkai Jiang, Xiaofeng Mao, Jiafu Wu, Weijian Cao, Chengjie Wang, Yanhao Ge, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Co-speech gesture generation is crucial for producing synchronized and
realistic human gestures that accompany speech, enhancing the animation of
lifelike avatars in virtual environments. While diffusion models have shown
impressive capabilities, current approaches often overlook a wide range of
modalities and their interactions, resulting in less dynamic and contextually
varied gestures. To address these challenges, we present MambaGesture, a novel
framework integrating a Mamba-based attention block, MambaAttn, with a
multi-modality feature fusion module, SEAD. The MambaAttn block combines the
sequential data processing strengths of the Mamba model with the contextual
richness of attention mechanisms, enhancing the temporal coherence of generated
gestures. SEAD adeptly fuses audio, text, style, and emotion modalities,
employing disentanglement to deepen the fusion process and yield gestures with
greater realism and diversity. Our approach, rigorously evaluated on the
multi-modal BEAT dataset, demonstrates significant improvements in Fr\'echet
Gesture Distance (FGD), diversity scores, and beat alignment, achieving
state-of-the-art performance in co-speech gesture generation. Project website:
$\href{https://fcchit.github.io/mambagesture/}{\textit{https://fcchit.github.io/mambagesture/}}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> AIM 2024 Challenge on Compressed Video Quality Assessment: Methods and
  Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11982v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11982v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksim Smirnov, Aleksandr Gushchin, Anastasia Antsiferova, Dmitry Vatolin, Radu Timofte, Zi<span class="highlight-author">heng Ji</span>a, Zicheng Zhang, Wei Sun, Jiaying Qian, Yuqin Cao, Yinan Sun, Yuxin Zhu, Xiongkuo Min, Guangtao Zhai, Kanjar De, Qing Luo, Ao-Xiang Zhang, Peng Zhang, Haibo Lei, Linyan Jiang, Yaqing Li, Wenhui Meng, Xiaoheng Tan, Haiqiang Wang, Xiaozhong Xu, Shan Liu, Zhenzhong Chen, Zhengxue Cheng, Jiahao Xiao, Jun Xu, Chenlong He, Qi Zheng, Ruoxi Zhu, Min Li, Yibo Fan, Zhengzhong Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video quality assessment (VQA) is a crucial task in the development of video
compression standards, as it directly impacts the viewer experience. This paper
presents the results of the Compressed Video Quality Assessment challenge, held
in conjunction with the Advances in Image Manipulation (AIM) workshop at ECCV
2024. The challenge aimed to evaluate the performance of VQA methods on a
diverse dataset of 459 videos, encoded with 14 codecs of various compression
standards (AVC/H.264, HEVC/H.265, AV1, and VVC/H.266) and containing a
comprehensive collection of compression artifacts. To measure the methods
performance, we employed traditional correlation coefficients between their
predictions and subjective scores, which were collected via large-scale
crowdsourced pairwise human comparisons. For training purposes, participants
were provided with the Compressed Video Quality Assessment Dataset (CVQAD), a
previously developed dataset of 1022 videos. Up to 30 participating teams
registered for the challenge, while we report the results of 6 teams, which
submitted valid final solutions and code for reproducing the results. Moreover,
we calculated and present the performance of state-of-the-art VQA methods on
the developed dataset, providing a comprehensive benchmark for future research.
The dataset, results, and online leaderboard are publicly available at
https://challenges.videoprocessing.ai/challenges/compressedvideo-quality-assessment.html.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-27T00:00:00Z">2024-08-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sec2Sec Co-attention for Video-Based Apparent Affective Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingwei Sun, Kunpeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-based apparent affect detection plays a crucial role in video
understanding, as it encompasses various elements such as vision, audio,
audio-visual interactions, and spatiotemporal information, which are essential
for accurate video predictions. However, existing approaches often focus on
extracting only a subset of these elements, resulting in the limited predictive
capacity of their models. To address this limitation, we propose a novel
LSTM-based network augmented with a Transformer co-attention mechanism for
predicting apparent affect in videos. We demonstrate that our proposed Sec2Sec
Co-attention Transformer surpasses multiple state-of-the-art methods in
predicting apparent affect on two widely used datasets: LIRIS-ACCEDE and First
Impressions. Notably, our model offers interpretability, allowing us to examine
the contributions of different time points to the overall prediction. The
implementation is available at: https://github.com/nestor-sun/sec2sec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alfie: Democratising RGBA Image Generation With No $$$ <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabio Quattrini, Vittorio Pippi, Silvia Cascianelli, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designs and artworks are ubiquitous across various creative fields, requiring
graphic design skills and dedicated software to create compositions that
include many graphical elements, such as logos, icons, symbols, and art scenes,
which are integral to visual storytelling. Automating the generation of such
visual elements improves graphic designers' productivity, democratizes and
innovates the creative industry, and helps generate more realistic synthetic
data for related tasks. These illustration elements are mostly RGBA images with
irregular shapes and cutouts, facilitating blending and scene composition.
However, most image generation models are incapable of generating such images
and achieving this capability requires expensive computational resources,
specific training recipes, or post-processing solutions. In this work, we
propose a fully-automated approach for obtaining RGBA illustrations by
modifying the inference-time behavior of a pre-trained Diffusion Transformer
model, exploiting the prompt-guided controllability and visual quality offered
by such models with no additional computational cost. We force the generation
of entire subjects without sharp croppings, whose background is easily removed
for seamless integration into design projects or artistic scenes. We show with
a user study that, in most cases, users prefer our solution over generating and
then matting an image, and we show that our generated illustrations yield good
results when used as inputs for composite scene generation pipelines. We
release the code at https://github.com/aimagelab/Alfie.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECCV AI for Visual Arts Workshop and Challenges</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive
  Streaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuang Shi, Simone Gasparini, Géraldine Morin, Wei Tsang Ooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Extended Reality (XR) requires efficient streaming of 3D online
worlds, challenging current 3DGS representations to adapt to
bandwidth-constrained environments. This paper proposes LapisGS, a layered 3DGS
that supports adaptive streaming and progressive rendering. Our method
constructs a layered structure for cumulative representation, incorporates
dynamic opacity optimization to maintain visual fidelity, and utilizes
occupancy maps to efficiently manage Gaussian splats. This proposed model
offers a progressive representation supporting a continuous rendering quality
adapted for bandwidth-aware streaming. Extensive experiments validate the
effectiveness of our approach in balancing visual fidelity with the compactness
of the model, with up to 50.71% improvement in SSIM, 286.53% improvement in
LPIPS, and 318.41% reduction in model size, and shows its potential for
bandwidth-adapted 3D streaming and rendering applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SynthDoc: Bilingual Documents Synthesis for Visual Document
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanghao Ding, Xuejing Liu, Wei Tang, Juan Li, Xiaoliang Wang, Rui Zhao, Cam-Tu Nguyen, Fei Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces SynthDoc, a novel synthetic document generation
pipeline designed to enhance Visual Document Understanding (VDU) by generating
high-quality, diverse datasets that include text, images, tables, and charts.
Addressing the challenges of data acquisition and the limitations of existing
datasets, SynthDoc leverages publicly available corpora and advanced rendering
tools to create a comprehensive and versatile dataset. Our experiments,
conducted using the Donut model, demonstrate that models trained with
SynthDoc's data achieve superior performance in pre-training read tasks and
maintain robustness in downstream tasks, despite language inconsistencies. The
release of a benchmark dataset comprising 5,000 image-text pairs not only
showcases the pipeline's capabilities but also provides a valuable resource for
the VDU community to advance research and development in document image
recognition. This work significantly contributes to the field by offering a
scalable solution to data scarcity and by validating the efficacy of end-to-end
models in parsing complex, real-world documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework
  with Correlated Differential Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianzhi Zhang, Yipeng Zhou, Di Wu, Quan Z. Sheng, Miao Hu, Linchang Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online video streaming has evolved into an integral component of the
contemporary Internet landscape. Yet, the disclosure of user requests presents
formidable privacy challenges. As users stream their preferred online videos,
their requests are automatically seized by video content providers, potentially
leaking users' privacy.
  Unfortunately, current protection methods are not well-suited to preserving
user request privacy from content providers while maintaining high-quality
online video services. To tackle this challenge, we introduce a novel
Privacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge
devices to pre-fetch and cache videos, ensuring the privacy of users' requests
while optimizing the efficiency of edge caching. More specifically, we design
PPVF with three core components: (1) \textit{Online privacy budget scheduler},
which employs a theoretically guaranteed online algorithm to select
non-requested videos as candidates with assigned privacy budgets. Alternative
videos are chosen by an online algorithm that is theoretically guaranteed to
consider both video utilities and available privacy budgets. (2) \textit{Noisy
video request generator}, which generates redundant video requests (in addition
to original ones) utilizing correlated differential privacy to obfuscate
request privacy. (3) \textit{Online video utility predictor}, which leverages
federated learning to collaboratively evaluate video utility in an online
fashion, aiding in video selection in (1) and noise generation in (2). Finally,
we conduct extensive experiments using real-world video request traces from
Tencent Video. The results demonstrate that PPVF effectively safeguards user
request privacy while upholding high video caching performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StyleSpeech: Parameter-efficient Fine Tuning for <span class="highlight-title">Pre-train</span>ed
  Controllable Text-to-Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Lou, Helen Paik, Wen Hu, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces StyleSpeech, a novel Text-to-Speech~(TTS) system that
enhances the naturalness and accuracy of synthesized speech. Building upon
existing TTS technologies, StyleSpeech incorporates a unique Style Decorator
structure that enables deep learning models to simultaneously learn style and
phoneme features, improving adaptability and efficiency through the principles
of Lower Rank Adaptation~(LoRA). LoRA allows efficient adaptation of style
features in pre-trained models. Additionally, we introduce a novel automatic
evaluation metric, the LLM-Guided Mean Opinion Score (LLM-MOS), which employs
large language models to offer an objective and robust protocol for
automatically assessing TTS system performance. Extensive testing on benchmark
datasets shows that our approach markedly outperforms existing state-of-the-art
baseline methods in producing natural, accurate, and high-quality speech. These
advancements not only pushes the boundaries of current TTS system capabilities,
but also facilitate the application of TTS system in more dynamic and
specialized, such as interactive virtual assistants, adaptive audiobooks, and
customized voice for gaming. Speech samples can be found in
https://style-speech.vercel.app
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-26T00:00:00Z">2024-08-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Image Captioning Training Paradigm via Direct CLIP-based
  Optimization <span class="chip">BMVC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Moratelli, Davide Caffagni, Marcella Cornia, Lorenzo Baraldi, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional training approach for image captioning involves pre-training
a network using teacher forcing and subsequent fine-tuning with Self-Critical
Sequence Training to maximize hand-crafted captioning metrics. However, when
attempting to optimize modern and higher-quality metrics like CLIP-Score and
PAC-Score, this training method often encounters instability and fails to
acquire the genuine descriptive capabilities needed to produce fluent and
informative captions. In this paper, we propose a new training paradigm termed
Direct CLIP-Based Optimization (DiCO). Our approach jointly learns and
optimizes a reward model that is distilled from a learnable captioning
evaluator with high human correlation. This is done by solving a weighted
classification problem directly inside the captioner. At the same time, DiCO
prevents divergence from the original model, ensuring that fluency is
maintained. DiCO not only exhibits improved stability and enhanced quality in
the generated captions but also aligns more closely with human preferences
compared to existing methods, especially in modern metrics. Additionally, it
maintains competitive performance in traditional metrics. Our source code and
trained models are publicly available at https://github.com/aimagelab/DiCO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BMVC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Fingerprinting on Multimedia: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wendi Chen, Wensheng Gan, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The explosive growth of multimedia content in the digital economy era has
brought challenges in content recognition, copyright protection, and data
management. As an emerging content management technology, perceptual hash-based
digital fingerprints, serving as compact summaries of multimedia content, have
been widely adopted for efficient multimedia content identification and
retrieval across different modalities (e.g., text, image, video, audio),
attracting significant attention from both academia and industry. Despite the
increasing applications of digital fingerprints, there is a lack of systematic
and comprehensive literature review on multimedia digital fingerprints. This
survey aims to fill this gap and provide an important resource for researchers
studying the details and related advancements of multimedia digital
fingerprints. The survey first introduces the definition, characteristics, and
related concepts (including hash functions, granularity, similarity measures,
etc.) of digital fingerprints. It then focuses on analyzing and summarizing the
algorithms for extracting unimodal fingerprints of different types of digital
content, including text fingerprints, image fingerprints, video fingerprints,
and audio fingerprints. Particularly, it provides an in-depth review and
summary of deep learning-based fingerprints. Additionally, the survey
elaborates on the various practical applications of digital fingerprints and
outlines the challenges and potential future research directions. The goal is
to promote the continued development of multimedia digital fingerprint
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. 5 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HABD: a houma alliance book ancient handwritten character recognition
  database 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyu Yuan, Xiaohua Huang, Zibo Zhang, Yabo Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Houma Alliance Book, one of history's earliest calligraphic examples, was
unearthed in the 1970s. These artifacts were meticulously organized,
reproduced, and copied by the Shanxi Provincial Institute of Cultural Relics.
However, because of their ancient origins and severe ink erosion, identifying
characters in the Houma Alliance Book is challenging, necessitating the use of
digital technology. In this paper, we propose a new ancient handwritten
character recognition database for the Houma alliance book, along with a novel
benchmark based on deep learning architectures. More specifically, a collection
of 26,732 characters samples from the Houma Alliance Book were gathered,
encompassing 327 different types of ancient characters through iterative
annotation. Furthermore, benchmark algorithms were proposed by combining four
deep neural network classifiers with two data augmentation methods. This
research provides valuable resources and technical support for further studies
on the Houma Alliance Book and other ancient characters. This contributes to
our understanding of ancient culture and history, as well as the preservation
and inheritance of humanity's cultural heritage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TSC-PCAC: Voxel Transformer and Sparse Convolution Based Point Cloud
  Attribute Compression for 3D Broadcasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixi Guo, Yun Zhang, Linwei Zhu, Hanli Wang, Gangyi Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud has been the mainstream representation for advanced 3D
applications, such as virtual reality and augmented reality. However, the
massive data amounts of point clouds is one of the most challenging issues for
transmission and storage. In this paper, we propose an end-to-end voxel
Transformer and Sparse Convolution based Point Cloud Attribute Compression
(TSC-PCAC) for 3D broadcasting. Firstly, we present a framework of the
TSC-PCAC, which include Transformer and Sparse Convolutional Module (TSCM)
based variational autoencoder and channel context module. Secondly, we propose
a two-stage TSCM, where the first stage focuses on modeling local dependencies
and feature representations of the point clouds, and the second stage captures
global features through spatial and channel pooling encompassing larger
receptive fields. This module effectively extracts global and local interpoint
relevance to reduce informational redundancy. Thirdly, we design a TSCM based
channel context module to exploit interchannel correlations, which improves the
predicted probability distribution of quantized latent representations and thus
reduces the bitrate. Experimental results indicate that the proposed TSC-PCAC
method achieves an average of 38.53%, 21.30%, and 11.19% Bjontegaard Delta
bitrate reductions compared to the Sparse-PCAC, NF-PCAC, and G-PCC v23 methods,
respectively. The encoding/decoding time costs are reduced up to 97.68%/98.78%
on average compared to the Sparse-PCAC. The source code and the trained models
of the TSC-PCAC are available at https://github.com/igizuxo/TSC-PCAC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaVEn: An Effective Multi-granularity Hybrid Visual Encoding Framework
  for Multimodal Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoya Jiang, Jia Hongrui, Haiyang Xu, Wei Ye, Mengfan Dong, Ming Yan, Ji Zhang, Fei Huang, Shikun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents MaVEn, an innovative Multi-granularity Visual Encoding
framework designed to enhance the capabilities of Multimodal Large Language
Models (MLLMs) in multi-image reasoning. Current MLLMs primarily focus on
single-image visual understanding, limiting their ability to interpret and
integrate information across multiple images. MaVEn addresses this limitation
by combining discrete visual symbol sequences, which abstract coarse-grained
semantic concepts, with traditional continuous representation sequences that
model fine-grained features. This dual approach bridges the semantic gap
between visual and textual data, thereby improving the model's ability to
process and interpret information from multiple images effectively.
Additionally, we design a dynamic reduction mechanism by for long-sequence
continuous features to enhance multi-image processing efficiency. Experimental
results demonstrate that MaVEn significantly enhances MLLMs' understanding in
complex multi-image scenarios, while also improving performance in single-image
contexts.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-25T00:00:00Z">2024-08-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Localization of Synthetic Manipulations in Western Blot Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anmol Manjunath, Viola Negroni, Sara Mandelli, Daniel Moreira, Paolo Bestagini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in deep learning and generative systems have
significantly fostered the creation of synthetic media, as well as the local
alteration of real content via the insertion of highly realistic synthetic
manipulations. Local image manipulation, in particular, poses serious
challenges to the integrity of digital content and societal trust. This problem
is not only confined to multimedia data, but also extends to biological images
included in scientific publications, like images depicting Western blots. In
this work, we address the task of localizing synthetic manipulations in Western
blot images. To discriminate between pristine and synthetic pixels of an
analyzed image, we propose a synthetic detector that operates on small patches
extracted from the image. We aggregate patch contributions to estimate a
tampering heatmap, highlighting synthetic pixels out of pristine ones. Our
methodology proves effective when tested over two manipulated Western blot
image datasets, one altered automatically and the other manually by exploiting
advanced AI-based image manipulation tools that are unknown at our training
stage. We also explore the robustness of our method over an external dataset of
other scientific images depicting different semantics, manipulated through
unseen generation techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing the Impact of Splicing Artifacts in Partially Fake Speech
  Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viola Negroni, Davide Salvi, Paolo Bestagini, Stefano Tubaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech deepfake detection has recently gained significant attention within
the multimedia forensics community. Related issues have also been explored,
such as the identification of partially fake signals, i.e., tracks that include
both real and fake speech segments. However, generating high-quality spliced
audio is not as straightforward as it may appear. Spliced signals are typically
created through basic signal concatenation. This process could introduce
noticeable artifacts that can make the generated data easier to detect. We
analyze spliced audio tracks resulting from signal concatenation, investigate
their artifacts and assess whether such artifacts introduce any bias in
existing datasets. Our findings reveal that by analyzing splicing artifacts, we
can achieve a detection EER of 6.16% and 7.36% on PartialSpoof and HAD
datasets, respectively, without needing to train any detector. These results
underscore the complexities of generating reliable spliced audio data and lead
to discussions that can help improve future research in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ASVspoof 5 Workshop (Interspeech2024 Satellite)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Visual Biases in Audio-Visual Localization Benchmarks <span class="chip">ECCV24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangyu Chen, Zihao Yue, Boshen Xu, Qin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-Visual Source Localization (AVSL) aims to localize the source of sound
within a video. In this paper, we identify a significant issue in existing
benchmarks: the sounding objects are often easily recognized based solely on
visual cues, which we refer to as visual bias. Such biases hinder these
benchmarks from effectively evaluating AVSL models. To further validate our
hypothesis regarding visual biases, we examine two representative AVSL
benchmarks, VGG-SS and EpicSounding-Object, where the vision-only models
outperform all audiovisual baselines. Our findings suggest that existing AVSL
benchmarks need further refinement to facilitate audio-visual learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECCV24 AVGenL Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Riemann-based Multi-scale Attention Reasoning Network for Text-3D
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenrui Li, Wei Han, Yandu Chen, Yeyu Chai, Yidan Lu, Xingtao Wang, Xiaopeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the challenges in acquiring paired Text-3D data and the inherent
irregularity of 3D data structures, combined representation learning of 3D
point clouds and text remains unexplored. In this paper, we propose a novel
Riemann-based Multi-scale Attention Reasoning Network (RMARN) for text-3D
retrieval. Specifically, the extracted text and point cloud features are
refined by their respective Adaptive Feature Refiner (AFR). Furthermore, we
introduce the innovative Riemann Local Similarity (RLS) module and the Global
Pooling Similarity (GPS) module. However, as 3D point cloud data and text data
often possess complex geometric structures in high-dimensional space, the
proposed RLS employs a novel Riemann Attention Mechanism to reflect the
intrinsic geometric relationships of the data. Without explicitly defining the
manifold, RMARN learns the manifold parameters to better represent the
distances between text-point cloud samples. To address the challenges of
lacking paired text-3D data, we have created the large-scale Text-3D Retrieval
dataset T3DR-HIT, which comprises over 3,380 pairs of text and point cloud
data. T3DR-HIT contains coarse-grained indoor 3D scenes and fine-grained
Chinese artifact scenes, consisting of 1,380 and over 2,000 text-3D pairs,
respectively. Experiments on our custom datasets demonstrate the superior
performance of the proposed method. Our code and proposed datasets are
available at \url{https://github.com/liwrui/RMARN}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SceneDreamer360: Text-Driven 3D-Consistent Scene Generation with
  Panoramic Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenrui Li, Yapeng Mi, Fucheng Cai, Zhe Yang, Wangmeng Zuo, Xingtao Wang, Xiaopeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-driven 3D scene generation has seen significant advancements recently.
However, most existing methods generate single-view images using generative
models and then stitch them together in 3D space. This independent generation
for each view often results in spatial inconsistency and implausibility in the
3D scenes. To address this challenge, we proposed a novel text-driven
3D-consistent scene generation model: SceneDreamer360. Our proposed method
leverages a text-driven panoramic image generation model as a prior for 3D
scene generation and employs 3D Gaussian Splatting (3DGS) to ensure consistency
across multi-view panoramic images. Specifically, SceneDreamer360 enhances the
fine-tuned Panfusion generator with a three-stage panoramic enhancement,
enabling the generation of high-resolution, detail-rich panoramic images.
During the 3D scene construction, a novel point cloud fusion initialization
method is used, producing higher quality and spatially consistent point clouds.
Our extensive experiments demonstrate that compared to other methods,
SceneDreamer360 with its panoramic image generation and 3DGS can produce higher
quality, spatially consistent, and visually appealing 3D scenes from any text
prompt. Our codes are available at
\url{https://github.com/liwrui/SceneDreamer360}.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-24T00:00:00Z">2024-08-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpeechCraft: A Fine-grained Expressive Speech <span class="highlight-title">Dataset</span> with Natural
  Language Description 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Jin, Jia Jia, Qixin Wang, Kehan Li, Shuoyi Zhou, Songtao Zhou, Xiaoyu Qin, Zhiyong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-language multi-modal learning presents a significant challenge due to
the fine nuanced information inherent in speech styles. Therefore, a
large-scale dataset providing elaborate comprehension of speech style is
urgently needed to facilitate insightful interplay between speech audio and
natural language. However, constructing such datasets presents a major
trade-off between large-scale data collection and high-quality annotation. To
tackle this challenge, we propose an automatic speech annotation system for
expressiveness interpretation that annotates in-the-wild speech clips with
expressive and vivid human language descriptions. Initially, speech audios are
processed by a series of expert classifiers and captioning models to capture
diverse speech characteristics, followed by a fine-tuned LLaMA for customized
annotation generation. Unlike previous tag/templet-based annotation frameworks
with limited information and diversity, our system provides in-depth
understandings of speech style through tailored natural language descriptions,
thereby enabling accurate and voluminous data generation for large model
training. With this system, we create SpeechCraft, a fine-grained bilingual
expressive speech dataset. It is distinguished by highly descriptive natural
language style prompts, containing approximately 2,000 hours of audio data and
encompassing over two million speech clips. Extensive experiments demonstrate
that the proposed dataset significantly boosts speech-language task performance
in stylist speech synthesis and speech style understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Multimedia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Open, Cross-Platform, Web-Based Metaverse Using WebXR and A-Frame 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Macario
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The metaverse has received much attention in the literature and industry in
the last few years, but the lack of an open and cross-platform architecture has
led to many distinct metaverses that cannot communicate with each other. This
work proposes a WebXR-based cross-platform architecture for developing spatial
web apps using the A-Frame and Networked-Aframe frameworks with a view to an
open and interoperable metaverse, accessible from both the web and extended
reality devices. A prototype was implemented and evaluated, supporting the
capability of the technology stack to enable immersive experiences across
different platforms and devices. Positive feedback on ease of use of the
immersive environment further corroborates the proposed approach, underscoring
its effectiveness in facilitating engaging and interactive virtual spaces. By
adhering to principles of interoperability and inclusivity, it lives up to Tim
Berners-Lee's vision of the World Wide Web as an open platform that transcends
geographical and technical boundaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2404.05317</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMoFusion: Multi-modal Co-Speech Motion Generation with Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02905v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02905v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sen Wang, Jiangning Zhang, Xin Tan, Zhifeng Xie, Chengjie Wang, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The body movements accompanying speech aid speakers in expressing their
ideas. Co-speech motion generation is one of the important approaches for
synthesizing realistic avatars. Due to the intricate correspondence between
speech and motion, generating realistic and diverse motion is a challenging
task. In this paper, we propose MMoFusion, a Multi-modal co-speech Motion
generation framework based on the diffusion model to ensure both the
authenticity and diversity of generated motion. We propose a progressive fusion
strategy to enhance the interaction of inter-modal and intra-modal, efficiently
integrating multi-modal information. Specifically, we employ a masked style
matrix based on emotion and identity information to control the generation of
different motion styles. Temporal modeling of speech and motion is partitioned
into style-guided specific feature encoding and shared feature encoding, aiming
to learn both inter-modal and intra-modal features. Besides, we propose a
geometric loss to enforce the joints' velocity and acceleration coherence among
frames. Our framework generates vivid, diverse, and style-controllable motion
of arbitrary length through inputting speech and editing identity and emotion.
Extensive experiments demonstrate that our method outperforms current co-speech
motion generation methods including upper body and challenging full body.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-23T00:00:00Z">2024-08-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghua Tang, Liyun Zhang, Yu Lu, Dian Ding, Lanqing Yang, YiChao Chen, Minjie Bian, Xiaoshan Li, Guangtao Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion recognition can enhance humanized machine responses to user commands,
while voiceprint-based perception systems can be easily integrated into
commonly used devices like smartphones and stereos. Despite having the largest
number of speakers, there is a noticeable absence of high-quality corpus
datasets for emotion recognition using Chinese voiceprints. Hence, this paper
introduces the VCEMO dataset to address this deficiency. The proposed dataset
is constructed from everyday conversations and comprises over 100 users and
7,747 textual samples. Furthermore, this paper proposes a multimodal-based
model as a benchmark, which effectively fuses speech, text, and external
knowledge using a co-attention structure. The system employs contrastive
learning-based regulation for the uneven distribution of the dataset and the
diversity of emotional expressions. The experiments demonstrate the significant
improvement of the proposed model over SOTA on the VCEMO and IEMOCAP datasets.
Code and dataset will be released for research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cam-Van Thi Nguyen, The-Son Le, Anh-Tuan Mai, Duc-Trong Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Emotion Recognition in Conversations (ERC) is a typical multimodal
learning task in exploiting various data modalities concurrently. Prior studies
on effective multimodal ERC encounter challenges in addressing modality
imbalances and optimizing learning across modalities. Dealing with these
problems, we present a novel framework named Ada2I, which consists of two
inseparable modules namely Adaptive Feature Weighting (AFW) and Adaptive
Modality Weighting (AMW) for feature-level and modality-level balancing
respectively via leveraging both Inter- and Intra-modal interactions.
Additionally, we introduce a refined disparity ratio as part of our training
optimization strategy, a simple yet effective measure to assess the overall
discrepancy of the model's learning process when handling multiple modalities
simultaneously. Experimental results validate the effectiveness of Ada2I with
state-of-the-art performance compared to baselines on three benchmark datasets,
particularly in addressing modality imbalances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACM Multimedia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cap2Sum: Learning to Summarize Videos by Generating Captions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cairong Zhao, Chutian Wang, Zifan Song, Guosheng Hu, Haonan Chen, Xiaofan Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth of video data on the internet, video summarization is
becoming a very important AI technology. However, due to the high labelling
cost of video summarization, existing studies have to be conducted on
small-scale datasets, leading to limited performance and generalization
capacity. In this work, we introduce the use of dense video captions as a
supervision signal to train video summarization models. Motivated by this, we
propose Cap2Sum, a model that learns to summarize videos by generating
captions, to exploit dense video caption annotations. This weakly-supervised
approach allows us to train the models on large-scale dense video caption
datasets to achieve better performance and generalization capacity. To further
improve the generalization capacity, we introduce a CLIP (a strong
vision-language model) Prior mechanism to enhance the learning of important
objects that captions may ignore in the videos. In practice, Cap2Sum can
perform zero-shot video summarization or be fine-tuned by the ground-truth
summary or video caption of the target dataset. To examine the performance of
Cap2Sum after weakly-supervised fine-tuning by the video captions, we propose
two new datasets, TVSum-Caption and SumMe-Caption, which are derived from two
common video summarization datasets and will be publicly released. We conduct
extensive experiments and the results demonstrate that our method achieves
significant improvements in performance and generalization capacity compared
with previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpeechEE: A Novel Benchmark for Speech <span class="highlight-title">Event</span> <span class="highlight-title">Extraction</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Wang, Meishan Zhang, Hao Fei, Yu Zhao, Bobo Li, Shengqiong Wu, Wei Ji, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event extraction (EE) is a critical direction in the field of information
extraction, laying an important foundation for the construction of structured
knowledge bases. EE from text has received ample research and attention for
years, yet there can be numerous real-world applications that require direct
information acquisition from speech signals, online meeting minutes, interview
summaries, press releases, etc. While EE from speech has remained
under-explored, this paper fills the gap by pioneering a SpeechEE, defined as
detecting the event predicates and arguments from a given audio speech. To
benchmark the SpeechEE task, we first construct a large-scale high-quality
dataset. Based on textual EE datasets under the sentence, document, and
dialogue scenarios, we convert texts into speeches through both manual
real-person narration and automatic synthesis, empowering the data with diverse
scenarios, languages, domains, ambiences, and speaker styles. Further, to
effectively address the key challenges in the task, we tailor an E2E SpeechEE
system based on the encoder-decoder architecture, where a novel Shrinking Unit
module and a retrieval-aided decoding mechanism are devised. Extensive
experimental results on all SpeechEE subsets demonstrate the efficacy of the
proposed model, offering a strong baseline for the task. At last, being the
first work on this topic, we shed light on key directions for future research.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-22T00:00:00Z">2024-08-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Methods for Analyzing Learning and Training Environments: A
  Systematic Literature <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clayton Cohn, Eduardo Davalos, Caleb Vatral, Joyce Horn Fonteles, Hanchen David Wang, Meiyi Ma, Gautam Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent technological advancements have enhanced our ability to collect and
analyze rich multimodal data (e.g., speech, video, and eye gaze) to better
inform learning and training experiences. While previous reviews have focused
on parts of the multimodal pipeline (e.g., conceptual models and data fusion),
a comprehensive literature review on the methods informing multimodal learning
and training environments has not been conducted. This literature review
provides an in-depth analysis of research methods in these environments,
proposing a taxonomy and framework that encapsulates recent methodological
advances in this field and characterizes the multimodal domain in terms of five
modality groups: Natural Language, Video, Sensors, Human-Centered, and
Environment Logs. We introduce a novel data fusion category -- mid fusion --
and a graph-based technique for refining literature reviews, termed citation
graph pruning. Our analysis reveals that leveraging multiple modalities offers
a more holistic understanding of the behaviors and outcomes of learners and
trainees. Even when multimodality does not enhance predictive accuracy, it
often uncovers patterns that contextualize and elucidate unimodal data,
revealing subtleties that a single modality may miss. However, there remains a
need for further research to bridge the divide between multimodal learning and
training studies and foundational AI research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ACM Computing Surveys. Currently under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiMed: Massively Multimodal and Multitask Medical Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shentong Mo, Paul Pu Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomedical data is inherently multimodal, consisting of electronic health
records, medical imaging, digital pathology, genome sequencing, wearable
sensors, and more. The application of artificial intelligence tools to these
multifaceted sensing technologies has the potential to revolutionize the
prognosis, diagnosis, and management of human health and disease. However,
current approaches to biomedical AI typically only train and evaluate with one
or a small set of medical modalities and tasks. This limitation hampers the
development of comprehensive tools that can leverage the rich interconnected
information across many heterogeneous biomedical sensors. To address this
challenge, we present MultiMed, a benchmark designed to evaluate and enable
large-scale learning across a wide spectrum of medical modalities and tasks.
MultiMed consists of 2.56 million samples across ten medical modalities such as
medical reports, pathology, genomics, and protein data, and is structured into
eleven challenging tasks, including disease prognosis, protein structure
prediction, and medical question answering. Using MultiMed, we conduct
comprehensive experiments benchmarking state-of-the-art unimodal, multimodal,
and multitask models. Our analysis highlights the advantages of training
large-scale medical models across many related modalities and tasks. Moreover,
MultiMed enables studies of generalization across related medical concepts,
robustness to real-world noisy data and distribution shifts, and novel modality
combinations to improve prediction performance. MultiMed will be publicly
available and regularly updated and welcomes inputs from the community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DreamCinema: Cinematic Transfer with Free Camera and 3D Character 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiliang Chen, Fangfu Liu, Diankun Wu, Haowen Sun, Haixu Song, Yueqi Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We are living in a flourishing era of digital media, where everyone has the
potential to become a personal filmmaker. Current research on cinematic
transfer empowers filmmakers to reproduce and manipulate the visual elements
(e.g., cinematography and character behaviors) from classic shots. However,
characters in the reimagined films still rely on manual crafting, which
involves significant technical complexity and high costs, making it
unattainable for ordinary users. Furthermore, their estimated cinematography
lacks smoothness due to inadequate capturing of inter-frame motion and modeling
of physical trajectories. Fortunately, the remarkable success of 2D and 3D AIGC
has opened up the possibility of efficiently generating characters tailored to
users' needs, diversifying cinematography. In this paper, we propose
DreamCinema, a novel cinematic transfer framework that pioneers generative AI
into the film production paradigm, aiming at facilitating user-friendly film
creation. Specifically, we first extract cinematic elements (i.e., human and
camera pose) and optimize the camera trajectory. Then, we apply a character
generator to efficiently create 3D high-quality characters with a human
structure prior. Finally, we develop a structure-guided motion transfer
strategy to incorporate generated characters into film creation and transfer it
via 3D graphics engines smoothly. Extensive experiments demonstrate the
effectiveness of our method for creating high-quality films with free camera
and 3D characters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://liuff19.github.io/DreamCinema</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Role of Audio in Multimodal Mis<span class="highlight-title">information</span> Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moyang Liu, Yukun Liu, Ruibo Fu, Zhengqi Wen, Jianhua Tao, Xuefei Liu, Guanjun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of deepfake technology, especially the deep audio
fake technology, misinformation detection on the social media scene meets a
great challenge. Social media data often contains multimodal information which
includes audio, video, text, and images. However, existing multimodal
misinformation detection methods tend to focus only on some of these
modalities, failing to comprehensively address information from all modalities.
To comprehensively address the various modal information that may appear on
social media, this paper constructs a comprehensive multimodal misinformation
detection framework. By employing corresponding neural network encoders for
each modality, the framework can fuse different modality information and
support the multimodal misinformation detection task. Based on the constructed
framework, this paper explores the importance of the audio modality in
multimodal misinformation detection tasks on social media. By adjusting the
architecture of the acoustic encoder, the effectiveness of different acoustic
feature encoders in the multimodal misinformation detection tasks is
investigated. Furthermore, this paper discovers that audio and video
information must be carefully aligned, otherwise the misalignment across
different audio and video modalities can severely impair the model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Face Forgery Detection via Adaptive Learning for <span class="highlight-title">Pre-train</span>ed
  Vision Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anwei Luo, Rizhao Cai, Chenqi Kong, Yakun Ju, Xiangui Kang, Jiwu Huang, Alex C. Kot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid progress of generative models, the current challenge in face
forgery detection is how to effectively detect realistic manipulated faces from
different unseen domains. Though previous studies show that pre-trained Vision
Transformer (ViT) based models can achieve some promising results after fully
fine-tuning on the Deepfake dataset, their generalization performances are
still unsatisfactory. One possible reason is that fully fine-tuned ViT-based
models may disrupt the pre-trained features [1, 2] and overfit to some
data-specific patterns [3]. To alleviate this issue, we present a
\textbf{F}orgery-aware \textbf{A}daptive \textbf{Vi}sion \textbf{T}ransformer
(FA-ViT) under the adaptive learning paradigm, where the parameters in the
pre-trained ViT are kept fixed while the designed adaptive modules are
optimized to capture forgery features. Specifically, a global adaptive module
is designed to model long-range interactions among input tokens, which takes
advantage of self-attention mechanism to mine global forgery clues. To further
explore essential local forgery clues, a local adaptive module is proposed to
expose local inconsistencies by enhancing the local contextual association. In
addition, we introduce a fine-grained adaptive learning module that emphasizes
the common compact representation of genuine faces through relationship
learning in fine-grained pairs, driving these proposed adaptive modules to be
aware of fine-grained forgery-aware information. Extensive experiments
demonstrate that our FA-ViT achieves state-of-the-arts results in the
cross-dataset evaluation, and enhances the robustness against unseen
perturbations. Particularly, FA-ViT achieves 93.83\% and 78.32\% AUC scores on
Celeb-DF and DFDC datasets in the cross-dataset evaluation. The code and
trained model have been released at: https://github.com/LoveSiameseCat/FAViT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lighthouse: A User-Friendly Library for Reproducible Video Moment
  Retrieval and Highlight Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taichi Nishimura, Shota Nakada, Hokuto Munakata, Tatsuya Komatsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Lighthouse, a user-friendly library for reproducible video moment
retrieval and highlight detection (MR-HD). Although researchers proposed
various MR-HD approaches, the research community holds two main issues. The
first is a lack of comprehensive and reproducible experiments across various
methods, datasets, and video-text features. This is because no unified training
and evaluation codebase covers multiple settings. The second is user-unfriendly
design. Because previous works use different libraries, researchers set up
individual environments. In addition, most works release only the training
codes, requiring users to implement the whole inference process of MR-HD.
Lighthouse addresses these issues by implementing a unified reproducible
codebase that includes six models, three features, and five datasets. In
addition, it provides an inference API and web demo to make these methods
easily accessible for researchers and developers. Our experiments demonstrate
that Lighthouse generally reproduces the reported scores in the reference
papers. The code is available at https://github.com/line/lighthouse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages; library tech report</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-21T00:00:00Z">2024-08-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Let Community Rules Be Reflected in Online Content Moderation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangjiaxuan Xin, Kanlun Wang, Zhe Fu, Lina Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content moderation is a widely used strategy to prevent the dissemination of
irregular information on social media platforms. Despite extensive research on
developing automated models to support decision-making in content moderation,
there remains a notable scarcity of studies that integrate the rules of online
communities into content moderation. This study addresses this gap by proposing
a community rule-based content moderation framework that directly integrates
community rules into the moderation of user-generated content. Our experiment
results with datasets collected from two domains demonstrate the superior
performance of models based on the framework to baseline models across all
evaluation metrics. In particular, incorporating community rules substantially
enhances model performance in content moderation. The findings of this research
have significant research and practical implications for improving the
effectiveness and generalizability of content moderation models in online
communities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video-Foley: Two-Stage Video-To-Sound Generation via Temporal <span class="highlight-title">Event</span>
  Condition For Foley Sound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwon Lee, Jaekwon Im, Dabin Kim, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foley sound synthesis is crucial for multimedia production, enhancing user
experience by synchronizing audio and video both temporally and semantically.
Recent studies on automating this labor-intensive process through
video-to-sound generation face significant challenges. Systems lacking explicit
temporal features suffer from poor controllability and alignment, while
timestamp-based models require costly and subjective human annotation. We
propose Video-Foley, a video-to-sound system using Root Mean Square (RMS) as a
temporal event condition with semantic timbre prompts (audio or text). RMS, a
frame-level intensity envelope feature closely related to audio semantics,
ensures high controllability and synchronization. The annotation-free
self-supervised learning framework consists of two stages, Video2RMS and
RMS2Sound, incorporating novel ideas including RMS discretization and
RMS-ControlNet with a pretrained text-to-audio model. Our extensive evaluation
shows that Video-Foley achieves state-of-the-art performance in audio-visual
alignment and controllability for sound timing, intensity, timbre, and nuance.
Code, model weights, and demonstrations are available on the accompanying
website. (https://jnwnlee.github.io/video-foley-demo)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for
  Multimodal Emotion Recognition <span class="chip">IJCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zebang Cheng, Shuyuan Tu, Dawei Huang, Minghan Li, Xiaojiang Peng, Zhi-Qi Cheng, Alexander G. Hauptmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents our winning approach for the MER-NOISE and MER-OV tracks
of the MER2024 Challenge on multimodal emotion recognition. Our system
leverages the advanced emotional understanding capabilities of Emotion-LLaMA to
generate high-quality annotations for unlabeled samples, addressing the
challenge of limited labeled data. To enhance multimodal fusion while
mitigating modality-specific noise, we introduce Conv-Attention, a lightweight
and efficient hybrid framework. Extensive experimentation vali-dates the
effectiveness of our approach. In the MER-NOISE track, our system achieves a
state-of-the-art weighted average F-score of 85.30%, surpassing the second and
third-place teams by 1.47% and 1.65%, respectively. For the MER-OV track, our
utilization of Emotion-LLaMA for open-vocabulary annotation yields an 8.52%
improvement in average accuracy and recall compared to GPT-4V, securing the
highest score among all participating large multimodal models. The code and
model for Emotion-LLaMA are available at
https://github.com/ZebangCheng/Emotion-LLaMA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ranked 1st in MER24@IJCAI and MRAC24@ACM MM (MER-NOISE & MER-OV
  (self-evaluated))</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ContextualStory: Consistent Visual Storytelling with Spatially-Enhanced
  and Storyline Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sixiao Zheng, Yanwei Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual storytelling involves generating a sequence of coherent frames from a
textual storyline while maintaining consistency in characters and scenes.
Existing autoregressive methods, which rely on previous frame-sentence pairs,
struggle with high memory usage, slow generation speeds, and limited context
integration. To address these issues, we propose ContextualStory, a novel
framework designed to generate coherent story frames and extend frames for
story continuation. ContextualStory utilizes Spatially-Enhanced Temporal
Attention to capture spatial and temporal dependencies, handling significant
character movements effectively. Additionally, we introduces a Storyline
Contextualizer to enrich context in storyline embedding and a StoryFlow Adapter
to measure scene changes between frames for guiding model. Extensive
experiments on PororoSV and FlintstonesSV benchmarks demonstrate that
ContextualStory significantly outperforms existing methods in both story
visualization and story continuation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Freehand Sketch Generation from Mechanical Components <span class="chip">ACM MM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05966v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05966v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Liao, Di Huang, Heming Fang, Yue Ma, Fengyuan Piao, Xinghui Li, Long Zeng, Pingfa Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drawing freehand sketches of mechanical components on multimedia devices for
AI-based engineering modeling has become a new trend. However, its development
is being impeded because existing works cannot produce suitable sketches for
data-driven research. These works either generate sketches lacking a freehand
style or utilize generative models not originally designed for this task
resulting in poor effectiveness. To address this issue, we design a two-stage
generative framework mimicking the human sketching behavior pattern, called
MSFormer, which is the first time to produce humanoid freehand sketches
tailored for mechanical components. The first stage employs Open CASCADE
technology to obtain multi-view contour sketches from mechanical components,
filtering perturbing signals for the ensuing generation process. Meanwhile, we
design a view selector to simulate viewpoint selection tasks during human
sketching for picking out information-rich sketches. The second stage
translates contour sketches into freehand sketches by a transformer-based
generator. To retain essential modeling features as much as possible and
rationalize stroke distribution, we introduce a novel edge-constraint stroke
initialization. Furthermore, we utilize a CLIP vision encoder and a new loss
function incorporating the Hausdorff distance to enhance the generalizability
and robustness of the model. Extensive experiments demonstrate that our
approach achieves state-of-the-art performance for generating freehand sketches
in the mechanical domain. Project page: https://mcfreeskegen.github.io .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ACM Multimedia (ACM MM) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ICE: Interactive 3D Game Character Editing via Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12667v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12667v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoqian Wu, Minda Zhao, Zhipeng Hu, Lincheng Li, Weijie Chen, Rui Zhao, Changjie Fan, Xin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ost recent popular Role-Playing Games (RPGs) allow players to create in-game
characters with hundreds of adjustable parameters, including bone positions and
various makeup options. Although text-driven auto-customization systems have
been developed to simplify the complex process of adjusting these intricate
character parameters, they are limited by their single-round generation and
lack the capability for further editing and fine-tuning. In this paper, we
propose an Interactive Character Editing framework (ICE) to achieve a
multi-round dialogue-based refinement process. In a nutshell, our ICE offers a
more user-friendly way to enable players to convey creative ideas iteratively
while ensuring that created characters align with the expectations of players.
Specifically, we propose an Instruction Parsing Module (IPM) that utilizes
large language models (LLMs) to parse multi-round dialogues into clear editing
instruction prompts in each round. To reliably and swiftly modify character
control parameters at a fine-grained level, we propose a Semantic-guided
Low-dimension Parameter Solver (SLPS) that edits character control parameters
according to prompts in a zero-shot manner. Our SLPS first localizes the
character control parameters related to the fine-grained modification, and then
optimizes the corresponding parameters in a low-dimension space to avoid
unrealistic results. Extensive experimental results demonstrate the
effectiveness of our proposed ICE for in-game character creation and the
superior editing performance of ICE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Medical M<span class="highlight-title">LLM</span> is Vulnerable: Cross-Modality Jailbreak and Mismatched
  Attacks on Medical Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20775v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20775v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xijie Huang, Xinyuan Wang, Hantao Zhang, Yinghao Zhu, Jiawen Xi, Jingkun An, Hao Wang, Hao Liang, Chengwei Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Security concerns related to Large Language Models (LLMs) have been
extensively explored, yet the safety implications for Multimodal Large Language
Models (MLLMs), particularly in medical contexts (MedMLLMs), remain
insufficiently studied. This paper delves into the underexplored security
vulnerabilities of MedMLLMs, especially when deployed in clinical environments
where the accuracy and relevance of question-and-answer interactions are
critically tested against complex medical challenges. By combining existing
clinical medical data with atypical natural phenomena, we define the mismatched
malicious attack (2M-attack) and introduce its optimized version, known as the
optimized mismatched malicious attack (O2M-attack or 2M-optimization). Using
the voluminous 3MAD dataset that we construct, which covers a wide range of
medical image modalities and harmful medical scenarios, we conduct a
comprehensive analysis and propose the MCM optimization method, which
significantly enhances the attack success rate on MedMLLMs. Evaluations with
this dataset and attack methods, including white-box attacks on LLaVA-Med and
transfer attacks (black-box) on four other SOTA models, indicate that even
MedMLLMs designed with enhanced security features remain vulnerable to security
breaches. Our work underscores the urgent need for a concerted effort to
implement robust security measures and enhance the safety and efficacy of
open-source MedMLLMs, particularly given the potential severity of jailbreak
attacks and other malicious or clinically significant exploits in medical
settings. Our code is available at https://github.com/dirtycomputer/O2M_attack.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-20T00:00:00Z">2024-08-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-supervised</span> Photographic Image Layout Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03740v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03740v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoran Zhao, Peng Lu, Xujun Peng, Wenhao Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the domain of image layout representation learning, the critical process
of translating image layouts into succinct vector forms is increasingly
significant across diverse applications, such as image retrieval, manipulation,
and generation. Most approaches in this area heavily rely on costly labeled
datasets and notably lack in adapting their modeling and learning methods to
the specific nuances of photographic image layouts. This shortfall makes the
learning process for photographic image layouts suboptimal. In our research, we
directly address these challenges. We innovate by defining basic layout
primitives that encapsulate various levels of layout information and by mapping
these, along with their interconnections, onto a heterogeneous graph structure.
This graph is meticulously engineered to capture the intricate layout
information within the pixel domain explicitly. Advancing further, we introduce
novel pretext tasks coupled with customized loss functions, strategically
designed for effective self-supervised learning of these layout graphs.
Building on this foundation, we develop an autoencoder-based network
architecture skilled in compressing these heterogeneous layout graphs into
precise, dimensionally-reduced layout representations. Additionally, we
introduce the LODB dataset, which features a broader range of layout categories
and richer semantics, serving as a comprehensive benchmark for evaluating the
effectiveness of layout representation learning methods. Our extensive
experimentation on this dataset demonstrates the superior performance of our
approach in the realm of photographic image layout representation learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The authors of the paper believe that there is an error in the
  measurement of the F1 curve in the metrics description</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ New Job, New Gender? Measuring the Social Bias in Image Generation
  Models <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Haonan Bai, Jen-tse Huang, Yuxuan Wan, Youliang Yuan, Haoyi Qiu, Nanyun Peng, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image generation models can generate or edit images from a given text. Recent
advancements in image generation technology, exemplified by DALL-E and
Midjourney, have been groundbreaking. These advanced models, despite their
impressive capabilities, are often trained on massive Internet datasets, making
them susceptible to generating content that perpetuates social stereotypes and
biases, which can lead to severe consequences. Prior research on assessing bias
within image generation models suffers from several shortcomings, including
limited accuracy, reliance on extensive human labor, and lack of comprehensive
analysis. In this paper, we propose BiasPainter, a novel evaluation framework
that can accurately, automatically and comprehensively trigger social bias in
image generation models. BiasPainter uses a diverse range of seed images of
individuals and prompts the image generation models to edit these images using
gender, race, and age-neutral queries. These queries span 62 professions, 39
activities, 57 types of objects, and 70 personality traits. The framework then
compares the edited images to the original seed images, focusing on the
significant changes related to gender, race, and age. BiasPainter adopts a key
insight that these characteristics should not be modified when subjected to
neutral prompts. Built upon this design, BiasPainter can trigger the social
bias and evaluate the fairness of image generation models. We use BiasPainter
to evaluate six widely-used image generation models, such as stable diffusion
and Midjourney. Experimental results show that BiasPainter can successfully
trigger social bias in image generation models. According to our human
evaluation, BiasPainter can achieve 90.8% accuracy on automatic bias detection,
which is significantly higher than the results reported in previous work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM MM 2024 Oral</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-19T00:00:00Z">2024-08-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu He, Yizhi Song, Hejun Huang, Daniel Aliaga, Xin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video generation has been dominated by end-to-end diffusion-based or
autoregressive models. On one hand, those novel models provide plausible
versatility, but they are criticized for physical correctness, shading and
illumination, camera motion, and temporal consistency. On the other hand, film
industry relies on manually-edited Computer-Generated Imagery (CGI) using 3D
modeling software. Human-directed 3D synthetic videos and animations address
the aforementioned shortcomings, but it is extremely tedious and requires tight
collaboration between movie makers and 3D rendering experts. In this paper, we
introduce an automatic synthetic video generation pipeline based on Vision
Large Language Model (VLM) agent collaborations. Given a natural language
description of a video, multiple VLM agents auto-direct various processes of
the generation pipeline. They cooperate to create Blender scripts which render
a video that best aligns with the given description. Based on film making
inspiration and augmented with Blender-based movie making knowledge, the
Director agent decomposes the input text-based video description into
sub-processes. For each sub-process, the Programmer agent produces Python-based
Blender scripts based on customized function composing and API calling. Then,
the Reviewer agent, augmented with knowledge of video reviewing, character
motion coordinates, and intermediate screenshots uses its compositional
reasoning ability to provide feedback to the Programmer agent. The Programmer
agent iteratively improves the scripts to yield the best overall video outcome.
Our generated videos show better quality than commercial video generation
models in 5 metrics on video quality and instruction-following performance.
Moreover, our framework outperforms other approaches in a comprehensive user
study on quality, consistency, and rationality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Webcam-based Pupil Diameter Prediction Benefits from Upscaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijul Shah, Brian B. Moser, Ko Watanabe, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing pupil diameter is essential for assessing psychological and
physiological states such as stress levels and cognitive load. However, the low
resolution of images in eye datasets often hampers precise measurement. This
study evaluates the impact of various upscaling methods, ranging from bicubic
interpolation to advanced super-resolution, on pupil diameter predictions. We
compare several pre-trained methods, including CodeFormer, GFPGAN, Real-ESRGAN,
HAT, and SRResNet. Our findings suggest that pupil diameter prediction models
trained on upscaled datasets are highly sensitive to the selected upscaling
method and scale. Our results demonstrate that upscaling methods consistently
enhance the accuracy of pupil diameter prediction models, highlighting the
importance of upscaling in pupilometry. Overall, our work provides valuable
insights for selecting upscaling techniques, paving the way for more accurate
assessments in psychological and physiological research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perceptual Depth Quality Assessment of Stereoscopic Omnidirectional
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhou, Zhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth perception plays an essential role in the viewer experience for
immersive virtual reality (VR) visual environments. However, previous research
investigations in the depth quality of 3D/stereoscopic images are rather
limited, and in particular, are largely lacking for 3D viewing of 360-degree
omnidirectional content. In this work, we make one of the first attempts to
develop an objective quality assessment model named depth quality index (DQI)
for efficient no-reference (NR) depth quality assessment of stereoscopic
omnidirectional images. Motivated by the perceptual characteristics of the
human visual system (HVS), the proposed DQI is built upon multi-color-channel,
adaptive viewport selection, and interocular discrepancy features. Experimental
results demonstrate that the proposed method outperforms state-of-the-art image
quality assessment (IQA) and depth quality assessment (DQA) approaches in
predicting the perceptual depth quality when tested using both single-viewport
and omnidirectional stereoscopic image databases. Furthermore, we demonstrate
that combining the proposed depth quality model with existing IQA methods
significantly boosts the performance in predicting the overall quality of 3D
omnidirectional images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TCSVT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sliced Maximal <span class="highlight-title">Information</span> Coefficient: A Training-Free Approach for
  Image Quality Assessment Enhancement <span class="chip">ICME2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Xiao, Xu Wang, Yulin He, Baoliang Chen, Xuelin Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Full-reference image quality assessment (FR-IQA) models generally operate by
measuring the visual differences between a degraded image and its reference.
However, existing FR-IQA models including both the classical ones (eg, PSNR and
SSIM) and deep-learning based measures (eg, LPIPS and DISTS) still exhibit
limitations in capturing the full perception characteristics of the human
visual system (HVS). In this paper, instead of designing a new FR-IQA measure,
we aim to explore a generalized human visual attention estimation strategy to
mimic the process of human quality rating and enhance existing IQA models. In
particular, we model human attention generation by measuring the statistical
dependency between the degraded image and the reference image. The dependency
is captured in a training-free manner by our proposed sliced maximal
information coefficient and exhibits surprising generalization in different IQA
measures. Experimental results verify the performance of existing IQA models
can be consistently improved when our attention module is incorporated. The
source code is available at https://github.com/KANGX99/SMIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, accepted by ICME2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anim-Director: A Large Multimodal Model Powered Agent for Controllable
  Animation Video Generation <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxin Li, Haoyuan Shi, Baotian Hu, Longyue Wang, Jiashun Zhu, Jinyi Xu, Zhen Zhao, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional animation generation methods depend on training generative models
with human-labelled data, entailing a sophisticated multi-stage pipeline that
demands substantial human effort and incurs high training costs. Due to limited
prompting plans, these methods typically produce brief, information-poor, and
context-incoherent animations. To overcome these limitations and automate the
animation process, we pioneer the introduction of large multimodal models
(LMMs) as the core processor to build an autonomous animation-making agent,
named Anim-Director. This agent mainly harnesses the advanced understanding and
reasoning capabilities of LMMs and generative AI tools to create animated
videos from concise narratives or simple instructions. Specifically, it
operates in three main stages: Firstly, the Anim-Director generates a coherent
storyline from user inputs, followed by a detailed director's script that
encompasses settings of character profiles and interior/exterior descriptions,
and context-coherent scene descriptions that include appearing characters,
interiors or exteriors, and scene events. Secondly, we employ LMMs with the
image generation tool to produce visual images of settings and scenes. These
images are designed to maintain visual consistency across different scenes
using a visual-language prompting method that combines scene descriptions and
images of the appearing character and setting. Thirdly, scene images serve as
the foundation for producing animated videos, with LMMs generating prompts to
guide this process. The whole process is notably autonomous without manual
intervention, as the LMMs interact seamlessly with generative tools to generate
prompts, evaluate visual quality, and select the best one to optimize the final
output.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGGRAPH Asia 2024, Project and Codes:
  https://github.com/HITsz-TMG/Anim-Director</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExpoMamba: Exploiting Frequency SSM Blocks for Efficient and Effective
  Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eashan Adhikarla, Kai Zhang, John Nicholson, Brian D. Davison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-light image enhancement remains a challenging task in computer vision,
with existing state-of-the-art models often limited by hardware constraints and
computational inefficiencies, particularly in handling high-resolution images.
Recent foundation models, such as transformers and diffusion models, despite
their efficacy in various domains, are limited in use on edge devices due to
their computational complexity and slow inference times. We introduce
ExpoMamba, a novel architecture that integrates components of the frequency
state space within a modified U-Net, offering a blend of efficiency and
effectiveness. This model is specifically optimized to address mixed exposure
challenges, a common issue in low-light image enhancement, while ensuring
computational efficiency. Our experiments demonstrate that ExpoMamba enhances
low-light images up to 2-3x faster than traditional models with an inference
time of 36.6 ms and achieves a PSNR improvement of approximately 15-20% over
competing models, making it highly suitable for real-time image processing
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-Stream Fusion Approach with One-Class Learning for Audio-Visual
  Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14176v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14176v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyungbok Lee, You Zhang, Zhiyao Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of developing a robust audio-visual
deepfake detection model. In practical use cases, new generation algorithms are
continually emerging, and these algorithms are not encountered during the
development of detection methods. This calls for the generalization ability of
the method. Additionally, to ensure the credibility of detection methods, it is
beneficial for the model to interpret which cues from the video indicate it is
fake. Motivated by these considerations, we then propose a multi-stream fusion
approach with one-class learning as a representation-level regularization
technique. We study the generalization problem of audio-visual deepfake
detection by creating a new benchmark by extending and re-splitting the
existing FakeAVCeleb dataset. The benchmark contains four categories of fake
videos (Real Audio-Fake Visual, Fake Audio-Fake Visual, Fake Audio-Real Visual,
and Unsynchronized videos). The experimental results demonstrate that our
approach surpasses the previous models by a large margin. Furthermore, our
proposed framework offers interpretability, indicating which modality the model
identifies as more likely to be fake. The source code is released at
https://github.com/bok-bok/MSOC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Resolution Guidance for Facial Expression Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.06365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.06365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songpan Wang, Xu Li, Tianxiang Jiang, Yuanlun Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facial expression recognition (FER) is vital for human-computer interaction
and emotion analysis, yet recognizing expressions in low-resolution images
remains challenging. This paper introduces a practical method called Dynamic
Resolution Guidance for Facial Expression Recognition (DRGFER) to effectively
recognize facial expressions in images with varying resolutions without
compromising FER model accuracy. Our framework comprises two main components:
the Resolution Recognition Network (RRN) and the Multi-Resolution Adaptation
Facial Expression Recognition Network (MRAFER). The RRN determines image
resolution, outputs a binary vector, and the MRAFER assigns images to suitable
facial expression recognition networks based on resolution. We evaluated DRGFER
on widely-used datasets RAFDB and FERPlus, demonstrating that our method
retains optimal model performance at each resolution and outperforms
alternative resolution approaches. The proposed framework exhibits robustness
against resolution variations and facial expressions, offering a promising
solution for real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-08-18T00:00:00Z">2024-08-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Modal Fusion by Alignment and Label Matching for Multimodal
  Emotion Recognition <span class="chip">INTERSPEECH 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qifei Li, Yingming Gao, Yuhua Wen, Cong Wang, Ya Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the limitation in multimodal emotion recognition (MER) performance
arising from inter-modal information fusion, we propose a novel MER framework
based on multitask learning where fusion occurs after alignment, called
Foal-Net. The framework is designed to enhance the effectiveness of modality
fusion and includes two auxiliary tasks: audio-video emotion alignment (AVEL)
and cross-modal emotion label matching (MEM). First, AVEL achieves alignment of
emotional information in audio-video representations through contrastive
learning. Then, a modal fusion network integrates the aligned features.
Meanwhile, MEM assesses whether the emotions of the current sample pair are the
same, providing assistance for modal information fusion and guiding the model
to focus more on emotional information. The experimental results conducted on
IEMOCAP corpus show that Foal-Net outperforms the state-of-the-art methods and
emotion alignment is necessary before modal fusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted by INTERSPEECH 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FD2Talk: Towards Generalized Talking Head Generation with Facial
  Decoupled Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Yao, Xuxin Cheng, Zhiqi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Talking head generation is a significant research topic that still faces
numerous challenges. Previous works often adopt generative adversarial networks
or regression models, which are plagued by generation quality and average
facial shape problem. Although diffusion models show impressive generative
ability, their exploration in talking head generation remains unsatisfactory.
This is because they either solely use the diffusion model to obtain an
intermediate representation and then employ another pre-trained renderer, or
they overlook the feature decoupling of complex facial details, such as
expressions, head poses and appearance textures. Therefore, we propose a Facial
Decoupled Diffusion model for Talking head generation called FD2Talk, which
fully leverages the advantages of diffusion models and decouples the complex
facial details through multi-stages. Specifically, we separate facial details
into motion and appearance. In the initial phase, we design the Diffusion
Transformer to accurately predict motion coefficients from raw audio. These
motions are highly decoupled from appearance, making them easier for the
network to learn compared to high-dimensional RGB images. Subsequently, in the
second phase, we encode the reference image to capture appearance textures. The
predicted facial and head motions and encoded appearance then serve as the
conditions for the Diffusion UNet, guiding the frame generation. Benefiting
from decoupling facial details and fully leveraging diffusion models, extensive
experiments substantiate that our approach excels in enhancing image quality
and generating more accurate and diverse results compared to previous
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM Multimedia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynopGround: A Large-Scale <span class="highlight-title">Dataset</span> for Multi-Paragraph Video Grounding
  from TV Dramas and Synopses <span class="chip">ACM MM 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01669v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01669v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaolei Tan, Zihang Lin, Junfu Pu, Zhongang Qi, Wei-Yi Pei, Zhi Qu, Yexin Wang, Ying Shan, Wei-Shi Zheng, Jian-Fang Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video grounding is a fundamental problem in multimodal content understanding,
aiming to localize specific natural language queries in an untrimmed video.
However, current video grounding datasets merely focus on simple events and are
either limited to shorter videos or brief sentences, which hinders the model
from evolving toward stronger multimodal understanding capabilities. To address
these limitations, we present a large-scale video grounding dataset named
SynopGround, in which more than 2800 hours of videos are sourced from popular
TV dramas and are paired with accurately localized human-written synopses. Each
paragraph in the synopsis serves as a language query and is manually annotated
with precise temporal boundaries in the long video. These paragraph queries are
tightly correlated to each other and contain a wealth of abstract expressions
summarizing video storylines and specific descriptions portraying event
details, which enables the model to learn multimodal perception on more
intricate concepts over longer context dependencies. Based on the dataset, we
further introduce a more complex setting of video grounding dubbed
Multi-Paragraph Video Grounding (MPVG), which takes as input multiple
paragraphs and a long video for grounding each paragraph query to its temporal
interval. In addition, we propose a novel Local-Global Multimodal Reasoner
(LGMR) to explicitly model the local-global structures of long-term multimodal
inputs for MPVG. Our method provides an effective baseline solution to the
multi-paragraph video grounding problem. Extensive experiments verify the
proposed model's effectiveness as well as its superiority in long-term
multi-paragraph video grounding over prior state-of-the-arts. Dataset and code
are publicly available. Project page: https://synopground.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2024. Project page: https://synopground.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Singer separation for karaoke content generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.06707v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.06707v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsuan-Yu Lin, Xuanjun Chen, Jyh-Shing Roger Jang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the rapid development of deep learning, we can now successfully
separate singing voice from mono audio music. However, this separation can only
extract human voices from other musical instruments, which is undesirable for
karaoke content generation applications that only require the separation of
lead singers. For this karaoke application, we need to separate the music
containing male and female duets into two vocals, or extract a single lead
vocal from the music containing vocal harmony. For this reason, we propose in
this article to use a singer separation system, which generates karaoke content
for one or two separated lead singers. In particular, we introduced three
models for the singer separation task and designed an automatic model selection
scheme to distinguish how many lead singers are in the song. We also collected
a large enough data set, MIR-SingerSeparation, which has been publicly released
to advance the frontier of this research. Our singer separation is most
suitable for sentimental ballads and can be directly applied to karaoke content
generation. As far as we know, this is the first singer-separation work for
real-world karaoke applications.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-09-18T01:50:23.263198423Z">
            2024-09-18 01:50:23 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
