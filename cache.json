{"2024-09-16T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2409.10516v1","updated":"2024-09-16T17:59:52Z","published":"2024-09-16T17:59:52Z","title":"RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval","summary":"  Transformer-based large Language Models (LLMs) become increasingly important\nin various domains. However, the quadratic time complexity of attention\noperation poses a significant challenge for scaling to longer contexts due to\nthe extremely high inference latency and GPU memory consumption for caching\nkey-value (KV) vectors. This paper proposes RetrievalAttention, a training-free\napproach to accelerate attention computation. To leverage the dynamic sparse\nproperty of attention, RetrievalAttention builds approximate nearest neighbor\nsearch (ANNS) indexes upon KV vectors in CPU memory and retrieves the most\nrelevant ones via vector search during generation. Due to the\nout-of-distribution (OOD) between query vectors and key vectors, off-the-shelf\nANNS indexes still need to scan O(N) (usually 30% of all keys) data for\naccurate retrieval, which fails to exploit the high sparsity.\nRetrievalAttention first identifies the OOD challenge of ANNS-based attention,\nand addresses it via an attention-aware vector search algorithm that can adapt\nto queries and only access 1--3% of data, thus achieving a sub-linear time\ncomplexity. RetrievalAttention greatly reduces the inference cost of\nlong-context LLM with much lower GPU memory requirements while maintaining the\nmodel accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for\nserving 128K tokens in LLMs with 8B parameters, which is capable of generating\none token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).\n","authors":["Di Liu","Meng Chen","Baotong Lu","Huiqiang Jiang","Zhenhua Han","Qianxi Zhang","Qi Chen","Chengruidong Zhang","Bailu Ding","Kai Zhang","Chen Chen","Fan Yang","Yuqing Yang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2409.10516v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2409.10515v1","updated":"2024-09-16T17:59:50Z","published":"2024-09-16T17:59:50Z","title":"An Efficient Self-Learning Framework For Interactive Spoken Dialog\n  Systems","summary":"  Dialog systems, such as voice assistants, are expected to engage with users\nin complex, evolving conversations. Unfortunately, traditional automatic speech\nrecognition (ASR) systems deployed in such applications are usually trained to\nrecognize each turn independently and lack the ability to adapt to the\nconversational context or incorporate user feedback. In this work, we introduce\na general framework for ASR in dialog systems that can go beyond learning from\nsingle-turn utterances and learn over time how to adapt to both explicit\nsupervision and implicit user feedback present in multi-turn conversations. We\naccomplish that by leveraging advances in student-teacher learning and\ncontext-aware dialog processing, and designing contrastive self-supervision\napproaches with Ohm, a new online hard-negative mining approach. We show that\nleveraging our new framework compared to traditional training leads to relative\nWER reductions of close to 10% in real-world dialog systems, and up to 26% on\npublic synthetic data.\n","authors":["Hitesh Tulsiani","David M. Chan","Shalini Ghosh","Garima Lalwani","Prabhat Pandey","Ankish Bansal","Sri Garimella","Ariya Rastrow","Bj√∂rn Hoffmeister"],"pdf_url":"https://arxiv.org/pdf/2409.10515v1.pdf","comment":"Presented at ICML 2024"},{"id":"http://arxiv.org/abs/2409.10504v1","updated":"2024-09-16T17:45:40Z","published":"2024-09-16T17:45:40Z","title":"DILA: Dictionary Label Attention for Mechanistic Interpretability in\n  High-dimensional Multi-label Medical Coding Prediction","summary":"  Predicting high-dimensional or extreme multilabels, such as in medical\ncoding, requires both accuracy and interpretability. Existing works often rely\non local interpretability methods, failing to provide comprehensive\nexplanations of the overall mechanism behind each label prediction within a\nmultilabel set. We propose a mechanistic interpretability module called\nDIctionary Label Attention (\\method) that disentangles uninterpretable dense\nembeddings into a sparse embedding space, where each nonzero element (a\ndictionary feature) represents a globally learned medical concept. Through\nhuman evaluations, we show that our sparse embeddings are more human\nunderstandable than its dense counterparts by at least 50 percent. Our\nautomated dictionary feature identification pipeline, leveraging large language\nmodels (LLMs), uncovers thousands of learned medical concepts by examining and\nsummarizing the highest activating tokens for each dictionary feature. We\nrepresent the relationships between dictionary features and medical codes\nthrough a sparse interpretable matrix, enhancing the mechanistic and global\nunderstanding of the model's predictions while maintaining competitive\nperformance and scalability without extensive human annotation.\n","authors":["John Wu","David Wu","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2409.10504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10527v2","updated":"2024-09-16T17:44:17Z","published":"2024-02-16T09:29:38Z","title":"Assessing biomedical knowledge robustness in large language models by\n  query-efficient sampling attacks","summary":"  The increasing depth of parametric domain knowledge in large language models\n(LLMs) is fueling their rapid deployment in real-world applications.\nUnderstanding model vulnerabilities in high-stakes and knowledge-intensive\ntasks is essential for quantifying the trustworthiness of model predictions and\nregulating their use. The recent discovery of named entities as adversarial\nexamples (i.e. adversarial entities) in natural language processing tasks\nraises questions about their potential impact on the knowledge robustness of\npre-trained and finetuned LLMs in high-stakes and specialized domains. We\nexamined the use of type-consistent entity substitution as a template for\ncollecting adversarial entities for billion-parameter LLMs with biomedical\nknowledge. To this end, we developed an embedding-space attack based on\npowerscaled distance-weighted sampling to assess the robustness of their\nbiomedical knowledge with a low query budget and controllable coverage. Our\nmethod has favorable query efficiency and scaling over alternative approaches\nbased on random sampling and blackbox gradient-guided search, which we\ndemonstrated for adversarial distractor generation in biomedical question\nanswering. Subsequent failure mode analysis uncovered two regimes of\nadversarial entities on the attack surface with distinct characteristics and we\nshowed that entity substitution attacks can manipulate token-wise Shapley value\nexplanations, which become deceptive in this setting. Our approach complements\nstandard evaluations for high-capacity models and the results highlight the\nbrittleness of domain knowledge in LLMs.\n","authors":["R. Patrick Xian","Alex J. Lee","Satvik Lolla","Vincent Wang","Qiming Cui","Russell Ro","Reza Abbasi-Asl"],"pdf_url":"https://arxiv.org/pdf/2402.10527v2.pdf","comment":"28 pages incl. appendix, updated version"},{"id":"http://arxiv.org/abs/2409.10502v1","updated":"2024-09-16T17:42:15Z","published":"2024-09-16T17:42:15Z","title":"Causal Language Modeling Can Elicit Search and Reasoning Capabilities on\n  Logic Puzzles","summary":"  Causal language modeling using the Transformer architecture has yielded\nremarkable capabilities in Large Language Models (LLMs) over the last few\nyears. However, the extent to which fundamental search and reasoning\ncapabilities emerged within LLMs remains a topic of ongoing debate. In this\nwork, we study if causal language modeling can learn a complex task such as\nsolving Sudoku puzzles. To solve a Sudoku, the model is first required to\nsearch over all empty cells of the puzzle to decide on a cell to fill and then\napply an appropriate strategy to fill the decided cell. Sometimes, the\napplication of a strategy only results in thinning down the possible values in\na cell rather than concluding the exact value of the cell. In such cases,\nmultiple strategies are applied one after the other to fill a single cell. We\nobserve that Transformer models trained on this synthetic task can indeed learn\nto solve Sudokus (our model solves $94.21\\%$ of the puzzles fully correctly)\nwhen trained on a logical sequence of steps taken by a solver. We find that\ntraining Transformers with the logical sequence of steps is necessary and\nwithout such training, they fail to learn Sudoku. We also extend our analysis\nto Zebra puzzles (known as Einstein puzzles) and show that the model solves\n$92.04 \\%$ of the puzzles fully correctly. In addition, we study the internal\nrepresentations of the trained Transformer and find that through linear\nprobing, we can decode information about the set of possible values in any\ngiven cell from them, pointing to the presence of a strong reasoning engine\nimplicit in the Transformer weights.\n","authors":["Kulin Shah","Nishanth Dikkala","Xin Wang","Rina Panigrahy"],"pdf_url":"https://arxiv.org/pdf/2409.10502v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2408.11006v2","updated":"2024-09-16T17:35:10Z","published":"2024-08-20T17:00:04Z","title":"Security Attacks on LLM-based Code Completion Tools","summary":"  The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs.\n","authors":["Wen Cheng","Ke Sun","Xinyu Zhang","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.11006v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10494v1","updated":"2024-09-16T17:27:27Z","published":"2024-09-16T17:27:27Z","title":"Incorporating Classifier-Free Guidance in Diffusion Model-Based\n  Recommendation","summary":"  This paper presents a diffusion-based recommender system that incorporates\nclassifier-free guidance. Most current recommender systems provide\nrecommendations using conventional methods such as collaborative or\ncontent-based filtering. Diffusion is a new approach to generative AI that\nimproves on previous generative AI approaches such as Variational Autoencoders\n(VAEs) and Generative Adversarial Networks (GANs). We incorporate diffusion in\na recommender system that mirrors the sequence users take when browsing and\nrating items. Although a few current recommender systems incorporate diffusion,\nthey do not incorporate classifier-free guidance, a new innovation in diffusion\nmodels as a whole. In this paper, we present a diffusion recommender system\nthat augments the underlying recommender system model for improved performance\nand also incorporates classifier-free guidance. Our findings show improvements\nover state-of-the-art recommender systems for most metrics for several\nrecommendation tasks on a variety of datasets. In particular, our approach\ndemonstrates the potential to provide better recommendations when data is\nsparse.\n","authors":["Noah Buchanan","Susan Gauch","Quan Mai"],"pdf_url":"https://arxiv.org/pdf/2409.10494v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2406.14891v2","updated":"2024-09-16T17:15:52Z","published":"2024-06-21T06:26:38Z","title":"Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop\n  Question Answering","summary":"  Multi-Hop Question Answering (MHQA) tasks present a significant challenge for\nlarge language models (LLMs) due to the intensive knowledge required. Current\nsolutions, like Retrieval-Augmented Generation, typically retrieve potential\ndocuments from an external corpus to read an answer. However, the performance\nof this retrieve-then-read paradigm is constrained by the retriever and the\ninevitable noise in the retrieved documents. To mitigate these challenges, we\nintroduce a novel generate-then-ground (GenGround) framework, synergizing the\nparametric knowledge of LLMs and external documents to solve a multi-hop\nquestion. GenGround empowers LLMs to alternate two phases until the final\nanswer is derived: (1) formulate a simpler, single-hop question and directly\ngenerate the answer; (2) ground the question-answer pair in retrieved\ndocuments, amending any wrong predictions in the answer. We also propose an\ninstructional grounding distillation method to generalize our method into\nsmaller models. Extensive experiments conducted on four datasets illustrate the\nsuperiority of our method.\n","authors":["Zhengliang Shi","Weiwei Sun","Shen Gao","Pengjie Ren","Zhumin Chen","Zhaochun Ren"],"pdf_url":"https://arxiv.org/pdf/2406.14891v2.pdf","comment":"ACL 2024 (main conference)"},{"id":"http://arxiv.org/abs/2401.13512v2","updated":"2024-09-16T16:44:11Z","published":"2024-01-24T15:10:13Z","title":"Can GPT-3.5 Generate and Code Discharge Summaries?","summary":"  Objective: To investigate GPT-3.5 in generating and coding medical documents\nwith ICD-10 codes for data augmentation on low-resources labels.\n  Materials and Methods: Employing GPT-3.5 we generated and coded 9,606\ndischarge summaries based on lists of ICD-10 code descriptions of patients with\ninfrequent (generation) codes within the MIMIC-IV dataset. Combined with the\nbaseline training set, this formed an augmented training set. Neural coding\nmodels were trained on baseline and augmented data and evaluated on a MIMIC-IV\ntest set. We report micro- and macro-F1 scores on the full codeset, generation\ncodes, and their families. Weak Hierarchical Confusion Matrices were employed\nto determine within-family and outside-of-family coding errors in the latter\ncodesets. The coding performance of GPT-3.5 was evaluated both on prompt-guided\nself-generated data and real MIMIC-IV data. Clinical professionals evaluated\nthe clinical acceptability of the generated documents.\n  Results: Augmentation slightly hinders the overall performance of the models\nbut improves performance for the generation candidate codes and their families,\nincluding one unseen in the baseline training data. Augmented models display\nlower out-of-family error rates. GPT-3.5 can identify ICD-10 codes by the\nprompted descriptions, but performs poorly on real data. Evaluators note the\ncorrectness of generated concepts while suffering in variety, supporting\ninformation, and narrative.\n  Discussion and Conclusion: GPT-3.5 alone is unsuitable for ICD-10 coding.\nAugmentation positively affects generation code families but mainly benefits\ncodes with existing examples. Augmentation reduces out-of-family errors.\nDischarge summaries generated by GPT-3.5 state prompted concepts correctly but\nlack variety, and authenticity in narratives. They are unsuitable for clinical\npractice.\n","authors":["Mat√∫≈° Falis","Aryo Pradipta Gema","Hang Dong","Luke Daines","Siddharth Basetti","Michael Holder","Rose S Penfold","Alexandra Birch","Beatrice Alex"],"pdf_url":"https://arxiv.org/pdf/2401.13512v2.pdf","comment":"15 pages; 250 words in abstract; 4,152 words in main body; 4 figures\n  (1 black and white, 3 colour); 4 tables; 34 references; Accepted and\n  published by the Journal of the American Medical Informatics Association"},{"id":"http://arxiv.org/abs/2406.05806v4","updated":"2024-09-16T16:26:49Z","published":"2024-06-09T14:44:59Z","title":"Do Prompts Really Prompt? Exploring the Prompt Understanding Capability\n  of Whisper","summary":"  This research explores how the information of prompts interacts with the\nhigh-performing speech recognition model, Whisper. We compare its performances\nwhen prompted by prompts with correct information and those corrupted with\nincorrect information. Our results unexpectedly show that Whisper may not\nunderstand the textual prompts in a human-expected way. Additionally, we find\nthat performance improvement is not guaranteed even with stronger adherence to\nthe topic information in textual prompts. It is also noted that English prompts\ngenerally outperform Mandarin ones on datasets of both languages, likely due to\ndifferences in training data distributions for these languages despite the\nmismatch with pre-training scenarios. Conversely, we discover that Whisper\nexhibits awareness of misleading information in language tokens by ignoring\nincorrect language tokens and focusing on the correct ones. In sum, We raise\ninsightful questions about Whisper's prompt understanding and reveal its\ncounter-intuitive behaviors. We encourage further studies.\n","authors":["Chih-Kai Yang","Kuan-Po Huang","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2406.05806v4.pdf","comment":"Accepted to 2024 IEEE Spoken Language Technology Workshop (SLT 2024)"},{"id":"http://arxiv.org/abs/2402.11628v2","updated":"2024-09-16T16:22:40Z","published":"2024-02-18T16:03:04Z","title":"Discrete Neural Algorithmic Reasoning","summary":"  Neural algorithmic reasoning aims to capture computations with neural\nnetworks via learning the models to imitate the execution of classic\nalgorithms. While common architectures are expressive enough to contain the\ncorrect model in the weights space, current neural reasoners are struggling to\ngeneralize well on out-of-distribution data. On the other hand, classic\ncomputations are not affected by distributional shifts as they can be described\nas transitions between discrete computational states. In this work, we propose\nto force neural reasoners to maintain the execution trajectory as a combination\nof finite predefined states. To achieve that, we separate discrete and\ncontinuous data flows and describe the interaction between them. Trained with\nsupervision on the algorithm's state transitions, such models are able to\nperfectly align with the original algorithm. To show this, we evaluate our\napproach on multiple algorithmic problems and get perfect test scores both in\nsingle-task and multitask setups. Moreover, the proposed architectural choice\nallows us to prove the correctness of the learned algorithms for any test~data.\n","authors":["Gleb Rodionov","Liudmila Prokhorenkova"],"pdf_url":"https://arxiv.org/pdf/2402.11628v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01256v2","updated":"2024-09-16T16:05:18Z","published":"2024-01-02T15:56:48Z","title":"VideoStudio: Generating Consistent-Content and Multi-Scene Videos","summary":"  The recent innovations and breakthroughs in diffusion models have\nsignificantly expanded the possibilities of generating high-quality videos for\nthe given prompts. Most existing works tackle the single-scene scenario with\nonly one video event occurring in a single background. Extending to generate\nmulti-scene videos nevertheless is not trivial and necessitates to nicely\nmanage the logic in between while preserving the consistent visual appearance\nof key content across video scenes. In this paper, we propose a novel\nframework, namely VideoStudio, for consistent-content and multi-scene video\ngeneration. Technically, VideoStudio leverages Large Language Models (LLM) to\nconvert the input prompt into comprehensive multi-scene script that benefits\nfrom the logical knowledge learnt by LLM. The script for each scene includes a\nprompt describing the event, the foreground/background entities, as well as\ncamera movement. VideoStudio identifies the common entities throughout the\nscript and asks LLM to detail each entity. The resultant entity description is\nthen fed into a text-to-image model to generate a reference image for each\nentity. Finally, VideoStudio outputs a multi-scene video by generating each\nscene video via a diffusion process that takes the reference images, the\ndescriptive prompt of the event and camera movement into account. The diffusion\nmodel incorporates the reference images as the condition and alignment to\nstrengthen the content consistency of multi-scene videos. Extensive experiments\ndemonstrate that VideoStudio outperforms the SOTA video generation models in\nterms of visual quality, content consistency, and user preference. Source code\nis available at \\url{https://github.com/FuchenUSTC/VideoStudio}.\n","authors":["Fuchen Long","Zhaofan Qiu","Ting Yao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2401.01256v2.pdf","comment":"ECCV 2024. Source code is available at\n  https://github.com/FuchenUSTC/VideoStudio"},{"id":"http://arxiv.org/abs/2409.10429v1","updated":"2024-09-16T16:04:16Z","published":"2024-09-16T16:04:16Z","title":"Meta-Whisper: Speech-Based Meta-ICL for ASR on Low-Resource Languages","summary":"  This paper presents Meta-Whisper, a novel approach to improve automatic\nspeech recognition (ASR) for low-resource languages using the Whisper model. By\nleveraging Meta In-Context Learning (Meta-ICL) and a k-Nearest Neighbors (KNN)\nalgorithm for sample selection, Meta-Whisper enhances Whisper's ability to\nrecognize speech in unfamiliar languages without extensive fine-tuning.\nExperiments on the ML-SUPERB dataset show that Meta-Whisper significantly\nreduces the Character Error Rate (CER) for low-resource languages compared to\nthe original Whisper model. This method offers a promising solution for\ndeveloping more adaptable multilingual ASR systems, particularly for languages\nwith limited resources.\n","authors":["Ming-Hao Hsu","Kuan Po Huang","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2409.10429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10403v1","updated":"2024-09-16T15:34:58Z","published":"2024-09-16T15:34:58Z","title":"A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning\n  and BERT Integration","summary":"  This paper proposes a knowledge-enhanced disease diagnosis method based on a\nprompt learning framework. The method retrieves structured knowledge from\nexternal knowledge graphs related to clinical cases, encodes it, and injects it\ninto the prompt templates to enhance the language model's understanding and\nreasoning capabilities for the task.We conducted experiments on three public\ndatasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that the\nproposed method significantly outperforms existing models across multiple\nevaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC\ndataset, 3.1% on the IMCS-V2-NER dataset,and 4.2% on the KUAKE-QTR dataset.\nAdditionally,ablation studies confirmed the critical role of the knowledge\ninjection module,as the removal of this module resulted in a significant drop\nin F1 score. The experimental results demonstrate that the proposed method not\nonly effectively improves the accuracy of disease diagnosis but also enhances\nthe interpretability of the predictions, providing more reliable support and\nevidence for clinical diagnosis.\n","authors":["Zhang Zheng"],"pdf_url":"https://arxiv.org/pdf/2409.10403v1.pdf","comment":"Knowledge Enhancement,Disease Diagnosis,Prompt\n  Learning,BERT,Knowledge Graph"},{"id":"http://arxiv.org/abs/2406.17639v3","updated":"2024-09-16T15:32:11Z","published":"2024-06-25T15:24:02Z","title":"Mitigate the Gap: Investigating Approaches for Improving Cross-Modal\n  Alignment in CLIP","summary":"  Contrastive Language--Image Pre-training (CLIP) has manifested remarkable\nimprovements in zero-shot classification and cross-modal vision-language tasks.\nYet, from a geometrical point of view, the CLIP embedding space has been found\nto have a pronounced modality gap. This gap renders the embedding space overly\nsparse and disconnected, with different modalities being densely distributed in\ndistinct subregions of the hypersphere. In this work, we aim at answering three\nmain questions: 1. Does sharing the parameter space between the multi-modal\nencoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart\nthe uni-modal embeddings via intra-modality separation? 3. How do these gap\nreduction approaches affect the downstream performance? We design AlignCLIP, in\norder to answer these questions and through extensive experiments, we show that\nAlignCLIP achieves noticeable enhancements in the cross-modal alignment of the\nembeddings, and thereby, reduces the modality gap, while improving the\nperformance across several zero-shot and fine-tuning downstream evaluations.\n","authors":["Sedigheh Eslami","Gerard de Melo"],"pdf_url":"https://arxiv.org/pdf/2406.17639v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10372v1","updated":"2024-09-16T15:15:51Z","published":"2024-09-16T15:15:51Z","title":"Instigating Cooperation among LLM Agents Using Adaptive Information\n  Modulation","summary":"  This paper introduces a novel framework combining LLM agents as proxies for\nhuman strategic behavior with reinforcement learning (RL) to engage these\nagents in evolving strategic interactions within team environments. Our\napproach extends traditional agent-based simulations by using strategic LLM\nagents (SLA) and introducing dynamic and adaptive governance through a\npro-social promoting RL agent (PPA) that modulates information access across\nagents in a network, optimizing social welfare and promoting pro-social\nbehavior. Through validation in iterative games, including the prisoner\ndilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.\nThe PPA agent effectively learns to adjust information transparency, resulting\nin enhanced cooperation rates. This framework offers significant insights into\nAI-mediated social dynamics, contributing to the deployment of AI in real-world\nteam settings.\n","authors":["Qiliang Chen"," Alireza"," Ilami","Nunzio Lore","Babak Heydari"],"pdf_url":"https://arxiv.org/pdf/2409.10372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10357v1","updated":"2024-09-16T15:06:12Z","published":"2024-09-16T15:06:12Z","title":"2D or not 2D: How Does the Dimensionality of Gesture Representation\n  Affect 3D Co-Speech Gesture Generation?","summary":"  Co-speech gestures are fundamental for communication. The advent of recent\ndeep learning techniques has facilitated the creation of lifelike, synchronous\nco-speech gestures for Embodied Conversational Agents. \"In-the-wild\" datasets,\naggregating video content from platforms like YouTube via human pose detection\ntechnologies, provide a feasible solution by offering 2D skeletal sequences\naligned with speech. Concurrent developments in lifting models enable the\nconversion of these 2D sequences into 3D gesture databases. However, it is\nimportant to note that the 3D poses estimated from the 2D extracted poses are,\nin essence, approximations of the ground-truth, which remains in the 2D domain.\nThis distinction raises questions about the impact of gesture representation\ndimensionality on the quality of generated motions - a topic that, to our\nknowledge, remains largely unexplored. Our study examines the effect of using\neither 2D or 3D joint coordinates as training data on the performance of\nspeech-to-gesture deep generative models. We employ a lifting model for\nconverting generated 2D pose sequences into 3D and assess how gestures created\ndirectly in 3D stack up against those initially generated in 2D and then\nconverted to 3D. We perform an objective evaluation using widely used metrics\nin the gesture generation field as well as a user study to qualitatively\nevaluate the different approaches.\n","authors":["T√©o Guichoux","Laure Soulier","Nicolas Obin","Catherine Pelachaud"],"pdf_url":"https://arxiv.org/pdf/2409.10357v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2406.15111"},{"id":"http://arxiv.org/abs/2409.05653v2","updated":"2024-09-16T14:58:07Z","published":"2024-09-09T14:19:21Z","title":"WinoPron: Revisiting English Winogender Schemas for Consistency,\n  Coverage, and Grammatical Case","summary":"  While measuring bias and robustness in coreference resolution are important\ngoals, such measurements are only as good as the tools we use to measure them\nwith. Winogender schemas (Rudinger et al., 2018) are an influential dataset\nproposed to evaluate gender bias in coreference resolution, but a closer look\nreveals issues with the data that compromise its use for reliable evaluation,\nincluding treating different pronominal forms as equivalent, violations of\ntemplate constraints, and typographical errors. We identify these issues and\nfix them, contributing a new dataset: WinoPron. Our changes affect performance\nwith state-of-the-art supervised coreference resolution systems as well as all\nmodel sizes of the language model FLAN-T5, with F1 dropping on average 10\npercentage points. We also propose a new method to evaluate pronominal bias in\ncoreference resolution that goes beyond the binary. With this method and our\nnew dataset which is balanced for grammatical case, we empirically demonstrate\nthat bias characteristics vary not just across pronoun sets, but also across\nsurface forms of those sets.\n","authors":["Vagrant Gautam","Julius Steuer","Eileen Bingert","Ray Johns","Anne Lauscher","Dietrich Klakow"],"pdf_url":"https://arxiv.org/pdf/2409.05653v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10341v1","updated":"2024-09-16T14:56:59Z","published":"2024-09-16T14:56:59Z","title":"Detecting Sexism in German Online Newspaper Comments with Open-Source\n  Text Embeddings (Team GDA, GermEval2024 Shared Task 1: GerMS-Detect, Subtasks\n  1 and 2, Closed Track)","summary":"  Sexism in online media comments is a pervasive challenge that often manifests\nsubtly, complicating moderation efforts as interpretations of what constitutes\nsexism can vary among individuals. We study monolingual and multilingual\nopen-source text embeddings to reliably detect sexism and misogyny in\nGerman-language online comments from an Austrian newspaper. We observed\nclassifiers trained on text embeddings to mimic closely the individual\njudgements of human annotators. Our method showed robust performance in the\nGermEval 2024 GerMS-Detect Subtask 1 challenge, achieving an average macro F1\nscore of 0.597 (4th place, as reported on Codabench). It also accurately\npredicted the distribution of human annotations in GerMS-Detect Subtask 2, with\nan average Jensen-Shannon distance of 0.301 (2nd place). The computational\nefficiency of our approach suggests potential for scalable applications across\nvarious languages and linguistic contexts.\n","authors":["Florian Bremm","Patrick Gustav Blaneck","Tobias Bornheim","Niklas Grieger","Stephan Bialonski"],"pdf_url":"https://arxiv.org/pdf/2409.10341v1.pdf","comment":"6 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2406.05870v2","updated":"2024-09-16T14:52:46Z","published":"2024-06-09T17:55:55Z","title":"Machine Against the RAG: Jamming Retrieval-Augmented Generation with\n  Blocker Documents","summary":"  Retrieval-augmented generation (RAG) systems respond to queries by retrieving\nrelevant documents from a knowledge database, then generating an answer by\napplying an LLM to the retrieved documents. We demonstrate that RAG systems\nthat operate on databases with untrusted content are vulnerable to a new class\nof denial-of-service attacks we call jamming. An adversary can add a single\n``blocker'' document to the database that will be retrieved in response to a\nspecific query and result in the RAG system not answering this query -\nostensibly because it lacks the information or because the answer is unsafe.\n  We describe and measure the efficacy of several methods for generating\nblocker documents, including a new method based on black-box optimization. This\nmethod (1) does not rely on instruction injection, (2) does not require the\nadversary to know the embedding or LLM used by the target RAG system, and (3)\ndoes not use an auxiliary LLM to generate blocker documents.\n  We evaluate jamming attacks on several LLMs and embeddings and demonstrate\nthat the existing safety metrics for LLMs do not capture their vulnerability to\njamming. We then discuss defenses against blocker documents.\n","authors":["Avital Shafran","Roei Schuster","Vitaly Shmatikov"],"pdf_url":"https://arxiv.org/pdf/2406.05870v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10338v1","updated":"2024-09-16T14:50:29Z","published":"2024-09-16T14:50:29Z","title":"The 20 questions game to distinguish large language models","summary":"  In a parallel with the 20 questions game, we present a method to determine\nwhether two large language models (LLMs), placed in a black-box context, are\nthe same or not. The goal is to use a small set of (benign) binary questions,\ntypically under 20. We formalize the problem and first establish a baseline\nusing a random selection of questions from known benchmark datasets, achieving\nan accuracy of nearly 100% within 20 questions. After showing optimal bounds\nfor this problem, we introduce two effective questioning heuristics able to\ndiscriminate 22 LLMs by using half as many questions for the same task. These\nmethods offer significant advantages in terms of stealth and are thus of\ninterest to auditors or copyright owners facing suspicions of model leaks.\n","authors":["Gurvan Richardeau","Erwan Le Merrer","Camilla Penzo","Gilles Tredan"],"pdf_url":"https://arxiv.org/pdf/2409.10338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10294v1","updated":"2024-09-16T14:01:03Z","published":"2024-09-16T14:01:03Z","title":"MGSA: Multi-granularity Graph Structure Attention for Knowledge\n  Graph-to-Text Generation","summary":"  The Knowledge Graph-to-Text Generation task aims to convert structured\nknowledge graphs into coherent and human-readable natural language text. Recent\nefforts in this field have focused on enhancing pre-trained language models\n(PLMs) by incorporating graph structure information to capture the intricate\nstructure details of knowledge graphs. However, most of these approaches tend\nto capture only single-granularity structure information, concentrating either\non the relationships between entities within the original graph or on the\nrelationships between words within the same entity or across different\nentities. This narrow focus results in a significant limitation: models that\nconcentrate solely on entity-level structure fail to capture the nuanced\nsemantic relationships between words, while those that focus only on word-level\nstructure overlook the broader relationships between original entire entities.\nTo overcome these limitations, this paper introduces the Multi-granularity\nGraph Structure Attention (MGSA), which is based on PLMs. The encoder of the\nmodel architecture features an entity-level structure encoding module, a\nword-level structure encoding module, and an aggregation module that\nsynthesizes information from both structure. This multi-granularity structure\nencoding approach allows the model to simultaneously capture both entity-level\nand word-level structure information, providing a more comprehensive\nunderstanding of the knowledge graph's structure information, thereby\nsignificantly improving the quality of the generated text. We conducted\nextensive evaluations of the MGSA model using two widely recognized KG-to-Text\nGeneration benchmark datasets, WebNLG and EventNarrative, where it consistently\noutperformed models that rely solely on single-granularity structure\ninformation, demonstrating the effectiveness of our approach.\n","authors":["Shanshan Wang","Chun Zhang","Ning Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.10294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10289v1","updated":"2024-09-16T13:56:17Z","published":"2024-09-16T13:56:17Z","title":"ReflectDiffu: Reflect between Emotion-intent Contagion and Mimicry for\n  Empathetic Response Generation via a RL-Diffusion Framework","summary":"  Empathetic response generation necessitates the integration of emotional and\nintentional dynamics to foster meaningful interactions. Existing research\neither neglects the intricate interplay between emotion and intent, leading to\nsuboptimal controllability of empathy, or resorts to large language models\n(LLMs), which incur significant computational overhead. In this paper, we\nintroduce ReflectDiffu, a lightweight and comprehensive framework for\nempathetic response generation. This framework incorporates emotion contagion\nto augment emotional expressiveness and employs an emotion-reasoning mask to\npinpoint critical emotional elements. Additionally, it integrates intent\nmimicry within reinforcement learning for refinement during diffusion. By\nharnessing an intent twice reflect the mechanism of\nExploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional\ndecision-making into precise intent actions, thereby addressing empathetic\nresponse misalignments stemming from emotional misrecognition. Through\nreflection, the framework maps emotional states to intents, markedly enhancing\nboth response empathy and flexibility. Comprehensive experiments reveal that\nReflectDiffu outperforms existing models regarding relevance, controllability,\nand informativeness, achieving state-of-the-art results in both automatic and\nhuman evaluations.\n","authors":["Jiahao Yuan","Zixiang Di","Zhiqing Cui","Guisong Yang","Usman Naseem"],"pdf_url":"https://arxiv.org/pdf/2409.10289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11477v2","updated":"2024-09-16T13:55:24Z","published":"2024-06-17T12:42:34Z","title":"How Can We Effectively Expand the Vocabulary of LLMs with 0.01GB of\n  Target Language Text?","summary":"  Large language models (LLMs) have shown remarkable capabilities in many\nlanguages beyond English. Yet, LLMs require more inference steps when\ngenerating non-English text due to their reliance on English-centric tokenizers\nand vocabulary, resulting in higher usage costs to non-English speakers.\nVocabulary expansion with target language tokens is a widely used cross-lingual\nvocabulary adaptation approach to remedy this issue. Despite its effectiveness\nin inference speedup, previous work on vocabulary expansion has focused on\nhigh-resource settings assuming access to a substantial amount of target\nlanguage data to effectively initialize the embeddings of the new tokens and\nadapt the LLM to the target language. However, vocabulary expansion in\nlow-resource settings has yet to be explored. In this paper, we investigate\nvocabulary expansion in low-resource settings by considering embedding\ninitialization methods and continual pre-training strategies. Through extensive\nexperiments across typologically diverse languages, tasks and models, we\nestablish a set of strategies to perform vocabulary expansion for faster\ninference, maintaining competitive downstream performance to baselines with\nonly 30K sentences ($\\sim$0.01GB text data) from the target language.\n","authors":["Atsuki Yamaguchi","Aline Villavicencio","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2406.11477v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07841v2","updated":"2024-09-16T13:18:23Z","published":"2024-02-12T17:52:05Z","title":"Do Membership Inference Attacks Work on Large Language Models?","summary":"  Membership inference attacks (MIAs) attempt to predict whether a particular\ndatapoint is a member of a target model's training data. Despite extensive\nresearch on traditional machine learning models, there has been limited work\nstudying MIA on the pre-training data of large language models (LLMs). We\nperform a large-scale evaluation of MIAs over a suite of language models (LMs)\ntrained on the Pile, ranging from 160M to 12B parameters. We find that MIAs\nbarely outperform random guessing for most settings across varying LLM sizes\nand domains. Our further analyses reveal that this poor performance can be\nattributed to (1) the combination of a large dataset and few training\niterations, and (2) an inherently fuzzy boundary between members and\nnon-members. We identify specific settings where LLMs have been shown to be\nvulnerable to membership inference and show that the apparent success in such\nsettings can be attributed to a distribution shift, such as when members and\nnon-members are drawn from the seemingly identical domain but with different\ntemporal ranges. We release our code and data as a unified benchmark package\nthat includes all existing MIAs, supporting future work.\n","authors":["Michael Duan","Anshuman Suri","Niloofar Mireshghallah","Sewon Min","Weijia Shi","Luke Zettlemoyer","Yulia Tsvetkov","Yejin Choi","David Evans","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2402.07841v2.pdf","comment":"Accepted at Conference on Language Modeling (COLM), 2024"},{"id":"http://arxiv.org/abs/2409.10245v1","updated":"2024-09-16T12:55:14Z","published":"2024-09-16T12:55:14Z","title":"From Text to Emoji: How PEFT-Driven Personality Manipulation Unleashes\n  the Emoji Potential in LLMs","summary":"  As the demand for human-like interactions with LLMs continues to grow, so\ndoes the interest in manipulating their personality traits, which has emerged\nas a key area of research. Methods like prompt-based In-Context Knowledge\nEditing (IKE) and gradient-based Model Editor Networks (MEND) have been\nexplored but show irregularity and variability. IKE depends on the prompt,\nleading to variability and sensitivity, while MEND yields inconsistent and\ngibberish outputs. To address this, we employed Opinion QA Based\nParameter-Efficient Fine-Tuning (PEFT), specifically Quantized Low-Rank\nAdaptation (QLORA), to manipulate the Big Five personality traits: Openness,\nConscientiousness, Extraversion, Agreeableness, and Neuroticism. After PEFT,\nmodels such as Mistral-7B-Instruct and Llama-2-7B-chat began generating emojis,\ndespite their absence in the PEFT data. For instance, Llama-2-7B-chat generated\nemojis in 99.5% of extraversion-related test instances, while\nMistral-8B-Instruct did so in 92.5% of openness-related test instances.\nExplainability analysis indicated that the LLMs used emojis intentionally to\nexpress these traits. This paper provides a number of novel contributions.\nFirst, introducing an Opinion QA dataset for PEFT-driven personality\nmanipulation; second, developing metric models to benchmark LLM personality\ntraits; third, demonstrating PEFT's superiority over IKE in personality\nmanipulation; and finally, analyzing and validating emoji usage through\nexplainability methods such as mechanistic interpretability and in-context\nlearning explainability methods.\n","authors":["Navya Jain","Zekun Wu","Cristian Munoz","Airlie Hilliard","Adriano Koshiyama","Emre Kazim","Philip Treleaven"],"pdf_url":"https://arxiv.org/pdf/2409.10245v1.pdf","comment":"Submitted to NeurIPS 2024 Workshop on Behavioral Machine Learning"},{"id":"http://arxiv.org/abs/2408.15512v2","updated":"2024-09-16T12:02:27Z","published":"2024-08-28T03:48:05Z","title":"Towards Fully Autonomous Research Powered by LLMs: Case Study on\n  Simulations","summary":"  The advent of Large Language Models (LLMs) has created new opportunities for\nthe automation of scientific research, spanning both experimental processes and\ncomputational simulations. This study explores the feasibility of constructing\nan autonomous simulation agent (ASA) powered by LLM, through sophisticated API\nintegration, to automate the entire research process, from experimental design,\nremote upload and simulation execution, data analysis, to report compilation.\nUsing a simulation problem of polymer chain conformations as a case study, we\nassessed the performance of ASAs powered by different LLMs including\nGPT-4-Turbo. Our findings revealed that ASA-GPT-4o achieved near-flawless\nexecution on designated research missions, underscoring the potential of LLMs\nto manage complete scientific investigations autonomously. The outlined\nautomation can be iteratively performed up to twenty cycles without human\nintervention, illustrating the potential of LLMs for large-scale autonomous\nresearch endeavors. Additionally, we discussed the intrinsic traits of ASAs in\nmanaging extensive tasks, focusing on self-validation mechanisms and the\nbalance between local attention and global oversight.\n","authors":["Zhihan Liu","Yubo Chai","Jianfeng Li"],"pdf_url":"https://arxiv.org/pdf/2408.15512v2.pdf","comment":"For additional code and data, please visit our GitHub repository:\n  https://github.com/zokaraa/autonomous_simulation_agent"},{"id":"http://arxiv.org/abs/2409.10197v1","updated":"2024-09-16T11:43:19Z","published":"2024-09-16T11:43:19Z","title":"Fit and Prune: Fast and Training-free Visual Token Pruning for\n  Multi-modal Large Language Models","summary":"  Recent progress in Multimodal Large Language Models(MLLMs) often use large\nimage tokens to compensate the visual shortcoming of MLLMs, which not only\nexhibits obvious redundancy but also greatly exacerbates the already high\ncomputation. Token pruning is an effective solution for speeding up MLLMs, but\nwhen and how to drop tokens still remains a challenge. In this paper, we\npropose a novel and training-free approach for the effective visual token\npruning of MLLMs, termed FitPrune, which can quickly produce a complete pruning\nrecipe for MLLMs according to a pre-defined budget. Specifically, FitPrune\nconsiders token pruning as a statistical problem of MLLM and its objective is\nto find out an optimal pruning scheme that can minimize the divergence of the\nattention distributions before and after pruning. In practice, FitPrune can be\nquickly accomplished based on the attention statistics from a small batch of\ninference data, avoiding the expensive trials of MLLMs. According to the\npruning recipe, an MLLM can directly remove the redundant visual tokens of\ndifferent examples during inference. To validate FitPrune, we apply it to a set\nof recent MLLMs, including LLaVA-1.5, LLaVA-HR and LLaVA-NEXT, and conduct\nextensive experiments on a set of benchmarks. The experimental results show\nthat our FitPrune can not only reduce the computational complexity to a large\nextent, while retaining high performance, e.g., -54.9% FLOPs for LLaVA-NEXT\nwith only 0.5% accuracy drop. Notably, the pruning recipe can be obtained in\nabout 5 minutes. Our code is available at https://github.com/ywh187/FitPrune.\n","authors":["Weihao Ye","Qiong Wu","Wenhao Lin","Yiyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.10197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10191v1","updated":"2024-09-16T11:34:40Z","published":"2024-09-16T11:34:40Z","title":"LLMs for clinical risk prediction","summary":"  This study compares the efficacy of GPT-4 and clinalytix Medical AI in\npredicting the clinical risk of delirium development. Findings indicate that\nGPT-4 exhibited significant deficiencies in identifying positive cases and\nstruggled to provide reliable probability estimates for delirium risk, while\nclinalytix Medical AI demonstrated superior accuracy. A thorough analysis of\nthe large language model's (LLM) outputs elucidated potential causes for these\ndiscrepancies, consistent with limitations reported in extant literature. These\nresults underscore the challenges LLMs face in accurately diagnosing conditions\nand interpreting complex clinical data. While LLMs hold substantial potential\nin healthcare, they are currently unsuitable for independent clinical\ndecision-making. Instead, they should be employed in assistive roles,\ncomplementing clinical expertise. Continued human oversight remains essential\nto ensure optimal outcomes for both patients and healthcare providers.\n","authors":["Mohamed Rezk","Patricia Cabanillas Silva","Fried-Michael Dahlweid"],"pdf_url":"https://arxiv.org/pdf/2409.10191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10164v1","updated":"2024-09-16T10:54:04Z","published":"2024-09-16T10:54:04Z","title":"Quantile Regression for Distributional Reward Models in RLHF","summary":"  Reinforcement learning from human feedback (RLHF) has become a key method for\naligning large language models (LLMs) with human preferences through the use of\nreward models. However, traditional reward models typically generate point\nestimates, which oversimplify the diversity and complexity of human values and\npreferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel\napproach to reward modeling that learns a distribution over rewards instead of\na single scalar value. Our method uses quantile regression to estimate a full,\npotentially multimodal distribution over preferences, providing a more powerful\nand nuanced representation of preferences. This distributional approach can\nbetter capture the diversity of human values, addresses label noise, and\naccommodates conflicting preferences by modeling them as distinct modes in the\ndistribution. Our experimental results show that QRM outperforms comparable\ntraditional point-estimate models on RewardBench. Furthermore, we demonstrate\nthat the additional information provided by the distributional estimates can be\nutilized in downstream applications, such as risk-aware reinforcement learning,\nresulting in LLM policies that generate fewer extremely negative responses. Our\ncode and model are released at https://github.com/Nicolinho/QRM.\n","authors":["Nicolai Dorka"],"pdf_url":"https://arxiv.org/pdf/2409.10164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08523v2","updated":"2024-09-16T10:50:30Z","published":"2024-09-13T04:06:00Z","title":"Eir: Thai Medical Large Language Models","summary":"  We present Eir-8B, a large language model with 8 billion parameters,\nspecifically designed to enhance the accuracy of handling medical tasks in the\nThai language. This model focuses on providing clear and easy-to-understand\nanswers for both healthcare professionals and patients, thereby improving the\nefficiency of diagnosis and treatment processes. Human evaluation was conducted\nto ensure that the model adheres to care standards and provides unbiased\nanswers.\n  To prioritize data security, the model is deployed within the hospital's\ninternal network, ensuring both high security and faster processing speeds. The\ninternal API connection is secured with encryption and strict authentication\nmeasures to prevent data leaks and unauthorized access.\n  We evaluated several open-source large language models with 8 billion\nparameters on four medical benchmarks: MedQA, MedMCQA, PubMedQA, and the\nmedical subset of MMLU. The best-performing baselines were used to develop\nEir-8B. Our evaluation employed multiple questioning strategies, including\nzero-shot, few-shot, chain-of-thought reasoning, and ensemble/self-consistency\nvoting methods. Our model outperformed commercially available Thai-language\nlarge language models by more than 10%. In addition, we developed enhanced\nmodel testing tailored for clinical use in Thai across 18 clinical tasks, where\nour model exceeded GPT-4o performance by more than 11%.\n","authors":["Yutthakorn Thiprak","Rungtam Ngodngamthaweesuk","Songtam Ngodngamtaweesuk"],"pdf_url":"https://arxiv.org/pdf/2409.08523v2.pdf","comment":"typos corrected, and references added"},{"id":"http://arxiv.org/abs/2409.10146v1","updated":"2024-09-16T10:15:30Z","published":"2024-09-16T10:15:30Z","title":"LLMs4OL 2024 Overview: The 1st Large Language Models for Ontology\n  Learning Challenge","summary":"  This paper outlines the LLMs4OL 2024, the first edition of the Large Language\nModels for Ontology Learning Challenge. LLMs4OL is a community development\ninitiative collocated with the 23rd International Semantic Web Conference\n(ISWC) to explore the potential of Large Language Models (LLMs) in Ontology\nLearning (OL), a vital process for enhancing the web with structured knowledge\nto improve interoperability. By leveraging LLMs, the challenge aims to advance\nunderstanding and innovation in OL, aligning with the goals of the Semantic Web\nto create a more intelligent and user-friendly web. In this paper, we give an\noverview of the 2024 edition of the LLMs4OL challenge and summarize the\ncontributions.\n","authors":["Hamed Babaei Giglou","Jennifer D'Souza","S√∂ren Auer"],"pdf_url":"https://arxiv.org/pdf/2409.10146v1.pdf","comment":"15 pages, 1 figure, Will appear in \"The 1st LLMs4OL Challenge @ ISWC\n  2024\" proceedings"},{"id":"http://arxiv.org/abs/2409.04964v2","updated":"2024-09-16T10:00:52Z","published":"2024-09-08T04:03:55Z","title":"Evaluation of Google Translate for Mandarin Chinese translation using\n  sentiment and semantic analysis","summary":"  Machine translation using large language models (LLMs) is having a\nsignificant global impact, making communication easier. Mandarin Chinese is the\nofficial language used for communication by the government and media in China.\nIn this study, we provide an automated assessment of translation quality of\nGoogle Translate with human experts using sentiment and semantic analysis. In\norder to demonstrate our framework, we select the classic early\ntwentieth-century novel 'The True Story of Ah Q' with selected Mandarin Chinese\nto English translations. We use Google Translate to translate the given text\ninto English and then conduct a chapter-wise sentiment analysis and semantic\nanalysis to compare the extracted sentiments across the different translations.\nOur results indicate that the precision of Google Translate differs both in\nterms of semantic and sentiment analysis when compared to human expert\ntranslations. We find that Google Translate is unable to translate some of the\nspecific words or phrases in Chinese, such as Chinese traditional allusions.\nThe mistranslations may be due to lack of contextual significance and\nhistorical knowledge of China.\n","authors":["Xuechun Wang","Rodney Beard","Rohitash Chandra"],"pdf_url":"https://arxiv.org/pdf/2409.04964v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08354v4","updated":"2024-09-16T09:51:21Z","published":"2024-04-12T09:48:58Z","title":"PMB5: Gaining More Insight into Neural Semantic Parsing with Challenging\n  Benchmarks","summary":"  The Parallel Meaning Bank (PMB) serves as a corpus for semantic processing\nwith a focus on semantic parsing and text generation. Currently, we witness an\nexcellent performance of neural parsers and generators on the PMB. This might\nsuggest that such semantic processing tasks have by and large been solved. We\nargue that this is not the case and that performance scores from the past on\nthe PMB are inflated by non-optimal data splits and test sets that are too\neasy. In response, we introduce several changes. First, instead of the prior\nrandom split, we propose a more systematic splitting approach to improve the\nreliability of the standard test data. Second, except for the standard test\nset, we also propose two challenge sets: one with longer texts including\ndiscourse structure, and one that addresses compositional generalization. We\nevaluate five neural models for semantic parsing and meaning-to-text\ngeneration. Our results show that model performance declines (in some cases\ndramatically) on the challenge sets, revealing the limitations of neural models\nwhen confronting such challenges.\n","authors":["Xiao Zhang","Chunliu Wang","Rik van Noord","Johan Bos"],"pdf_url":"https://arxiv.org/pdf/2404.08354v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10132v1","updated":"2024-09-16T09:48:56Z","published":"2024-09-16T09:48:56Z","title":"StruEdit: Structured Outputs Enable the Fast and Accurate Knowledge\n  Editing for Large Language Models","summary":"  As the modern tool of choice for question answering, large language models\n(LLMs) are expected to deliver answers with up-to-date knowledge. To achieve\nsuch ideal question-answering systems, locating and then editing outdated\nknowledge in the natural language outputs is a general target of popular\nknowledge editing methods. However, this target is challenging, as both\nidentifying which tokens to edit in the reasoning steps and ensuring the\ncoherence of the revised reasoning chain are difficult tasks. We argue that\nthese challenges stem from the unstructured nature of natural language outputs.\nTo address the above challenges, we propose $\\textbf{Stru}$ctural\n$\\textbf{Edit}$ing ($\\textbf{StruEdit}$), an improved baseline for knowledge\nediting. We first prompt LLMs to produce structured outputs consisting of\nreasoning triplets. Then, StruEdit removes any potentially outdated knowledge\nand efficiently refills the structured outputs with up-to-date information in a\nsingle step. Experimental results show that StruEdit consistently delivers the\nhighest accuracy with lowest latency compared with other knowledge editing\nmethods.\n","authors":["Baolong Bi","Shenghua Liu","Yiwei Wang","Lingrui Mei","Hongcheng Gao","Junfeng Fang","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2409.10132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10103v1","updated":"2024-09-16T09:07:08Z","published":"2024-09-16T09:07:08Z","title":"Self-Supervised Syllable Discovery Based on Speaker-Disentangled HuBERT","summary":"  Self-supervised speech representation learning has become essential for\nextracting meaningful features from untranscribed audio. Recent advances\nhighlight the potential of deriving discrete symbols from the features\ncorrelated with linguistic units, which enables text-less training across\ndiverse tasks. In particular, sentence-level Self-Distillation of the\npretrained HuBERT (SD-HuBERT) induces syllabic structures within latent speech\nframe representations extracted from an intermediate Transformer layer. In\nSD-HuBERT, sentence-level representation is accumulated from speech frame\nfeatures through self-attention layers using a special CLS token. However, we\nobserve that the information aggregated in the CLS token correlates more with\nspeaker identity than with linguistic content. To address this, we propose a\nspeech-only self-supervised fine-tuning approach that separates syllabic units\nfrom speaker information. Our method introduces speaker perturbation as data\naugmentation and adopts a frame-level training objective to prevent the CLS\ntoken from aggregating paralinguistic information. Experimental results show\nthat our approach surpasses the current state-of-the-art method in most\nsyllable segmentation and syllabic unit quality metrics on Librispeech,\nunderscoring its effectiveness in promoting syllabic organization within\nspeech-only models.\n","authors":["Ryota Komatsu","Takahiro Shinozaki"],"pdf_url":"https://arxiv.org/pdf/2409.10103v1.pdf","comment":"Accepted by IEEE SLT 2024"},{"id":"http://arxiv.org/abs/2409.10102v1","updated":"2024-09-16T09:06:44Z","published":"2024-09-16T09:06:44Z","title":"Trustworthiness in Retrieval-Augmented Generation Systems: A Survey","summary":"  Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal\nparadigm in the development of Large Language Models (LLMs). While much of the\ncurrent research in this field focuses on performance optimization,\nparticularly in terms of accuracy and efficiency, the trustworthiness of RAG\nsystems remains an area still under exploration. From a positive perspective,\nRAG systems are promising to enhance LLMs by providing them with useful and\nup-to-date knowledge from vast external databases, thereby mitigating the\nlong-standing problem of hallucination. While from a negative perspective, RAG\nsystems are at the risk of generating undesirable contents if the retrieved\ninformation is either inappropriate or poorly utilized. To address these\nconcerns, we propose a unified framework that assesses the trustworthiness of\nRAG systems across six key dimensions: factuality, robustness, fairness,\ntransparency, accountability, and privacy. Within this framework, we thoroughly\nreview the existing literature on each dimension. Additionally, we create the\nevaluation benchmark regarding the six dimensions and conduct comprehensive\nevaluations for a variety of proprietary and open-source models. Finally, we\nidentify the potential challenges for future research based on our\ninvestigation results. Through this work, we aim to lay a structured foundation\nfor future investigations and provide practical insights for enhancing the\ntrustworthiness of RAG systems in real-world applications.\n","authors":["Yujia Zhou","Yan Liu","Xiaoxi Li","Jiajie Jin","Hongjin Qian","Zheng Liu","Chaozhuo Li","Zhicheng Dou","Tsung-Yi Ho","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2409.10102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09576v2","updated":"2024-09-16T08:35:51Z","published":"2024-04-15T08:37:26Z","title":"Large language models and linguistic intentionality","summary":"  Do large language models like Chat-GPT or LLaMa meaningfully use the words\nthey produce? Or are they merely clever prediction machines, simulating\nlanguage use by producing statistically plausible text? There have already been\nsome initial attempts to answer this question by showing that these models meet\nthe criteria for entering meaningful states according to metasemantic theories\nof mental content. In this paper, I will argue for a different approach - that\nwe should instead consider whether language models meet the criteria given by\nour best metasemantic theories of linguistic content. In that vein, I will\nillustrate how this can be done by applying two such theories to the case of\nlanguage models: Gareth Evans' (1982) account of naming practices and Ruth\nMillikan's (1984, 2004, 2005) teleosemantics. In doing so, I will argue that it\nis a mistake to think that the failure of LLMs to meet plausible conditions for\nmental intentionality thereby renders their outputs meaningless, and that a\ndistinguishing feature of linguistic intentionality - dependency on a\npre-existing linguistic system - allows for the plausible result LLM outputs\nare meaningful.\n","authors":["Jumbly Grindrod"],"pdf_url":"https://arxiv.org/pdf/2404.09576v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10077v1","updated":"2024-09-16T08:28:05Z","published":"2024-09-16T08:28:05Z","title":"LLM-DER:A Named Entity Recognition Method Based on Large Language Models\n  for Chinese Coal Chemical Domain","summary":"  Domain-specific Named Entity Recognition (NER), whose goal is to recognize\ndomain-specific entities and their categories, provides an important support\nfor constructing domain knowledge graphs. Currently, deep learning-based\nmethods are widely used and effective in NER tasks, but due to the reliance on\nlarge-scale labeled data. As a result, the scarcity of labeled data in a\nspecific domain will limit its application.Therefore, many researches started\nto introduce few-shot methods and achieved some results. However, the entity\nstructures in specific domains are often complex, and the current few-shot\nmethods are difficult to adapt to NER tasks with complex features.Taking the\nChinese coal chemical industry domain as an example,there exists a complex\nstructure of multiple entities sharing a single entity, as well as multiple\nrelationships for the same pair of entities, which affects the NER task under\nthe sample less condition.In this paper, we propose a Large Language Models\n(LLMs)-based entity recognition framework LLM-DER for the domain-specific\nentity recognition problem in Chinese, which enriches the entity information by\ngenerating a list of relationships containing entity types through LLMs, and\ndesigning a plausibility and consistency evaluation method to remove\nmisrecognized entities, which can effectively solve the complex structural\nentity recognition problem in a specific domain.The experimental results of\nthis paper on the Resume dataset and the self-constructed coal chemical dataset\nCoal show that LLM-DER performs outstandingly in domain-specific entity\nrecognition, not only outperforming the existing GPT-3.5-turbo baseline, but\nalso exceeding the fully-supervised baseline, verifying its effectiveness in\nentity recognition.\n","authors":["Le Xiao","Yunfei Xu","Jing Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.10077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13292v2","updated":"2024-09-16T08:23:30Z","published":"2024-07-18T08:46:47Z","title":"Low-Resourced Speech Recognition for Iu Mien Language via\n  Weakly-Supervised Phoneme-based Multilingual Pre-training","summary":"  The mainstream automatic speech recognition (ASR) technology usually requires\nhundreds to thousands of hours of annotated speech data. Three approaches to\nlow-resourced ASR are phoneme or subword based supervised pre-training, and\nself-supervised pre-training over multilingual data. The Iu Mien language is\nthe main ethnic language of the Yao ethnic group in China and is low-resourced\nin the sense that the annotated speech is very limited. With less than 10 hours\nof transcribed Iu Mien language, this paper investigates and compares the three\napproaches for Iu Mien speech recognition. Our experiments are based on the\nrecently released, three backbone models pretrained over the 10 languages from\nthe CommonVoice dataset (CV-Lang10), which correspond to the three approaches\nfor low-resourced ASR. It is found that phoneme supervision can achieve better\nresults compared to subword supervision and self-supervision, thereby providing\nhigher data-efficiency. Particularly, the Whistle models, i.e., obtained by the\nweakly-supervised phoneme-based multilingual pre-training, obtain the most\ncompetitive results.\n","authors":["Lukuan Dong","Donghong Qin","Fengbo Bai","Fanhua Song","Yan Liu","Chen Xu","Zhijian Ou"],"pdf_url":"https://arxiv.org/pdf/2407.13292v2.pdf","comment":"Accepted into ISCSLP 2024"},{"id":"http://arxiv.org/abs/2409.10070v1","updated":"2024-09-16T08:15:35Z","published":"2024-09-16T08:15:35Z","title":"Increasing faithfulness in human-human dialog summarization with Spoken\n  Language Understanding tasks","summary":"  Dialogue summarization aims to provide a concise and coherent summary of\nconversations between multiple speakers. While recent advancements in language\nmodels have enhanced this process, summarizing dialogues accurately and\nfaithfully remains challenging due to the need to understand speaker\ninteractions and capture relevant information. Indeed, abstractive models used\nfor dialog summarization may generate summaries that contain inconsistencies.\nWe suggest using the semantic information proposed for performing Spoken\nLanguage Understanding (SLU) in human-machine dialogue systems for\ngoal-oriented human-human dialogues to obtain a more semantically faithful\nsummary regarding the task. This study introduces three key contributions:\nFirst, we propose an exploration of how incorporating task-related information\ncan enhance the summarization process, leading to more semantically accurate\nsummaries. Then, we introduce a new evaluation criterion based on task\nsemantics. Finally, we propose a new dataset version with increased annotated\ndata standardized for research on task-oriented dialogue summarization. The\nstudy evaluates these methods using the DECODA corpus, a collection of French\nspoken dialogues from a call center. Results show that integrating models with\ntask-related information improves summary accuracy, even with varying word\nerror rates.\n","authors":["Eunice Akani","Benoit Favre","Frederic Bechet","Romain Gemignani"],"pdf_url":"https://arxiv.org/pdf/2409.10070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10064v1","updated":"2024-09-16T07:58:56Z","published":"2024-09-16T07:58:56Z","title":"MindGuard: Towards Accessible and Sitgma-free Mental Health First Aid\n  via Edge LLM","summary":"  Mental health disorders are among the most prevalent diseases worldwide,\naffecting nearly one in four people. Despite their widespread impact, the\nintervention rate remains below 25%, largely due to the significant cooperation\nrequired from patients for both diagnosis and intervention. The core issue\nbehind this low treatment rate is stigma, which discourages over half of those\naffected from seeking help. This paper presents MindGuard, an accessible,\nstigma-free, and professional mobile mental healthcare system designed to\nprovide mental health first aid. The heart of MindGuard is an innovative edge\nLLM, equipped with professional mental health knowledge, that seamlessly\nintegrates objective mobile sensor data with subjective Ecological Momentary\nAssessment records to deliver personalized screening and intervention\nconversations. We conduct a broad evaluation of MindGuard using open datasets\nspanning four years and real-world deployment across various mobile devices\ninvolving 20 subjects for two weeks. Remarkably, MindGuard achieves results\ncomparable to GPT-4 and outperforms its counterpart with more than 10 times the\nmodel size. We believe that MindGuard paves the way for mobile LLM\napplications, potentially revolutionizing mental healthcare practices by\nsubstituting self-reporting and intervention conversations with passive,\nintegrated monitoring within daily life, thus ensuring accessible and\nstigma-free mental health support.\n","authors":["Sijie Ji","Xinzhe Zheng","Jiawei Sun","Renqi Chen","Wei Gao","Mani Srivastava"],"pdf_url":"https://arxiv.org/pdf/2409.10064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10053v1","updated":"2024-09-16T07:29:40Z","published":"2024-09-16T07:29:40Z","title":"Householder Pseudo-Rotation: A Novel Approach to Activation Editing in\n  LLMs with Direction-Magnitude Perspective","summary":"  Activation Editing, which involves directly editting the internal\nrepresentations of large language models (LLMs) to alter their behaviors and\nachieve desired properties, has emerged as a promising area of research.\nExisting works primarily treat LLMs' activations as points in space and modify\nthem by adding steering vectors. However, this approach is limited in its\nability to achieve greater performance improvement while maintaining the\nnecessary consistency of activation magnitudes. To overcome these issues, we\npropose a novel editing method that views activations in terms of their\ndirections and magnitudes. Our method, named Householder Pseudo-Rotation (HPR),\nmimics the rotation transformation, thus preserving activation norms and\nresulting in an improved performance on various safety benchmarks.\n","authors":["Van-Cuong Pham","Thien Huu Nguyen"],"pdf_url":"https://arxiv.org/pdf/2409.10053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10058v2","updated":"2024-09-16T07:20:13Z","published":"2024-07-14T03:05:53Z","title":"Learning to Refuse: Towards Mitigating Privacy Risks in LLMs","summary":"  Large language models (LLMs) exhibit remarkable capabilities in understanding\nand generating natural language. However, these models can inadvertently\nmemorize private information, posing significant privacy risks. This study\naddresses the challenge of enabling LLMs to protect specific individuals'\nprivate data without the need for complete retraining. We propose \\return, a\nReal-world pErsonal daTa UnleaRNing dataset, comprising 2,492 individuals from\nWikipedia with associated QA pairs, to evaluate machine unlearning (MU) methods\nfor protecting personal data in a realistic scenario. Additionally, we\nintroduce the Name-Aware Unlearning Framework (NAUF) for Privacy Protection,\nwhich enables the model to learn which individuals' information should be\nprotected without affecting its ability to answer questions related to other\nunrelated individuals. Our extensive experiments demonstrate that NAUF achieves\na state-of-the-art average unlearning score, surpassing the best baseline\nmethod by 5.65 points, effectively protecting target individuals' personal data\nwhile maintaining the model's general capabilities.\n","authors":["Zhenhua Liu","Tong Zhu","Chuanyuan Tan","Wenliang Chen"],"pdf_url":"https://arxiv.org/pdf/2407.10058v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10044v1","updated":"2024-09-16T07:13:30Z","published":"2024-09-16T07:13:30Z","title":"Benchmarking Large Language Model Uncertainty for Prompt Optimization","summary":"  Prompt optimization algorithms for Large Language Models (LLMs) excel in\nmulti-step reasoning but still lack effective uncertainty estimation. This\npaper introduces a benchmark dataset to evaluate uncertainty metrics, focusing\non Answer, Correctness, Aleatoric, and Epistemic Uncertainty. Through analysis\nof models like GPT-3.5-Turbo and Meta-Llama-3.1-8B-Instruct, we show that\ncurrent metrics align more with Answer Uncertainty, which reflects output\nconfidence and diversity, rather than Correctness Uncertainty, highlighting the\nneed for improved metrics that are optimization-objective-aware to better guide\nprompt optimization. Our code and dataset are available at\nhttps://github.com/0Frett/PO-Uncertainty-Benchmarking.\n","authors":["Pei-Fu Guo","Yun-Da Tsai","Shou-De Lin"],"pdf_url":"https://arxiv.org/pdf/2409.10044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10918v4","updated":"2024-09-16T07:12:12Z","published":"2024-06-16T12:46:40Z","title":"Central Answer Modeling for an Embodied Multi-LLM System","summary":"  Embodied Question Answering (EQA) is an important problem, which involves an\nagent exploring the environment to answer user queries. In the existing\nliterature, EQA has exclusively been studied in single-agent scenarios, where\nexploration can be time-consuming and costly. In this work, we consider EQA in\na multi-agent framework involving multiple large language models (LLM) based\nagents independently answering queries about a household environment. To\ngenerate one answer for each query, we use the individual responses to train a\nCentral Answer Model (CAM) that aggregates responses for a robust answer. While\nprior Question Answering (QA) work has used a central module based on answers\nfrom multiple LLM-based experts, we specifically look at applying this\nframework to embodied LLM-based agents that must physically explore the\nenvironment first to become experts on their given environment to answer\nquestions. Our work is the first to utilize a central answer model framework\nwith embodied agents that must rely on exploring an unknown environment. We set\nup a variation of EQA where instead of the agents exploring the environment\nafter the question is asked, the agents first explore the environment for a set\namount of time and then answer a set of queries. Using CAM, we observe a $46\\%$\nhigher EQA accuracy when compared against aggregation methods for ensemble LLM,\nsuch as voting schemes and debates. CAM does not require any form of agent\ncommunication, alleviating it from the associated costs. We ablate CAM with\nvarious nonlinear (neural network, random forest, decision tree, XGBoost) and\nlinear (logistic regression classifier, SVM) algorithms. We experiment in\nvarious topological graph environments and examine the case where one of the\nagents is malicious and purposes contribute responses it believes to be wrong.\n","authors":["Bhrij Patel","Vishnu Sashank Dorbala","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2406.10918v4.pdf","comment":"15 pages, 11 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2409.10038v1","updated":"2024-09-16T07:01:41Z","published":"2024-09-16T07:01:41Z","title":"On the Diagram of Thought","summary":"  We introduce Diagram of Thought (DoT), a framework that models iterative\nreasoning in large language models (LLMs) as the construction of a directed\nacyclic graph (DAG) within a single model. Unlike traditional approaches that\nrepresent reasoning as linear chains or trees, DoT organizes propositions,\ncritiques, refinements, and verifications into a cohesive DAG structure,\nallowing the model to explore complex reasoning pathways while maintaining\nlogical consistency. Each node in the diagram corresponds to a proposition that\nhas been proposed, critiqued, refined, or verified, enabling the LLM to\niteratively improve its reasoning through natural language feedback. By\nleveraging auto-regressive next-token prediction with role-specific tokens, DoT\nfacilitates seamless transitions between proposing ideas and critically\nevaluating them, providing richer feedback than binary signals. Furthermore, we\nformalize the DoT framework using Topos Theory, providing a mathematical\nfoundation that ensures logical consistency and soundness in the reasoning\nprocess. This approach enhances both the training and inference processes\nwithin a single LLM, eliminating the need for multiple models or external\ncontrol mechanisms. DoT offers a conceptual framework for designing\nnext-generation reasoning-specialized models, emphasizing training efficiency,\nrobust reasoning capabilities, and theoretical grounding. The code is available\nat https://github.com/diagram-of-thought/diagram-of-thought.\n","authors":["Yifan Zhang","Yang Yuan","Andrew Chi-Chih Yao"],"pdf_url":"https://arxiv.org/pdf/2409.10038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10016v1","updated":"2024-09-16T06:06:34Z","published":"2024-09-16T06:06:34Z","title":"AceParse: A Comprehensive Dataset with Diverse Structured Texts for\n  Academic Literature Parsing","summary":"  With the development of data-centric AI, the focus has shifted from\nmodel-driven approaches to improving data quality. Academic literature, as one\nof the crucial types, is predominantly stored in PDF formats and needs to be\nparsed into texts before further processing. However, parsing diverse\nstructured texts in academic literature remains challenging due to the lack of\ndatasets that cover various text structures. In this paper, we introduce\nAceParse, the first comprehensive dataset designed to support the parsing of a\nwide range of structured texts, including formulas, tables, lists, algorithms,\nand sentences with embedded mathematical expressions. Based on AceParse, we\nfine-tuned a multimodal model, named AceParser, which accurately parses various\nstructured texts within academic literature. This model outperforms the\nprevious state-of-the-art by 4.1% in terms of F1 score and by 5% in Jaccard\nSimilarity, demonstrating the potential of multimodal models in academic\nliterature parsing. Our dataset is available at\nhttps://github.com/JHW5981/AceParse.\n","authors":["Huawei Ji","Cheng Deng","Bo Xue","Zhouyang Jin","Jiaxin Ding","Xiaoying Gan","Luoyi Fu","Xinbing Wang","Chenghu Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.10016v1.pdf","comment":"5 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2407.16166v2","updated":"2024-09-16T06:02:00Z","published":"2024-07-23T04:20:14Z","title":"Robust Privacy Amidst Innovation with Large Language Models Through a\n  Critical Assessment of the Risks","summary":"  This study examines integrating EHRs and NLP with large language models\n(LLMs) to improve healthcare data management and patient care. It focuses on\nusing advanced models to create secure, HIPAA-compliant synthetic patient notes\nfor biomedical research. The study used de-identified and re-identified MIMIC\nIII datasets with GPT-3.5, GPT-4, and Mistral 7B to generate synthetic notes.\nText generation employed templates and keyword extraction for contextually\nrelevant notes, with one-shot generation for comparison. Privacy assessment\nchecked PHI occurrence, while text utility was tested using an ICD-9 coding\ntask. Text quality was evaluated with ROUGE and cosine similarity metrics to\nmeasure semantic similarity with source notes. Analysis of PHI occurrence and\ntext utility via the ICD-9 coding task showed that the keyword-based method had\nlow risk and good performance. One-shot generation showed the highest PHI\nexposure and PHI co-occurrence, especially in geographic location and date\ncategories. The Normalized One-shot method achieved the highest classification\naccuracy. Privacy analysis revealed a critical balance between data utility and\nprivacy protection, influencing future data use and sharing. Re-identified data\nconsistently outperformed de-identified data. This study demonstrates the\neffectiveness of keyword-based methods in generating privacy-protecting\nsynthetic clinical notes that retain data usability, potentially transforming\nclinical data-sharing practices. The superior performance of re-identified over\nde-identified data suggests a shift towards methods that enhance utility and\nprivacy by using dummy PHIs to perplex privacy attacks.\n","authors":["Yao-Shun Chuang","Atiquer Rahman Sarkar","Yu-Chun Hsu","Noman Mohammed","Xiaoqian Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.16166v2.pdf","comment":"13 pages, 4 figures, 1 table, 1 supplementary, under review"},{"id":"http://arxiv.org/abs/2409.10011v1","updated":"2024-09-16T05:50:39Z","published":"2024-09-16T05:50:39Z","title":"HALO: Hallucination Analysis and Learning Optimization to Empower LLMs\n  with Retrieval-Augmented Context for Guided Clinical Decision Making","summary":"  Large language models (LLMs) have significantly advanced natural language\nprocessing tasks, yet they are susceptible to generating inaccurate or\nunreliable responses, a phenomenon known as hallucination. In critical domains\nsuch as health and medicine, these hallucinations can pose serious risks. This\npaper introduces HALO, a novel framework designed to enhance the accuracy and\nreliability of medical question-answering (QA) systems by focusing on the\ndetection and mitigation of hallucinations. Our approach generates multiple\nvariations of a given query using LLMs and retrieves relevant information from\nexternal open knowledge bases to enrich the context. We utilize maximum\nmarginal relevance scoring to prioritize the retrieved context, which is then\nprovided to LLMs for answer generation, thereby reducing the risk of\nhallucinations. The integration of LangChain further streamlines this process,\nresulting in a notable and robust increase in the accuracy of both open-source\nand commercial LLMs, such as Llama-3.1 (from 44% to 65%) and ChatGPT (from 56%\nto 70%). This framework underscores the critical importance of addressing\nhallucinations in medical QA systems, ultimately improving clinical\ndecision-making and patient care. The open-source HALO is available at:\nhttps://github.com/ResponsibleAILab/HALO.\n","authors":["Sumera Anjum","Hanzhi Zhang","Wenjun Zhou","Eun Jin Paek","Xiaopeng Zhao","Yunhe Feng"],"pdf_url":"https://arxiv.org/pdf/2409.10011v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.10007v1","updated":"2024-09-16T05:40:18Z","published":"2024-09-16T05:40:18Z","title":"SelECT-SQL: Self-correcting ensemble Chain-of-Thought for Text-to-SQL","summary":"  In recent years,Text-to-SQL, the problem of automatically converting\nquestions posed in natural language to formal SQL queries, has emerged as an\nimportant problem at the intersection of natural language processing and data\nmanagement research. Large language models (LLMs) have delivered impressive\nperformance when used in an off-the-shelf performance, but still fall\nsignificantly short of expected expert-level performance. Errors are especially\nprobable when a nuanced understanding is needed of database schemas, questions,\nand SQL clauses to do proper Text-to-SQL conversion. We introduce SelECT-SQL, a\nnovel in-context learning solution that uses an algorithmic combination of\nchain-of-thought (CoT) prompting, self-correction, and ensemble methods to\nyield a new state-of-the-art result on challenging Text-to-SQL benchmarks.\nSpecifically, when configured using GPT-3.5-Turbo as the base LLM, SelECT-SQL\nachieves 84.2% execution accuracy on the Spider leaderboard's development set,\nexceeding both the best results of other baseline GPT-3.5-Turbo-based solutions\n(81.1%), and the peak performance (83.5%) of the GPT-4 result reported on the\nleaderboard.\n","authors":["Ke Shen","Mayank Kejriwal"],"pdf_url":"https://arxiv.org/pdf/2409.10007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11764v2","updated":"2024-09-16T05:28:43Z","published":"2024-02-19T01:28:48Z","title":"ChatGPT Based Data Augmentation for Improved Parameter-Efficient\n  Debiasing of LLMs","summary":"  Large Language models (LLMs), while powerful, exhibit harmful social biases.\nDebiasing is often challenging due to computational costs, data constraints,\nand potential degradation of multi-task language capabilities. This work\nintroduces a novel approach utilizing ChatGPT to generate synthetic training\ndata, aiming to enhance the debiasing of LLMs. We propose two strategies:\nTargeted Prompting, which provides effective debiasing for known biases but\nnecessitates prior specification of bias in question; and General Prompting,\nwhich, while slightly less effective, offers debiasing across various\ncategories. We leverage resource-efficient LLM debiasing using adapter tuning\nand compare the effectiveness of our synthetic data to existing debiasing\ndatasets. Our results reveal that: (1) ChatGPT can efficiently produce\nhigh-quality training data for debiasing other LLMs; (2) data produced via our\napproach surpasses existing datasets in debiasing performance while also\npreserving internal knowledge of a pre-trained LLM; and (3) synthetic data\nexhibits generalizability across categories, effectively mitigating various\nbiases, including intersectional ones. These findings underscore the potential\nof synthetic data in advancing the fairness of LLMs with minimal retraining\ncost.\n","authors":["Pengrui Han","Rafal Kocielnik","Adhithya Saravanan","Roy Jiang","Or Sharir","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2402.11764v2.pdf","comment":"To Appear in the Proceedings of the 1st Conference on Language\n  Modeling (COLM) 2024"},{"id":"http://arxiv.org/abs/2409.09989v1","updated":"2024-09-16T04:44:52Z","published":"2024-09-16T04:44:52Z","title":"Comprehensive Study on Sentiment Analysis: From Rule-based to modern LLM\n  based system","summary":"  This paper provides a comprehensive survey of sentiment analysis within the\ncontext of artificial intelligence (AI) and large language models (LLMs).\nSentiment analysis, a critical aspect of natural language processing (NLP), has\nevolved significantly from traditional rule-based methods to advanced deep\nlearning techniques. This study examines the historical development of\nsentiment analysis, highlighting the transition from lexicon-based and\npattern-based approaches to more sophisticated machine learning and deep\nlearning models. Key challenges are discussed, including handling bilingual\ntexts, detecting sarcasm, and addressing biases. The paper reviews\nstate-of-the-art approaches, identifies emerging trends, and outlines future\nresearch directions to advance the field. By synthesizing current methodologies\nand exploring future opportunities, this survey aims to understand sentiment\nanalysis in the AI and LLM context thoroughly.\n","authors":["Shailja Gupta","Rajesh Ranjan","Surya Narayan Singh"],"pdf_url":"https://arxiv.org/pdf/2409.09989v1.pdf","comment":"2 Images"},{"id":"http://arxiv.org/abs/2409.09947v1","updated":"2024-09-16T02:38:38Z","published":"2024-09-16T02:38:38Z","title":"Gaps or Hallucinations? Gazing into Machine-Generated Legal Analysis for\n  Fine-grained Text Evaluations","summary":"  Large Language Models (LLMs) show promise as a writing aid for professionals\nperforming legal analyses. However, LLMs can often hallucinate in this setting,\nin ways difficult to recognize by non-professionals and existing text\nevaluation metrics. In this work, we pose the question: when can\nmachine-generated legal analysis be evaluated as acceptable? We introduce the\nneutral notion of gaps, as opposed to hallucinations in a strict erroneous\nsense, to refer to the difference between human-written and machine-generated\nlegal analysis. Gaps do not always equate to invalid generation. Working with\nlegal experts, we consider the CLERC generation task proposed in Hou et al.\n(2024b), leading to a taxonomy, a fine-grained detector for predicting gap\ncategories, and an annotated dataset for automatic evaluation. Our best\ndetector achieves 67% F1 score and 80% precision on the test set. Employing\nthis detector as an automated metric on legal analysis generated by SOTA LLMs,\nwe find around 80% contain hallucinations of different kinds.\n","authors":["Abe Bohan Hou","William Jurayj","Nils Holzenberger","Andrew Blair-Stanek","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2409.09947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09927v1","updated":"2024-09-16T02:04:33Z","published":"2024-09-16T02:04:33Z","title":"Towards Data Contamination Detection for Modern Large Language Models:\n  Limitations, Inconsistencies, and Oracle Challenges","summary":"  As large language models achieve increasingly impressive results, questions\narise about whether such performance is from generalizability or mere data\nmemorization. Thus, numerous data contamination detection methods have been\nproposed. However, these approaches are often validated with traditional\nbenchmarks and early-stage LLMs, leaving uncertainty about their effectiveness\nwhen evaluating state-of-the-art LLMs on the contamination of more challenging\nbenchmarks. To address this gap and provide a dual investigation of SOTA LLM\ncontamination status and detection method robustness, we evaluate five\ncontamination detection approaches with four state-of-the-art LLMs across eight\nchallenging datasets often used in modern LLM evaluation. Our analysis reveals\nthat (1) Current methods have non-trivial limitations in their assumptions and\npractical applications; (2) Notable difficulties exist in detecting\ncontamination introduced during instruction fine-tuning with answer\naugmentation; and (3) Limited consistencies between SOTA contamination\ndetection techniques. These findings highlight the complexity of contamination\ndetection in advanced LLMs and the urgent need for further research on robust\nand generalizable contamination evaluation. Our code is available at\nhttps://github.com/vsamuel2003/data-contamination.\n","authors":["Vinay Samuel","Yue Zhou","Henry Peng Zou"],"pdf_url":"https://arxiv.org/pdf/2409.09927v1.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2409.09916v1","updated":"2024-09-16T01:08:18Z","published":"2024-09-16T01:08:18Z","title":"SFR-RAG: Towards Contextually Faithful LLMs","summary":"  Retrieval Augmented Generation (RAG), a paradigm that integrates external\ncontextual information with large language models (LLMs) to enhance factual\naccuracy and relevance, has emerged as a pivotal area in generative AI. The\nLLMs used in RAG applications are required to faithfully and completely\ncomprehend the provided context and users' questions, avoid hallucination,\nhandle unanswerable, counterfactual or otherwise low-quality and irrelevant\ncontexts, perform complex multi-hop reasoning and produce reliable citations.\nIn this paper, we introduce SFR-RAG, a small LLM that is instruction-tuned with\nan emphasis on context-grounded generation and hallucination minimization. We\nalso present ContextualBench, a new evaluation framework compiling multiple\npopular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with\nconsistent RAG settings to ensure reproducibility and consistency in model\nassessments. Experimental results demonstrate that our SFR-RAG-9B model\noutperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving\nstate-of-the-art results in 3 out of 7 benchmarks in ContextualBench with\nsignificantly fewer parameters. The model is also shown to be resilient to\nalteration in the contextual information and behave appropriately when relevant\ncontext is removed. Additionally, the SFR-RAG model maintains competitive\nperformance in general instruction-following tasks and function-calling\ncapabilities.\n","authors":["Xuan-Phi Nguyen","Shrey Pandit","Senthil Purushwalkam","Austin Xu","Hailin Chen","Yifei Ming","Zixuan Ke","Silvio Savarese","Caiming Xong","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2409.09916v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2409.09905v1","updated":"2024-09-16T00:24:40Z","published":"2024-09-16T00:24:40Z","title":"Rediscovering the Latent Dimensions of Personality with Large Language\n  Models as Trait Descriptors","summary":"  Assessing personality traits using large language models (LLMs) has emerged\nas an interesting and challenging area of research. While previous methods\nemploy explicit questionnaires, often derived from the Big Five model of\npersonality, we hypothesize that LLMs implicitly encode notions of personality\nwhen modeling next-token responses. To demonstrate this, we introduce a novel\napproach that uncovers latent personality dimensions in LLMs by applying\nsingular value de-composition (SVD) to the log-probabilities of\ntrait-descriptive adjectives. Our experiments show that LLMs \"rediscover\" core\npersonality traits such as extraversion, agreeableness, conscientiousness,\nneuroticism, and openness without relying on direct questionnaire inputs, with\nthe top-5 factors corresponding to Big Five traits explaining 74.3% of the\nvariance in the latent space. Moreover, we can use the derived principal\ncomponents to assess personality along the Big Five dimensions, and achieve\nimprovements in average personality prediction accuracy of up to 5% over\nfine-tuned models, and up to 21% over direct LLM-based scoring techniques.\n","authors":["Joseph Suh","Suhong Moon","Minwoo Kang","David M. Chan"],"pdf_url":"https://arxiv.org/pdf/2409.09905v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2312.15268v2","updated":"2024-09-16T17:45:13Z","published":"2023-12-23T14:36:27Z","title":"Manydepth2: Motion-Aware Self-Supervised Monocular Depth Estimation in\n  Dynamic Scenes","summary":"  Despite advancements in self-supervised monocular depth estimation,\nchallenges persist in dynamic scenarios due to the dependence on assumptions\nabout a static world. In this paper, we present Manydepth2, a Motion-Guided\nCost Volume Depth Net, to achieve precise depth estimation for both dynamic\nobjects and static backgrounds, all while maintaining computational efficiency.\nTo tackle the challenges posed by dynamic content, we incorporate optical flow\nand coarse monocular depth to create a novel static reference frame. This frame\nis then utilized to build a motion-guided cost volume in collaboration with the\ntarget frame. Additionally, to enhance the accuracy and resilience of the\nnetwork structure, we introduce an attention-based depth net architecture to\neffectively integrate information from feature maps with varying resolutions.\nCompared to methods with similar computational costs, Manydepth2 achieves a\nsignificant reduction of approximately five percent in root-mean-square error\nfor self-supervised monocular depth estimation on the KITTI-2015 dataset. The\ncode could be found: https://github.com/kaichen-z/Manydepth2\n","authors":["Kaichen Zhou","Jia-Wang Bian","Qian Xie","Jian-Qing Zheng","Niki Trigoni","Andrew Markham"],"pdf_url":"https://arxiv.org/pdf/2312.15268v2.pdf","comment":"Monocular Depth Estimation, Self-Supervised, Optical Flow"},{"id":"http://arxiv.org/abs/2401.00935v3","updated":"2024-09-16T17:42:17Z","published":"2024-01-01T19:00:55Z","title":"Boundary Attention: Learning curves, corners, junctions and grouping","summary":"  We present a lightweight network that infers grouping and boundaries,\nincluding curves, corners and junctions. It operates in a bottom-up fashion,\nanalogous to classical methods for sub-pixel edge localization and\nedge-linking, but with a higher-dimensional representation of local boundary\nstructure, and notions of local scale and spatial consistency that are learned\ninstead of designed. Our network uses a mechanism that we call boundary\nattention: a geometry-aware local attention operation that, when applied\ndensely and repeatedly, progressively refines a pixel-resolution field of\nvariables that specify the boundary structure in every overlapping patch within\nan image. Unlike many edge detectors that produce rasterized binary edge maps,\nour model provides a rich, unrasterized representation of the geometric\nstructure in every local region. We find that its intentional geometric bias\nallows it to be trained on simple synthetic shapes and then generalize to\nextracting boundaries from noisy low-light photographs.\n","authors":["Mia Gaia Polansky","Charles Herrmann","Junhwa Hur","Deqing Sun","Dor Verbin","Todd Zickler"],"pdf_url":"https://arxiv.org/pdf/2401.00935v3.pdf","comment":"Project website at boundaryattention.github.io:\n  http://boundaryattention.github.io"},{"id":"http://arxiv.org/abs/2311.12539v5","updated":"2024-09-16T17:41:55Z","published":"2023-11-21T11:33:15Z","title":"GMISeg: General Medical Image Segmentation without Re-Training","summary":"  Deep learning models have become the dominant method for medical image\nsegmentation. However, they often struggle to be generalisable to unknown tasks\ninvolving new anatomical structures, labels, or shapes. In these cases, the\nmodel needs to be re-trained for the new tasks, posing a significant challenge\nfor non-machine learning experts and requiring a considerable time investment.\nHere I developed a general model that can solve unknown medical image\nsegmentation tasks without requiring additional training. Given an example set\nof images and visual prompts for defining new segmentation tasks, GMISeg\n(General Medical Image Segmentation) leverages a pre-trained image encoder\nbased on ViT and applies a low-rank fine-tuning strategy to the prompt encoder\nand mask decoder to fine-tune the model without in an efficient manner. I\nevaluated the performance of the proposed method on medical image datasets with\ndifferent imaging modalities and anatomical structures. The proposed method\nfacilitated the deployment of pre-trained AI models to new segmentation works\nin a user-friendly way.\n","authors":["Jing Xu"],"pdf_url":"https://arxiv.org/pdf/2311.12539v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07128v5","updated":"2024-09-16T17:40:31Z","published":"2023-12-12T10:04:11Z","title":"MS-Twins: Multi-Scale Deep Self-Attention Networks for Medical Image\n  Segmentation","summary":"  Although transformer is preferred in natural language processing, some\nstudies has only been applied to the field of medical imaging in recent years.\nFor its long-term dependency, the transformer is expected to contribute to\nunconventional convolution neural net conquer their inherent spatial induction\nbias. The lately suggested transformer-based segmentation method only uses the\ntransformer as an auxiliary module to help encode the global context into a\nconvolutional representation. How to optimally integrate self-attention with\nconvolution has not been investigated in depth. To solve the problem, this\npaper proposes MS-Twins (Multi-Scale Twins), which is a powerful segmentation\nmodel on account of the bond of self-attention and convolution. MS-Twins can\nbetter capture semantic and fine-grained information by combining different\nscales and cascading features. Compared with the existing network structure,\nMS-Twins has made progress on the previous method based on the transformer of\ntwo in common use data sets, Synapse and ACDC. In particular, the performance\nof MS-Twins on Synapse is 8% higher than SwinUNet. Even compared with nnUNet,\nthe best entirely convoluted medical image segmentation network, the\nperformance of MS-Twins on Synapse and ACDC still has a bit advantage.\n","authors":["Jing Xu"],"pdf_url":"https://arxiv.org/pdf/2312.07128v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10488v1","updated":"2024-09-16T17:22:18Z","published":"2024-09-16T17:22:18Z","title":"Do Pre-trained Vision-Language Models Encode Object States?","summary":"  For a vision-language model (VLM) to understand the physical world, such as\ncause and effect, a first step is to capture the temporal dynamics of the\nvisual world, for example how the physical states of objects evolve over time\n(e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs\npre-trained on web-scale data learn to encode object states, which can be\nextracted with zero-shot text prompts. We curate an object state recognition\ndataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models\ntrained with contrastive and generative objectives. We observe that while these\nstate-of-the-art vision-language models can reliably perform object\nrecognition, they consistently fail to accurately distinguish the objects'\nphysical states. Through extensive experiments, we identify three areas for\nimprovements for VLMs to better encode object states, namely the quality of\nobject localization, the architecture to bind concepts to objects, and the\nobjective to learn discriminative visual and language encoders on object\nstates. Data and code are released.\n","authors":["Kaleb Newman","Shijie Wang","Yuan Zang","David Heffren","Chen Sun"],"pdf_url":"https://arxiv.org/pdf/2409.10488v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10481v1","updated":"2024-09-16T17:17:47Z","published":"2024-09-16T17:17:47Z","title":"Exploring 3D Face Reconstruction and Fusion Methods for Face\n  Verification: A Case-Study in Video Surveillance","summary":"  3D face reconstruction (3DFR) algorithms are based on specific assumptions\ntailored to distinct application scenarios. These assumptions limit their use\nwhen acquisition conditions, such as the subject's distance from the camera or\nthe camera's characteristics, are different than expected, as typically happens\nin video surveillance. Additionally, 3DFR algorithms follow various strategies\nto address the reconstruction of a 3D shape from 2D data, such as statistical\nmodel fitting, photometric stereo, or deep learning. In the present study, we\nexplore the application of three 3DFR algorithms representative of the SOTA,\nemploying each one as the template set generator for a face verification\nsystem. The scores provided by each system are combined by score-level fusion.\nWe show that the complementarity induced by different 3DFR algorithms improves\nperformance when tests are conducted at never-seen-before distances from the\ncamera and camera characteristics (cross-distance and cross-camera settings),\nthus encouraging further investigations on multiple 3DFR-based approaches.\n","authors":["Simone Maurizio La Cava","Sara Concas","Ruben Tolosana","Roberto Casula","Giulia Orr√π","Martin Drahansky","Julian Fierrez","Gian Luca Marcialis"],"pdf_url":"https://arxiv.org/pdf/2409.10481v1.pdf","comment":"Accepted at T-CAP - Towards a Complete Analysis of People:\n  Fine-grained Understanding for Real-World Applications, workshop in\n  conjunction with the 18th European Conference on Computer Vision ECCV 2024"},{"id":"http://arxiv.org/abs/2409.10476v1","updated":"2024-09-16T17:10:50Z","published":"2024-09-16T17:10:50Z","title":"SimInversion: A Simple Framework for Inversion-Based Text-to-Image\n  Editing","summary":"  Diffusion models demonstrate impressive image generation performance with\ntext guidance. Inspired by the learning process of diffusion, existing images\ncan be edited according to text by DDIM inversion. However, the vanilla DDIM\ninversion is not optimized for classifier-free guidance and the accumulated\nerror will result in the undesired performance. While many algorithms are\ndeveloped to improve the framework of DDIM inversion for editing, in this work,\nwe investigate the approximation error in DDIM inversion and propose to\ndisentangle the guidance scale for the source and target branches to reduce the\nerror while keeping the original framework. Moreover, a better guidance scale\n(i.e., 0.5) than default settings can be derived theoretically. Experiments on\nPIE-Bench show that our proposal can improve the performance of DDIM inversion\ndramatically without sacrificing efficiency.\n","authors":["Qi Qian","Haiyang Xu","Ming Yan","Juhua Hu"],"pdf_url":"https://arxiv.org/pdf/2409.10476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10473v1","updated":"2024-09-16T17:06:10Z","published":"2024-09-16T17:06:10Z","title":"MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion","summary":"  Self-supervised learning has proved effective for skeleton-based human action\nunderstanding. However, previous works either rely on contrastive learning that\nsuffers false negative problems or are based on reconstruction that learns too\nmuch unessential low-level clues, leading to limited representations for\ndownstream tasks. Recently, great advances have been made in generative\nlearning, which is naturally a challenging yet meaningful pretext task to model\nthe general underlying data distributions. However, the representation learning\ncapacity of generative models is under-explored, especially for the skeletons\nwith spacial sparsity and temporal redundancy. To this end, we propose Masked\nConditional Diffusion (MacDiff) as a unified framework for human skeleton\nmodeling. For the first time, we leverage diffusion models as effective\nskeleton representation learners. Specifically, we train a diffusion decoder\nconditioned on the representations extracted by a semantic encoder. Random\nmasking is applied to encoder inputs to introduce a information bottleneck and\nremove redundancy of skeletons. Furthermore, we theoretically demonstrate that\nour generative objective involves the contrastive learning objective which\naligns the masked and noisy views. Meanwhile, it also enforces the\nrepresentation to complement for the noisy view, leading to better\ngeneralization performance. MacDiff achieves state-of-the-art performance on\nrepresentation learning benchmarks while maintaining the competence for\ngenerative tasks. Moreover, we leverage the diffusion model for data\naugmentation, significantly enhancing the fine-tuning performance in scenarios\nwith scarce labeled data. Our project is available at\nhttps://lehongwu.github.io/ECCV24MacDiff/.\n","authors":["Lehong Wu","Lilang Lin","Jiahang Zhang","Yiyang Ma","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2409.10473v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2409.02007v2","updated":"2024-09-16T16:51:50Z","published":"2024-09-03T15:54:34Z","title":"PMT-MAE: Dual-Branch Self-Supervised Learning with Distillation for\n  Efficient Point Cloud Classification","summary":"  Advances in self-supervised learning are essential for enhancing feature\nextraction and understanding in point cloud processing. This paper introduces\nPMT-MAE (Point MLP-Transformer Masked Autoencoder), a novel self-supervised\nlearning framework for point cloud classification. PMT-MAE features a\ndual-branch architecture that integrates Transformer and MLP components to\ncapture rich features. The Transformer branch leverages global self-attention\nfor intricate feature interactions, while the parallel MLP branch processes\ntokens through shared fully connected layers, offering a complementary feature\ntransformation pathway. A fusion mechanism then combines these features,\nenhancing the model's capacity to learn comprehensive 3D representations.\nGuided by the sophisticated teacher model Point-M2AE, PMT-MAE employs a\ndistillation strategy that includes feature distillation during pre-training\nand logit distillation during fine-tuning, ensuring effective knowledge\ntransfer. On the ModelNet40 classification task, achieving an accuracy of\n93.6\\% without employing voting strategy, PMT-MAE surpasses the baseline\nPoint-MAE (93.2\\%) and the teacher Point-M2AE (93.4\\%), underscoring its\nability to learn discriminative 3D point cloud representations. Additionally,\nthis framework demonstrates high efficiency, requiring only 40 epochs for both\npre-training and fine-tuning. PMT-MAE's effectiveness and efficiency render it\nwell-suited for scenarios with limited computational resources, positioning it\nas a promising solution for practical point cloud analysis.\n","authors":["Qiang Zheng","Chao Zhang","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2409.02007v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05508v2","updated":"2024-09-16T16:44:58Z","published":"2024-08-10T10:16:03Z","title":"PointMT: Efficient Point Cloud Analysis with Hybrid MLP-Transformer\n  Architecture","summary":"  In recent years, point cloud analysis methods based on the Transformer\narchitecture have made significant progress, particularly in the context of\nmultimedia applications such as 3D modeling, virtual reality, and autonomous\nsystems. However, the high computational resource demands of the Transformer\narchitecture hinder its scalability, real-time processing capabilities, and\ndeployment on mobile devices and other platforms with limited computational\nresources. This limitation remains a significant obstacle to its practical\napplication in scenarios requiring on-device intelligence and multimedia\nprocessing. To address this challenge, we propose an efficient point cloud\nanalysis architecture, \\textbf{Point} \\textbf{M}LP-\\textbf{T}ransformer\n(PointMT). This study tackles the quadratic complexity of the self-attention\nmechanism by introducing a linear complexity local attention mechanism for\neffective feature aggregation. Additionally, to counter the Transformer's focus\non token differences while neglecting channel differences, we introduce a\nparameter-free channel temperature adaptation mechanism that adaptively adjusts\nthe attention weight distribution in each channel, enhancing the precision of\nfeature aggregation. To improve the Transformer's slow convergence speed due to\nthe limited scale of point cloud datasets, we propose an MLP-Transformer hybrid\nmodule, which significantly enhances the model's convergence speed.\nFurthermore, to boost the feature representation capability of point tokens, we\nrefine the classification head, enabling point tokens to directly participate\nin prediction. Experimental results on multiple evaluation benchmarks\ndemonstrate that PointMT achieves performance comparable to state-of-the-art\nmethods while maintaining an optimal balance between performance and accuracy.\n","authors":["Qiang Zheng","Chao Zhang","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2408.05508v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10445v1","updated":"2024-09-16T16:29:41Z","published":"2024-09-16T16:29:41Z","title":"Deep-Wide Learning Assistance for Insect Pest Classification","summary":"  Accurate insect pest recognition plays a critical role in agriculture. It is\na challenging problem due to the intricate characteristics of insects. In this\npaper, we present DeWi, novel learning assistance for insect pest\nclassification. With a one-stage and alternating training strategy, DeWi\nsimultaneously improves several Convolutional Neural Networks in two\nperspectives: discrimination (by optimizing a triplet margin loss in a\nsupervised training manner) and generalization (via data augmentation). From\nthat, DeWi can learn discriminative and in-depth features of insect pests\n(deep) yet still generalize well to a large number of insect categories (wide).\nExperimental results show that DeWi achieves the highest performances on two\ninsect pest classification benchmarks (76.44\\% accuracy on the IP102 dataset\nand 99.79\\% accuracy on the D0 dataset, respectively). In addition, extensive\nevaluations and ablation studies are conducted to thoroughly investigate our\nDeWi and demonstrate its superiority. Our source code is available at\nhttps://github.com/toannguyen1904/DeWi.\n","authors":["Toan Nguyen","Huy Nguyen","Huy Ung","Hieu Ung","Binh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2409.10445v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10441v1","updated":"2024-09-16T16:22:43Z","published":"2024-09-16T16:22:43Z","title":"CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using\n  a Single Camera","summary":"  Camera-to-robot calibration is crucial for vision-based robot control and\nrequires effort to make it accurate. Recent advancements in markerless pose\nestimation methods have eliminated the need for time-consuming physical setups\nfor camera-to-robot calibration. While the existing markerless pose estimation\nmethods have demonstrated impressive accuracy without the need for cumbersome\nsetups, they rely on the assumption that all the robot joints are visible\nwithin the camera's field of view. However, in practice, robots usually move in\nand out of view, and some portion of the robot may stay out-of-frame during the\nwhole manipulation task due to real-world constraints, leading to a lack of\nsufficient visual features and subsequent failure of these approaches. To\naddress this challenge and enhance the applicability to vision-based robot\ncontrol, we propose a novel framework capable of estimating the robot pose with\npartially visible robot manipulators. Our approach leverages the\nVision-Language Models for fine-grained robot components detection, and\nintegrates it into a keypoint-based pose estimation network, which enables more\nrobust performance in varied operational conditions. The framework is evaluated\non both public robot datasets and self-collected partial-view datasets to\ndemonstrate our robustness and generalizability. As a result, this method is\neffective for robot pose estimation in a wider range of real-world manipulation\nscenarios.\n","authors":["Jingpei Lu","Zekai Liang","Tristin Xie","Florian Ritcher","Shan Lin","Sainan Liu","Michael C. Yip"],"pdf_url":"https://arxiv.org/pdf/2409.10441v1.pdf","comment":"7 pages, 5 figures, project website:\n  https://sites.google.com/ucsd.edu/ctrnet-x"},{"id":"http://arxiv.org/abs/2401.01256v2","updated":"2024-09-16T16:05:18Z","published":"2024-01-02T15:56:48Z","title":"VideoStudio: Generating Consistent-Content and Multi-Scene Videos","summary":"  The recent innovations and breakthroughs in diffusion models have\nsignificantly expanded the possibilities of generating high-quality videos for\nthe given prompts. Most existing works tackle the single-scene scenario with\nonly one video event occurring in a single background. Extending to generate\nmulti-scene videos nevertheless is not trivial and necessitates to nicely\nmanage the logic in between while preserving the consistent visual appearance\nof key content across video scenes. In this paper, we propose a novel\nframework, namely VideoStudio, for consistent-content and multi-scene video\ngeneration. Technically, VideoStudio leverages Large Language Models (LLM) to\nconvert the input prompt into comprehensive multi-scene script that benefits\nfrom the logical knowledge learnt by LLM. The script for each scene includes a\nprompt describing the event, the foreground/background entities, as well as\ncamera movement. VideoStudio identifies the common entities throughout the\nscript and asks LLM to detail each entity. The resultant entity description is\nthen fed into a text-to-image model to generate a reference image for each\nentity. Finally, VideoStudio outputs a multi-scene video by generating each\nscene video via a diffusion process that takes the reference images, the\ndescriptive prompt of the event and camera movement into account. The diffusion\nmodel incorporates the reference images as the condition and alignment to\nstrengthen the content consistency of multi-scene videos. Extensive experiments\ndemonstrate that VideoStudio outperforms the SOTA video generation models in\nterms of visual quality, content consistency, and user preference. Source code\nis available at \\url{https://github.com/FuchenUSTC/VideoStudio}.\n","authors":["Fuchen Long","Zhaofan Qiu","Ting Yao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2401.01256v2.pdf","comment":"ECCV 2024. Source code is available at\n  https://github.com/FuchenUSTC/VideoStudio"},{"id":"http://arxiv.org/abs/2312.03018v4","updated":"2024-09-16T16:02:34Z","published":"2023-12-05T03:16:31Z","title":"DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention\n  and Text Guidance","summary":"  Image-to-video generation, which aims to generate a video starting from a\ngiven reference image, has drawn great attention. Existing methods try to\nextend pre-trained text-guided image diffusion models to image-guided video\ngeneration models. Nevertheless, these methods often result in either low\nfidelity or flickering over time due to their limitation to shallow image\nguidance and poor temporal consistency. To tackle these problems, we propose a\nhigh-fidelity image-to-video generation method by devising a frame retention\nbranch based on a pre-trained video diffusion model, named DreamVideo. Instead\nof integrating the reference image into the diffusion process at a semantic\nlevel, our DreamVideo perceives the reference image via convolution layers and\nconcatenates the features with the noisy latents as model input. By this means,\nthe details of the reference image can be preserved to the greatest extent. In\naddition, by incorporating double-condition classifier-free guidance, a single\nimage can be directed to videos of different actions by providing varying\nprompt texts. This has significant implications for controllable video\ngeneration and holds broad application prospects. We conduct comprehensive\nexperiments on the public dataset, and both quantitative and qualitative\nresults indicate that our method outperforms the state-of-the-art method.\nEspecially for fidelity, our model has a powerful image retention ability and\ndelivers the best results in UCF101 compared to other image-to-video models to\nour best knowledge. Also, precise control can be achieved by giving different\ntext prompts. Further details and comprehensive results of our model will be\npresented in https://anonymous0769.github.io/DreamVideo/.\n","authors":["Cong Wang","Jiaxi Gu","Panwen Hu","Songcen Xu","Hang Xu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2312.03018v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13005v2","updated":"2024-09-16T15:56:34Z","published":"2024-08-23T11:48:29Z","title":"EasyControl: Transfer ControlNet to Video Diffusion for Controllable\n  Generation and Interpolation","summary":"  Following the advancements in text-guided image generation technology\nexemplified by Stable Diffusion, video generation is gaining increased\nattention in the academic community. However, relying solely on text guidance\nfor video generation has serious limitations, as videos contain much richer\ncontent than images, especially in terms of motion. This information can hardly\nbe adequately described with plain text. Fortunately, in computer vision,\nvarious visual representations can serve as additional control signals to guide\ngeneration. With the help of these signals, video generation can be controlled\nin finer detail, allowing for greater flexibility for different applications.\nIntegrating various controls, however, is nontrivial. In this paper, we propose\na universal framework called EasyControl. By propagating and injecting\ncondition features through condition adapters, our method enables users to\ncontrol video generation with a single condition map. With our framework,\nvarious conditions including raw pixels, depth, HED, etc., can be integrated\ninto different Unet-based pre-trained video diffusion models at a low practical\ncost. We conduct comprehensive experiments on public datasets, and both\nquantitative and qualitative results indicate that our method outperforms\nstate-of-the-art methods. EasyControl significantly improves various evaluation\nmetrics across multiple validation datasets compared to previous works.\nSpecifically, for the sketch-to-video generation task, EasyControl achieves an\nimprovement of 152.0 on FVD and 19.9 on IS, respectively, in UCF101 compared\nwith VideoComposer. For fidelity, our model demonstrates powerful image\nretention ability, resulting in high FVD and IS in UCF101 and MSR-VTT compared\nto other image-to-video models.\n","authors":["Cong Wang","Jiaxi Gu","Panwen Hu","Haoyu Zhao","Yuanfan Guo","Jianhua Han","Hang Xu","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2408.13005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10422v1","updated":"2024-09-16T15:52:41Z","published":"2024-09-16T15:52:41Z","title":"Learning Semi-Supervised Medical Image Segmentation from Spatial\n  Registration","summary":"  Semi-supervised medical image segmentation has shown promise in training\nmodels with limited labeled data and abundant unlabeled data. However,\nstate-of-the-art methods ignore a potentially valuable source of unsupervised\nsemantic information -- spatial registration transforms between image volumes.\nTo address this, we propose CCT-R, a contrastive cross-teaching framework\nincorporating registration information. To leverage the semantic information\navailable in registrations between volume pairs, CCT-R incorporates two\nproposed modules: Registration Supervision Loss (RSL) and Registration-Enhanced\nPositive Sampling (REPS). The RSL leverages segmentation knowledge derived from\ntransforms between labeled and unlabeled volume pairs, providing an additional\nsource of pseudo-labels. REPS enhances contrastive learning by identifying\nanatomically-corresponding positives across volumes using registration\ntransforms. Experimental results on two challenging medical segmentation\nbenchmarks demonstrate the effectiveness and superiority of CCT-R across\nvarious semi-supervised settings, with as few as one labeled case. Our code is\navailable at\nhttps://github.com/kathyliu579/ContrastiveCross-teachingWithRegistration.\n","authors":["Qianying Liu","Paul Henderson","Xiao Gu","Hang Dai","Fani Deligianni"],"pdf_url":"https://arxiv.org/pdf/2409.10422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02020v2","updated":"2024-09-16T15:51:35Z","published":"2024-09-03T16:12:12Z","title":"Efficient Point Cloud Classification via Offline Distillation Framework\n  and Negative-Weight Self-Distillation Technique","summary":"  The rapid advancement in point cloud processing technologies has\nsignificantly increased the demand for efficient and compact models that\nachieve high-accuracy classification. Knowledge distillation has emerged as a\npotent model compression technique. However, traditional KD often requires\nextensive computational resources for forward inference of large teacher\nmodels, thereby reducing training efficiency for student models and increasing\nresource demands. To address these challenges, we introduce an innovative\noffline recording strategy that avoids the simultaneous loading of both teacher\nand student models, thereby reducing hardware demands. This approach feeds a\nmultitude of augmented samples into the teacher model, recording both the data\naugmentation parameters and the corresponding logit outputs. By applying\nshape-level augmentation operations such as random scaling and translation,\nwhile excluding point-level operations like random jittering, the size of the\nrecords is significantly reduced. Additionally, to mitigate the issue of small\nstudent model over-imitating the teacher model's outputs and converging to\nsuboptimal solutions, we incorporate a negative-weight self-distillation\nstrategy. Experimental results demonstrate that the proposed distillation\nstrategy enables the student model to achieve performance comparable to\nstate-of-the-art models while maintaining lower parameter count. This approach\nstrikes an optimal balance between performance and complexity. This study\nhighlights the potential of our method to optimize knowledge distillation for\npoint cloud classification tasks, particularly in resource-constrained\nenvironments, providing a novel solution for efficient point cloud analysis.\n","authors":["Qiang Zheng","Chao Zhang","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2409.02020v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07113v3","updated":"2024-09-16T15:47:45Z","published":"2024-06-11T09:57:04Z","title":"Beyond Bare Queries: Open-Vocabulary Object Grounding with 3D Scene\n  Graph","summary":"  Locating objects described in natural language presents a significant\nchallenge for autonomous agents. Existing CLIP-based open-vocabulary methods\nsuccessfully perform 3D object grounding with simple (bare) queries, but cannot\ncope with ambiguous descriptions that demand an understanding of object\nrelations. To tackle this problem, we propose a modular approach called BBQ\n(Beyond Bare Queries), which constructs 3D scene graph representation with\nmetric and semantic edges and utilizes a large language model as a\nhuman-to-agent interface through our deductive scene reasoning algorithm. BBQ\nemploys robust DINO-powered associations to construct 3D object-centric map and\nan advanced raycasting algorithm with a 2D vision-language model to describe\nthem as graph nodes. On the Replica and ScanNet datasets, we have demonstrated\nthat BBQ takes a leading place in open-vocabulary 3D semantic segmentation\ncompared to other zero-shot methods. Also, we show that leveraging spatial\nrelations is especially effective for scenes containing multiple entities of\nthe same semantic class. On challenging Sr3D+, Nr3D and ScanRefer benchmarks,\nour deductive approach demonstrates a significant improvement, enabling objects\ngrounding by complex queries compared to other state-of-the-art methods. The\ncombination of our design choices and software implementation has resulted in\nsignificant data processing speed in experiments on the robot on-board\ncomputer. This promising performance enables the application of our approach in\nintelligent robotics projects. We made the code publicly available at\nhttps://linukc.github.io/BeyondBareQueries/.\n","authors":["Sergey Linok","Tatiana Zemskova","Svetlana Ladanova","Roman Titkov","Dmitry Yudin","Maxim Monastyrny","Aleksei Valenkov"],"pdf_url":"https://arxiv.org/pdf/2406.07113v3.pdf","comment":"6 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2406.17639v3","updated":"2024-09-16T15:32:11Z","published":"2024-06-25T15:24:02Z","title":"Mitigate the Gap: Investigating Approaches for Improving Cross-Modal\n  Alignment in CLIP","summary":"  Contrastive Language--Image Pre-training (CLIP) has manifested remarkable\nimprovements in zero-shot classification and cross-modal vision-language tasks.\nYet, from a geometrical point of view, the CLIP embedding space has been found\nto have a pronounced modality gap. This gap renders the embedding space overly\nsparse and disconnected, with different modalities being densely distributed in\ndistinct subregions of the hypersphere. In this work, we aim at answering three\nmain questions: 1. Does sharing the parameter space between the multi-modal\nencoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart\nthe uni-modal embeddings via intra-modality separation? 3. How do these gap\nreduction approaches affect the downstream performance? We design AlignCLIP, in\norder to answer these questions and through extensive experiments, we show that\nAlignCLIP achieves noticeable enhancements in the cross-modal alignment of the\nembeddings, and thereby, reduces the modality gap, while improving the\nperformance across several zero-shot and fine-tuning downstream evaluations.\n","authors":["Sedigheh Eslami","Gerard de Melo"],"pdf_url":"https://arxiv.org/pdf/2406.17639v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00921v2","updated":"2024-09-16T15:28:31Z","published":"2024-07-01T02:55:45Z","title":"PointViG: A Lightweight GNN-based Model for Efficient Point Cloud\n  Analysis","summary":"  In the domain of point cloud analysis, despite the significant capabilities\nof Graph Neural Networks (GNNs) in managing complex 3D datasets, existing\napproaches encounter challenges like high computational costs and scalability\nissues with extensive scenarios. These limitations restrict the practical\ndeployment of GNNs, notably in resource-constrained environments. To address\nthese issues, this study introduce <b>Point<\\b> <b>Vi<\\b>sion <b>G<\\b>NN\n(PointViG), an efficient framework for point cloud analysis. PointViG\nincorporates a lightweight graph convolutional module to efficiently aggregate\nlocal features and mitigate over-smoothing. For large-scale point cloud scenes,\nwe propose an adaptive dilated graph convolution technique that searches for\nsparse neighboring nodes within a dilated neighborhood based on semantic\ncorrelation, thereby expanding the receptive field and ensuring computational\nefficiency. Experiments demonstrate that PointViG achieves performance\ncomparable to state-of-the-art models while balancing performance and\ncomplexity. On the ModelNet40 classification task, PointViG achieved 94.3%\naccuracy with 1.5M parameters. For the S3DIS segmentation task, it achieved an\nmIoU of 71.7% with 5.3M parameters. These results underscore the potential and\nefficiency of PointViG in point cloud analysis.\n","authors":["Qiang Zheng","Yafei Qi","Chen Wang","Chao Zhang","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2407.00921v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10389v1","updated":"2024-09-16T15:24:26Z","published":"2024-09-16T15:24:26Z","title":"Prompt-and-Transfer: Dynamic Class-aware Enhancement for Few-shot\n  Segmentation","summary":"  For more efficient generalization to unseen domains (classes), most Few-shot\nSegmentation (FSS) would directly exploit pre-trained encoders and only\nfine-tune the decoder, especially in the current era of large models. However,\nsuch fixed feature encoders tend to be class-agnostic, inevitably activating\nobjects that are irrelevant to the target class. In contrast, humans can\neffortlessly focus on specific objects in the line of sight. This paper mimics\nthe visual perception pattern of human beings and proposes a novel and powerful\nprompt-driven scheme, called ``Prompt and Transfer\" (PAT), which constructs a\ndynamic class-aware prompting paradigm to tune the encoder for focusing on the\ninterested object (target class) in the current task. Three key points are\nelaborated to enhance the prompting: 1) Cross-modal linguistic information is\nintroduced to initialize prompts for each task. 2) Semantic Prompt Transfer\n(SPT) that precisely transfers the class-specific semantics within the images\nto prompts. 3) Part Mask Generator (PMG) that works in conjunction with SPT to\nadaptively generate different but complementary part prompts for different\nindividuals. Surprisingly, PAT achieves competitive performance on 4 different\ntasks including standard FSS, Cross-domain FSS (e.g., CV, medical, and remote\nsensing domains), Weak-label FSS, and Zero-shot Segmentation, setting new\nstate-of-the-arts on 11 benchmarks.\n","authors":["Hanbo Bi","Yingchao Feng","Wenhui Diao","Peijin Wang","Yongqiang Mao","Kun Fu","Hongqi Wang","Xian Sun"],"pdf_url":"https://arxiv.org/pdf/2409.10389v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10385v1","updated":"2024-09-16T15:20:48Z","published":"2024-09-16T15:20:48Z","title":"Mamba-ST: State Space Model for Efficient Style Transfer","summary":"  The goal of style transfer is, given a content image and a style source,\ngenerating a new image preserving the content but with the artistic\nrepresentation of the style source. Most of the state-of-the-art architectures\nuse transformers or diffusion-based models to perform this task, despite the\nheavy computational burden that they require. In particular, transformers use\nself- and cross-attention layers which have large memory footprint, while\ndiffusion models require high inference time. To overcome the above, this paper\nexplores a novel design of Mamba, an emergent State-Space Model (SSM), called\nMamba-ST, to perform style transfer. To do so, we adapt Mamba linear equation\nto simulate the behavior of cross-attention layers, which are able to combine\ntwo separate embeddings into a single output, but drastically reducing memory\nusage and time complexity. We modified the Mamba's inner equations so to accept\ninputs from, and combine, two separate data streams. To the best of our\nknowledge, this is the first attempt to adapt the equations of SSMs to a vision\ntask like style transfer without requiring any other module like\ncross-attention or custom normalization layers. An extensive set of experiments\ndemonstrates the superiority and efficiency of our method in performing style\ntransfer compared to transformers and diffusion models. Results show improved\nquality in terms of both ArtFID and FID metrics. Code is available at\nhttps://github.com/FilippoBotti/MambaST.\n","authors":["Filippo Botti","Alex Ergasti","Leonardo Rossi","Tomaso Fontanini","Claudio Ferrari","Massimo Bertozzi","Andrea Prati"],"pdf_url":"https://arxiv.org/pdf/2409.10385v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10365v1","updated":"2024-09-16T15:11:00Z","published":"2024-09-16T15:11:00Z","title":"Robust image representations with counterfactual contrastive learning","summary":"  Contrastive pretraining can substantially increase model generalisation and\ndownstream performance. However, the quality of the learned representations is\nhighly dependent on the data augmentation strategy applied to generate positive\npairs. Positive contrastive pairs should preserve semantic meaning while\ndiscarding unwanted variations related to the data acquisition domain.\nTraditional contrastive pipelines attempt to simulate domain shifts through\npre-defined generic image transformations. However, these do not always mimic\nrealistic and relevant domain variations for medical imaging such as scanner\ndifferences. To tackle this issue, we herein introduce counterfactual\ncontrastive learning, a novel framework leveraging recent advances in causal\nimage synthesis to create contrastive positive pairs that faithfully capture\nrelevant domain variations. Our method, evaluated across five datasets\nencompassing both chest radiography and mammography data, for two established\ncontrastive objectives (SimCLR and DINO-v2), outperforms standard contrastive\nlearning in terms of robustness to acquisition shift. Notably, counterfactual\ncontrastive learning achieves superior downstream performance on both\nin-distribution and on external datasets, especially for images acquired with\nscanners under-represented in the training set. Further experiments show that\nthe proposed framework extends beyond acquisition shifts, with models trained\nwith counterfactual contrastive learning substantially improving subgroup\nperformance across biological sex.\n","authors":["M√©lanie Roschewitz","Fabio De Sousa Ribeiro","Tian Xia","Galvin Khara","Ben Glocker"],"pdf_url":"https://arxiv.org/pdf/2409.10365v1.pdf","comment":"Code available at\n  https://github.com/biomedia-mira/counterfactual-contrastive/"},{"id":"http://arxiv.org/abs/2409.10362v1","updated":"2024-09-16T15:10:07Z","published":"2024-09-16T15:10:07Z","title":"Frequency-Guided Masking for Enhanced Vision Self-Supervised Learning","summary":"  We present a novel frequency-based Self-Supervised Learning (SSL) approach\nthat significantly enhances its efficacy for pre-training. Prior work in this\ndirection masks out pre-defined frequencies in the input image and employs a\nreconstruction loss to pre-train the model. While achieving promising results,\nsuch an implementation has two fundamental limitations as identified in our\npaper. First, using pre-defined frequencies overlooks the variability of image\nfrequency responses. Second, pre-trained with frequency-filtered images, the\nresulting model needs relatively more data to adapt to naturally looking images\nduring fine-tuning. To address these drawbacks, we propose FOurier transform\ncompression with seLf-Knowledge distillation (FOLK), integrating two dedicated\nideas. First, inspired by image compression, we adaptively select the\nmasked-out frequencies based on image frequency responses, creating more\nsuitable SSL tasks for pre-training. Second, we employ a two-branch framework\nempowered by knowledge distillation, enabling the model to take both the\nfiltered and original images as input, largely reducing the burden of\ndownstream tasks. Our experimental results demonstrate the effectiveness of\nFOLK in achieving competitive performance to many state-of-the-art SSL methods\nacross various downstream tasks, including image classification, few-shot\nlearning, and semantic segmentation.\n","authors":["Amin Karimi Monsefi","Mengxi Zhou","Nastaran Karimi Monsefi","Ser-Nam Lim","Wei-Lun Chao","Rajiv Ramnath"],"pdf_url":"https://arxiv.org/pdf/2409.10362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10357v1","updated":"2024-09-16T15:06:12Z","published":"2024-09-16T15:06:12Z","title":"2D or not 2D: How Does the Dimensionality of Gesture Representation\n  Affect 3D Co-Speech Gesture Generation?","summary":"  Co-speech gestures are fundamental for communication. The advent of recent\ndeep learning techniques has facilitated the creation of lifelike, synchronous\nco-speech gestures for Embodied Conversational Agents. \"In-the-wild\" datasets,\naggregating video content from platforms like YouTube via human pose detection\ntechnologies, provide a feasible solution by offering 2D skeletal sequences\naligned with speech. Concurrent developments in lifting models enable the\nconversion of these 2D sequences into 3D gesture databases. However, it is\nimportant to note that the 3D poses estimated from the 2D extracted poses are,\nin essence, approximations of the ground-truth, which remains in the 2D domain.\nThis distinction raises questions about the impact of gesture representation\ndimensionality on the quality of generated motions - a topic that, to our\nknowledge, remains largely unexplored. Our study examines the effect of using\neither 2D or 3D joint coordinates as training data on the performance of\nspeech-to-gesture deep generative models. We employ a lifting model for\nconverting generated 2D pose sequences into 3D and assess how gestures created\ndirectly in 3D stack up against those initially generated in 2D and then\nconverted to 3D. We perform an objective evaluation using widely used metrics\nin the gesture generation field as well as a user study to qualitatively\nevaluate the different approaches.\n","authors":["T√©o Guichoux","Laure Soulier","Nicolas Obin","Catherine Pelachaud"],"pdf_url":"https://arxiv.org/pdf/2409.10357v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2406.15111"},{"id":"http://arxiv.org/abs/2409.10353v1","updated":"2024-09-16T15:04:14Z","published":"2024-09-16T15:04:14Z","title":"Taming Diffusion Models for Image Restoration: A Review","summary":"  Diffusion models have achieved remarkable progress in generative modelling,\nparticularly in enhancing image quality to conform to human preferences.\nRecently, these models have also been applied to low-level computer vision for\nphoto-realistic image restoration (IR) in tasks such as image denoising,\ndeblurring, dehazing, etc. In this review paper, we introduce key constructions\nin diffusion models and survey contemporary techniques that make use of\ndiffusion models in solving general IR tasks. Furthermore, we point out the\nmain challenges and limitations of existing diffusion-based IR frameworks and\nprovide potential directions for future work.\n","authors":["Ziwei Luo","Fredrik K. Gustafsson","Zheng Zhao","Jens Sj√∂lund","Thomas B. Sch√∂n"],"pdf_url":"https://arxiv.org/pdf/2409.10353v1.pdf","comment":"Review paper; any comments and suggestions are most welcome!"},{"id":"http://arxiv.org/abs/2409.10350v1","updated":"2024-09-16T15:01:28Z","published":"2024-09-16T15:01:28Z","title":"Point2Graph: An End-to-end Point Cloud-based 3D Open-Vocabulary Scene\n  Graph for Robot Navigation","summary":"  Current open-vocabulary scene graph generation algorithms highly rely on both\n3D scene point cloud data and posed RGB-D images and thus have limited\napplications in scenarios where RGB-D images or camera poses are not readily\navailable. To solve this problem, we propose Point2Graph, a novel end-to-end\npoint cloud-based 3D open-vocabulary scene graph generation framework in which\nthe requirement of posed RGB-D image series is eliminated. This hierarchical\nframework contains room and object detection/segmentation and open-vocabulary\nclassification. For the room layer, we leverage the advantage of merging the\ngeometry-based border detection algorithm with the learning-based region\ndetection to segment rooms and create a \"Snap-Lookup\" framework for\nopen-vocabulary room classification. In addition, we create an end-to-end\npipeline for the object layer to detect and classify 3D objects based solely on\n3D point cloud data. Our evaluation results show that our framework can\noutperform the current state-of-the-art (SOTA) open-vocabulary object and room\nsegmentation and classification algorithm on widely used real-scene datasets.\n","authors":["Yifan Xu","Ziming Luo","Qianwei Wang","Vineet Kamat","Carol Menassa"],"pdf_url":"https://arxiv.org/pdf/2409.10350v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2211.01783v2","updated":"2024-09-16T15:00:17Z","published":"2022-11-03T13:17:53Z","title":"Quantifying and Learning Static vs. Dynamic Information in Deep\n  Spatiotemporal Networks","summary":"  There is limited understanding of the information captured by deep\nspatiotemporal models in their intermediate representations. For example, while\nevidence suggests that action recognition algorithms are heavily influenced by\nvisual appearance in single frames, no quantitative methodology exists for\nevaluating such static bias in the latent representation compared to bias\ntoward dynamics. We tackle this challenge by proposing an approach for\nquantifying the static and dynamic biases of any spatiotemporal model, and\napply our approach to three tasks, action recognition, automatic video object\nsegmentation (AVOS) and video instance segmentation (VIS). Our key findings\nare: (i) Most examined models are biased toward static information. (ii) Some\ndatasets that are assumed to be biased toward dynamics are actually biased\ntoward static information. (iii) Individual channels in an architecture can be\nbiased toward static, dynamic or a combination of the two. (iv) Most models\nconverge to their culminating biases in the first half of training. We then\nexplore how these biases affect performance on dynamically biased datasets. For\naction recognition, we propose StaticDropout, a semantically guided dropout\nthat debiases a model from static information toward dynamics. For AVOS, we\ndesign a better combination of fusion and cross connection layers compared with\nprevious architectures.\n","authors":["Matthew Kowal","Mennatullah Siam","Md Amirul Islam","Neil D. B. Bruce","Richard P. Wildes","Konstantinos G. Derpanis"],"pdf_url":"https://arxiv.org/pdf/2211.01783v2.pdf","comment":"TPAMI 2024. arXiv admin note: substantial text overlap with\n  arXiv:2206.02846"},{"id":"http://arxiv.org/abs/2409.10339v1","updated":"2024-09-16T14:52:22Z","published":"2024-09-16T14:52:22Z","title":"VAE-QWGAN: Improving Quantum GANs for High Resolution Image Generation","summary":"  This paper presents a novel hybrid quantum generative model, the VAE-QWGAN,\nwhich combines the strengths of a classical Variational AutoEncoder (VAE) with\na hybrid Quantum Wasserstein Generative Adversarial Network (QWGAN). The\nVAE-QWGAN integrates the VAE decoder and QGAN generator into a single quantum\nmodel with shared parameters, utilizing the VAE's encoder for latent vector\nsampling during training. To generate new data from the trained model at\ninference, input latent vectors are sampled from a Gaussian Mixture Model\n(GMM), learnt on the training latent vectors. This, in turn, enhances the\ndiversity and quality of generated images. We evaluate the model's performance\non MNIST/Fashion-MNIST datasets, and demonstrate improved quality and diversity\nof generated images compared to existing approaches.\n","authors":["Aaron Mark Thomas","Sharu Theresa Jose"],"pdf_url":"https://arxiv.org/pdf/2409.10339v1.pdf","comment":"5 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.10335v1","updated":"2024-09-16T14:46:36Z","published":"2024-09-16T14:46:36Z","title":"Phys3DGS: Physically-based 3D Gaussian Splatting for Inverse Rendering","summary":"  We propose two novel ideas (adoption of deferred rendering and mesh-based\nrepresentation) to improve the quality of 3D Gaussian splatting (3DGS) based\ninverse rendering. We first report a problem incurred by hidden Gaussians,\nwhere Gaussians beneath the surface adversely affect the pixel color in the\nvolume rendering adopted by the existing methods. In order to resolve the\nproblem, we propose applying deferred rendering and report new problems\nincurred in a naive application of deferred rendering to the existing\n3DGS-based inverse rendering. In an effort to improve the quality of 3DGS-based\ninverse rendering under deferred rendering, we propose a novel two-step\ntraining approach which (1) exploits mesh extraction and utilizes a hybrid\nmesh-3DGS representation and (2) applies novel regularization methods to better\nexploit the mesh. Our experiments show that, under relighting, the proposed\nmethod offers significantly better rendering quality than the existing\n3DGS-based inverse rendering methods. Compared with the SOTA voxel grid-based\ninverse rendering method, it gives better rendering quality while offering\nreal-time rendering.\n","authors":["Euntae Choi","Sungjoo Yoo"],"pdf_url":"https://arxiv.org/pdf/2409.10335v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.10330v1","updated":"2024-09-16T14:40:47Z","published":"2024-09-16T14:40:47Z","title":"DRIVE: Dependable Robust Interpretable Visionary Ensemble Framework in\n  Autonomous Driving","summary":"  Recent advancements in autonomous driving have seen a paradigm shift towards\nend-to-end learning paradigms, which map sensory inputs directly to driving\nactions, thereby enhancing the robustness and adaptability of autonomous\nvehicles. However, these models often sacrifice interpretability, posing\nsignificant challenges to trust, safety, and regulatory compliance. To address\nthese issues, we introduce DRIVE -- Dependable Robust Interpretable Visionary\nEnsemble Framework in Autonomous Driving, a comprehensive framework designed to\nimprove the dependability and stability of explanations in end-to-end\nunsupervised autonomous driving models. Our work specifically targets the\ninherent instability problems observed in the Driving through the Concept\nGridlock (DCG) model, which undermine the trustworthiness of its explanations\nand decision-making processes. We define four key attributes of DRIVE:\nconsistent interpretability, stable interpretability, consistent output, and\nstable output. These attributes collectively ensure that explanations remain\nreliable and robust across different scenarios and perturbations. Through\nextensive empirical evaluations, we demonstrate the effectiveness of our\nframework in enhancing the stability and dependability of explanations, thereby\naddressing the limitations of current models. Our contributions include an\nin-depth analysis of the dependability issues within the DCG model, a rigorous\ndefinition of DRIVE with its fundamental properties, a framework to implement\nDRIVE, and novel metrics for evaluating the dependability of concept-based\nexplainable autonomous driving models. These advancements lay the groundwork\nfor the development of more reliable and trusted autonomous driving systems,\npaving the way for their broader acceptance and deployment in real-world\napplications.\n","authors":["Songning Lai","Tianlang Xue","Hongru Xiao","Lijie Hu","Jiemin Wu","Ninghui Feng","Runwei Guan","Haicheng Liao","Zhenning Li","Yutao Yue"],"pdf_url":"https://arxiv.org/pdf/2409.10330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10329v1","updated":"2024-09-16T14:39:15Z","published":"2024-09-16T14:39:15Z","title":"InfoDisent: Explainability of Image Classification Models by Information\n  Disentanglement","summary":"  Understanding the decisions made by image classification networks is a\ncritical area of research in deep learning. This task is traditionally divided\ninto two distinct approaches: post-hoc methods and intrinsic methods. Post-hoc\nmethods, such as GradCam, aim to interpret the decisions of pre-trained models\nby identifying regions of the image where the network focuses its attention.\nHowever, these methods provide only a high-level overview, making it difficult\nto fully understand the network's decision-making process. Conversely,\nintrinsic methods, like prototypical parts models, offer a more detailed\nunderstanding of network predictions but are constrained by specific\narchitectures, training methods, and datasets.\n  In this paper, we introduce InfoDisent, a hybrid model that combines the\nadvantages of both approaches. By utilizing an information bottleneck,\nInfoDisent disentangles the information in the final layer of a pre-trained\ndeep network, enabling the breakdown of classification decisions into basic,\nunderstandable atomic components. Unlike standard prototypical parts\napproaches, InfoDisent can interpret the decisions of pre-trained\nclassification networks and be used for making classification decisions,\nsimilar to intrinsic models. We validate the effectiveness of InfoDisent on\nbenchmark datasets such as ImageNet, CUB-200-2011, Stanford Cars, and Stanford\nDogs for both convolutional and transformer backbones.\n","authors":["≈Åukasz Struski","Jacek Tabor"],"pdf_url":"https://arxiv.org/pdf/2409.10329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10327v1","updated":"2024-09-16T14:38:26Z","published":"2024-09-16T14:38:26Z","title":"Baking Relightable NeRF for Real-time Direct/Indirect Illumination\n  Rendering","summary":"  Relighting, which synthesizes a novel view under a given lighting condition\n(unseen in training time), is a must feature for immersive photo-realistic\nexperience. However, real-time relighting is challenging due to high\ncomputation cost of the rendering equation which requires shape and material\ndecomposition and visibility test to model shadow. Additionally, for indirect\nillumination, additional computation of rendering equation on each secondary\nsurface point (where reflection occurs) is required rendering real-time\nrelighting challenging. We propose a novel method that executes a CNN renderer\nto compute primary surface points and rendering parameters, required for direct\nillumination. We also present a lightweight hash grid-based renderer, for\nindirect illumination, which is recursively executed to perform the secondary\nray tracing process. Both renderers are trained in a distillation from a\npre-trained teacher model and provide real-time physically-based rendering\nunder unseen lighting condition at a negligible loss of rendering quality.\n","authors":["Euntae Choi","Vincent Carpentier","Seunghun Shin","Sungjoo Yoo"],"pdf_url":"https://arxiv.org/pdf/2409.10327v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2310.10461v3","updated":"2024-09-16T14:24:48Z","published":"2023-10-16T14:42:22Z","title":"Model Selection of Anomaly Detectors in the Absence of Labeled\n  Validation Data","summary":"  Anomaly detection is the task of identifying abnormal samples in large\nunlabeled datasets. While the advent of foundation models has produced powerful\nzero-shot anomaly detection methods, their deployment in practice is often\nhindered by the absence of labeled validation data -- without it, their\ndetection performance cannot be evaluated reliably. In this work, we propose\nSWSA (Selection With Synthetic Anomalies): a general-purpose framework to\nselect image-based anomaly detectors without labeled validation data. Instead\nof collecting labeled validation data, we generate synthetic anomalies without\nany training or fine-tuning, using only a small support set of normal images.\nOur synthetic anomalies are used to create detection tasks that compose a\nvalidation framework for model selection. In an empirical study, we evaluate\nSWSA with three types of synthetic anomalies and on two selection tasks: model\nselection of image-based anomaly detectors and prompt selection for CLIP-based\nanomaly detection. SWSA often selects models and prompts that match selections\nmade with a ground-truth validation set, outperforming baseline selection\nstrategies.\n","authors":["Clement Fung","Chen Qiu","Aodong Li","Maja Rudolph"],"pdf_url":"https://arxiv.org/pdf/2310.10461v3.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2409.10297v1","updated":"2024-09-16T14:02:18Z","published":"2024-09-16T14:02:18Z","title":"On Synthetic Texture Datasets: Challenges, Creation, and Curation","summary":"  The influence of textures on machine learning models has been an ongoing\ninvestigation, specifically in texture bias/learning, interpretability, and\nrobustness. However, due to the lack of large and diverse texture data\navailable, the findings in these works have been limited, as more comprehensive\nevaluations have not been feasible. Image generative models are able to provide\ndata creation at scale, but utilizing these models for texture synthesis has\nbeen unexplored and poses additional challenges both in creating accurate\ntexture images and validating those images. In this work, we introduce an\nextensible methodology and corresponding new dataset for generating\nhigh-quality, diverse texture images capable of supporting a broad set of\ntexture-based tasks. Our pipeline consists of: (1) developing prompts from a\nrange of descriptors to serve as input to text-to-image models, (2) adopting\nand adapting Stable Diffusion pipelines to generate and filter the\ncorresponding images, and (3) further filtering down to the highest quality\nimages. Through this, we create the Prompted Textures Dataset (PTD), a dataset\nof 362,880 texture images that span 56 textures. During the process of\ngenerating images, we find that NSFW safety filters in image generation\npipelines are highly sensitive to texture (and flag up to 60\\% of our texture\nimages), uncovering a potential bias in these models and presenting unique\nchallenges when working with texture data. Through both standard metrics and a\nhuman evaluation, we find that our dataset is high quality and diverse.\n","authors":["Blaine Hoak","Patrick McDaniel"],"pdf_url":"https://arxiv.org/pdf/2409.10297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10293v1","updated":"2024-09-16T13:59:43Z","published":"2024-09-16T13:59:43Z","title":"SPAC: Sampling-based Progressive Attribute Compression for Dense Point\n  Clouds","summary":"  We propose an end-to-end attribute compression method for dense point clouds.\nThe proposed method combines a frequency sampling module, an adaptive scale\nfeature extraction module with geometry assistance, and a global hyperprior\nentropy model. The frequency sampling module uses a Hamming window and the Fast\nFourier Transform to extract high-frequency components of the point cloud. The\ndifference between the original point cloud and the sampled point cloud is\ndivided into multiple sub-point clouds. These sub-point clouds are then\npartitioned using an octree, providing a structured input for feature\nextraction. The feature extraction module integrates adaptive convolutional\nlayers and uses offset-attention to capture both local and global features.\nThen, a geometry-assisted attribute feature refinement module is used to refine\nthe extracted attribute features. Finally, a global hyperprior model is\nintroduced for entropy encoding. This model propagates hyperprior parameters\nfrom the deepest (base) layer to the other layers, further enhancing the\nencoding efficiency. At the decoder, a mirrored network is used to\nprogressively restore features and reconstruct the color attribute through\ntransposed convolutional layers. The proposed method encodes base layer\ninformation at a low bitrate and progressively adds enhancement layer\ninformation to improve reconstruction accuracy. Compared to the latest G-PCC\ntest model (TMC13v23) under the MPEG common test conditions (CTCs), the\nproposed method achieved an average Bjontegaard delta bitrate reduction of\n24.58% for the Y component (21.23% for YUV combined) on the MPEG Category Solid\ndataset and 22.48% for the Y component (17.19% for YUV combined) on the MPEG\nCategory Dense dataset. This is the first instance of a learning-based codec\noutperforming the G-PCC standard on these datasets under the MPEG CTCs.\n","authors":["Xiaolong Mao","Hui Yuan","Tian Guo","Shiqi Jiang","Raouf Hamzaoui","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2409.10293v1.pdf","comment":"136pages, 13 figures"},{"id":"http://arxiv.org/abs/2409.10291v1","updated":"2024-09-16T13:58:42Z","published":"2024-09-16T13:58:42Z","title":"Anatomical Positional Embeddings","summary":"  We propose a self-supervised model producing 3D anatomical positional\nembeddings (APE) of individual medical image voxels. APE encodes voxels'\nanatomical closeness, i.e., voxels of the same organ or nearby organs always\nhave closer positional embeddings than the voxels of more distant body parts.\nIn contrast to the existing models of anatomical positional embeddings, our\nmethod is able to efficiently produce a map of voxel-wise embeddings for a\nwhole volumetric input image, which makes it an optimal choice for different\ndownstream applications. We train our APE model on 8400 publicly available CT\nimages of abdomen and chest regions. We demonstrate its superior performance\ncompared with the existing models on anatomical landmark retrieval and\nweakly-supervised few-shot localization of 13 abdominal organs. As a practical\napplication, we show how to cheaply train APE to crop raw CT images to\ndifferent anatomical regions of interest with 0.99 recall, while reducing the\nimage volume by 10-100 times. The code and the pre-trained APE model are\navailable at https://github.com/mishgon/ape .\n","authors":["Mikhail Goncharov","Valentin Samokhin","Eugenia Soboleva","Roman Sokolov","Boris Shirokikh","Mikhail Belyaev","Anvar Kurmukov","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2409.10291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10286v1","updated":"2024-09-16T13:47:52Z","published":"2024-09-16T13:47:52Z","title":"Enhancing Image Classification in Small and Unbalanced Datasets through\n  Synthetic Data Augmentation","summary":"  Accurate and robust medical image classification is a challenging task,\nespecially in application domains where available annotated datasets are small\nand present high imbalance between target classes. Considering that data\nacquisition is not always feasible, especially for underrepresented classes,\nour approach introduces a novel synthetic augmentation strategy using\nclass-specific Variational Autoencoders (VAEs) and latent space interpolation\nto improve discrimination capabilities.\n  By generating realistic, varied synthetic data that fills feature space gaps,\nwe address issues of data scarcity and class imbalance. The method presented in\nthis paper relies on the interpolation of latent representations within each\nclass, thus enriching the training set and improving the model's\ngeneralizability and diagnostic accuracy. The proposed strategy was tested in a\nsmall dataset of 321 images created to train and validate an automatic method\nfor assessing the quality of cleanliness of esophagogastroduodenoscopy images.\nBy combining real and synthetic data, an increase of over 18\\% in the accuracy\nof the most challenging underrepresented class was observed. The proposed\nstrategy not only benefited the underrepresented class but also led to a\ngeneral improvement in other metrics, including a 6\\% increase in global\naccuracy and precision.\n","authors":["Neil De La Fuente","Mireia Maj√≥","Irina Luzko","Henry C√≥rdova","Gloria Fern√°ndez-Esparrach","Jorge Bernal"],"pdf_url":"https://arxiv.org/pdf/2409.10286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10272v1","updated":"2024-09-16T13:34:26Z","published":"2024-09-16T13:34:26Z","title":"Performance of Human Annotators in Object Detection and Segmentation of\n  Remotely Sensed Data","summary":"  This study introduces a laboratory experiment designed to assess the\ninfluence of annotation strategies, levels of imbalanced data, and prior\nexperience, on the performance of human annotators. The experiment focuses on\nlabeling aerial imagery, using ArcGIS Pro tools, to detect and segment\nsmall-scale photovoltaic solar panels, selected as a case study for rectangular\nobjects. The experiment is conducted using images with a pixel size of\n0.15\\textbf{$m$}, involving both expert and non-expert participants, across\ndifferent setup strategies and target-background ratio datasets. Our findings\nindicate that human annotators generally perform more effectively in object\ndetection than in segmentation tasks. A marked tendency to commit more Type II\nerrors (False Negatives, i.e., undetected objects) than Type I errors (False\nPositives, i.e. falsely detecting objects that do not exist) was observed\nacross all experimental setups and conditions, suggesting a consistent bias in\ndetection and segmentation processes. Performance was better in tasks with\nhigher target-background ratios (i.e., more objects per unit area). Prior\nexperience did not significantly impact performance and may, in some cases,\neven lead to overestimation in segmentation. These results provide evidence\nthat human annotators are relatively cautious and tend to identify objects only\nwhen they are confident about them, prioritizing underestimation over\noverestimation. Annotators' performance is also influenced by object scarcity,\nshowing a decline in areas with extremely imbalanced datasets and a low ratio\nof target-to-background. These findings may enhance annotation strategies for\nremote sensing research while efficient human annotators are crucial in an era\ncharacterized by growing demands for high-quality training data to improve\nsegmentation and detection models.\n","authors":["Roni Blushtein-Livnon","Tal Svoray","Michael Dorman"],"pdf_url":"https://arxiv.org/pdf/2409.10272v1.pdf","comment":"14 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2409.10269v1","updated":"2024-09-16T13:25:42Z","published":"2024-09-16T13:25:42Z","title":"BAFNet: Bilateral Attention Fusion Network for Lightweight Semantic\n  Segmentation of Urban Remote Sensing Images","summary":"  Large-scale semantic segmentation networks often achieve high performance,\nwhile their application can be challenging when faced with limited sample sizes\nand computational resources. In scenarios with restricted network size and\ncomputational complexity, models encounter significant challenges in capturing\nlong-range dependencies and recovering detailed information in images. We\npropose a lightweight bilateral semantic segmentation network called bilateral\nattention fusion network (BAFNet) to efficiently segment high-resolution urban\nremote sensing images. The model consists of two paths, namely dependency path\nand remote-local path. The dependency path utilizes large kernel attention to\nacquire long-range dependencies in the image. Besides, multi-scale local\nattention and efficient remote attention are designed to construct remote-local\npath. Finally, a feature aggregation module is designed to effectively utilize\nthe different features of the two paths. Our proposed method was tested on\npublic high-resolution urban remote sensing datasets Vaihingen and Potsdam,\nwith mIoU reaching 83.20% and 86.53%, respectively. As a lightweight semantic\nsegmentation model, BAFNet not only outperforms advanced lightweight models in\naccuracy but also demonstrates comparable performance to non-lightweight\nstate-of-the-art methods on two datasets, despite a tenfold variance in\nfloating-point operations and a fifteenfold difference in network parameters.\n","authors":["Wentao Wang","Xili Wang"],"pdf_url":"https://arxiv.org/pdf/2409.10269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00346v2","updated":"2024-09-16T13:23:55Z","published":"2024-08-31T04:23:33Z","title":"SMAFormer: Synergistic Multi-Attention Transformer for Medical Image\n  Segmentation","summary":"  In medical image segmentation, specialized computer vision techniques,\nnotably transformers grounded in attention mechanisms and residual networks\nemploying skip connections, have been instrumental in advancing performance.\nNonetheless, previous models often falter when segmenting small, irregularly\nshaped tumors. To this end, we introduce SMAFormer, an efficient,\nTransformer-based architecture that fuses multiple attention mechanisms for\nenhanced segmentation of small tumors and organs. SMAFormer can capture both\nlocal and global features for medical image segmentation. The architecture\ncomprises two pivotal components. First, a Synergistic Multi-Attention (SMA)\nTransformer block is proposed, which has the benefits of Pixel Attention,\nChannel Attention, and Spatial Attention for feature enrichment. Second,\naddressing the challenge of information loss incurred during attention\nmechanism transitions and feature fusion, we design a Feature Fusion Modulator.\nThis module bolsters the integration between the channel and spatial attention\nby mitigating reshaping-induced information attrition. To evaluate our method,\nwe conduct extensive experiments on various medical image segmentation tasks,\nincluding multi-organ, liver tumor, and bladder tumor segmentation, achieving\nstate-of-the-art results. Code and models are available at:\n\\url{https://github.com/CXH-Research/SMAFormer}.\n","authors":["Fuchen Zheng","Xuhang Chen","Weihuang Liu","Haolun Li","Yingtie Lei","Jiahui He","Chi-Man Pun","Shounjun Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.00346v2.pdf","comment":"Accepted by IEEE BIBM 2024"},{"id":"http://arxiv.org/abs/2409.06509v2","updated":"2024-09-16T13:22:16Z","published":"2024-09-10T13:41:08Z","title":"Aligning Machine and Human Visual Representations across Abstraction\n  Levels","summary":"  Deep neural networks have achieved success across a wide range of\napplications, including as models of human behavior in vision tasks. However,\nneural network training and human learning differ in fundamental ways, and\nneural networks often fail to generalize as robustly as humans do, raising\nquestions regarding the similarity of their underlying representations. What is\nmissing for modern learning systems to exhibit more human-like behavior? We\nhighlight a key misalignment between vision models and humans: whereas human\nconceptual knowledge is hierarchically organized from fine- to coarse-scale\ndistinctions, model representations do not accurately capture all these levels\nof abstraction. To address this misalignment, we first train a teacher model to\nimitate human judgments, then transfer human-like structure from its\nrepresentations into pretrained state-of-the-art vision foundation models.\nThese human-aligned models more accurately approximate human behavior and\nuncertainty across a wide range of similarity tasks, including a new dataset of\nhuman judgments spanning multiple levels of semantic abstractions. They also\nperform better on a diverse set of machine learning tasks, increasing\ngeneralization and out-of-distribution robustness. Thus, infusing neural\nnetworks with additional human knowledge yields a best-of-both-worlds\nrepresentation that is both more consistent with human cognition and more\npractically useful, thus paving the way toward more robust, interpretable, and\nhuman-like artificial intelligence systems.\n","authors":["Lukas Muttenthaler","Klaus Greff","Frieda Born","Bernhard Spitzer","Simon Kornblith","Michael C. Mozer","Klaus-Robert M√ºller","Thomas Unterthiner","Andrew K. Lampinen"],"pdf_url":"https://arxiv.org/pdf/2409.06509v2.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2409.10262v1","updated":"2024-09-16T13:13:06Z","published":"2024-09-16T13:13:06Z","title":"Hydra-SGG: Hybrid Relation Assignment for One-stage Scene Graph\n  Generation","summary":"  DETR introduces a simplified one-stage framework for scene graph generation\n(SGG). However, DETR-based SGG models face two challenges: i) Sparse\nsupervision, as each image typically contains fewer than 10 relation\nannotations, while the models employ over 100 relation queries. This sparsity\narises because each ground truth relation is assigned to only one single query\nduring training. ii) False negative samples, since one ground truth relation\nmay have multiple queries with similar matching scores. These suboptimally\nmatched queries are simply treated as negative samples, causing the loss of\nvaluable supervisory signals. As a response, we devise Hydra-SGG, a one-stage\nSGG method that adopts a new Hybrid Relation Assignment. This assignment\ncombines a One-to-One Relation Assignment with a newly introduced IoU-based\nOne-to-Many Relation Assignment. Specifically, each ground truth is assigned to\nmultiple relation queries with high IoU subject-object boxes. This Hybrid\nRelation Assignment increases the number of positive training samples,\nalleviating sparse supervision. Moreover, we, for the first time, empirically\nshow that self-attention over relation queries helps reduce duplicated relation\npredictions. We, therefore, propose Hydra Branch, a parameter-sharing auxiliary\ndecoder without a self-attention layer. This design promotes One-to-Many\nRelation Assignment by enabling different queries to predict the same relation.\nHydra-SGG achieves state-of-the-art performance with 10.6 mR@20 and 16.0 mR@50\non VG150, while only requiring 12 training epochs. It also sets a new\nstate-of-the-art on Open Images V6 and and GQA.\n","authors":["Minghan Chen","Guikun Chen","Wenguan Wang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2409.10262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10259v1","updated":"2024-09-16T13:10:58Z","published":"2024-09-16T13:10:58Z","title":"Self-Updating Vehicle Monitoring Framework Employing Distributed\n  Acoustic Sensing towards Real-World Settings","summary":"  The recent emergence of Distributed Acoustic Sensing (DAS) technology has\nfacilitated the effective capture of traffic-induced seismic data. The\ntraffic-induced seismic wave is a prominent contributor to urban vibrations and\ncontain crucial information to advance urban exploration and governance.\nHowever, identifying vehicular movements within massive noisy data poses a\nsignificant challenge. In this study, we introduce a real-time semi-supervised\nvehicle monitoring framework tailored to urban settings. It requires only a\nsmall fraction of manual labels for initial training and exploits unlabeled\ndata for model improvement. Additionally, the framework can autonomously adapt\nto newly collected unlabeled data. Before DAS data undergo object detection as\ntwo-dimensional images to preserve spatial information, we leveraged\ncomprehensive one-dimensional signal preprocessing to mitigate noise.\nFurthermore, we propose a novel prior loss that incorporates the shapes of\nvehicular traces to track a single vehicle with varying speeds. To evaluate our\nmodel, we conducted experiments with seismic data from the Stanford 2 DAS\nArray. The results showed that our model outperformed the baseline model\nEfficient Teacher and its supervised counterpart, YOLO (You Only Look Once), in\nboth accuracy and robustness. With only 35 labeled images, our model surpassed\nYOLO's mAP 0.5:0.95 criterion by 18% and showed a 7% increase over Efficient\nTeacher. We conducted comparative experiments with multiple update strategies\nfor self-updating and identified an optimal approach. This approach surpasses\nthe performance of non-overfitting training conducted with all data in a single\npass.\n","authors":["Xi Wang","Xin Liu","Songming Zhu","Zhanwen Li","Lina Gao"],"pdf_url":"https://arxiv.org/pdf/2409.10259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10247v1","updated":"2024-09-16T12:58:03Z","published":"2024-09-16T12:58:03Z","title":"SOLVR: Submap Oriented LiDAR-Visual Re-Localisation","summary":"  This paper proposes SOLVR, a unified pipeline for learning based LiDAR-Visual\nre-localisation which performs place recognition and 6-DoF registration across\nsensor modalities. We propose a strategy to align the input sensor modalities\nby leveraging stereo image streams to produce metric depth predictions with\npose information, followed by fusing multiple scene views from a local window\nusing a probabilistic occupancy framework to expand the limited field-of-view\nof the camera. Additionally, SOLVR adopts a flexible definition of what\nconstitutes positive examples for different training losses, allowing us to\nsimultaneously optimise place recognition and registration performance.\nFurthermore, we replace RANSAC with a registration function that weights a\nsimple least-squares fitting with the estimated inlier likelihood of sparse\nkeypoint correspondences, improving performance in scenarios with a low inlier\nratio between the query and retrieved place. Our experiments on the KITTI and\nKITTI360 datasets show that SOLVR achieves state-of-the-art performance for\nLiDAR-Visual place recognition and registration, particularly improving\nregistration accuracy over larger distances between the query and retrieved\nplace.\n","authors":["Joshua Knights","Sebasti√°n Barbas Laina","Peyman Moghadam","Stefan Leutenegger"],"pdf_url":"https://arxiv.org/pdf/2409.10247v1.pdf","comment":"Submitted to ICRA2025"},{"id":"http://arxiv.org/abs/2409.10246v1","updated":"2024-09-16T12:56:23Z","published":"2024-09-16T12:56:23Z","title":"FGR-Net:Interpretable fundus imagegradeability classification based on\n  deepreconstruction learning","summary":"  The performance of diagnostic Computer-Aided Design (CAD) systems for retinal\ndiseases depends on the quality of the retinal images being screened. Thus,\nmany studies have been developed to evaluate and assess the quality of such\nretinal images. However, most of them did not investigate the relationship\nbetween the accuracy of the developed models and the quality of the\nvisualization of interpretability methods for distinguishing between gradable\nand non-gradable retinal images. Consequently, this paper presents a novel\nframework called FGR-Net to automatically assess and interpret underlying\nfundus image quality by merging an autoencoder network with a classifier\nnetwork. The FGR-Net model also provides an interpretable quality assessment\nthrough visualizations. In particular, FGR-Net uses a deep autoencoder to\nreconstruct the input image in order to extract the visual characteristics of\nthe input fundus images based on self-supervised learning. The extracted\nfeatures by the autoencoder are then fed into a deep classifier network to\ndistinguish between gradable and ungradable fundus images. FGR-Net is evaluated\nwith different interpretability methods, which indicates that the autoencoder\nis a key factor in forcing the classifier to focus on the relevant structures\nof the fundus images, such as the fovea, optic disk, and prominent blood\nvessels. Additionally, the interpretability methods can provide visual feedback\nfor ophthalmologists to understand how our model evaluates the quality of\nfundus images. The experimental results showed the superiority of FGR-Net over\nthe state-of-the-art quality assessment methods, with an accuracy of 89% and an\nF1-score of 87%.\n","authors":["Saif Khalid","Hatem A. Rashwan","Saddam Abdulwahab","Mohamed Abdel-Nasser","Facundo Manuel Quiroga","Domenec Puig"],"pdf_url":"https://arxiv.org/pdf/2409.10246v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10164v2","updated":"2024-09-16T12:42:47Z","published":"2024-03-15T10:18:06Z","title":"CoReEcho: Continuous Representation Learning for 2D+time\n  Echocardiography Analysis","summary":"  Deep learning (DL) models have been advancing automatic medical image\nanalysis on various modalities, including echocardiography, by offering a\ncomprehensive end-to-end training pipeline. This approach enables DL models to\nregress ejection fraction (EF) directly from 2D+time echocardiograms, resulting\nin superior performance. However, the end-to-end training pipeline makes the\nlearned representations less explainable. The representations may also fail to\ncapture the continuous relation among echocardiogram clips, indicating the\nexistence of spurious correlations, which can negatively affect the\ngeneralization. To mitigate this issue, we propose CoReEcho, a novel training\nframework emphasizing continuous representations tailored for direct EF\nregression. Our extensive experiments demonstrate that CoReEcho: 1) outperforms\nthe current state-of-the-art (SOTA) on the largest echocardiography dataset\n(EchoNet-Dynamic) with MAE of 3.90 & R2 of 82.44, and 2) provides robust and\ngeneralizable features that transfer more effectively in related downstream\ntasks. The code is publicly available at https://github.com/fadamsyah/CoReEcho.\n","authors":["Fadillah Adamsyah Maani","Numan Saeed","Aleksandr Matsun","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.10164v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10228v1","updated":"2024-09-16T12:23:35Z","published":"2024-09-16T12:23:35Z","title":"Robust Bird's Eye View Segmentation by Adapting DINOv2","summary":"  Extracting a Bird's Eye View (BEV) representation from multiple camera images\noffers a cost-effective, scalable alternative to LIDAR-based solutions in\nautonomous driving. However, the performance of the existing BEV methods drops\nsignificantly under various corruptions such as brightness and weather changes\nor camera failures. To improve the robustness of BEV perception, we propose to\nadapt a large vision foundational model, DINOv2, to BEV estimation using Low\nRank Adaptation (LoRA). Our approach builds on the strong representation space\nof DINOv2 by adapting it to the BEV task in a state-of-the-art framework,\nSimpleBEV. Our experiments show increased robustness of BEV perception under\nvarious corruptions, with increasing gains from scaling up the model and the\ninput resolution. We also showcase the effectiveness of the adapted\nrepresentations in terms of fewer learnable parameters and faster convergence\nduring training.\n","authors":["Merve Rabia Barƒ±n","G√∂rkay Aydemir","Fatma G√ºney"],"pdf_url":"https://arxiv.org/pdf/2409.10228v1.pdf","comment":"ECCV 2024 - 2nd Workshop on Vision-Centric Autonomous Driving (VCAD)"},{"id":"http://arxiv.org/abs/2409.10213v1","updated":"2024-09-16T12:04:26Z","published":"2024-09-16T12:04:26Z","title":"Neuromorphic Facial Analysis with Cross-Modal Supervision","summary":"  Traditional approaches for analyzing RGB frames are capable of providing a\nfine-grained understanding of a face from different angles by inferring\nemotions, poses, shapes, landmarks. However, when it comes to subtle movements\nstandard RGB cameras might fall behind due to their latency, making it hard to\ndetect micro-movements that carry highly informative cues to infer the true\nemotions of a subject. To address this issue, the usage of event cameras to\nanalyze faces is gaining increasing interest. Nonetheless, all the expertise\nmatured for RGB processing is not directly transferrable to neuromorphic data\ndue to a strong domain shift and intrinsic differences in how data is\nrepresented. The lack of labeled data can be considered one of the main causes\nof this gap, yet gathering data is harder in the event domain since it cannot\nbe crawled from the web and labeling frames should take into account event\naggregation rates and the fact that static parts might not be visible in\ncertain frames. In this paper, we first present FACEMORPHIC, a multimodal\ntemporally synchronized face dataset comprising both RGB videos and event\nstreams. The data is labeled at a video level with facial Action Units and also\ncontains streams collected with a variety of applications in mind, ranging from\n3D shape estimation to lip-reading. We then show how temporal synchronization\ncan allow effective neuromorphic face analysis without the need to manually\nannotate videos: we instead leverage cross-modal supervision bridging the\ndomain gap by representing face shapes in a 3D space.\n","authors":["Federico Becattini","Luca Cultrera","Lorenzo Berlincioni","Claudio Ferrari","Andrea Leonardo","Alberto Del Bimbo"],"pdf_url":"https://arxiv.org/pdf/2409.10213v1.pdf","comment":"Accepted for publication at the ECCV 2024 workshop on Neuromorphic\n  Vision: Advantages and Applications of Event Cameras (NEVI)"},{"id":"http://arxiv.org/abs/2407.15051v3","updated":"2024-09-16T12:01:22Z","published":"2024-07-21T04:39:06Z","title":"Prior Knowledge Integration via LLM Encoding and Pseudo Event Regulation\n  for Video Moment Retrieval","summary":"  In this paper, we investigate the feasibility of leveraging large language\nmodels (LLMs) for integrating general knowledge and incorporating pseudo-events\nas priors for temporal content distribution in video moment retrieval (VMR)\nmodels. The motivation behind this study arises from the limitations of using\nLLMs as decoders for generating discrete textual descriptions, which hinders\ntheir direct application to continuous outputs like salience scores and\ninter-frame embeddings that capture inter-frame relations. To overcome these\nlimitations, we propose utilizing LLM encoders instead of decoders. Through a\nfeasibility study, we demonstrate that LLM encoders effectively refine\ninter-concept relations in multimodal embeddings, even without being trained on\ntextual embeddings. We also show that the refinement capability of LLM encoders\ncan be transferred to other embeddings, such as BLIP and T5, as long as these\nembeddings exhibit similar inter-concept similarity patterns to CLIP\nembeddings. We present a general framework for integrating LLM encoders into\nexisting VMR architectures, specifically within the fusion module. Through\nexperimental validation, we demonstrate the effectiveness of our proposed\nmethods by achieving state-of-the-art performance in VMR. The source code can\nbe accessed at https://github.com/fletcherjiang/LLMEPET.\n","authors":["Yiyang Jiang","Wengyu Zhang","Xulu Zhang","Xiaoyong Wei","Chang Wen Chen","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2407.15051v3.pdf","comment":"Accepted to ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2409.10206v1","updated":"2024-09-16T11:55:45Z","published":"2024-09-16T11:55:45Z","title":"Garment Attribute Manipulation with Multi-level Attention","summary":"  In the rapidly evolving field of online fashion shopping, the need for more\npersonalized and interactive image retrieval systems has become paramount.\nExisting methods often struggle with precisely manipulating specific garment\nattributes without inadvertently affecting others. To address this challenge,\nwe propose GAMMA (Garment Attribute Manipulation with Multi-level Attention), a\nnovel framework that integrates attribute-disentangled representations with a\nmulti-stage attention-based architecture. GAMMA enables targeted manipulation\nof fashion image attributes, allowing users to refine their searches with high\naccuracy. By leveraging a dual-encoder Transformer and memory block, our model\nachieves state-of-the-art performance on popular datasets like Shopping100k and\nDeepFashion.\n","authors":["Vittorio Casula","Lorenzo Berlincioni","Luca Cultrera","Federico Becattini","Chiara Pero","Carmen Bisogni","Marco Bertini","Alberto Del Bimbo"],"pdf_url":"https://arxiv.org/pdf/2409.10206v1.pdf","comment":"Accepted for publication at the ECCV 2024 workshop FashionAI"},{"id":"http://arxiv.org/abs/2409.10202v1","updated":"2024-09-16T11:52:13Z","published":"2024-09-16T11:52:13Z","title":"SteeredMarigold: Steering Diffusion Towards Depth Completion of Largely\n  Incomplete Depth Maps","summary":"  Even if the depth maps captured by RGB-D sensors deployed in real\nenvironments are often characterized by large areas missing valid depth\nmeasurements, the vast majority of depth completion methods still assumes depth\nvalues covering all areas of the scene. To address this limitation, we\nintroduce SteeredMarigold, a training-free, zero-shot depth completion method\ncapable of producing metric dense depth, even for largely incomplete depth\nmaps. SteeredMarigold achieves this by using the available sparse depth points\nas conditions to steer a denoising diffusion probabilistic model. Our method\noutperforms relevant top-performing methods on the NYUv2 dataset, in tests\nwhere no depth was provided for a large area, achieving state-of-art\nperformance and exhibiting remarkable robustness against depth map\nincompleteness. Our code will be publicly available.\n","authors":["Jakub Gregorek","Lazaros Nalpantidis"],"pdf_url":"https://arxiv.org/pdf/2409.10202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10012v2","updated":"2024-09-16T11:46:36Z","published":"2024-08-19T14:05:58Z","title":"CLIPCleaner: Cleaning Noisy Labels with CLIP","summary":"  Learning with Noisy labels (LNL) poses a significant challenge for the\nMachine Learning community. Some of the most widely used approaches that select\nas clean samples for which the model itself (the in-training model) has high\nconfidence, e.g., `small loss', can suffer from the so called\n`self-confirmation' bias. This bias arises because the in-training model, is at\nleast partially trained on the noisy labels. Furthermore, in the classification\ncase, an additional challenge arises because some of the label noise is between\nclasses that are visually very similar (`hard noise'). This paper addresses\nthese challenges by proposing a method (\\textit{CLIPCleaner}) that leverages\nCLIP, a powerful Vision-Language (VL) model for constructing a zero-shot\nclassifier for efficient, offline, clean sample selection. This has the\nadvantage that the sample selection is decoupled from the in-training model and\nthat the sample selection is aware of the semantic and visual similarities\nbetween the classes due to the way that CLIP is trained. We provide theoretical\njustifications and empirical evidence to demonstrate the advantages of CLIP for\nLNL compared to conventional pre-trained models. Compared to current methods\nthat combine iterative sample selection with various techniques,\n\\textit{CLIPCleaner} offers a simple, single-step approach that achieves\ncompetitive or superior performance on benchmark datasets. To the best of our\nknowledge, this is the first time a VL model has been used for sample selection\nto address the problem of Learning with Noisy Labels (LNL), highlighting their\npotential in the domain.\n","authors":["Chen Feng","Georgios Tzimiropoulos","Ioannis Patras"],"pdf_url":"https://arxiv.org/pdf/2408.10012v2.pdf","comment":"Accepted to ACMMM2024. Codes are available at\n  https://github.com/MrChenFeng/CLIPCleaner_ACMMM2024"},{"id":"http://arxiv.org/abs/2409.10197v1","updated":"2024-09-16T11:43:19Z","published":"2024-09-16T11:43:19Z","title":"Fit and Prune: Fast and Training-free Visual Token Pruning for\n  Multi-modal Large Language Models","summary":"  Recent progress in Multimodal Large Language Models(MLLMs) often use large\nimage tokens to compensate the visual shortcoming of MLLMs, which not only\nexhibits obvious redundancy but also greatly exacerbates the already high\ncomputation. Token pruning is an effective solution for speeding up MLLMs, but\nwhen and how to drop tokens still remains a challenge. In this paper, we\npropose a novel and training-free approach for the effective visual token\npruning of MLLMs, termed FitPrune, which can quickly produce a complete pruning\nrecipe for MLLMs according to a pre-defined budget. Specifically, FitPrune\nconsiders token pruning as a statistical problem of MLLM and its objective is\nto find out an optimal pruning scheme that can minimize the divergence of the\nattention distributions before and after pruning. In practice, FitPrune can be\nquickly accomplished based on the attention statistics from a small batch of\ninference data, avoiding the expensive trials of MLLMs. According to the\npruning recipe, an MLLM can directly remove the redundant visual tokens of\ndifferent examples during inference. To validate FitPrune, we apply it to a set\nof recent MLLMs, including LLaVA-1.5, LLaVA-HR and LLaVA-NEXT, and conduct\nextensive experiments on a set of benchmarks. The experimental results show\nthat our FitPrune can not only reduce the computational complexity to a large\nextent, while retaining high performance, e.g., -54.9% FLOPs for LLaVA-NEXT\nwith only 0.5% accuracy drop. Notably, the pruning recipe can be obtained in\nabout 5 minutes. Our code is available at https://github.com/ywh187/FitPrune.\n","authors":["Weihao Ye","Qiong Wu","Wenhao Lin","Yiyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.10197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10196v1","updated":"2024-09-16T11:42:15Z","published":"2024-09-16T11:42:15Z","title":"NEUSIS: A Compositional Neuro-Symbolic Framework for Autonomous\n  Perception, Reasoning, and Planning in Complex UAV Search Missions","summary":"  This paper addresses the problem of autonomous UAV search missions, where a\nUAV must locate specific Entities of Interest (EOIs) within a time limit, based\non brief descriptions in large, hazard-prone environments with keep-out zones.\nThe UAV must perceive, reason, and make decisions with limited and uncertain\ninformation. We propose NEUSIS, a compositional neuro-symbolic system designed\nfor interpretable UAV search and navigation in realistic scenarios. NEUSIS\nintegrates neuro-symbolic visual perception, reasoning, and grounding (GRiD) to\nprocess raw sensory inputs, maintains a probabilistic world model for\nenvironment representation, and uses a hierarchical planning component (SNaC)\nfor efficient path planning. Experimental results from simulated urban search\nmissions using AirSim and Unreal Engine show that NEUSIS outperforms a\nstate-of-the-art (SOTA) vision-language model and a SOTA search planning model\nin success rate, search efficiency, and 3D localization. These results\ndemonstrate the effectiveness of our compositional neuro-symbolic approach in\nhandling complex, real-world scenarios, making it a promising solution for\nautonomous UAV systems in search missions.\n","authors":["Zhixi Cai","Cristian Rojas Cardenas","Kevin Leo","Chenyuan Zhang","Kal Backman","Hanbing Li","Boying Li","Mahsa Ghorbanali","Stavya Datta","Lizhen Qu","Julian Gutierrez Santiago","Alexey Ignatiev","Yuan-Fang Li","Mor Vered","Peter J Stuckey","Maria Garcia de la Banda","Hamid Rezatofighi"],"pdf_url":"https://arxiv.org/pdf/2409.10196v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15002v2","updated":"2024-09-16T11:38:10Z","published":"2024-08-27T12:34:41Z","title":"Knowledge Discovery in Optical Music Recognition: Enhancing Information\n  Retrieval with Instance Segmentation","summary":"  Optical Music Recognition (OMR) automates the transcription of musical\nnotation from images into machine-readable formats like MusicXML, MEI, or MIDI,\nsignificantly reducing the costs and time of manual transcription. This study\nexplores knowledge discovery in OMR by applying instance segmentation using\nMask R-CNN to enhance the detection and delineation of musical symbols in sheet\nmusic. Unlike Optical Character Recognition (OCR), OMR must handle the\nintricate semantics of Common Western Music Notation (CWMN), where symbol\nmeanings depend on shape, position, and context. Our approach leverages\ninstance segmentation to manage the density and overlap of musical symbols,\nfacilitating more precise information retrieval from music scores. Evaluations\non the DoReMi and MUSCIMA++ datasets demonstrate substantial improvements, with\nour method achieving a mean Average Precision (mAP) of up to 59.70\\% in dense\nsymbol environments, achieving comparable results to object detection.\nFurthermore, using traditional computer vision techniques, we add a parallel\nstep for staff detection to infer the pitch for the recognised symbols. This\nstudy emphasises the role of pixel-wise segmentation in advancing accurate\nmusic symbol recognition, contributing to knowledge discovery in OMR. Our\nfindings indicate that instance segmentation provides more precise\nrepresentations of musical symbols, particularly in densely populated scores,\nadvancing OMR technology. We make our implementation, pre-processing scripts,\ntrained models, and evaluation results publicly available to support further\nresearch and development.\n","authors":["Elona Shatri","George Fazekas"],"pdf_url":"https://arxiv.org/pdf/2408.15002v2.pdf","comment":"8 pages content and one references, accepted version at the\n  International Conference on Knowledge Discovery and Information Retrieval\n  2024, Porto, Portugal"},{"id":"http://arxiv.org/abs/2409.10180v1","updated":"2024-09-16T11:18:57Z","published":"2024-09-16T11:18:57Z","title":"RealDiff: Real-world 3D Shape Completion using Self-Supervised Diffusion\n  Models","summary":"  Point cloud completion aims to recover the complete 3D shape of an object\nfrom partial observations. While approaches relying on synthetic shape priors\nachieved promising results in this domain, their applicability and\ngeneralizability to real-world data are still limited. To tackle this problem,\nwe propose a self-supervised framework, namely RealDiff, that formulates point\ncloud completion as a conditional generation problem directly on real-world\nmeasurements. To better deal with noisy observations without resorting to\ntraining on synthetic data, we leverage additional geometric cues.\nSpecifically, RealDiff simulates a diffusion process at the missing object\nparts while conditioning the generation on the partial input to address the\nmultimodal nature of the task. We further regularize the training by matching\nobject silhouettes and depth maps, predicted by our method, with the externally\nestimated ones. Experimental results show that our method consistently\noutperforms state-of-the-art methods in real-world point cloud completion.\n","authors":["Ba≈üak Melis √ñcal","Maxim Tatarchenko","Sezer Karaoglu","Theo Gevers"],"pdf_url":"https://arxiv.org/pdf/2409.10180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10178v1","updated":"2024-09-16T11:17:33Z","published":"2024-09-16T11:17:33Z","title":"ExelMap: Explainable Element-based HD-Map Change Detection and Update","summary":"  Acquisition and maintenance are central problems in deploying high-definition\n(HD) maps for autonomous driving, with two lines of research prevalent in\ncurrent literature: Online HD map generation and HD map change detection.\nHowever, the generated map's quality is currently insufficient for safe\ndeployment, and many change detection approaches fail to precisely localize and\nextract the changed map elements, hence lacking explainability and hindering a\npotential fleet-based cooperative HD map update. In this paper, we propose the\nnovel task of explainable element-based HD map change detection and update. In\nextending recent approaches that use online mapping techniques informed with an\noutdated map prior for HD map updating, we present ExelMap, an explainable\nelement-based map updating strategy that specifically identifies changed map\nelements. In this context, we discuss how currently used metrics fail to\ncapture change detection performance, while allowing for unfair comparison\nbetween prior-less and prior-informed map generation methods. Finally, we\npresent an experimental study on real-world changes related to pedestrian\ncrossings of the Argoverse 2 Map Change Dataset. To the best of our knowledge,\nthis is the first comprehensive problem investigation of real-world end-to-end\nelement-based HD map change detection and update, and ExelMap the first\nproposed solution.\n","authors":["Lena Wild","Ludvig Ericson","Rafael Valencia","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2409.10178v1.pdf","comment":"17 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.10175v1","updated":"2024-09-16T11:10:48Z","published":"2024-09-16T11:10:48Z","title":"VideoRun2D: Cost-Effective Markerless Motion Capture for Sprint\n  Biomechanics","summary":"  Sprinting is a determinant ability, especially in team sports. The kinematics\nof the sprint have been studied in the past using different methods specially\ndeveloped considering human biomechanics and, among those methods, markerless\nsystems stand out as very cost-effective. On the other hand, we have now\nmultiple general methods for pixel and body tracking based on recent machine\nlearning breakthroughs with excellent performance in body tracking, but these\nexcellent trackers do not generally consider realistic human biomechanics. This\ninvestigation first adapts two of these general trackers (MoveNet and\nCoTracker) for realistic biomechanical analysis and then evaluate them in\ncomparison to manual tracking (with key points manually marked using the\nsoftware Kinovea).\n  Our best resulting markerless body tracker particularly adapted for sprint\nbiomechanics is termed VideoRun2D. The experimental development and assessment\nof VideoRun2D is reported on forty sprints recorded with a video camera from 5\ndifferent subjects, focusing our analysis in 3 key angles in sprint\nbiomechanics: inclination of the trunk, flex extension of the hip and the knee.\nThe CoTracker method showed huge differences compared to the manual labeling\napproach. However, the angle curves were correctly estimated by the MoveNet\nmethod, finding errors between 3.2{\\deg} and 5.5{\\deg}.\n  In conclusion, our proposed VideoRun2D based on MoveNet core seems to be a\nhelpful tool for evaluating sprint kinematics in some scenarios. On the other\nhand, the observed precision of this first version of VideoRun2D as a\nmarkerless sprint analysis system may not be yet enough for highly demanding\napplications. Future research lines towards that purpose are also discussed at\nthe end: better tracking post-processing and user- and time-dependent\nadaptation.\n","authors":["Gonzalo Garrido-Lopez","Luis F. Gomez","Julian Fierrez","Aythami Morales","Ruben Tolosana","Javier Rueda","Enrique Navarro"],"pdf_url":"https://arxiv.org/pdf/2409.10175v1.pdf","comment":"Preprint of the paper presented to the Workshop on IAPR International\n  Conference on Pattern Recognition (ICPR) 2024"},{"id":"http://arxiv.org/abs/2406.09407v2","updated":"2024-09-16T10:55:09Z","published":"2024-06-13T17:59:44Z","title":"Towards Evaluating the Robustness of Visual State Space Models","summary":"  Vision State Space Models (VSSMs), a novel architecture that combines the\nstrengths of recurrent neural networks and latent variable models, have\ndemonstrated remarkable performance in visual perception tasks by efficiently\ncapturing long-range dependencies and modeling complex visual dynamics.\nHowever, their robustness under natural and adversarial perturbations remains a\ncritical concern. In this work, we present a comprehensive evaluation of VSSMs'\nrobustness under various perturbation scenarios, including occlusions, image\nstructure, common corruptions, and adversarial attacks, and compare their\nperformance to well-established architectures such as transformers and\nConvolutional Neural Networks. Furthermore, we investigate the resilience of\nVSSMs to object-background compositional changes on sophisticated benchmarks\ndesigned to test model performance in complex visual scenes. We also assess\ntheir robustness on object detection and segmentation tasks using corrupted\ndatasets that mimic real-world scenarios. To gain a deeper understanding of\nVSSMs' adversarial robustness, we conduct a frequency-based analysis of\nadversarial attacks, evaluating their performance against low-frequency and\nhigh-frequency perturbations. Our findings highlight the strengths and\nlimitations of VSSMs in handling complex visual corruptions, offering valuable\ninsights for future research. Our code and models will be available at\nhttps://github.com/HashmatShadab/MambaRobustness.\n","authors":["Hashmat Shadab Malik","Fahad Shamshad","Muzammal Naseer","Karthik Nandakumar","Fahad Shahbaz Khan","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2406.09407v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10161v1","updated":"2024-09-16T10:52:16Z","published":"2024-09-16T10:52:16Z","title":"SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using\n  Gaussian Splatting","summary":"  Sim2Real transfer, particularly for manipulation policies relying on RGB\nimages, remains a critical challenge in robotics due to the significant domain\nshift between synthetic and real-world visual data. In this paper, we propose\nSplatSim, a novel framework that leverages Gaussian Splatting as the primary\nrendering primitive to reduce the Sim2Real gap for RGB-based manipulation\npolicies. By replacing traditional mesh representations with Gaussian Splats in\nsimulators, SplatSim produces highly photorealistic synthetic data while\nmaintaining the scalability and cost-efficiency of simulation. We demonstrate\nthe effectiveness of our framework by training manipulation policies within\nSplatSim}and deploying them in the real world in a zero-shot manner, achieving\nan average success rate of 86.25%, compared to 97.5% for policies trained on\nreal-world data.\n","authors":["Mohammad Nomaan Qureshi","Sparsh Garg","Francisco Yandun","David Held","George Kantor","Abhishesh Silwal"],"pdf_url":"https://arxiv.org/pdf/2409.10161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10156v1","updated":"2024-09-16T10:41:29Z","published":"2024-09-16T10:41:29Z","title":"Contrastive Learning for Character Detection in Ancient Greek Papyri","summary":"  This thesis investigates the effectiveness of SimCLR, a contrastive learning\ntechnique, in Greek letter recognition, focusing on the impact of various\naugmentation techniques. We pretrain the SimCLR backbone using the Alpub\ndataset (pretraining dataset) and fine-tune it on a smaller ICDAR dataset\n(finetuning dataset) to compare SimCLR's performance against traditional\nbaseline models, which use cross-entropy and triplet loss functions.\nAdditionally, we explore the role of different data augmentation strategies,\nessential for the SimCLR training process. Methodologically, we examine three\nprimary approaches: (1) a baseline model using cross-entropy loss, (2) a\ntriplet embedding model with a classification layer, and (3) a SimCLR\npretrained model with a classification layer. Initially, we train the baseline,\ntriplet, and SimCLR models using 93 augmentations on ResNet-18 and ResNet-50\nnetworks with the ICDAR dataset. From these, the top four augmentations are\nselected using a statistical t-test. Pretraining of SimCLR is conducted on the\nAlpub dataset, followed by fine-tuning on the ICDAR dataset. The triplet loss\nmodel undergoes a similar process, being pretrained on the top four\naugmentations before fine-tuning on ICDAR. Our experiments show that SimCLR\ndoes not outperform the baselines in letter recognition tasks. The baseline\nmodel with cross-entropy loss demonstrates better performance than both SimCLR\nand the triplet loss model. This study provides a detailed evaluation of\ncontrastive learning for letter recognition, highlighting SimCLR's limitations\nwhile emphasizing the strengths of traditional supervised learning models in\nthis task. We believe SimCLR's cropping strategies may cause a semantic shift\nin the input image, reducing training effectiveness despite the large\npretraining dataset. Our code is available at\nhttps://github.com/DIVA-DIA/MT_augmentation_and_contrastive_learning/.\n","authors":["Vedasri Nakka","Andreas Fischer","Rolf Ingold","Lars Vogtlin"],"pdf_url":"https://arxiv.org/pdf/2409.10156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10151v1","updated":"2024-09-16T10:27:30Z","published":"2024-09-16T10:27:30Z","title":"AutoPET Challenge III: Testing the Robustness of Generalized Dice Focal\n  Loss trained 3D Residual UNet for FDG and PSMA Lesion Segmentation from\n  Whole-Body PET/CT Images","summary":"  Automated segmentation of cancerous lesions in PET/CT scans is a crucial\nfirst step in quantitative image analysis. However, training deep learning\nmodels for segmentation with high accuracy is particularly challenging due to\nthe variations in lesion size, shape, and radiotracer uptake. These lesions can\nappear in different parts of the body, often near healthy organs that also\nexhibit considerable uptake, making the task even more complex. As a result,\ncreating an effective segmentation model for routine PET/CT image analysis is\nchallenging. In this study, we utilized a 3D Residual UNet model and employed\nthe Generalized Dice Focal Loss function to train the model on the AutoPET\nChallenge 2024 dataset. We conducted a 5-fold cross-validation and used an\naverage ensembling technique using the models from the five folds. In the\npreliminary test phase for Task-1, the average ensemble achieved a mean Dice\nSimilarity Coefficient (DSC) of 0.6687, mean false negative volume (FNV) of\n10.9522 ml and mean false positive volume (FPV) 2.9684 ml. More details about\nthe algorithm can be found on our GitHub repository:\nhttps://github.com/ahxmeds/autosegnet2024.git. The training code has been\nshared via the repository: https://github.com/ahxmeds/autopet2024.git.\n","authors":["Shadab Ahamed"],"pdf_url":"https://arxiv.org/pdf/2409.10151v1.pdf","comment":"11 pages, 5 figures, 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:2309.13553"},{"id":"http://arxiv.org/abs/2308.05659v2","updated":"2024-09-16T10:25:47Z","published":"2023-08-10T15:58:28Z","title":"AD-CLIP: Adapting Domains in Prompt Space Using CLIP","summary":"  Although deep learning models have shown impressive performance on supervised\nlearning tasks, they often struggle to generalize well when the training\n(source) and test (target) domains differ. Unsupervised domain adaptation (DA)\nhas emerged as a popular solution to this problem. However, current DA\ntechniques rely on visual backbones, which may lack semantic richness. Despite\nthe potential of large-scale vision-language foundation models like CLIP, their\neffectiveness for DA has yet to be fully explored. To address this gap, we\nintroduce \\textsc{AD-CLIP}, a domain-agnostic prompt learning strategy for CLIP\nthat aims to solve the DA problem in the prompt space. We leverage the frozen\nvision backbone of CLIP to extract both image style (domain) and content\ninformation, which we apply to learn prompt tokens. Our prompts are designed to\nbe domain-invariant and class-generalizable, by conditioning prompt learning on\nimage style and content features simultaneously. We use standard supervised\ncontrastive learning in the source domain, while proposing an entropy\nminimization strategy to align domains in the embedding space given the target\ndomain data. We also consider a scenario where only target domain samples are\navailable during testing, without any source domain data, and propose a\ncross-domain style mapping network to hallucinate domain-agnostic tokens. Our\nextensive experiments on three benchmark DA datasets demonstrate the\neffectiveness of \\textsc{AD-CLIP} compared to existing literature. Code is\navailable at \\url{https://github.com/mainaksingha01/AD-CLIP}\n","authors":["Mainak Singha","Harsh Pal","Ankit Jha","Biplab Banerjee"],"pdf_url":"https://arxiv.org/pdf/2308.05659v2.pdf","comment":"10 pages, 8 figures, 4 tables. Accepted at OOD-CV, ICCV Workshop,\n  2023"},{"id":"http://arxiv.org/abs/2409.10143v1","updated":"2024-09-16T10:13:34Z","published":"2024-09-16T10:13:34Z","title":"P2U-SLAM: A Monocular Wide-FoV SLAM System Based on Point Uncertainty\n  and Pose Uncertainty","summary":"  This paper presents P2U-SLAM, a visual Simultaneous Localization And Mapping\n(SLAM) system with a wide Field of View (FoV) camera, which utilizes pose\nuncertainty and point uncertainty. While the wide FoV enables considerable\nrepetitive observations of historical map points for matching cross-view\nfeatures, the data properties of the historical map points and the poses of\nhistorical keyframes have changed during the optimization process. The neglect\nof data property changes triggers the absence of a partial information matrix\nin optimization and leads to the risk of long-term positioning performance\ndegradation. The purpose of our research is to reduce the risk of the wide\nfield of view visual input to the SLAM system. Based on the conditional\nprobability model, this work reveals the definite impact of the above data\nproperties changes on the optimization process, concretizes it as point\nuncertainty and pose uncertainty, and gives a specific mathematical form.\nP2U-SLAM respectively embeds point uncertainty and pose uncertainty into the\ntracking module and local mapping, and updates these uncertainties after each\noptimization operation including local mapping, map merging, and loop closing.\nWe present an exhaustive evaluation in 27 sequences from two popular public\ndatasets with wide-FoV visual input. P2U-SLAM shows excellent performance\ncompared with other state-of-the-art methods. The source code will be made\npublicly available at https://github.com/BambValley/P2U-SLAM.\n","authors":["Yufan Zhang","Kailun Yang","Ze Wang","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2409.10143v1.pdf","comment":"The source code will be made publicly available at\n  https://github.com/BambValley/P2U-SLAM"},{"id":"http://arxiv.org/abs/2409.10141v1","updated":"2024-09-16T10:13:06Z","published":"2024-09-16T10:13:06Z","title":"PSHuman: Photorealistic Single-view Human Reconstruction using\n  Cross-Scale Diffusion","summary":"  Detailed and photorealistic 3D human modeling is essential for various\napplications and has seen tremendous progress. However, full-body\nreconstruction from a monocular RGB image remains challenging due to the\nill-posed nature of the problem and sophisticated clothing topology with\nself-occlusions. In this paper, we propose PSHuman, a novel framework that\nexplicitly reconstructs human meshes utilizing priors from the multiview\ndiffusion model. It is found that directly applying multiview diffusion on\nsingle-view human images leads to severe geometric distortions, especially on\ngenerated faces. To address it, we propose a cross-scale diffusion that models\nthe joint probability distribution of global full-body shape and local facial\ncharacteristics, enabling detailed and identity-preserved novel-view generation\nwithout any geometric distortion. Moreover, to enhance cross-view body shape\nconsistency of varied human poses, we condition the generative model on\nparametric models like SMPL-X, which provide body priors and prevent unnatural\nviews inconsistent with human anatomy. Leveraging the generated multi-view\nnormal and color images, we present SMPLX-initialized explicit human carving to\nrecover realistic textured human meshes efficiently. Extensive experimental\nresults and quantitative evaluations on CAPE and THuman2.1 datasets demonstrate\nPSHumans superiority in geometry details, texture fidelity, and generalization\ncapability.\n","authors":["Peng Li","Wangguandong Zheng","Yuan Liu","Tao Yu","Yangguang Li","Xingqun Qi","Mengfei Li","Xiaowei Chi","Siyu Xia","Wei Xue","Wenhan Luo","Qifeng Liu","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2409.10141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07392v2","updated":"2024-09-16T09:33:17Z","published":"2024-05-12T23:00:53Z","title":"NGD-SLAM: Towards Real-Time Dynamic SLAM without GPU","summary":"  Existing SLAM (Simultaneous Localization and Mapping) algorithms have\nachieved remarkable localization accuracy in dynamic environments by using deep\nlearning techniques to identify dynamic objects. However, they usually require\nGPUs to operate in real-time. Therefore, this paper proposes an open-source\nreal-time dynamic SLAM system that runs solely on CPU by incorporating a mask\nprediction mechanism, which allows the deep learning method and the camera\ntracking to run entirely in parallel at different frequencies. Our SLAM system\nfurther introduces a dual-stage optical flow tracking approach and employs a\nhybrid usage of optical flow and ORB features, enhancing efficiency and\nrobustness by selectively allocating computational resources to input frames.\nCompared with previous methods, our system maintains high localization accuracy\nin dynamic environments while achieving a tracking frame rate of 56 FPS on a\nlaptop CPU, proving that deep learning methods are feasible for dynamic SLAM\nwithout GPU support. To the best of our knowledge, this is the first SLAM\nsystem to achieve this.\n","authors":["Yuhao Zhang","Mihai Bujanca","Mikel Luj√°n"],"pdf_url":"https://arxiv.org/pdf/2405.07392v2.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.10120v1","updated":"2024-09-16T09:32:04Z","published":"2024-09-16T09:32:04Z","title":"Data-Centric Strategies for Overcoming PET/CT Heterogeneity: Insights\n  from the AutoPET III Lesion Segmentation Challenge","summary":"  The third autoPET challenge introduced a new data-centric task this year,\nshifting the focus from model development to improving metastatic lesion\nsegmentation on PET/CT images through data quality and handling strategies. In\nresponse, we developed targeted methods to enhance segmentation performance\ntailored to the characteristics of PET/CT imaging. Our approach encompasses two\nkey elements. First, to address potential alignment errors between CT and PET\nmodalities as well as the prevalence of punctate lesions, we modified the\nbaseline data augmentation scheme and extended it with misalignment\naugmentation. This adaptation aims to improve segmentation accuracy,\nparticularly for tiny metastatic lesions. Second, to tackle the variability in\nimage dimensions significantly affecting the prediction time, we implemented a\ndynamic ensembling and test-time augmentation (TTA) strategy. This method\noptimizes the use of ensembling and TTA within a 5-minute prediction time\nlimit, effectively leveraging the generalization potential for both small and\nlarge images. Both of our solutions are designed to be robust across different\ntracers and institutional settings, offering a general, yet imaging-specific\napproach to the multi-tracer and multi-institutional challenges of the\ncompetition. We made the challenge repository with our modifications publicly\navailable at \\url{https://github.com/MIC-DKFZ/miccai2024_autopet3_datacentric}.\n","authors":["Balint Kovacs","Shuhan Xiao","Maximilian Rokuss","Constantin Ulrich","Fabian Isensee","Klaus H. Maier-Hein"],"pdf_url":"https://arxiv.org/pdf/2409.10120v1.pdf","comment":"Contribution to the data-centric task of the autoPET III Challenge\n  2024"},{"id":"http://arxiv.org/abs/2304.05653v2","updated":"2024-09-16T09:10:00Z","published":"2023-04-12T07:16:55Z","title":"A Closer Look at the Explainability of Contrastive Language-Image\n  Pre-training","summary":"  Contrastive language-image pre-training (CLIP) is a powerful vision-language\nmodel that has shown great benefits for various tasks. However, we have\nidentified some issues with its explainability, which undermine its credibility\nand limit the capacity for related tasks. Specifically, we find that CLIP tends\nto focus on background regions rather than foregrounds, with noisy activations\nat irrelevant positions on the visualization results. These phenomena conflict\nwith conventional explainability methods based on the class attention map\n(CAM), where the raw model can highlight the local foreground regions using\nglobal supervision without alignment. To address these problems, we take a\ncloser look at its architecture and features. Based on thorough analyses, we\nfind the raw self-attentions link to inconsistent semantic regions, resulting\nin the opposite visualization. Besides, the noisy activations are owing to\nredundant features among categories. Building on these insights, we propose the\nCLIP Surgery for reliable CAM, a method that allows surgery-like modifications\nto the inference architecture and features, without further fine-tuning as\nclassical CAM methods. This approach significantly improves the explainability\nof CLIP, surpassing existing methods by large margins. Besides, it enables\nmultimodal visualization and extends the capacity of raw CLIP on\nopen-vocabulary tasks without extra alignment. The code is available at\nhttps://github.com/xmed-lab/CLIP_Surgery.\n","authors":["Yi Li","Hualiang Wang","Yiqun Duan","Jiheng Zhang","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2304.05653v2.pdf","comment":"30 pages, 11 figures, under review"},{"id":"http://arxiv.org/abs/2409.10104v1","updated":"2024-09-16T09:07:31Z","published":"2024-09-16T09:07:31Z","title":"A Comparative Study of Open Source Computer Vision Models for\n  Application on Small Data: The Case of CFRP Tape Laying","summary":"  In the realm of industrial manufacturing, Artificial Intelligence (AI) is\nplaying an increasing role, from automating existing processes to aiding in the\ndevelopment of new materials and techniques. However, a significant challenge\narises in smaller, experimental processes characterized by limited training\ndata availability, questioning the possibility to train AI models in such small\ndata contexts. In this work, we explore the potential of Transfer Learning to\naddress this challenge, specifically investigating the minimum amount of data\nrequired to develop a functional AI model. For this purpose, we consider the\nuse case of quality control of Carbon Fiber Reinforced Polymer (CFRP) tape\nlaying in aerospace manufacturing using optical sensors. We investigate the\nbehavior of different open-source computer vision models with a continuous\nreduction of the training data. Our results show that the amount of data\nrequired to successfully train an AI model can be drastically reduced, and the\nuse of smaller models does not necessarily lead to a loss of performance.\n","authors":["Thomas Fraunholz","Dennis Rall","Tim K√∂hler","Alfons Schuster","Monika Mayer","Lars Larsen"],"pdf_url":"https://arxiv.org/pdf/2409.10104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10101v1","updated":"2024-09-16T09:05:40Z","published":"2024-09-16T09:05:40Z","title":"Adaptive Segmentation-Based Initialization for Steered Mixture of\n  Experts Image Regression","summary":"  Kernel image regression methods have shown to provide excellent efficiency in\nmany image processing task, such as image and light-field compression, Gaussian\nSplatting, denoising and super-resolution. The estimation of parameters for\nthese methods frequently employ gradient descent iterative optimization, which\nposes significant computational burden for many applications. In this paper, we\nintroduce a novel adaptive segmentation-based initialization method targeted\nfor optimizing Steered-Mixture-of Experts (SMoE) gating networks and\nRadial-Basis-Function (RBF) networks with steering kernels. The novel\ninitialization method allocates kernels into pre-calculated image segments. The\noptimal number of kernels, kernel positions, and steering parameters are\nderived per segment in an iterative optimization and kernel sparsification\nprocedure. The kernel information from \"local\" segments is then transferred\ninto a \"global\" initialization, ready for use in iterative optimization of\nSMoE, RBF, and related kernel image regression methods. Results show that\ndrastic objective and subjective quality improvements are achievable compared\nto widely used regular grid initialization, \"state-of-the-art\" K-Means\ninitialization and previously introduced segmentation-based initialization\nmethods, while also drastically improving the sparsity of the regression\nmodels. For same quality, the novel initialization results in models with\naround 50% reduction of kernels. In addition, a significant reduction of\nconvergence time is achieved, with overall run-time savings of up to 50%. The\nsegmentation-based initialization strategy itself admits heavy parallel\ncomputation; in theory, it may be divided into as many tasks as there are\nsegments in the images. By accessing only four parallel GPUs, run-time savings\nof already 50% for initialization are achievable.\n","authors":["Yi-Hsin Li","Sebastian Knorr","M√•rten Sj√∂str√∂m","Thomas Sikora"],"pdf_url":"https://arxiv.org/pdf/2409.10101v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10095v1","updated":"2024-09-16T08:54:03Z","published":"2024-09-16T08:54:03Z","title":"Human Insights Driven Latent Space for Different Driving Perspectives: A\n  Unified Encoder for Efficient Multi-Task Inference","summary":"  Autonomous driving holds great potential to transform road safety and traffic\nefficiency by minimizing human error and reducing congestion. A key challenge\nin realizing this potential is the accurate estimation of steering angles,\nwhich is essential for effective vehicle navigation and control. Recent\nbreakthroughs in deep learning have made it possible to estimate steering\nangles directly from raw camera inputs. However, the limited available\nnavigation data can hinder optimal feature learning, impacting the system's\nperformance in complex driving scenarios. In this paper, we propose a shared\nencoder trained on multiple computer vision tasks critical for urban\nnavigation, such as depth, pose, and 3D scene flow estimation, as well as\nsemantic, instance, panoptic, and motion segmentation. By incorporating diverse\nvisual information used by humans during navigation, this unified encoder might\nenhance steering angle estimation. To achieve effective multi-task learning\nwithin a single encoder, we introduce a multi-scale feature network for pose\nestimation to improve depth learning. Additionally, we employ knowledge\ndistillation from a multi-backbone model pretrained on these navigation tasks\nto stabilize training and boost performance. Our findings demonstrate that a\nshared backbone trained on diverse visual tasks is capable of providing overall\nperception capabilities. While our performance in steering angle estimation is\ncomparable to existing methods, the integration of human-like perception\nthrough multi-task learning holds significant potential for advancing\nautonomous driving systems. More details and the pretrained model are available\nat https://hi-computervision.github.io/uni-encoder/.\n","authors":["Huy-Dung Nguyen","Anass Bairouk","Mirjana Maras","Wei Xiao","Tsun-Hsuan Wang","Patrick Chareyre","Ramin Hasani","Marc Blanchon","Daniela Rus"],"pdf_url":"https://arxiv.org/pdf/2409.10095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10094v1","updated":"2024-09-16T08:50:47Z","published":"2024-09-16T08:50:47Z","title":"DDoS: Diffusion Distribution Similarity for Out-of-Distribution\n  Detection","summary":"  Out-of-Distribution (OoD) detection determines whether the given samples are\nfrom the training distribution of the classifier-under-protection, i.e., the\nIn-Distribution (InD), or from a different OoD. Latest researches introduce\ndiffusion models pre-trained on InD data to advocate OoD detection by\ntransferring an OoD image into a generated one that is close to InD, so that\none could capture the distribution disparities between original and generated\nimages to detect OoD data. Existing diffusion-based detectors adopt perceptual\nmetrics on the two images to measure such disparities, but ignore a fundamental\nfact: Perceptual metrics are devised essentially for human-perceived\nsimilarities of low-level image patterns, e.g., textures and colors, and are\nnot advisable in evaluating distribution disparities, since images with\ndifferent low-level patterns could possibly come from the same distribution. To\naddress this issue, we formulate a diffusion-based detection framework that\nconsiders the distribution similarity between a tested image and its generated\ncounterpart via a novel proper similarity metric in the informative feature\nspace and probability space learned by the classifier-under-protection. An\nanomaly-removal strategy is further presented to enlarge such distribution\ndisparities by removing abnormal OoD information in the feature space to\nfacilitate the detection. Extensive empirical results unveil the insufficiency\nof perceptual metrics and the effectiveness of our distribution similarity\nframework with new state-of-the-art detection performance.\n","authors":["Kun Fang","Qinghua Tao","Zuopeng Yang","Xiaolin Huang","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2409.10094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10090v1","updated":"2024-09-16T08:44:17Z","published":"2024-09-16T08:44:17Z","title":"MotionCom: Automatic and Motion-Aware Image Composition with LLM and\n  Video Diffusion Prior","summary":"  This work presents MotionCom, a training-free motion-aware diffusion based\nimage composition, enabling automatic and seamless integration of target\nobjects into new scenes with dynamically coherent results without finetuning or\noptimization. Traditional approaches in this area suffer from two significant\nlimitations: they require manual planning for object placement and often\ngenerate static compositions lacking motion realism. MotionCom addresses these\nissues by utilizing a Large Vision Language Model (LVLM) for intelligent\nplanning, and a Video Diffusion prior for motion-infused image synthesis,\nstreamlining the composition process. Our multi-modal Chain-of-Thought (CoT)\nprompting with LVLM automates the strategic placement planning of foreground\nobjects, considering their potential motion and interaction within the scenes.\nComplementing this, we propose a novel method MotionPaint to distill\nmotion-aware information from pretrained video diffusion models in the\ngeneration phase, ensuring that these objects are not only seamlessly\nintegrated but also endowed with realistic motion. Extensive quantitative and\nqualitative results highlight MotionCom's superiority, showcasing its\nefficiency in streamlining the planning process and its capability to produce\ncompositions that authentically depict motion and interaction.\n","authors":["Weijing Tao","Xiaofeng Yang","Miaomiao Cui","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2409.10090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10089v1","updated":"2024-09-16T08:43:37Z","published":"2024-09-16T08:43:37Z","title":"Cross-modality image synthesis from TOF-MRA to CTA using diffusion-based\n  models","summary":"  Cerebrovascular disease often requires multiple imaging modalities for\naccurate diagnosis, treatment, and monitoring. Computed Tomography Angiography\n(CTA) and Time-of-Flight Magnetic Resonance Angiography (TOF-MRA) are two\ncommon non-invasive angiography techniques, each with distinct strengths in\naccessibility, safety, and diagnostic accuracy. While CTA is more widely used\nin acute stroke due to its faster acquisition times and higher diagnostic\naccuracy, TOF-MRA is preferred for its safety, as it avoids radiation exposure\nand contrast agent-related health risks. Despite the predominant role of CTA in\nclinical workflows, there is a scarcity of open-source CTA data, limiting the\nresearch and development of AI models for tasks such as large vessel occlusion\ndetection and aneurysm segmentation. This study explores diffusion-based\nimage-to-image translation models to generate synthetic CTA images from TOF-MRA\ninput. We demonstrate the modality conversion from TOF-MRA to CTA and show that\ndiffusion models outperform a traditional U-Net-based approach. Our work\ncompares different state-of-the-art diffusion architectures and samplers,\noffering recommendations for optimal model performance in this cross-modality\ntranslation task.\n","authors":["Alexander Koch","Orhun Utku Aydin","Adam Hilbert","Jana Rieger","Satoru Tanioka","Fujimaro Ishida","Dietmar Frey"],"pdf_url":"https://arxiv.org/pdf/2409.10089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03703v2","updated":"2024-09-16T08:43:13Z","published":"2024-04-04T07:49:39Z","title":"Mitigating analytical variability in fMRI results with style transfer","summary":"  We propose a novel approach to improve the reproducibility of neuroimaging\nresults by converting statistic maps across different functional MRI pipelines.\nWe make the assumption that pipelines used to compute fMRI statistic maps can\nbe considered as a style component and we propose to use different generative\nmodels, among which, Generative Adversarial Networks (GAN) and Diffusion Models\n(DM) to convert statistic maps across different pipelines. We explore the\nperformance of multiple GAN frameworks, and design a new DM framework for\nunsupervised multi-domain styletransfer. We constrain the generation of 3D fMRI\nstatistic maps using the latent space of an auxiliary classifier that\ndistinguishes statistic maps from different pipelines and extend traditional\nsampling techniques used in DM to improve the transition performance. Our\nexperiments demonstrate that our proposed methods aresuccessful: pipelines can\nindeed be transferred as a style component, providing animportant source of\ndata augmentation for future medical studies.\n","authors":["Elodie Germani","Camille Maumet","Elisa Fromont"],"pdf_url":"https://arxiv.org/pdf/2404.03703v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10080v1","updated":"2024-09-16T08:37:09Z","published":"2024-09-16T08:37:09Z","title":"DAE-Fuse: An Adaptive Discriminative Autoencoder for Multi-Modality\n  Image Fusion","summary":"  Multi-modality image fusion aims to integrate complementary data information\nfrom different imaging modalities into a single image. Existing methods often\ngenerate either blurry fused images that lose fine-grained semantic information\nor unnatural fused images that appear perceptually cropped from the inputs. In\nthis work, we propose a novel two-phase discriminative autoencoder framework,\ntermed DAE-Fuse, that generates sharp and natural fused images. In the\nadversarial feature extraction phase, we introduce two discriminative blocks\ninto the encoder-decoder architecture, providing an additional adversarial loss\nto better guide feature extraction by reconstructing the source images. While\nthe two discriminative blocks are adapted in the attention-guided\ncross-modality fusion phase to distinguish the structural differences between\nthe fused output and the source inputs, injecting more naturalness into the\nresults. Extensive experiments on public infrared-visible, medical image\nfusion, and downstream object detection datasets demonstrate our method's\nsuperiority and generalizability in both quantitative and qualitative\nevaluations.\n","authors":["Yuchen Guo","Ruoxiang Xu","Rongcheng Li","Zhenghao Wu","Weifeng Su"],"pdf_url":"https://arxiv.org/pdf/2409.10080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10071v1","updated":"2024-09-16T08:21:22Z","published":"2024-09-16T08:21:22Z","title":"Towards Physically-Realizable Adversarial Attacks in Embodied Vision\n  Navigation","summary":"  The deployment of embodied navigation agents in safety-critical environments\nraises concerns about their vulnerability to adversarial attacks on deep neural\nnetworks. However, current attack methods often lack practicality due to\nchallenges in transitioning from the digital to the physical world, while\nexisting physical attacks for object detection fail to achieve both multi-view\neffectiveness and naturalness. To address this, we propose a practical attack\nmethod for embodied navigation by attaching adversarial patches with learnable\ntextures and opacity to objects. Specifically, to ensure effectiveness across\nvarying viewpoints, we employ a multi-view optimization strategy based on\nobject-aware sampling, which uses feedback from the navigation model to\noptimize the patch's texture. To make the patch inconspicuous to human\nobservers, we introduce a two-stage opacity optimization mechanism, where\nopacity is refined after texture optimization. Experimental results show our\nadversarial patches reduce navigation success rates by about 40%, outperforming\nprevious methods in practicality, effectiveness, and naturalness. Code is\navailable at:\n[https://github.com/chen37058/Physical-Attacks-in-Embodied-Navigation].\n","authors":["Meng Chen","Jiawei Tu","Chao Qi","Yonghao Dang","Feng Zhou","Wei Wei","Jianqin Yin"],"pdf_url":"https://arxiv.org/pdf/2409.10071v1.pdf","comment":"8 pages, 6 figures, submitted to the 2025 IEEE International\n  Conference on Robotics & Automation (ICRA)"},{"id":"http://arxiv.org/abs/2406.10723v3","updated":"2024-09-16T07:52:59Z","published":"2024-06-15T19:32:00Z","title":"Eye in the Sky: Detection and Compliance Monitoring of Brick Kilns using\n  Satellite Imagery","summary":"  Air pollution kills 7 million people annually. The brick manufacturing\nindustry accounts for 8%-14% of air pollution in the densely populated\nIndo-Gangetic plain. Due to the unorganized nature of brick kilns, policy\nviolation detection, such as proximity to human habitats, remains challenging.\nWhile previous studies have utilized computer vision-based machine learning\nmethods for brick kiln detection from satellite imagery, they utilize\nproprietary satellite data and rarely focus on compliance with government\npolicies. In this research, we introduce a scalable framework for brick kiln\ndetection and automatic compliance monitoring. We use Google Maps Static API to\ndownload the satellite imagery followed by the YOLOv8x model for detection. We\nidentified and hand-verified 19579 new brick kilns across 9 states within the\nIndo-Gangetic plain. Furthermore, we automate and test the compliance to the\npolicies affecting human habitats, rivers and hospitals. Our results show that\na substantial number of brick kilns do not meet the compliance requirements.\nOur framework offers a valuable tool for governments worldwide to automate and\nenforce policy regulations for brick kilns, addressing critical environmental\nand public health concerns.\n","authors":["Rishabh Mondal","Shataxi Dubey","Vannsh Jani","Shrimay Shah","Suraj Jaiswal","Zeel B Patel","Nipun Batra"],"pdf_url":"https://arxiv.org/pdf/2406.10723v3.pdf","comment":"The PI was not in favor of making the work public on arXiv as the\n  content is not yet ready to be released"},{"id":"http://arxiv.org/abs/2409.10041v1","updated":"2024-09-16T07:11:58Z","published":"2024-09-16T07:11:58Z","title":"DENSER: 3D Gaussians Splatting for Scene Reconstruction of Dynamic Urban\n  Environments","summary":"  This paper presents DENSER, an efficient and effective approach leveraging 3D\nGaussian splatting (3DGS) for the reconstruction of dynamic urban environments.\nWhile several methods for photorealistic scene representations, both implicitly\nusing neural radiance fields (NeRF) and explicitly using 3DGS have shown\npromising results in scene reconstruction of relatively complex dynamic scenes,\nmodeling the dynamic appearance of foreground objects tend to be challenging,\nlimiting the applicability of these methods to capture subtleties and details\nof the scenes, especially far dynamic objects. To this end, we propose DENSER,\na framework that significantly enhances the representation of dynamic objects\nand accurately models the appearance of dynamic objects in the driving scene.\nInstead of directly using Spherical Harmonics (SH) to model the appearance of\ndynamic objects, we introduce and integrate a new method aiming at dynamically\nestimating SH bases using wavelets, resulting in better representation of\ndynamic objects appearance in both space and time. Besides object appearance,\nDENSER enhances object shape representation through densification of its point\ncloud across multiple scene frames, resulting in faster convergence of model\ntraining. Extensive evaluations on KITTI dataset show that the proposed\napproach significantly outperforms state-of-the-art methods by a wide margin.\nSource codes and models will be uploaded to this repository\nhttps://github.com/sntubix/denser\n","authors":["Mahmud A. Mohamad","Gamal Elghazaly","Arthur Hubert","Raphael Frank"],"pdf_url":"https://arxiv.org/pdf/2409.10041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14997v2","updated":"2024-09-16T07:05:56Z","published":"2024-08-27T12:25:12Z","title":"Depth Restoration of Hand-Held Transparent Objects for Human-to-Robot\n  Handover","summary":"  Transparent objects are common in daily life, while their optical properties\npose challenges for RGB-D cameras to capture accurate depth information. This\nissue is further amplified when these objects are hand-held, as hand occlusions\nfurther complicate depth estimation. For assistant robots, however, accurately\nperceiving hand-held transparent objects is critical to effective human-robot\ninteraction. This paper presents a Hand-Aware Depth Restoration (HADR) method\nbased on creating an implicit neural representation function from a single\nRGB-D image. The proposed method utilizes hand posture as an important guidance\nto leverage semantic and geometric information of hand-object interaction. To\ntrain and evaluate the proposed method, we create a high-fidelity synthetic\ndataset named TransHand-14K with a real-to-sim data generation scheme.\nExperiments show that our method has better performance and generalization\nability compared with existing methods. We further develop a real-world\nhuman-to-robot handover system based on HADR, demonstrating its potential in\nhuman-robot interaction applications.\n","authors":["Ran Yu","Haixin Yu","Shoujie Li","Huang Yan","Ziwu Song","Wenbo Ding"],"pdf_url":"https://arxiv.org/pdf/2408.14997v2.pdf","comment":"7 pages, 7 figures, conference"},{"id":"http://arxiv.org/abs/2403.14376v2","updated":"2024-09-16T07:03:42Z","published":"2024-03-21T13:06:57Z","title":"InfNeRF: Towards Infinite Scale NeRF Rendering with O(log n) Space\n  Complexity","summary":"  The conventional mesh-based Level of Detail (LoD) technique, exemplified by\napplications such as Google Earth and many game engines, exhibits the\ncapability to holistically represent a large scene even the Earth, and achieves\nrendering with a space complexity of O(log n). This constrained data\nrequirement not only enhances rendering efficiency but also facilitates dynamic\ndata fetching, thereby enabling a seamless 3D navigation experience for users.\nIn this work, we extend this proven LoD technique to Neural Radiance Fields\n(NeRF) by introducing an octree structure to represent the scenes in different\nscales. This innovative approach provides a mathematically simple and elegant\nrepresentation with a rendering space complexity of O(log n), aligned with the\nefficiency of mesh-based LoD techniques. We also present a novel training\nstrategy that maintains a complexity of O(n). This strategy allows for parallel\ntraining with minimal overhead, ensuring the scalability and efficiency of our\nproposed method. Our contribution is not only in extending the capabilities of\nexisting techniques but also in establishing a foundation for scalable and\nefficient large-scale scene representation using NeRF and octree structures.\n","authors":["Jiabin Liang","Lanqing Zhang","Zhuoran Zhao","Xiangyu Xu"],"pdf_url":"https://arxiv.org/pdf/2403.14376v2.pdf","comment":"10 pages, version accepted by Siggraph Asia"},{"id":"http://arxiv.org/abs/2408.02245v2","updated":"2024-09-16T06:40:25Z","published":"2024-08-05T05:33:59Z","title":"A Two-Stage Progressive Pre-training using Multi-Modal Contrastive\n  Masked Autoencoders","summary":"  In this paper, we propose a new progressive pre-training method for image\nunderstanding tasks which leverages RGB-D datasets. The method utilizes\nMulti-Modal Contrastive Masked Autoencoder and Denoising techniques. Our\nproposed approach consists of two stages. In the first stage, we pre-train the\nmodel using contrastive learning to learn cross-modal representations. In the\nsecond stage, we further pre-train the model using masked autoencoding and\ndenoising/noise prediction used in diffusion models. Masked autoencoding\nfocuses on reconstructing the missing patches in the input modality using local\nspatial correlations, while denoising learns high frequency components of the\ninput data. Moreover, it incorporates global distillation in the second stage\nby leveraging the knowledge acquired in stage one. Our approach is scalable,\nrobust and suitable for pre-training RGB-D datasets. Extensive experiments on\nmultiple datasets such as ScanNet, NYUv2 and SUN RGB-D show the efficacy and\nsuperior performance of our approach. Specifically, we show an improvement of\n+1.3% mIoU against Mask3D on ScanNet semantic segmentation. We further\ndemonstrate the effectiveness of our approach in low-data regime by evaluating\nit for semantic segmentation task against the state-of-the-art methods.\n","authors":["Muhammad Abdullah Jamal","Omid Mohareri"],"pdf_url":"https://arxiv.org/pdf/2408.02245v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08513v2","updated":"2024-09-16T06:39:54Z","published":"2024-09-13T03:23:52Z","title":"Mamba-YOLO-World: Marrying YOLO-World with Mamba for Open-Vocabulary\n  Detection","summary":"  Open-vocabulary detection (OVD) aims to detect objects beyond a predefined\nset of categories. As a pioneering model incorporating the YOLO series into\nOVD, YOLO-World is well-suited for scenarios prioritizing speed and efficiency.\nHowever, its performance is hindered by its neck feature fusion mechanism,\nwhich causes the quadratic complexity and the limited guided receptive fields.\nTo address these limitations, we present Mamba-YOLO-World, a novel YOLO-based\nOVD model employing the proposed MambaFusion Path Aggregation Network\n(MambaFusion-PAN) as its neck architecture. Specifically, we introduce an\ninnovative State Space Model-based feature fusion mechanism consisting of a\nParallel-Guided Selective Scan algorithm and a Serial-Guided Selective Scan\nalgorithm with linear complexity and globally guided receptive fields. It\nleverages multi-modal input sequences and mamba hidden states to guide the\nselective scanning process. Experiments demonstrate that our model outperforms\nthe original YOLO-World on the COCO and LVIS benchmarks in both zero-shot and\nfine-tuning settings while maintaining comparable parameters and FLOPs.\nAdditionally, it surpasses existing state-of-the-art OVD methods with fewer\nparameters and FLOPs.\n","authors":["Haoxuan Wang","Qingdong He","Jinlong Peng","Hao Yang","Mingmin Chi","Yabiao Wang"],"pdf_url":"https://arxiv.org/pdf/2409.08513v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2409.10028v1","updated":"2024-09-16T06:38:25Z","published":"2024-09-16T06:38:25Z","title":"AttnMod: Attention-Based New Art Styles","summary":"  Imagine a human artist looking at the generated photo of a diffusion model,\nand hoping to create a painting out of it. There could be some feature of the\nobject in the photo that the artist wants to emphasize, some color to disperse,\nsome silhouette to twist, or some part of the scene to be materialized. These\nintentions can be viewed as the modification of the cross attention from the\ntext prompt onto UNet, during the desoising diffusion. This work presents\nAttnMod, to modify attention for creating new unpromptable art styles out of\nexisting diffusion models. The style-creating behavior is studied across\ndifferent setups.\n","authors":["Shih-Chieh Su"],"pdf_url":"https://arxiv.org/pdf/2409.10028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10021v1","updated":"2024-09-16T06:17:53Z","published":"2024-09-16T06:17:53Z","title":"LithoHoD: A Litho Simulator-Powered Framework for IC Layout Hotspot\n  Detection","summary":"  Recent advances in VLSI fabrication technology have led to die shrinkage and\nincreased layout density, creating an urgent demand for advanced hotspot\ndetection techniques. However, by taking an object detection network as the\nbackbone, recent learning-based hotspot detectors learn to recognize only the\nproblematic layout patterns in the training data. This fact makes these hotspot\ndetectors difficult to generalize to real-world scenarios. We propose a novel\nlithography simulator-powered hotspot detection framework to overcome this\ndifficulty. Our framework integrates a lithography simulator with an object\ndetection backbone, merging the extracted latent features from both the\nsimulator and the object detector via well-designed cross-attention blocks.\nConsequently, the proposed framework can be used to detect potential hotspot\nregions based on I) the variation of possible circuit shape deformation\nestimated by the lithography simulator, and ii) the problematic layout patterns\nalready known. To this end, we utilize RetinaNet with a feature pyramid network\nas the object detection backbone and leverage LithoNet as the lithography\nsimulator. Extensive experiments demonstrate that our proposed simulator-guided\nhotspot detection framework outperforms previous state-of-the-art methods on\nreal-world data.\n","authors":["Hao-Chiang Shao","Guan-Yu Chen","Yu-Hsien Lin","Chia-Wen Lin","Shao-Yun Fang","Pin-Yian Tsai","Yan-Hsiu Liu"],"pdf_url":"https://arxiv.org/pdf/2409.10021v1.pdf","comment":"14 pages to appear in IEEE Transactions on Computer-Aided Design of\n  Integrated Circuits and Systems"},{"id":"http://arxiv.org/abs/2403.15918v3","updated":"2024-09-16T05:49:02Z","published":"2024-03-23T19:21:31Z","title":"Towards Adversarial Robustness And Backdoor Mitigation in SSL","summary":"  Self-Supervised Learning (SSL) has shown great promise in learning\nrepresentations from unlabeled data. The power of learning representations\nwithout the need for human annotations has made SSL a widely used technique in\nreal-world problems. However, SSL methods have recently been shown to be\nvulnerable to backdoor attacks, where the learned model can be exploited by\nadversaries to manipulate the learned representations, either through tampering\nthe training data distribution, or via modifying the model itself. This work\naims to address defending against backdoor attacks in SSL, where the adversary\nhas access to a realistic fraction of the SSL training data, and no access to\nthe model. We use novel methods that are computationally efficient as well as\ngeneralizable across different problem settings. We also investigate the\nadversarial robustness of SSL models when trained with our method, and show\ninsights into increased robustness in SSL via frequency domain augmentations.\nWe demonstrate the effectiveness of our method on a variety of SSL benchmarks,\nand show that our method is able to mitigate backdoor attacks while maintaining\nhigh performance on downstream tasks. Code for our work is available at\ngithub.com/Aryan-Satpathy/Backdoor\n","authors":["Aryan Satpathy","Nilaksh Singh","Dhruva Rajwade","Somesh Kumar"],"pdf_url":"https://arxiv.org/pdf/2403.15918v3.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.03556v3","updated":"2024-09-16T05:14:14Z","published":"2024-06-05T18:10:49Z","title":"Npix2Cpix: A GAN-Based Image-to-Image Translation Network With\n  Retrieval- Classification Integration for Watermark Retrieval From Historical\n  Document Images","summary":"  The identification and restoration of ancient watermarks have long been a\nmajor topic in codicology and history. Classifying historical documents based\non watermarks is challenging due to their diversity, noisy samples, multiple\nrepresentation modes, and minor distinctions between classes and intra-class\nvariations. This paper proposes a modified U-net-based conditional generative\nadversarial network (GAN) named Npix2Cpix to translate noisy raw historical\nwatermarked images into clean, handwriting-free watermarked images by\nperforming image translation from degraded (noisy) pixels to clean pixels.\nUsing image-to-image translation and adversarial learning, the network creates\nclutter-free images for watermark restoration and categorization. The generator\nand discriminator of the proposed GAN are trained using two separate loss\nfunctions, each based on the distance between images, to learn the mapping from\nthe input noisy image to the output clean image. After using the proposed GAN\nto pre-process noisy watermarked images, Siamese-based one-shot learning is\nemployed for watermark classification. Experimental results on a large-scale\nhistorical watermark dataset demonstrate that cleaning the noisy watermarked\nimages can help to achieve high one-shot classification accuracy. The\nqualitative and quantitative evaluation of the retrieved watermarked image\nhighlights the effectiveness of the proposed approach.\n","authors":["Utsab Saha","Sawradip Saha","Shaikh Anowarul Fattah","Mohammad Saquib"],"pdf_url":"https://arxiv.org/pdf/2406.03556v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18097v2","updated":"2024-09-16T05:07:54Z","published":"2024-07-25T15:02:24Z","title":"SSTD: Stripe-Like Space Target Detection Using Single-Point Weak\n  Supervision","summary":"  Stripe-like space target detection (SSTD) plays a key role in enhancing space\nsituational awareness and assessing spacecraft behaviour. This domain faces\nthree challenges: the lack of publicly available datasets, interference from\nstray light and stars, and the variability of stripe-like targets, which makes\nmanual labeling both inaccurate and labor-intensive. In response, we introduces\n`AstroStripeSet', a pioneering dataset designed for SSTD, aiming to bridge the\ngap in academic resources and advance research in SSTD. Furthermore, we propose\na novel teacher-student label evolution framework with single-point weak\nsupervision, providing a new solution to the challenges of manual labeling.\nThis framework starts with generating initial pseudo-labels using the zero-shot\ncapabilities of the Segment Anything Model (SAM) in a single-point setting.\nAfter that, the fine-tuned StripeSAM serves as the teacher and the newly\ndeveloped StripeNet as the student, consistently improving segmentation\nperformance through label evolution, which iteratively refines these labels. We\nalso introduce `GeoDice', a new loss function customized for the linear\ncharacteristics of stripe-like targets. Extensive experiments show that our\nmethod matches fully supervised approaches, exhibits strong zero-shot\ngeneralization for diverse space-based and ground-based real-world images, and\nsets a new state-of-the-art (SOTA) benchmark. Our AstroStripeSet dataset and\ncode will be made publicly available.\n","authors":["Zijian Zhu","Ali Zia","Xuesong Li","Bingbing Dan","Yuebo Ma","Enhai Liu","Rujin Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.18097v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13722v2","updated":"2024-09-16T04:25:50Z","published":"2024-05-22T15:14:00Z","title":"LightningDrag: Lightning Fast and Accurate Drag-based Image Editing\n  Emerging from Videos","summary":"  Accuracy and speed are critical in image editing tasks. Pan et al. introduced\na drag-based image editing framework that achieves pixel-level control using\nGenerative Adversarial Networks (GANs). A flurry of subsequent studies enhanced\nthis framework's generality by leveraging large-scale diffusion models.\nHowever, these methods often suffer from inordinately long processing times\n(exceeding 1 minute per edit) and low success rates. Addressing these issues\nhead on, we present LightningDrag, a rapid approach enabling high quality\ndrag-based image editing in ~1 second. Unlike most previous methods, we\nredefine drag-based editing as a conditional generation task, eliminating the\nneed for time-consuming latent optimization or gradient-based guidance during\ninference. In addition, the design of our pipeline allows us to train our model\non large-scale paired video frames, which contain rich motion information such\nas object translations, changing poses and orientations, zooming in and out,\netc. By learning from videos, our approach can significantly outperform\nprevious methods in terms of accuracy and consistency. Despite being trained\nsolely on videos, our model generalizes well to perform local shape\ndeformations not presented in the training data (e.g., lengthening of hair,\ntwisting rainbows, etc.). Extensive qualitative and quantitative evaluations on\nbenchmark datasets corroborate the superiority of our approach. The code and\nmodel will be released at https://github.com/magic-research/LightningDrag.\n","authors":["Yujun Shi","Jun Hao Liew","Hanshu Yan","Vincent Y. F. Tan","Jiashi Feng"],"pdf_url":"https://arxiv.org/pdf/2405.13722v2.pdf","comment":"Project page: https://lightning-drag.github.io/"},{"id":"http://arxiv.org/abs/2409.09969v1","updated":"2024-09-16T04:01:10Z","published":"2024-09-16T04:01:10Z","title":"2S-ODIS: Two-Stage Omni-Directional Image Synthesis by Geometric\n  Distortion Correction","summary":"  Omni-directional images have been increasingly used in various applications,\nincluding virtual reality and SNS (Social Networking Services). However, their\navailability is comparatively limited in contrast to normal field of view\n(NFoV) images, since specialized cameras are required to take omni-directional\nimages. Consequently, several methods have been proposed based on generative\nadversarial networks (GAN) to synthesize omni-directional images, but these\napproaches have shown difficulties in training of the models, due to\ninstability and/or significant time consumption in the training. To address\nthese problems, this paper proposes a novel omni-directional image synthesis\nmethod, 2S-ODIS (Two-Stage Omni-Directional Image Synthesis), which generated\nhigh-quality omni-directional images but drastically reduced the training time.\nThis was realized by utilizing the VQGAN (Vector Quantized GAN) model\npre-trained on a large-scale NFoV image database such as ImageNet without\nfine-tuning. Since this pre-trained model does not represent distortions of\nomni-directional images in the equi-rectangular projection (ERP), it cannot be\napplied directly to the omni-directional image synthesis in ERP. Therefore,\ntwo-stage structure was adopted to first create a global coarse image in ERP\nand then refine the image by integrating multiple local NFoV images in the\nhigher resolution to compensate the distortions in ERP, both of which are based\non the pre-trained VQGAN model. As a result, the proposed method, 2S-ODIS,\nachieved the reduction of the training time from 14 days in OmniDreamer to four\ndays in higher image quality.\n","authors":["Atsuya Nakata","Takao Yamanaka"],"pdf_url":"https://arxiv.org/pdf/2409.09969v1.pdf","comment":"ECCV2024 https://github.com/islab-sophia/2S-ODIS"},{"id":"http://arxiv.org/abs/2409.09968v1","updated":"2024-09-16T03:59:01Z","published":"2024-09-16T03:59:01Z","title":"Artificial Intelligence-Based Opportunistic Coronary Calcium Screening\n  in the Veterans Affairs National Healthcare System","summary":"  Coronary artery calcium (CAC) is highly predictive of cardiovascular events.\nWhile millions of chest CT scans are performed annually in the United States,\nCAC is not routinely quantified from scans done for non-cardiac purposes. A\ndeep learning algorithm was developed using 446 expert segmentations to\nautomatically quantify CAC on non-contrast, non-gated CT scans (AI-CAC). Our\nstudy differs from prior works as we leverage imaging data across the Veterans\nAffairs national healthcare system, from 98 medical centers, capturing\nextensive heterogeneity in imaging protocols, scanners, and patients. AI-CAC\nperformance on non-gated scans was compared against clinical standard ECG-gated\nCAC scoring. Non-gated AI-CAC differentiated zero vs. non-zero and less than\n100 vs. 100 or greater Agatston scores with accuracies of 89.4% (F1 0.93) and\n87.3% (F1 0.89), respectively, in 795 patients with paired gated scans within a\nyear of a non-gated CT scan. Non-gated AI-CAC was predictive of 10-year\nall-cause mortality (CAC 0 vs. >400 group: 25.4% vs. 60.2%, Cox HR 3.49, p <\n0.005), and composite first-time stroke, MI, or death (CAC 0 vs. >400 group:\n33.5% vs. 63.8%, Cox HR 3.00, p < 0.005). In a screening dataset of 8,052\npatients with low-dose lung cancer-screening CTs (LDCT), 3,091/8,052 (38.4%)\nindividuals had AI-CAC >400. Four cardiologists qualitatively reviewed LDCT\nimages from a random sample of >400 AI-CAC patients and verified that 527/531\n(99.2%) would benefit from lipid-lowering therapy. To the best of our\nknowledge, this is the first non-gated CT CAC algorithm developed across a\nnational healthcare system, on multiple imaging protocols, without filtering\nintra-cardiac hardware, and compared against a strong gated CT reference. We\nreport superior performance relative to previous CAC algorithms evaluated\nagainst paired gated scans that included patients with intra-cardiac hardware.\n","authors":["Raffi Hagopian","Timothy Strebel","Simon Bernatz","Gregory A Myers","Erik Offerman","Eric Zuniga","Cy Y Kim","Angie T Ng","James A Iwaz","Sunny P Singh","Evan P Carey","Michael J Kim","R Spencer Schaefer","Jeannie Yu","Amilcare Gentili","Hugo JWL Aerts"],"pdf_url":"https://arxiv.org/pdf/2409.09968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07914v2","updated":"2024-09-16T03:34:47Z","published":"2024-09-12T10:30:44Z","title":"InterACT: Inter-dependency Aware Action Chunking with Hierarchical\n  Attention Transformers for Bimanual Manipulation","summary":"  We present InterACT: Inter-dependency aware Action Chunking with Hierarchical\nAttention Transformers, a novel imitation learning framework for bimanual\nmanipulation that integrates hierarchical attention to capture\ninter-dependencies between dual-arm joint states and visual inputs. InterACT\nconsists of a Hierarchical Attention Encoder and a Multi-arm Decoder, both\ndesigned to enhance information aggregation and coordination. The encoder\nprocesses multi-modal inputs through segment-wise and cross-segment attention\nmechanisms, while the decoder leverages synchronization blocks to refine\nindividual action predictions, providing the counterpart's prediction as\ncontext. Our experiments on a variety of simulated and real-world bimanual\nmanipulation tasks demonstrate that InterACT significantly outperforms existing\nmethods. Detailed ablation studies validate the contributions of key components\nof our work, including the impact of CLS tokens, cross-segment encoders, and\nsynchronization blocks.\n","authors":["Andrew Lee","Ian Chuang","Ling-Yuan Chen","Iman Soltani"],"pdf_url":"https://arxiv.org/pdf/2409.07914v2.pdf","comment":"Accepted at Conference on Robot Learning (CoRL) 2024"},{"id":"http://arxiv.org/abs/2409.09953v1","updated":"2024-09-16T02:53:49Z","published":"2024-09-16T02:53:49Z","title":"Uncertainty-Guided Appearance-Motion Association Network for\n  Out-of-Distribution Action Detection","summary":"  Out-of-distribution (OOD) detection targets to detect and reject test samples\nwith semantic shifts, to prevent models trained on in-distribution (ID) dataset\nfrom producing unreliable predictions. Existing works only extract the\nappearance features on image datasets, and cannot handle dynamic multimedia\nscenarios with much motion information. Therefore, we target a more realistic\nand challenging OOD detection task: OOD action detection (ODAD). Given an\nuntrimmed video, ODAD first classifies the ID actions and recognizes the OOD\nactions, and then localizes ID and OOD actions. To this end, in this paper, we\npropose a novel Uncertainty-Guided Appearance-Motion Association Network\n(UAAN), which explores both appearance features and motion contexts to reason\nspatial-temporal inter-object interaction for ODAD.Firstly, we design separate\nappearance and motion branches to extract corresponding appearance-oriented and\nmotion-aspect object representations. In each branch, we construct a\nspatial-temporal graph to reason appearance-guided and motion-driven\ninter-object interaction. Then, we design an appearance-motion attention module\nto fuse the appearance and motion features for final action detection.\nExperimental results on two challenging datasets show that UAAN beats\nstate-of-the-art methods by a significant margin, illustrating its\neffectiveness.\n","authors":["Xiang Fang","Arvind Easwaran","Blaise Genest"],"pdf_url":"https://arxiv.org/pdf/2409.09953v1.pdf","comment":"Accepted by MIPR 2024"},{"id":"http://arxiv.org/abs/2404.04556v2","updated":"2024-09-16T02:02:15Z","published":"2024-04-06T08:45:07Z","title":"Rethinking Self-training for Semi-supervised Landmark Detection: A\n  Selection-free Approach","summary":"  Self-training is a simple yet effective method for semi-supervised learning,\nduring which pseudo-label selection plays an important role for handling\nconfirmation bias. Despite its popularity, applying self-training to landmark\ndetection faces three problems: 1) The selected confident pseudo-labels often\ncontain data bias, which may hurt model performance; 2) It is not easy to\ndecide a proper threshold for sample selection as the localization task can be\nsensitive to noisy pseudo-labels; 3) coordinate regression does not output\nconfidence, making selection-based self-training infeasible. To address the\nabove issues, we propose Self-Training for Landmark Detection (STLD), a method\nthat does not require explicit pseudo-label selection. Instead, STLD constructs\na task curriculum to deal with confirmation bias, which progressively\ntransitions from more confident to less confident tasks over the rounds of\nself-training. Pseudo pretraining and shrink regression are two essential\ncomponents for such a curriculum, where the former is the first task of the\ncurriculum for providing a better model initialization and the latter is\nfurther added in the later rounds to directly leverage the pseudo-labels in a\ncoarse-to-fine manner. Experiments on three facial and one medical landmark\ndetection benchmark show that STLD outperforms the existing methods\nconsistently in both semi- and omni-supervised settings. The code is available\nat https://github.com/jhb86253817/STLD.\n","authors":["Haibo Jin","Haoxuan Che","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2404.04556v2.pdf","comment":"Accepted to IEEE Transactions on Image Processing (TIP)"},{"id":"http://arxiv.org/abs/2409.09921v1","updated":"2024-09-16T01:39:50Z","published":"2024-09-16T01:39:50Z","title":"Towards Real-Time Generation of Delay-Compensated Video Feeds for\n  Outdoor Mobile Robot Teleoperation","summary":"  Teleoperation is an important technology to enable supervisors to control\nagricultural robots remotely. However, environmental factors in dense crop rows\nand limitations in network infrastructure hinder the reliability of data\nstreamed to teleoperators. These issues result in delayed and variable frame\nrate video feeds that often deviate significantly from the robot's actual\nviewpoint. We propose a modular learning-based vision pipeline to generate\ndelay-compensated images in real-time for supervisors. Our extensive offline\nevaluations demonstrate that our method generates more accurate images compared\nto state-of-the-art approaches in our setting. Additionally, we are one of the\nfew works to evaluate a delay-compensation method in outdoor field environments\nwith complex terrain on data from a real robot in real-time. Additional videos\nare provided at https://sites.google.com/illinois.edu/comp-teleop.\n","authors":["Neeloy Chakraborty","Yixiao Fang","Andre Schreiber","Tianchen Ji","Zhe Huang","Aganze Mihigo","Cassidy Wall","Abdulrahman Almana","Katherine Driggs-Campbell"],"pdf_url":"https://arxiv.org/pdf/2409.09921v1.pdf","comment":"8 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2409.09915v1","updated":"2024-09-16T01:07:16Z","published":"2024-09-16T01:07:16Z","title":"Forearm Ultrasound based Gesture Recognition on Edge","summary":"  Ultrasound imaging of the forearm has demonstrated significant potential for\naccurate hand gesture classification. Despite this progress, there has been\nlimited focus on developing a stand-alone end- to-end gesture recognition\nsystem which makes it mobile, real-time and more user friendly. To bridge this\ngap, this paper explores the deployment of deep neural networks for forearm\nultrasound-based hand gesture recognition on edge devices. Utilizing\nquantization techniques, we achieve substantial reductions in model size while\nmaintaining high accuracy and low latency. Our best model, with Float16\nquantization, achieves a test accuracy of 92% and an inference time of 0.31\nseconds on a Raspberry Pi. These results demonstrate the feasibility of\nefficient, real-time gesture recognition on resource-limited edge devices,\npaving the way for wearable ultrasound-based systems.\n","authors":["Keshav Bimbraw","Haichong K. Zhang","Bashima Islam"],"pdf_url":"https://arxiv.org/pdf/2409.09915v1.pdf","comment":"Please contact the authors for code and any additional questions\n  pertaining to the project. You can reach Keshav Bimbraw at bimbrawkeshav at\n  gmail dot com"},{"id":"http://arxiv.org/abs/2401.03771v2","updated":"2024-09-16T00:53:50Z","published":"2024-01-08T09:50:54Z","title":"NeRFmentation: NeRF-based Augmentation for Monocular Depth Estimation","summary":"  The capabilities of monocular depth estimation (MDE) models are limited by\nthe availability of sufficient and diverse datasets. In the case of MDE models\nfor autonomous driving, this issue is exacerbated by the linearity of the\ncaptured data trajectories. We propose a NeRF-based data augmentation pipeline\nto introduce synthetic data with more diverse viewing directions into training\ndatasets and demonstrate the benefits of our approach to model performance and\nrobustness. Our data augmentation pipeline, which we call\n\\textit{NeRFmentation}, trains NeRFs on each scene in a dataset, filters out\nsubpar NeRFs based on relevant metrics, and uses them to generate synthetic\nRGB-D images captured from new viewing directions. In this work, we apply our\ntechnique in conjunction with three state-of-the-art MDE architectures on the\npopular autonomous driving dataset, KITTI, augmenting its training set of the\nEigen split. We evaluate the resulting performance gain on the original test\nset, a separate popular driving dataset, and our own synthetic test set.\n","authors":["Casimir Feldmann","Niall Siegenheim","Nikolas Hars","Lovro Rabuzin","Mert Ertugrul","Luca Wolfart","Marc Pollefeys","Zuria Bauer","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2401.03771v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09907v1","updated":"2024-09-16T00:42:45Z","published":"2024-09-16T00:42:45Z","title":"Rapid Adaptation of Earth Observation Foundation Models for Segmentation","summary":"  This study investigates the efficacy of Low-Rank Adaptation (LoRA) in\nfine-tuning Earth Observation (EO) foundation models for flood segmentation. We\nhypothesize that LoRA, a parameter-efficient technique, can significantly\naccelerate the adaptation of large-scale EO models to this critical task while\nmaintaining high performance. We apply LoRA to fine-tune a state-of-the-art EO\nfoundation model pre-trained on diverse satellite imagery, using a curated\ndataset of flood events. Our results demonstrate that LoRA-based fine-tuning\n(r-256) improves F1 score by 6.66 points and IoU by 0.11 compared to a frozen\nencoder baseline, while significantly reducing computational costs. Notably,\nLoRA outperforms full fine-tuning, which proves computationally infeasible on\nour hardware. We further assess generalization through out-of-distribution\n(OOD) testing on a geographically distinct flood event. While LoRA\nconfigurations show improved OOD performance over the baseline. This work\ncontributes to research on efficient adaptation of foundation models for\nspecialized EO tasks, with implications for rapid response systems in disaster\nmanagement. Our findings demonstrate LoRA's potential for enabling faster\ndeployment of accurate flood segmentation models in resource-constrained,\ntime-critical scenarios.\n","authors":["Karthick Panner Selvam","Raul Ramos-Pollan","Freddie Kalaitzis"],"pdf_url":"https://arxiv.org/pdf/2409.09907v1.pdf","comment":"9 pages 2 figures"},{"id":"http://arxiv.org/abs/2409.09904v1","updated":"2024-09-16T00:15:59Z","published":"2024-09-16T00:15:59Z","title":"Enhancing Visual Inertial SLAM with Magnetic Measurements","summary":"  This paper presents an extension to visual inertial odometry (VIO) by\nintroducing tightly-coupled fusion of magnetometer measurements. A sliding\nwindow of keyframes is optimized by minimizing re-projection errors, relative\ninertial errors, and relative magnetometer orientation errors. The results of\nIMU orientation propagation are used to efficiently transform magnetometer\nmeasurements between frames producing relative orientation constraints between\nconsecutive frames. The soft and hard iron effects are calibrated using an\nellipsoid fitting algorithm. The introduction of magnetometer data results in\nsignificant reductions in the orientation error and also in recovery of the\ntrue yaw orientation with respect to the magnetic north. The proposed framework\noperates in all environments with slow-varying magnetic fields, mainly outdoors\nand underwater. We have focused our work on the underwater domain, especially\nin underwater caves, as the narrow passage and turbulent flow make it difficult\nto perform loop closures and reset the localization drift. The underwater caves\npresent challenges to VIO due to the absence of ambient light and the confined\nnature of the environment, while also being a crucial source of fresh water and\nproviding valuable historical records. Experimental results from underwater\ncaves demonstrate the improvements in accuracy and robustness introduced by the\nproposed VIO extension.\n","authors":["Bharat Joshi","Ioannis Rekleitis"],"pdf_url":"https://arxiv.org/pdf/2409.09904v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.10494v1","updated":"2024-09-16T17:27:27Z","published":"2024-09-16T17:27:27Z","title":"Incorporating Classifier-Free Guidance in Diffusion Model-Based\n  Recommendation","summary":"  This paper presents a diffusion-based recommender system that incorporates\nclassifier-free guidance. Most current recommender systems provide\nrecommendations using conventional methods such as collaborative or\ncontent-based filtering. Diffusion is a new approach to generative AI that\nimproves on previous generative AI approaches such as Variational Autoencoders\n(VAEs) and Generative Adversarial Networks (GANs). We incorporate diffusion in\na recommender system that mirrors the sequence users take when browsing and\nrating items. Although a few current recommender systems incorporate diffusion,\nthey do not incorporate classifier-free guidance, a new innovation in diffusion\nmodels as a whole. In this paper, we present a diffusion recommender system\nthat augments the underlying recommender system model for improved performance\nand also incorporates classifier-free guidance. Our findings show improvements\nover state-of-the-art recommender systems for most metrics for several\nrecommendation tasks on a variety of datasets. In particular, our approach\ndemonstrates the potential to provide better recommendations when data is\nsparse.\n","authors":["Noah Buchanan","Susan Gauch","Quan Mai"],"pdf_url":"https://arxiv.org/pdf/2409.10494v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2406.14891v2","updated":"2024-09-16T17:15:52Z","published":"2024-06-21T06:26:38Z","title":"Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop\n  Question Answering","summary":"  Multi-Hop Question Answering (MHQA) tasks present a significant challenge for\nlarge language models (LLMs) due to the intensive knowledge required. Current\nsolutions, like Retrieval-Augmented Generation, typically retrieve potential\ndocuments from an external corpus to read an answer. However, the performance\nof this retrieve-then-read paradigm is constrained by the retriever and the\ninevitable noise in the retrieved documents. To mitigate these challenges, we\nintroduce a novel generate-then-ground (GenGround) framework, synergizing the\nparametric knowledge of LLMs and external documents to solve a multi-hop\nquestion. GenGround empowers LLMs to alternate two phases until the final\nanswer is derived: (1) formulate a simpler, single-hop question and directly\ngenerate the answer; (2) ground the question-answer pair in retrieved\ndocuments, amending any wrong predictions in the answer. We also propose an\ninstructional grounding distillation method to generalize our method into\nsmaller models. Extensive experiments conducted on four datasets illustrate the\nsuperiority of our method.\n","authors":["Zhengliang Shi","Weiwei Sun","Shen Gao","Pengjie Ren","Zhumin Chen","Zhaochun Ren"],"pdf_url":"https://arxiv.org/pdf/2406.14891v2.pdf","comment":"ACL 2024 (main conference)"},{"id":"http://arxiv.org/abs/2409.10343v1","updated":"2024-09-16T14:57:09Z","published":"2024-09-16T14:57:09Z","title":"Large Language Model Enhanced Hard Sample Identification for Denoising\n  Recommendation","summary":"  Implicit feedback, often used to build recommender systems, unavoidably\nconfronts noise due to factors such as misclicks and position bias. Previous\nstudies have attempted to alleviate this by identifying noisy samples based on\ntheir diverged patterns, such as higher loss values, and mitigating the noise\nthrough sample dropping or reweighting. Despite the progress, we observe\nexisting approaches struggle to distinguish hard samples and noise samples, as\nthey often exhibit similar patterns, thereby limiting their effectiveness in\ndenoising recommendations. To address this challenge, we propose a Large\nLanguage Model Enhanced Hard Sample Denoising (LLMHD) framework. Specifically,\nwe construct an LLM-based scorer to evaluate the semantic consistency of items\nwith the user preference, which is quantified based on summarized historical\nuser interactions. The resulting scores are used to assess the hardness of\nsamples for the pointwise or pairwise training objectives. To ensure\nefficiency, we introduce a variance-based sample pruning strategy to filter\npotential hard samples before scoring. Besides, we propose an iterative\npreference update module designed to continuously refine summarized user\npreference, which may be biased due to false-positive user-item interactions.\nExtensive experiments on three real-world datasets and four backbone\nrecommenders demonstrate the effectiveness of our approach.\n","authors":["Tianrui Song","Wenshuo Chao","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2409.10343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10309v1","updated":"2024-09-16T14:15:42Z","published":"2024-09-16T14:15:42Z","title":"beeFormer: Bridging the Gap Between Semantic and Interaction Similarity\n  in Recommender Systems","summary":"  Recommender systems often use text-side information to improve their\npredictions, especially in cold-start or zero-shot recommendation scenarios,\nwhere traditional collaborative filtering approaches cannot be used. Many\napproaches to text-mining side information for recommender systems have been\nproposed over recent years, with sentence Transformers being the most prominent\none. However, these models are trained to predict semantic similarity without\nutilizing interaction data with hidden patterns specific to recommender\nsystems. In this paper, we propose beeFormer, a framework for training sentence\nTransformer models with interaction data. We demonstrate that our models\ntrained with beeFormer can transfer knowledge between datasets while\noutperforming not only semantic similarity sentence Transformers but also\ntraditional collaborative filtering methods. We also show that training on\nmultiple datasets from different domains accumulates knowledge in a single\nmodel, unlocking the possibility of training universal, domain-agnostic\nsentence Transformer models to mine text representations for recommender\nsystems. We release the source code, trained models, and additional details\nallowing replication of our experiments at\nhttps://github.com/recombee/beeformer.\n","authors":["Vojtƒõch Vanƒçura","Pavel Kord√≠k","Milan Straka"],"pdf_url":"https://arxiv.org/pdf/2409.10309v1.pdf","comment":"Accepted to RecSys 2024"},{"id":"http://arxiv.org/abs/2409.10271v1","updated":"2024-09-16T13:31:04Z","published":"2024-09-16T13:31:04Z","title":"Causal Discovery in Recommender Systems: Example and Discussion","summary":"  Causality is receiving increasing attention by the artificial intelligence\nand machine learning communities. This paper gives an example of modelling a\nrecommender system problem using causal graphs. Specifically, we approached the\ncausal discovery task to learn a causal graph by combining observational data\nfrom an open-source dataset with prior knowledge. The resulting causal graph\nshows that only a few variables effectively influence the analysed feedback\nsignals. This contrasts with the recent trend in the machine learning community\nto include more and more variables in massive models, such as neural networks.\n","authors":["Emanuele Cavenaghi","Fabio Stella","Markus Zanker"],"pdf_url":"https://arxiv.org/pdf/2409.10271v1.pdf","comment":"Accepted at the CONSEQUENCES '24 workshop, co-located with ACM RecSys\n  '24"},{"id":"http://arxiv.org/abs/2409.10267v1","updated":"2024-09-16T13:21:09Z","published":"2024-09-16T13:21:09Z","title":"Enhancing Personalized Recipe Recommendation Through Multi-Class\n  Classification","summary":"  This paper intends to address the challenge of personalized recipe\nrecommendation in the realm of diverse culinary preferences. The problem domain\ninvolves recipe recommendations, utilizing techniques such as association\nanalysis and classification. Association analysis explores the relationships\nand connections between different ingredients to enhance the user experience.\nMeanwhile, the classification aspect involves categorizing recipes based on\nuser-defined ingredients and preferences. A unique aspect of the paper is the\nconsideration of recipes and ingredients belonging to multiple classes,\nrecognizing the complexity of culinary combinations. This necessitates a\nsophisticated approach to classification and recommendation, ensuring the\nsystem accommodates the nature of recipe categorization. The paper seeks not\nonly to recommend recipes but also to explore the process involved in achieving\naccurate and personalized recommendations.\n","authors":["Harish Neelam","Koushik Sai Veerella"],"pdf_url":"https://arxiv.org/pdf/2409.10267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15002v2","updated":"2024-09-16T11:38:10Z","published":"2024-08-27T12:34:41Z","title":"Knowledge Discovery in Optical Music Recognition: Enhancing Information\n  Retrieval with Instance Segmentation","summary":"  Optical Music Recognition (OMR) automates the transcription of musical\nnotation from images into machine-readable formats like MusicXML, MEI, or MIDI,\nsignificantly reducing the costs and time of manual transcription. This study\nexplores knowledge discovery in OMR by applying instance segmentation using\nMask R-CNN to enhance the detection and delineation of musical symbols in sheet\nmusic. Unlike Optical Character Recognition (OCR), OMR must handle the\nintricate semantics of Common Western Music Notation (CWMN), where symbol\nmeanings depend on shape, position, and context. Our approach leverages\ninstance segmentation to manage the density and overlap of musical symbols,\nfacilitating more precise information retrieval from music scores. Evaluations\non the DoReMi and MUSCIMA++ datasets demonstrate substantial improvements, with\nour method achieving a mean Average Precision (mAP) of up to 59.70\\% in dense\nsymbol environments, achieving comparable results to object detection.\nFurthermore, using traditional computer vision techniques, we add a parallel\nstep for staff detection to infer the pitch for the recognised symbols. This\nstudy emphasises the role of pixel-wise segmentation in advancing accurate\nmusic symbol recognition, contributing to knowledge discovery in OMR. Our\nfindings indicate that instance segmentation provides more precise\nrepresentations of musical symbols, particularly in densely populated scores,\nadvancing OMR technology. We make our implementation, pre-processing scripts,\ntrained models, and evaluation results publicly available to support further\nresearch and development.\n","authors":["Elona Shatri","George Fazekas"],"pdf_url":"https://arxiv.org/pdf/2408.15002v2.pdf","comment":"8 pages content and one references, accepted version at the\n  International Conference on Knowledge Discovery and Information Retrieval\n  2024, Porto, Portugal"},{"id":"http://arxiv.org/abs/2405.10024v2","updated":"2024-09-16T09:30:40Z","published":"2024-05-16T12:04:55Z","title":"$Œî\\text{-}{\\rm OPE}$: Off-Policy Estimation with Pairs of Policies","summary":"  The off-policy paradigm casts recommendation as a counterfactual\ndecision-making task, allowing practitioners to unbiasedly estimate online\nmetrics using offline data. This leads to effective evaluation metrics, as well\nas learning procedures that directly optimise online success. Nevertheless, the\nhigh variance that comes with unbiasedness is typically the crux that\ncomplicates practical applications. An important insight is that the difference\nbetween policy values can often be estimated with significantly reduced\nvariance, if said policies have positive covariance. This allows us to\nformulate a pairwise off-policy estimation task: $\\Delta\\text{-}{\\rm OPE}$.\n  $\\Delta\\text{-}{\\rm OPE}$ subsumes the common use-case of estimating\nimprovements of a learnt policy over a production policy, using data collected\nby a stochastic logging policy. We introduce $\\Delta\\text{-}{\\rm OPE}$ methods\nbased on the widely used Inverse Propensity Scoring estimator and its\nextensions. Moreover, we characterise a variance-optimal additive control\nvariate that further enhances efficiency. Simulated, offline, and online\nexperiments show that our methods significantly improve performance for both\nevaluation and learning tasks.\n","authors":["Olivier Jeunen","Aleksei Ustimenko"],"pdf_url":"https://arxiv.org/pdf/2405.10024v2.pdf","comment":"Accepted as a short paper in the 2024 ACM Conference on Recommender\n  Systems (RecSys '24)"},{"id":"http://arxiv.org/abs/2405.02141v2","updated":"2024-09-16T09:21:15Z","published":"2024-05-03T14:44:04Z","title":"Multi-Objective Recommendation via Multivariate Policy Learning","summary":"  Real-world recommender systems often need to balance multiple objectives when\ndeciding which recommendations to present to users. These include behavioural\nsignals (e.g. clicks, shares, dwell time), as well as broader objectives (e.g.\ndiversity, fairness). Scalarisation methods are commonly used to handle this\nbalancing task, where a weighted average of per-objective reward signals\ndetermines the final score used for ranking. Naturally, how these weights are\ncomputed exactly, is key to success for any online platform. We frame this as a\ndecision-making task, where the scalarisation weights are actions taken to\nmaximise an overall North Star reward (e.g. long-term user retention or\ngrowth). We extend existing policy learning methods to the continuous\nmultivariate action domain, proposing to maximise a pessimistic lower bound on\nthe North Star reward that the learnt policy will yield. Typical lower bounds\nbased on normal approximations suffer from insufficient coverage, and we\npropose an efficient and effective policy-dependent correction for this. We\nprovide guidance to design stochastic data collection policies, as well as\nhighly sensitive reward signals. Empirical observations from simulations,\noffline and online experiments highlight the efficacy of our deployed approach.\n","authors":["Olivier Jeunen","Jatin Mandav","Ivan Potapov","Nakul Agarwal","Sourabh Vaid","Wenzhe Shi","Aleksei Ustimenko"],"pdf_url":"https://arxiv.org/pdf/2405.02141v2.pdf","comment":"Accepted as a full paper in the 2024 ACM Conference on Recommender\n  Systems (RecSys '24)"},{"id":"http://arxiv.org/abs/2409.10102v1","updated":"2024-09-16T09:06:44Z","published":"2024-09-16T09:06:44Z","title":"Trustworthiness in Retrieval-Augmented Generation Systems: A Survey","summary":"  Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal\nparadigm in the development of Large Language Models (LLMs). While much of the\ncurrent research in this field focuses on performance optimization,\nparticularly in terms of accuracy and efficiency, the trustworthiness of RAG\nsystems remains an area still under exploration. From a positive perspective,\nRAG systems are promising to enhance LLMs by providing them with useful and\nup-to-date knowledge from vast external databases, thereby mitigating the\nlong-standing problem of hallucination. While from a negative perspective, RAG\nsystems are at the risk of generating undesirable contents if the retrieved\ninformation is either inappropriate or poorly utilized. To address these\nconcerns, we propose a unified framework that assesses the trustworthiness of\nRAG systems across six key dimensions: factuality, robustness, fairness,\ntransparency, accountability, and privacy. Within this framework, we thoroughly\nreview the existing literature on each dimension. Additionally, we create the\nevaluation benchmark regarding the six dimensions and conduct comprehensive\nevaluations for a variety of proprietary and open-source models. Finally, we\nidentify the potential challenges for future research based on our\ninvestigation results. Through this work, we aim to lay a structured foundation\nfor future investigations and provide practical insights for enhancing the\ntrustworthiness of RAG systems in real-world applications.\n","authors":["Yujia Zhou","Yan Liu","Xiaoxi Li","Jiajie Jin","Hongjin Qian","Zheng Liu","Chaozhuo Li","Zhicheng Dou","Tsung-Yi Ho","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2409.10102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10046v1","updated":"2024-09-16T07:19:08Z","published":"2024-09-16T07:19:08Z","title":"Global Lightning-Ignited Wildfires Prediction and Climate Change\n  Projections based on Explainable Machine Learning Models","summary":"  Wildfires pose a significant natural disaster risk to populations and\ncontribute to accelerated climate change. As wildfires are also affected by\nclimate change, extreme wildfires are becoming increasingly frequent. Although\nthey occur less frequently globally than those sparked by human activities,\nlightning-ignited wildfires play a substantial role in carbon emissions and\naccount for the majority of burned areas in certain regions. While existing\ncomputational models, especially those based on machine learning, aim to\npredict lightning-ignited wildfires, they are typically tailored to specific\nregions with unique characteristics, limiting their global applicability. In\nthis study, we present machine learning models designed to characterize and\npredict lightning-ignited wildfires on a global scale. Our approach involves\nclassifying lightning-ignited versus anthropogenic wildfires, and estimating\nwith high accuracy the probability of lightning to ignite a fire based on a\nwide spectrum of factors such as meteorological conditions and vegetation.\nUtilizing these models, we analyze seasonal and spatial trends in\nlightning-ignited wildfires shedding light on the impact of climate change on\nthis phenomenon. We analyze the influence of various features on the models\nusing eXplainable Artificial Intelligence (XAI) frameworks. Our findings\nhighlight significant global differences between anthropogenic and\nlightning-ignited wildfires. Moreover, we demonstrate that, even over a short\ntime span of less than a decade, climate changes have steadily increased the\nglobal risk of lightning-ignited wildfires. This distinction underscores the\nimperative need for dedicated predictive models and fire weather indices\ntailored specifically to each type of wildfire.\n","authors":["Assaf Shmuel","Teddy Lazebnik","Oren Glickman","Eyal Heifetz","Colin Price"],"pdf_url":"https://arxiv.org/pdf/2409.10046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10025v1","updated":"2024-09-16T06:33:26Z","published":"2024-09-16T06:33:26Z","title":"DiffATR: Diffusion-based Generative Modeling for Audio-Text Retrieval","summary":"  Existing audio-text retrieval (ATR) methods are essentially discriminative\nmodels that aim to maximize the conditional likelihood, represented as\np(candidates|query). Nevertheless, this methodology fails to consider the\nintrinsic data distribution p(query), leading to difficulties in discerning\nout-of-distribution data. In this work, we attempt to tackle this constraint\nthrough a generative perspective and model the relationship between audio and\ntext as their joint probability p(candidates,query). To this end, we present a\ndiffusion-based ATR framework (DiffATR), which models ATR as an iterative\nprocedure that progressively generates joint distribution from noise.\nThroughout its training phase, DiffATR is optimized from both generative and\ndiscriminative viewpoints: the generator is refined through a generation loss,\nwhile the feature extractor benefits from a contrastive loss, thus combining\nthe merits of both methodologies. Experiments on the AudioCaps and Clotho\ndatasets with superior performances, verify the effectiveness of our approach.\nNotably, without any alterations, our DiffATR consistently exhibits strong\nperformance in out-of-domain retrieval settings.\n","authors":["Yifei Xin","Xuxin Cheng","Zhihong Zhu","Xusheng Yang","Yuexian Zou"],"pdf_url":"https://arxiv.org/pdf/2409.10025v1.pdf","comment":"Accepted by Interspeech2024"},{"id":"http://arxiv.org/abs/2409.09913v1","updated":"2024-09-16T01:06:23Z","published":"2024-09-16T01:06:23Z","title":"Practical and Asymptotically Optimal Quantization of High-Dimensional\n  Vectors in Euclidean Space for Approximate Nearest Neighbor Search","summary":"  Approximate nearest neighbor (ANN) query in high-dimensional Euclidean space\nis a key operator in database systems. For this query, quantization is a\npopular family of methods developed for compressing vectors and reducing memory\nconsumption. Recently, a method called RaBitQ achieves the state-of-the-art\nperformance among these methods. It produces better empirical performance in\nboth accuracy and efficiency when using the same compression rate and provides\nrigorous theoretical guarantees. However, the method is only designed for\ncompressing vectors at high compression rates (32x) and lacks support for\nachieving higher accuracy by using more space. In this paper, we introduce a\nnew quantization method to address this limitation by extending RaBitQ. The new\nmethod inherits the theoretical guarantees of RaBitQ and achieves the\nasymptotic optimality in terms of the trade-off between space and error bounds\nas to be proven in this study. Additionally, we present efficient\nimplementations of the method, enabling its application to ANN queries to\nreduce both space and time consumption. Extensive experiments on real-world\ndatasets confirm that our method consistently outperforms the state-of-the-art\nbaselines in both accuracy and efficiency when using the same amount of memory.\n","authors":["Jianyang Gao","Yutong Gou","Yuexuan Xu","Yongyi Yang","Cheng Long","Raymond Chi-Wing Wong"],"pdf_url":"https://arxiv.org/pdf/2409.09913v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2408.13378v3","updated":"2024-09-16T22:13:30Z","published":"2024-08-23T21:24:59Z","title":"DrugAgent: Explainable Drug Repurposing Agent with Large Language\n  Model-based Reasoning","summary":"  Drug repurposing offers a promising avenue for accelerating drug development\nby identifying new therapeutic potentials of existing drugs. In this paper, we\npropose a multi-agent framework to enhance the drug repurposing process using\nstate-of-the-art machine learning techniques and knowledge integration. Our\nframework comprises several specialized agents: an AI Agent trains robust\ndrug-target interaction (DTI) models; a Knowledge Graph Agent utilizes the\ndrug-gene interaction database (DGIdb), DrugBank, Comparative Toxicogenomics\nDatabase (CTD), and Search Tool for Interactions of Chemicals (STITCH) to\nsystematically extract DTIs; and a Search Agent interacts with biomedical\nliterature to annotate and verify computational predictions. By integrating\noutputs from these agents, our system effectively harnesses diverse data\nsources, including external databases, to propose viable repurposing\ncandidates. Preliminary results demonstrate the potential of our approach in\nnot only predicting drug-disease interactions but also in reducing the time and\ncost associated with traditional drug discovery methods. This paper highlights\nthe scalability of multi-agent systems in biomedical research and their role in\ndriving innovation in drug repurposing. Our approach not only outperforms\nexisting methods in predicting drug repurposing potential but also provides\ninterpretable results, paving the way for more efficient and cost-effective\ndrug discovery processes.\n","authors":["Yoshitaka Inoue","Tianci Song","Tianfan Fu"],"pdf_url":"https://arxiv.org/pdf/2408.13378v3.pdf","comment":"18 pages, 1 figure"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2409.10516v1","updated":"2024-09-16T17:59:52Z","published":"2024-09-16T17:59:52Z","title":"RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval","summary":"  Transformer-based large Language Models (LLMs) become increasingly important\nin various domains. However, the quadratic time complexity of attention\noperation poses a significant challenge for scaling to longer contexts due to\nthe extremely high inference latency and GPU memory consumption for caching\nkey-value (KV) vectors. This paper proposes RetrievalAttention, a training-free\napproach to accelerate attention computation. To leverage the dynamic sparse\nproperty of attention, RetrievalAttention builds approximate nearest neighbor\nsearch (ANNS) indexes upon KV vectors in CPU memory and retrieves the most\nrelevant ones via vector search during generation. Due to the\nout-of-distribution (OOD) between query vectors and key vectors, off-the-shelf\nANNS indexes still need to scan O(N) (usually 30% of all keys) data for\naccurate retrieval, which fails to exploit the high sparsity.\nRetrievalAttention first identifies the OOD challenge of ANNS-based attention,\nand addresses it via an attention-aware vector search algorithm that can adapt\nto queries and only access 1--3% of data, thus achieving a sub-linear time\ncomplexity. RetrievalAttention greatly reduces the inference cost of\nlong-context LLM with much lower GPU memory requirements while maintaining the\nmodel accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for\nserving 128K tokens in LLMs with 8B parameters, which is capable of generating\none token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).\n","authors":["Di Liu","Meng Chen","Baotong Lu","Huiqiang Jiang","Zhenhua Han","Qianxi Zhang","Qi Chen","Chengruidong Zhang","Bailu Ding","Kai Zhang","Chen Chen","Fan Yang","Yuqing Yang","Lili Qiu"],"pdf_url":"https://arxiv.org/pdf/2409.10516v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2407.21787v2","updated":"2024-09-16T17:58:42Z","published":"2024-07-31T17:57:25Z","title":"Large Language Monkeys: Scaling Inference Compute with Repeated Sampling","summary":"  Scaling the amount of compute used to train language models has dramatically\nimproved their capabilities. However, when it comes to inference, we often\nlimit the amount of compute to only one attempt per problem. Here, we explore\ninference compute as another axis for scaling by increasing the number of\ngenerated samples. Across multiple tasks and models, we observe that coverage -\nthe fraction of problems solved by any attempt - scales with the number of\nsamples over four orders of magnitude. In domains like coding and formal\nproofs, where all answers can be automatically verified, these increases in\ncoverage directly translate into improved performance. When we apply repeated\nsampling to SWE-bench Lite, the fraction of issues solved with\nDeepSeek-V2-Coder-Instruct increases from 15.9% with one sample to 56% with 250\nsamples, outperforming the single-attempt state-of-the-art of 43% which uses\nmore capable frontier models. Moreover, using current API pricing, amplifying\nthe cheaper DeepSeek model with five samples is more cost-effective and solves\nmore issues than paying a premium for one sample from GPT-4o or Claude 3.5\nSonnet. Interestingly, the relationship between coverage and the number of\nsamples is often log-linear and can be modelled with an exponentiated power\nlaw, suggesting the existence of inference-time scaling laws. Finally, we find\nthat identifying correct samples out of many generations remains an important\ndirection for future research in domains without automatic verifiers. When\nsolving math word problems from GSM8K and MATH, coverage with Llama-3 models\ngrows to over 95% with 10,000 samples. However, common methods to pick correct\nsolutions from a sample collection, such as majority voting or reward models,\nplateau beyond several hundred samples and fail to fully scale with the sample\nbudget.\n","authors":["Bradley Brown","Jordan Juravsky","Ryan Ehrlich","Ronald Clark","Quoc V. Le","Christopher R√©","Azalia Mirhoseini"],"pdf_url":"https://arxiv.org/pdf/2407.21787v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14220v4","updated":"2024-09-16T17:47:54Z","published":"2023-11-23T22:41:30Z","title":"Assumption-Lean and Data-Adaptive Post-Prediction Inference","summary":"  A primary challenge facing modern scientific research is the limited\navailability of gold-standard data which can be costly, labor-intensive, or\ninvasive to obtain. With the rapid development of machine learning (ML),\nscientists can now employ ML algorithms to predict gold-standard outcomes with\nvariables that are easier to obtain. However, these predicted outcomes are\noften used directly in subsequent statistical analyses, ignoring imprecision\nand heterogeneity introduced by the prediction procedure. This will likely\nresult in false positive findings and invalid scientific conclusions. In this\nwork, we introduce PoSt-Prediction Adaptive inference (PSPA) that allows valid\nand powerful inference based on ML-predicted data. Its \"assumption-lean\"\nproperty guarantees reliable statistical inference without assumptions on the\nML prediction. Its \"data-adaptive\" feature guarantees an efficiency gain over\nexisting methods, regardless of the accuracy of ML prediction. We demonstrate\nthe statistical superiority and broad applicability of our method through\nsimulations and real-data applications.\n","authors":["Jiacheng Miao","Xinran Miao","Yixuan Wu","Jiwei Zhao","Qiongshi Lu"],"pdf_url":"https://arxiv.org/pdf/2311.14220v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10502v1","updated":"2024-09-16T17:42:15Z","published":"2024-09-16T17:42:15Z","title":"Causal Language Modeling Can Elicit Search and Reasoning Capabilities on\n  Logic Puzzles","summary":"  Causal language modeling using the Transformer architecture has yielded\nremarkable capabilities in Large Language Models (LLMs) over the last few\nyears. However, the extent to which fundamental search and reasoning\ncapabilities emerged within LLMs remains a topic of ongoing debate. In this\nwork, we study if causal language modeling can learn a complex task such as\nsolving Sudoku puzzles. To solve a Sudoku, the model is first required to\nsearch over all empty cells of the puzzle to decide on a cell to fill and then\napply an appropriate strategy to fill the decided cell. Sometimes, the\napplication of a strategy only results in thinning down the possible values in\na cell rather than concluding the exact value of the cell. In such cases,\nmultiple strategies are applied one after the other to fill a single cell. We\nobserve that Transformer models trained on this synthetic task can indeed learn\nto solve Sudokus (our model solves $94.21\\%$ of the puzzles fully correctly)\nwhen trained on a logical sequence of steps taken by a solver. We find that\ntraining Transformers with the logical sequence of steps is necessary and\nwithout such training, they fail to learn Sudoku. We also extend our analysis\nto Zebra puzzles (known as Einstein puzzles) and show that the model solves\n$92.04 \\%$ of the puzzles fully correctly. In addition, we study the internal\nrepresentations of the trained Transformer and find that through linear\nprobing, we can decode information about the set of possible values in any\ngiven cell from them, pointing to the presence of a strong reasoning engine\nimplicit in the Transformer weights.\n","authors":["Kulin Shah","Nishanth Dikkala","Xin Wang","Rina Panigrahy"],"pdf_url":"https://arxiv.org/pdf/2409.10502v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2409.10499v1","updated":"2024-09-16T17:41:45Z","published":"2024-09-16T17:41:45Z","title":"Partial Distribution Matching via Partial Wasserstein Adversarial\n  Networks","summary":"  This paper studies the problem of distribution matching (DM), which is a\nfundamental machine learning problem seeking to robustly align two probability\ndistributions. Our approach is established on a relaxed formulation, called\npartial distribution matching (PDM), which seeks to match a fraction of the\ndistributions instead of matching them completely. We theoretically derive the\nKantorovich-Rubinstein duality for the partial Wasserstain-1 (PW) discrepancy,\nand develop a partial Wasserstein adversarial network (PWAN) that efficiently\napproximates the PW discrepancy based on this dual form. Partial matching can\nthen be achieved by optimizing the network using gradient descent. Two\npractical tasks, point set registration and partial domain adaptation are\ninvestigated, where the goals are to partially match distributions in 3D space\nand high-dimensional feature space respectively. The experiment results confirm\nthat the proposed PWAN effectively produces highly robust matching results,\nperforming better or on par with the state-of-the-art methods.\n","authors":["Zi-Ming Wang","Nan Xue","Ling Lei","Rebecka J√∂rnsten","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2409.10499v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2203.02227"},{"id":"http://arxiv.org/abs/2409.09003v2","updated":"2024-09-16T17:34:26Z","published":"2024-09-13T17:32:05Z","title":"Model-independent variable selection via the rule-based variable\n  priority","summary":"  While achieving high prediction accuracy is a fundamental goal in machine\nlearning, an equally important task is finding a small number of features with\nhigh explanatory power. One popular selection technique is permutation\nimportance, which assesses a variable's impact by measuring the change in\nprediction error after permuting the variable. However, this can be problematic\ndue to the need to create artificial data, a problem shared by other methods as\nwell. Another problem is that variable selection methods can be limited by\nbeing model-specific. We introduce a new model-independent approach, Variable\nPriority (VarPro), which works by utilizing rules without the need to generate\nartificial data or evaluate prediction error. The method is relatively easy to\nuse, requiring only the calculation of sample averages of simple statistics,\nand can be applied to many data settings, including regression, classification,\nand survival. We investigate the asymptotic properties of VarPro and show,\namong other things, that VarPro has a consistent filtering property for noise\nvariables. Empirical studies using synthetic and real-world data show the\nmethod achieves a balanced performance and compares favorably to many\nstate-of-the-art procedures currently used for variable selection.\n","authors":["Min Lu","Hemant Ishwaran"],"pdf_url":"https://arxiv.org/pdf/2409.09003v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10496v1","updated":"2024-09-16T17:28:21Z","published":"2024-09-16T17:28:21Z","title":"MusicLIME: Explainable Multimodal Music Understanding","summary":"  Multimodal models are critical for music understanding tasks, as they capture\nthe complex interplay between audio and lyrics. However, as these models become\nmore prevalent, the need for explainability grows-understanding how these\nsystems make decisions is vital for ensuring fairness, reducing bias, and\nfostering trust. In this paper, we introduce MusicLIME, a model-agnostic\nfeature importance explanation method designed for multimodal music models.\nUnlike traditional unimodal methods, which analyze each modality separately\nwithout considering the interaction between them, often leading to incomplete\nor misleading explanations, MusicLIME reveals how audio and lyrical features\ninteract and contribute to predictions, providing a holistic view of the\nmodel's decision-making. Additionally, we enhance local explanations by\naggregating them into global explanations, giving users a broader perspective\nof model behavior. Through this work, we contribute to improving the\ninterpretability of multimodal music models, empowering users to make informed\nchoices, and fostering more equitable, fair, and transparent music\nunderstanding systems.\n","authors":["Theodoros Sotirou","Vassilis Lyberatos","Orfeas Menis Mastromichalakis","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2409.10496v1.pdf","comment":"GitHub repository: https://github.com/IamTheo2000/MusicLIME"},{"id":"http://arxiv.org/abs/2409.10470v1","updated":"2024-09-16T17:01:27Z","published":"2024-09-16T17:01:27Z","title":"Online Nonconvex Bilevel Optimization with Bregman Divergences","summary":"  Bilevel optimization methods are increasingly relevant within machine\nlearning, especially for tasks such as hyperparameter optimization and\nmeta-learning. Compared to the offline setting, online bilevel optimization\n(OBO) offers a more dynamic framework by accommodating time-varying functions\nand sequentially arriving data. This study addresses the online\nnonconvex-strongly convex bilevel optimization problem. In deterministic\nsettings, we introduce a novel online Bregman bilevel optimizer (OBBO) that\nutilizes adaptive Bregman divergences. We demonstrate that OBBO enhances the\nknown sublinear rates for bilevel local regret through a novel hypergradient\nerror decomposition that adapts to the underlying geometry of the problem. In\nstochastic contexts, we introduce the first stochastic online bilevel optimizer\n(SOBBO), which employs a window averaging method for updating outer-level\nvariables using a weighted average of recent stochastic approximations of\nhypergradients. This approach not only achieves sublinear rates of bilevel\nlocal regret but also serves as an effective variance reduction strategy,\nobviating the need for additional stochastic gradient samples at each timestep.\nExperiments on online hyperparameter optimization and online meta-learning\nhighlight the superior performance, efficiency, and adaptability of our\nBregman-based algorithms compared to established online and offline bilevel\nbenchmarks.\n","authors":["Jason Bohne","David Rosenberg","Gary Kazantsev","Pawel Polak"],"pdf_url":"https://arxiv.org/pdf/2409.10470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10463v1","updated":"2024-09-16T16:56:08Z","published":"2024-09-16T16:56:08Z","title":"Kolmogorov-Arnold Networks in Low-Data Regimes: A Comparative Study with\n  Multilayer Perceptrons","summary":"  Multilayer Perceptrons (MLPs) have long been a cornerstone in deep learning,\nknown for their capacity to model complex relationships. Recently,\nKolmogorov-Arnold Networks (KANs) have emerged as a compelling alternative,\nutilizing highly flexible learnable activation functions directly on network\nedges, a departure from the neuron-centric approach of MLPs. However, KANs\nsignificantly increase the number of learnable parameters, raising concerns\nabout their effectiveness in data-scarce environments. This paper presents a\ncomprehensive comparative study of MLPs and KANs from both algorithmic and\nexperimental perspectives, with a focus on low-data regimes. We introduce an\neffective technique for designing MLPs with unique, parameterized activation\nfunctions for each neuron, enabling a more balanced comparison with KANs. Using\nempirical evaluations on simulated data and two real-world data sets from\nmedicine and engineering, we explore the trade-offs between model complexity\nand accuracy, with particular attention to the role of network depth. Our\nfindings show that MLPs with individualized activation functions achieve\nsignificantly higher predictive accuracy with only a modest increase in\nparameters, especially when the sample size is limited to around one hundred.\nFor example, in a three-class classification problem within additive\nmanufacturing, MLPs achieve a median accuracy of 0.91, significantly\noutperforming KANs, which only reach a median accuracy of 0.53 with default\nhyperparameters. These results offer valuable insights into the impact of\nactivation function selection in neural networks.\n","authors":["Farhad Pourkamali-Anaraki"],"pdf_url":"https://arxiv.org/pdf/2409.10463v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2211.15856v5","updated":"2024-09-16T16:43:41Z","published":"2022-11-29T01:11:04Z","title":"Beyond Ensemble Averages: Leveraging Climate Model Ensembles for\n  Subseasonal Forecasting","summary":"  Producing high-quality forecasts of key climate variables, such as\ntemperature and precipitation, on subseasonal time scales has long been a gap\nin operational forecasting. This study explores an application of machine\nlearning (ML) models as post-processing tools for subseasonal forecasting.\nLagged numerical ensemble forecasts (i.e., an ensemble where the members have\ndifferent initialization dates) and observational data, including relative\nhumidity, pressure at sea level, and geopotential height, are incorporated into\nvarious ML methods to predict monthly average precipitation and two-meter\ntemperature two weeks in advance for the continental United States. For\nregression, quantile regression, and tercile classification tasks, we consider\nusing linear models, random forests, convolutional neural networks, and stacked\nmodels (a multi-model approach based on the prediction of the individual ML\nmodels). Unlike previous ML approaches that often use ensemble mean alone, we\nleverage information embedded in the ensemble forecasts to enhance prediction\naccuracy. Additionally, we investigate extreme event predictions that are\ncrucial for planning and mitigation efforts. Considering ensemble members as a\ncollection of spatial forecasts, we explore different approaches to using\nspatial information. Trade-offs between different approaches may be mitigated\nwith model stacking. Our proposed models outperform standard baselines such as\nclimatological forecasts and ensemble means. In addition, we investigate\nfeature importance, trade-offs between using the full ensemble or only the\nensemble mean, and different modes of accounting for spatial variability.\n","authors":["Elena Orlova","Haokun Liu","Raphael Rossellini","Benjamin A. Cash","Rebecca Willett"],"pdf_url":"https://arxiv.org/pdf/2211.15856v5.pdf","comment":"This Work has been accepted to Artificial Intelligence for the Earth\n  Systems journal. The AMS does not guarantee that the copy provided here is an\n  accurate copy of the Version of Record (VoR).\n  https://doi.org/10.1175/AIES-D-23-0103.1"},{"id":"http://arxiv.org/abs/2409.10452v1","updated":"2024-09-16T16:40:40Z","published":"2024-09-16T16:40:40Z","title":"Signed Graph Autoencoder for Explainable and Polarization-Aware Network\n  Embeddings","summary":"  Autoencoders based on Graph Neural Networks (GNNs) have garnered significant\nattention in recent years for their ability to extract informative latent\nrepresentations, characterizing the structure of complex topologies, such as\ngraphs. Despite the prevalence of Graph Autoencoders, there has been limited\nfocus on developing and evaluating explainable neural-based graph generative\nmodels specifically designed for signed networks. To address this gap, we\npropose the Signed Graph Archetypal Autoencoder (SGAAE) framework. SGAAE\nextracts node-level representations that express node memberships over distinct\nextreme profiles, referred to as archetypes, within the network. This is\nachieved by projecting the graph onto a learned polytope, which governs its\npolarization. The framework employs a recently proposed likelihood for\nanalyzing signed networks based on the Skellam distribution, combined with\nrelational archetypal analysis and GNNs. Our experimental evaluation\ndemonstrates the SGAAEs' capability to successfully infer node memberships over\nthe different underlying latent structures while extracting competing\ncommunities formed through the participation of the opposing views in the\nnetwork. Additionally, we introduce the 2-level network polarization problem\nand show how SGAAE is able to characterize such a setting. The proposed model\nachieves high performance in different tasks of signed link prediction across\nfour real-world datasets, outperforming several baseline models.\n","authors":["Nikolaos Nakis","Chrysoula Kosma","Giannis Nikolentzos","Michalis Chatzianastasis","Iakovos Evdaimon","Michalis Vazirgiannis"],"pdf_url":"https://arxiv.org/pdf/2409.10452v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2206.09563v6","updated":"2024-09-16T16:39:48Z","published":"2022-06-20T04:17:32Z","title":"Scalable Distributed Algorithms for Size-Constrained Submodular\n  Maximization in the MapReduce and Adaptive Complexity Models","summary":"  Distributed maximization of a submodular function in the MapReduce (MR) model\nhas received much attention, culminating in two frameworks that allow a\ncentralized algorithm to be run in the MR setting without loss of\napproximation, as long as the centralized algorithm satisfies a certain\nconsistency property -- which had previously only been known to be satisfied by\nthe standard greedy and continous greedy algorithms. A separate line of work\nhas studied parallelizability of submodular maximization in the adaptive\ncomplexity model, where each thread may have access to the entire ground set.\nFor the size-constrained maximization of a monotone and submodular function, we\nshow that several sublinearly adaptive (highly parallelizable) algorithms\nsatisfy the consistency property required to work in the MR setting, which\nyields practical, parallelizable and distributed algorithms. Separately, we\ndevelop the first distributed algorithm with linear query complexity for this\nproblem. Finally, we provide a method to increase the maximum cardinality\nconstraint for MR algorithms at the cost of additional MR rounds.\n","authors":["Yixin Chen","Tonmoy Dey","Alan Kuhnle"],"pdf_url":"https://arxiv.org/pdf/2206.09563v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00846v3","updated":"2024-09-16T16:30:09Z","published":"2024-06-02T19:50:05Z","title":"Local Methods with Adaptivity via Scaling","summary":"  The rapid development of machine learning and deep learning has introduced\nincreasingly complex optimization challenges that must be addressed. Indeed,\ntraining modern, advanced models has become difficult to implement without\nleveraging multiple computing nodes in a distributed environment. Distributed\noptimization is also fundamental to emerging fields such as federated learning.\nSpecifically, there is a need to organize the training process to minimize the\ntime lost due to communication. A widely used and extensively researched\ntechnique to mitigate the communication bottleneck involves performing local\ntraining before communication. This approach is the focus of our paper.\nConcurrently, adaptive methods that incorporate scaling, notably led by Adam,\nhave gained significant popularity in recent years. Therefore, this paper aims\nto merge the local training technique with the adaptive approach to develop\nefficient distributed learning methods. We consider the classical Local SGD\nmethod and enhance it with a scaling feature. A crucial aspect is that the\nscaling is described generically, allowing us to analyze various approaches,\nincluding Adam, RMSProp, and OASIS, in a unified manner. In addition to\ntheoretical analysis, we validate the performance of our methods in practice by\ntraining a neural network.\n","authors":["Savelii Chezhegov","Sergey Skorik","Nikolas Khachaturov","Danil Shalagin","Aram Avetisyan","Martin Tak√°ƒç","Yaroslav Kholodov","Aleksandr Beznosikov"],"pdf_url":"https://arxiv.org/pdf/2406.00846v3.pdf","comment":"41 pages, 2 algorithms, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2402.11628v2","updated":"2024-09-16T16:22:40Z","published":"2024-02-18T16:03:04Z","title":"Discrete Neural Algorithmic Reasoning","summary":"  Neural algorithmic reasoning aims to capture computations with neural\nnetworks via learning the models to imitate the execution of classic\nalgorithms. While common architectures are expressive enough to contain the\ncorrect model in the weights space, current neural reasoners are struggling to\ngeneralize well on out-of-distribution data. On the other hand, classic\ncomputations are not affected by distributional shifts as they can be described\nas transitions between discrete computational states. In this work, we propose\nto force neural reasoners to maintain the execution trajectory as a combination\nof finite predefined states. To achieve that, we separate discrete and\ncontinuous data flows and describe the interaction between them. Trained with\nsupervision on the algorithm's state transitions, such models are able to\nperfectly align with the original algorithm. To show this, we evaluate our\napproach on multiple algorithmic problems and get perfect test scores both in\nsingle-task and multitask setups. Moreover, the proposed architectural choice\nallows us to prove the correctness of the learned algorithms for any test~data.\n","authors":["Gleb Rodionov","Liudmila Prokhorenkova"],"pdf_url":"https://arxiv.org/pdf/2402.11628v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10432v1","updated":"2024-09-16T16:07:21Z","published":"2024-09-16T16:07:21Z","title":"Structure-preserving learning for multi-symplectic PDEs","summary":"  This paper presents an energy-preserving machine learning method for\ninferring reduced-order models (ROMs) by exploiting the multi-symplectic form\nof partial differential equations (PDEs). The vast majority of\nenergy-preserving reduced-order methods use symplectic Galerkin projection to\nconstruct reduced-order Hamiltonian models by projecting the full models onto a\nsymplectic subspace. However, symplectic projection requires the existence of\nfully discrete operators, and in many cases, such as black-box PDE solvers,\nthese operators are inaccessible. In this work, we propose an energy-preserving\nmachine learning method that can infer the dynamics of the given PDE using data\nonly, so that the proposed framework does not depend on the fully discrete\noperators. In this context, the proposed method is non-intrusive. The proposed\nmethod is grey box in the sense that it requires only some basic knowledge of\nthe multi-symplectic model at the partial differential equation level. We prove\nthat the proposed method satisfies spatially discrete local energy conservation\nand preserves the multi-symplectic conservation laws. We test our method on the\nlinear wave equation, the Korteweg-de Vries equation, and the\nZakharov-Kuznetsov equation. We test the generalization of our learned models\nby testing them far outside the training time interval.\n","authors":["S√ºleyman Yƒ±ldƒ±z","Pawan Goyal","Peter Benner"],"pdf_url":"https://arxiv.org/pdf/2409.10432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07684v2","updated":"2024-09-16T16:02:46Z","published":"2024-07-10T14:08:27Z","title":"Towards Human-Like Driving: Active Inference in Autonomous Vehicle\n  Control","summary":"  This paper presents a novel approach to Autonomous Vehicle (AV) control\nthrough the application of active inference, a theory derived from neuroscience\nthat conceptualizes the brain as a predictive machine. Traditional autonomous\ndriving systems rely heavily on Modular Pipelines, Imitation Learning, or\nReinforcement Learning, each with inherent limitations in adaptability,\ngeneralization, and computational efficiency. Active inference addresses these\nchallenges by minimizing prediction error (termed \"surprise\") through a dynamic\nmodel that balances perception and action. Our method integrates active\ninference with deep learning to manage lateral control in AVs, enabling them to\nperform lane following maneuvers within a simulated urban environment. We\ndemonstrate that our model, despite its simplicity, effectively learns and\ngeneralizes from limited data without extensive retraining, significantly\nreducing computational demands. The proposed approach not only enhances the\nadaptability and performance of AVs in dynamic scenarios but also aligns\nclosely with human-like driving behavior, leveraging a generative model to\npredict and adapt to environmental changes. Results from extensive experiments\nin the CARLA simulator show promising outcomes, outperforming traditional\nmethods in terms of adaptability and efficiency, thereby advancing the\npotential of active inference in real-world autonomous driving applications.\n","authors":["Elahe Delavari","John Moore","Junho Hong","Jaerock Kwon"],"pdf_url":"https://arxiv.org/pdf/2407.07684v2.pdf","comment":"The work is partly supported by a sponsor. Authors need to complete\n  the final report submission before any type of publication according to the\n  sponsor. The final report will be submitted in few weeks. Then, authors will\n  reinstate this paper after that"},{"id":"http://arxiv.org/abs/2408.03539v3","updated":"2024-09-16T15:41:05Z","published":"2024-08-07T04:35:38Z","title":"Deep Reinforcement Learning for Robotics: A Survey of Real-World\n  Successes","summary":"  Reinforcement learning (RL), particularly its combination with deep neural\nnetworks referred to as deep RL (DRL), has shown tremendous promise across a\nwide range of applications, suggesting its potential for enabling the\ndevelopment of sophisticated robotic behaviors. Robotics problems, however,\npose fundamental difficulties for the application of RL, stemming from the\ncomplexity and cost of interacting with the physical world. This article\nprovides a modern survey of DRL for robotics, with a particular focus on\nevaluating the real-world successes achieved with DRL in realizing several key\nrobotic competencies. Our analysis aims to identify the key factors underlying\nthose exciting successes, reveal underexplored areas, and provide an overall\ncharacterization of the status of DRL in robotics. We highlight several\nimportant avenues for future work, emphasizing the need for stable and\nsample-efficient real-world RL paradigms, holistic approaches for discovering\nand integrating various competencies to tackle complex long-horizon, open-world\ntasks, and principled development and evaluation procedures. This survey is\ndesigned to offer insights for both RL practitioners and roboticists toward\nharnessing RL's power to create generally capable real-world robotic systems.\n","authors":["Chen Tang","Ben Abbatematteo","Jiaheng Hu","Rohan Chandra","Roberto Mart√≠n-Mart√≠n","Peter Stone"],"pdf_url":"https://arxiv.org/pdf/2408.03539v3.pdf","comment":"The first three authors contributed equally. Accepted to Annual\n  Review of Control, Robotics, and Autonomous Systems"},{"id":"http://arxiv.org/abs/2406.17639v3","updated":"2024-09-16T15:32:11Z","published":"2024-06-25T15:24:02Z","title":"Mitigate the Gap: Investigating Approaches for Improving Cross-Modal\n  Alignment in CLIP","summary":"  Contrastive Language--Image Pre-training (CLIP) has manifested remarkable\nimprovements in zero-shot classification and cross-modal vision-language tasks.\nYet, from a geometrical point of view, the CLIP embedding space has been found\nto have a pronounced modality gap. This gap renders the embedding space overly\nsparse and disconnected, with different modalities being densely distributed in\ndistinct subregions of the hypersphere. In this work, we aim at answering three\nmain questions: 1. Does sharing the parameter space between the multi-modal\nencoders reduce the modality gap? 2. Can the gap be mitigated by pushing apart\nthe uni-modal embeddings via intra-modality separation? 3. How do these gap\nreduction approaches affect the downstream performance? We design AlignCLIP, in\norder to answer these questions and through extensive experiments, we show that\nAlignCLIP achieves noticeable enhancements in the cross-modal alignment of the\nembeddings, and thereby, reduces the modality gap, while improving the\nperformance across several zero-shot and fine-tuning downstream evaluations.\n","authors":["Sedigheh Eslami","Gerard de Melo"],"pdf_url":"https://arxiv.org/pdf/2406.17639v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07745v2","updated":"2024-09-16T15:28:42Z","published":"2023-10-11T16:24:14Z","title":"Deep Reinforcement Learning for Autonomous Cyber Operations: A Survey","summary":"  The rapid increase in the number of cyber-attacks in recent years raises the\nneed for principled methods for defending networks against malicious actors.\nDeep reinforcement learning (DRL) has emerged as a promising approach for\nmitigating these attacks. However, while DRL has shown much potential for cyber\ndefence, numerous challenges must be overcome before DRL can be applied to\nautonomous cyber operations (ACO) at scale. Principled methods are required for\nenvironments that confront learners with very high-dimensional state spaces,\nlarge multi-discrete action spaces, and adversarial learning. Recent works have\nreported success in solving these problems individually. There have also been\nimpressive engineering efforts towards solving all three for real-time strategy\ngames. However, applying DRL to the full ACO problem remains an open challenge.\nHere, we survey the relevant DRL literature and conceptualize an idealised\nACO-DRL agent. We provide: i.) A summary of the domain properties that define\nthe ACO problem; ii.) A comprehensive comparison of current ACO environments\nused for benchmarking DRL approaches; iii.) An overview of state-of-the-art\napproaches for scaling DRL to domains that confront learners with the curse of\ndimensionality, and; iv.) A survey and critique of current methods for limiting\nthe exploitability of agents within adversarial settings from the perspective\nof ACO. We conclude with open research questions that we hope will motivate\nfuture directions for researchers and practitioners working on ACO.\n","authors":["Gregory Palmer","Chris Parry","Daniel J. B. Harrold","Chris Willis"],"pdf_url":"https://arxiv.org/pdf/2310.07745v2.pdf","comment":"89 pages, 14 figures, 4 tables"},{"id":"http://arxiv.org/abs/2409.10392v1","updated":"2024-09-16T15:27:35Z","published":"2024-09-16T15:27:35Z","title":"TPFL: Tsetlin-Personalized Federated Learning with Confidence-Based\n  Clustering","summary":"  The world of Machine Learning (ML) has witnessed rapid changes in terms of\nnew models and ways to process users data. The majority of work that has been\ndone is focused on Deep Learning (DL) based approaches. However, with the\nemergence of new algorithms such as the Tsetlin Machine (TM) algorithm, there\nis growing interest in exploring alternative approaches that may offer unique\nadvantages in certain domains or applications. One of these domains is\nFederated Learning (FL), in which users privacy is of utmost importance. Due to\nits novelty, FL has seen a surge in the incorporation of personalization\ntechniques to enhance model accuracy while maintaining user privacy under\npersonalized conditions. In this work, we propose a novel approach dubbed TPFL:\nTsetlin-Personalized Federated Learning, in which models are grouped into\nclusters based on their confidence towards a specific class. In this way,\nclustering can benefit from two key advantages. Firstly, clients share only\nwhat they are confident about, resulting in the elimination of wrongful weight\naggregation among clients whose data for a specific class may have not been\nenough during the training. This phenomenon is prevalent when the data are\nnon-Independent and Identically Distributed (non-IID). Secondly, by sharing\nonly weights towards a specific class, communication cost is substantially\nreduced, making TPLF efficient in terms of both accuracy and communication\ncost. The results of TPFL demonstrated the highest accuracy on three different\ndatasets; namely MNIST, FashionMNIST and FEMNIST.\n","authors":["Rasoul Jafari Gohari","Laya Aliahmadipour","Ezat Valipour"],"pdf_url":"https://arxiv.org/pdf/2409.10392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10388v1","updated":"2024-09-16T15:24:25Z","published":"2024-09-16T15:24:25Z","title":"Revising the Structure of Recurrent Neural Networks to Eliminate\n  Numerical Derivatives in Forming Physics Informed Loss Terms with Respect to\n  Time","summary":"  Solving unsteady partial differential equations (PDEs) using recurrent neural\nnetworks (RNNs) typically requires numerical derivatives between each block of\nthe RNN to form the physics informed loss function. However, this introduces\nthe complexities of numerical derivatives into the training process of these\nmodels. In this study, we propose modifying the structure of the traditional\nRNN to enable the prediction of each block over a time interval, making it\npossible to calculate the derivative of the output with respect to time using\nthe backpropagation algorithm. To achieve this, the time intervals of these\nblocks are overlapped, defining a mutual loss function between them.\nAdditionally, the employment of conditional hidden states enables us to achieve\na unique solution for each block. The forget factor is utilized to control the\ninfluence of the conditional hidden state on the prediction of the subsequent\nblock. This new model, termed the Mutual Interval RNN (MI-RNN), is applied to\nsolve three different benchmarks: the Burgers equation, unsteady heat\nconduction in an irregular domain, and the Green vortex problem. Our results\ndemonstrate that MI-RNN can find the exact solution more accurately compared to\nexisting RNN models. For instance, in the second problem, MI-RNN achieved one\norder of magnitude less relative error compared to the RNN model with numerical\nderivatives.\n","authors":["Mahyar Jahani-nasab","Mohamad Ali Bijarchi"],"pdf_url":"https://arxiv.org/pdf/2409.10388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10371v1","updated":"2024-09-16T15:14:53Z","published":"2024-09-16T15:14:53Z","title":"Learning Gentle Grasping from Human-Free Force Control Demonstration","summary":"  Humans can steadily and gently grasp unfamiliar objects based on tactile\nperception. Robots still face challenges in achieving similar performance due\nto the difficulty of learning accurate grasp-force predictions and force\ncontrol strategies that can be generalized from limited data. In this article,\nwe propose an approach for learning grasping from ideal force control\ndemonstrations, to achieve similar performance of human hands with limited data\nsize. Our approach utilizes objects with known contact characteristics to\nautomatically generate reference force curves without human demonstrations. In\naddition, we design the dual convolutional neural networks (Dual-CNN)\narchitecture which incorporating a physics-based mechanics module for learning\ntarget grasping force predictions from demonstrations. The described method can\nbe effectively applied in vision-based tactile sensors and enables gentle and\nstable grasping of objects from the ground. The described prediction model and\ngrasping strategy were validated in offline evaluations and online experiments,\nand the accuracy and generalizability were demonstrated.\n","authors":["Mingxuan Li","Lunwei Zhang","Tiemin Li","Yao Jiang"],"pdf_url":"https://arxiv.org/pdf/2409.10371v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2409.10370v1","updated":"2024-09-16T15:13:39Z","published":"2024-09-16T15:13:39Z","title":"Uncovering the Mechanism of Hepatotoxiciy of PFAS Targeting L-FABP Using\n  GCN and Computational Modeling","summary":"  Per- and polyfluoroalkyl substances (PFAS) are persistent environmental\npollutants with known toxicity and bioaccumulation issues. Their widespread\nindustrial use and resistance to degradation have led to global environmental\ncontamination and significant health concerns. While a minority of PFAS have\nbeen extensively studied, the toxicity of many PFAS remains poorly understood\ndue to limited direct toxicological data. This study advances the predictive\nmodeling of PFAS toxicity by combining semi-supervised graph convolutional\nnetworks (GCNs) with molecular descriptors and fingerprints. We propose a novel\napproach to enhance the prediction of PFAS binding affinities by isolating\nmolecular fingerprints to construct graphs where then descriptors are set as\nthe node features. This approach specifically captures the structural,\nphysicochemical, and topological features of PFAS without overfitting due to an\nabundance of features. Unsupervised clustering then identifies representative\ncompounds for detailed binding studies. Our results provide a more accurate\nability to estimate PFAS hepatotoxicity to provide guidance in chemical\ndiscovery of new PFAS and the development of new safety regulations.\n","authors":["Lucas Jividen","Tibo Duran","Xi-Zhi Niu","Jun Bai"],"pdf_url":"https://arxiv.org/pdf/2409.10370v1.pdf","comment":"8 pages, 9 figures, submitted to IEEE BIBM 2024"},{"id":"http://arxiv.org/abs/2409.10357v1","updated":"2024-09-16T15:06:12Z","published":"2024-09-16T15:06:12Z","title":"2D or not 2D: How Does the Dimensionality of Gesture Representation\n  Affect 3D Co-Speech Gesture Generation?","summary":"  Co-speech gestures are fundamental for communication. The advent of recent\ndeep learning techniques has facilitated the creation of lifelike, synchronous\nco-speech gestures for Embodied Conversational Agents. \"In-the-wild\" datasets,\naggregating video content from platforms like YouTube via human pose detection\ntechnologies, provide a feasible solution by offering 2D skeletal sequences\naligned with speech. Concurrent developments in lifting models enable the\nconversion of these 2D sequences into 3D gesture databases. However, it is\nimportant to note that the 3D poses estimated from the 2D extracted poses are,\nin essence, approximations of the ground-truth, which remains in the 2D domain.\nThis distinction raises questions about the impact of gesture representation\ndimensionality on the quality of generated motions - a topic that, to our\nknowledge, remains largely unexplored. Our study examines the effect of using\neither 2D or 3D joint coordinates as training data on the performance of\nspeech-to-gesture deep generative models. We employ a lifting model for\nconverting generated 2D pose sequences into 3D and assess how gestures created\ndirectly in 3D stack up against those initially generated in 2D and then\nconverted to 3D. We perform an objective evaluation using widely used metrics\nin the gesture generation field as well as a user study to qualitatively\nevaluate the different approaches.\n","authors":["T√©o Guichoux","Laure Soulier","Nicolas Obin","Catherine Pelachaud"],"pdf_url":"https://arxiv.org/pdf/2409.10357v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2406.15111"},{"id":"http://arxiv.org/abs/2406.02285v2","updated":"2024-09-16T14:58:01Z","published":"2024-06-04T12:58:19Z","title":"Towards Supervised Performance on Speaker Verification with\n  Self-Supervised Learning by Leveraging Large-Scale ASR Models","summary":"  Recent advancements in Self-Supervised Learning (SSL) have shown promising\nresults in Speaker Verification (SV). However, narrowing the performance gap\nwith supervised systems remains an ongoing challenge. Several studies have\nobserved that speech representations from large-scale ASR models contain\nvaluable speaker information. This work explores the limitations of fine-tuning\nthese models for SV using an SSL contrastive objective in an end-to-end\napproach. Then, we propose a framework to learn speaker representations in an\nSSL context by fine-tuning a pre-trained WavLM with a supervised loss using\npseudo-labels. Initial pseudo-labels are derived from an SSL DINO-based model\nand are iteratively refined by clustering the model embeddings. Our method\nachieves 0.99% EER on VoxCeleb1-O, establishing the new state-of-the-art on\nself-supervised SV. As this performance is close to our supervised baseline of\n0.94% EER, this contribution is a step towards supervised performance on SV\nwith SSL.\n","authors":["Victor Miara","Theo Lepage","Reda Dehak"],"pdf_url":"https://arxiv.org/pdf/2406.02285v2.pdf","comment":"accepted at INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2409.10340v1","updated":"2024-09-16T14:56:10Z","published":"2024-09-16T14:56:10Z","title":"Hyperedge Modeling in Hypergraph Neural Networks by using Densest\n  Overlapping Subgraphs","summary":"  Hypergraphs tackle the limitations of traditional graphs by introducing {\\em\nhyperedges}. While graph edges connect only two nodes, hyperedges connect an\narbitrary number of nodes along their edges. Also, the underlying\nmessage-passing mechanisms in Hypergraph Neural Networks (HGNNs) are in the\nform of vertex-hyperedge-vertex, which let HGNNs capture and utilize richer and\nmore complex structural information than traditional Graph Neural Networks\n(GNNs). More recently, the idea of overlapping subgraphs has emerged. These\nsubgraphs can capture more information about subgroups of vertices without\nlimiting one vertex belonging to just one group, allowing vertices to belong to\nmultiple groups or subgraphs. In addition, one of the most important problems\nin graph clustering is to find densest overlapping subgraphs (DOS). In this\npaper, we propose a solution to the DOS problem via Agglomerative Greedy\nEnumeration (DOSAGE) algorithm as a novel approach to enhance the process of\ngenerating the densest overlapping subgraphs and, hence, a robust construction\nof the hypergraphs. Experiments on standard benchmarks show that the DOSAGE\nalgorithm significantly outperforms the HGNNs and six other methods on the node\nclassification task.\n","authors":["Mehrad Soltani","Luis Rueda"],"pdf_url":"https://arxiv.org/pdf/2409.10340v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10996v2","updated":"2024-09-16T14:52:47Z","published":"2024-03-16T18:47:04Z","title":"A Scalable and Parallelizable Digital Twin Framework for Sustainable\n  Sim2Real Transition of Multi-Agent Reinforcement Learning Systems","summary":"  Multi-agent reinforcement learning (MARL) systems usually require\nsignificantly long training times due to their inherent complexity.\nFurthermore, deploying them in the real world demands a feature-rich\nenvironment along with multiple embodied agents, which may not be feasible due\nto budget or space limitations, not to mention energy consumption and safety\nissues. This work tries to address these pain points by presenting a\nsustainable digital twin framework capable of accelerating MARL training by\nselectively scaling parallelized workloads on-demand, and transferring the\ntrained policies from simulation to reality using minimal hardware resources.\nThe applicability of the proposed digital twin framework is highlighted through\ntwo representative use cases, which cover cooperative as well as competitive\nclasses of MARL problems. We study the effect of agent and environment\nparallelization on training time and that of systematic domain randomization on\nzero-shot sim2real transfer across both the case studies. Results indicate up\nto 76.3% reduction in training time with the proposed parallelization scheme\nand as low as 2.9% sim2real gap using the suggested deployment method.\n","authors":["Chinmay Vilas Samak","Tanmay Vilas Samak","Venkat Krovi"],"pdf_url":"https://arxiv.org/pdf/2403.10996v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05870v2","updated":"2024-09-16T14:52:46Z","published":"2024-06-09T17:55:55Z","title":"Machine Against the RAG: Jamming Retrieval-Augmented Generation with\n  Blocker Documents","summary":"  Retrieval-augmented generation (RAG) systems respond to queries by retrieving\nrelevant documents from a knowledge database, then generating an answer by\napplying an LLM to the retrieved documents. We demonstrate that RAG systems\nthat operate on databases with untrusted content are vulnerable to a new class\nof denial-of-service attacks we call jamming. An adversary can add a single\n``blocker'' document to the database that will be retrieved in response to a\nspecific query and result in the RAG system not answering this query -\nostensibly because it lacks the information or because the answer is unsafe.\n  We describe and measure the efficacy of several methods for generating\nblocker documents, including a new method based on black-box optimization. This\nmethod (1) does not rely on instruction injection, (2) does not require the\nadversary to know the embedding or LLM used by the target RAG system, and (3)\ndoes not use an auxiliary LLM to generate blocker documents.\n  We evaluate jamming attacks on several LLMs and embeddings and demonstrate\nthat the existing safety metrics for LLMs do not capture their vulnerability to\njamming. We then discuss defenses against blocker documents.\n","authors":["Avital Shafran","Roei Schuster","Vitaly Shmatikov"],"pdf_url":"https://arxiv.org/pdf/2406.05870v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10339v1","updated":"2024-09-16T14:52:22Z","published":"2024-09-16T14:52:22Z","title":"VAE-QWGAN: Improving Quantum GANs for High Resolution Image Generation","summary":"  This paper presents a novel hybrid quantum generative model, the VAE-QWGAN,\nwhich combines the strengths of a classical Variational AutoEncoder (VAE) with\na hybrid Quantum Wasserstein Generative Adversarial Network (QWGAN). The\nVAE-QWGAN integrates the VAE decoder and QGAN generator into a single quantum\nmodel with shared parameters, utilizing the VAE's encoder for latent vector\nsampling during training. To generate new data from the trained model at\ninference, input latent vectors are sampled from a Gaussian Mixture Model\n(GMM), learnt on the training latent vectors. This, in turn, enhances the\ndiversity and quality of generated images. We evaluate the model's performance\non MNIST/Fashion-MNIST datasets, and demonstrate improved quality and diversity\nof generated images compared to existing approaches.\n","authors":["Aaron Mark Thomas","Sharu Theresa Jose"],"pdf_url":"https://arxiv.org/pdf/2409.10339v1.pdf","comment":"5 pages, 8 figures"},{"id":"http://arxiv.org/abs/2406.12709v2","updated":"2024-09-16T14:44:53Z","published":"2024-06-18T15:23:10Z","title":"Enhancing Spatio-temporal Quantile Forecasting with Curriculum Learning:\n  Lessons Learned","summary":"  Training models on spatio-temporal (ST) data poses an open problem due to the\ncomplicated and diverse nature of the data itself, and it is challenging to\nensure the model's performance directly trained on the original ST data. While\nlimiting the variety of training data can make training easier, it can also\nlead to a lack of knowledge and information for the model, resulting in a\ndecrease in performance. To address this challenge, we presented an innovative\nparadigm that incorporates three separate forms of curriculum learning\nspecifically targeting from spatial, temporal, and quantile perspectives.\nFurthermore, our framework incorporates a stacking fusion module to combine\ndiverse information from three types of curriculum learning, resulting in a\nstrong and thorough learning process. We demonstrated the effectiveness of this\nframework with extensive empirical evaluations, highlighting its better\nperformance in addressing complex ST challenges. We provided thorough ablation\nstudies to investigate the effectiveness of our curriculum and to explain how\nit contributes to the improvement of learning efficiency on ST data.\n","authors":["Du Yin","Jinliang Deng","Shuang Ao","Zechen Li","Hao Xue","Arian Prabowo","Renhe Jiang","Xuan Song","Flora Salim"],"pdf_url":"https://arxiv.org/pdf/2406.12709v2.pdf","comment":"accept by sigspatial 2024"},{"id":"http://arxiv.org/abs/2409.10331v1","updated":"2024-09-16T14:41:41Z","published":"2024-09-16T14:41:41Z","title":"Research and Design of a Financial Intelligent Risk Control Platform\n  Based on Big Data Analysis and Deep Machine Learning","summary":"  In the financial field of the United States, the application of big data\ntechnology has become one of the important means for financial institutions to\nenhance competitiveness and reduce risks. The core objective of this article is\nto explore how to fully utilize big data technology to achieve complete\nintegration of internal and external data of financial institutions, and create\nan efficient and reliable platform for big data collection, storage, and\nanalysis. With the continuous expansion and innovation of financial business,\ntraditional risk management models are no longer able to meet the increasingly\ncomplex market demands. This article adopts big data mining and real-time\nstreaming data processing technology to monitor, analyze, and alert various\nbusiness data. Through statistical analysis of historical data and precise\nmining of customer transaction behavior and relationships, potential risks can\nbe more accurately identified and timely responses can be made. This article\ndesigns and implements a financial big data intelligent risk control platform.\nThis platform not only achieves effective integration, storage, and analysis of\ninternal and external data of financial institutions, but also intelligently\ndisplays customer characteristics and their related relationships, as well as\nintelligent supervision of various risk information\n","authors":["Shuochen Bi","Yufan Lian","Ziyue Wang"],"pdf_url":"https://arxiv.org/pdf/2409.10331v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.12830v2","updated":"2024-09-16T14:40:16Z","published":"2024-01-23T15:07:49Z","title":"Enhancing Next Destination Prediction: A Novel Long Short-Term Memory\n  Neural Network Approach Using Real-World Airline Data","summary":"  In the modern transportation industry, accurate prediction of travelers' next\ndestinations brings multiple benefits to companies, such as customer\nsatisfaction and targeted marketing. This study focuses on developing a precise\nmodel that captures the sequential patterns and dependencies in travel data,\nenabling accurate predictions of individual travelers' future destinations. To\nachieve this, a novel model architecture with a sliding window approach based\non Long Short-Term Memory (LSTM) is proposed for destination prediction in the\ntransportation industry. The experimental results highlight satisfactory\nperformance and high scores achieved by the proposed model across different\ndata sizes and performance metrics. This research contributes to advancing\ndestination prediction methods, empowering companies to deliver personalized\nrecommendations and optimize customer experiences in the dynamic travel\nlandscape.\n","authors":["Salih Salihoglu","Gulser Koksal","Orhan Abar"],"pdf_url":"https://arxiv.org/pdf/2401.12830v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10323v1","updated":"2024-09-16T14:35:00Z","published":"2024-09-16T14:35:00Z","title":"On the Hardness of Meaningful Local Guarantees in Nonsmooth Nonconvex\n  Optimization","summary":"  We study the oracle complexity of nonsmooth nonconvex optimization, with the\nalgorithm assumed to have access only to local function information. It has\nbeen shown by Davis, Drusvyatskiy, and Jiang (2023) that for nonsmooth\nLipschitz functions satisfying certain regularity and strictness conditions,\nperturbed gradient descent converges to local minimizers asymptotically.\nMotivated by this result and by other recent algorithmic advances in nonconvex\nnonsmooth optimization concerning Goldstein stationarity, we consider the\nquestion of obtaining a non-asymptotic rate of convergence to local minima for\nthis problem class.\n  We provide the following negative answer to this question: Local algorithms\nacting on regular Lipschitz functions cannot, in the worst case, provide\nmeaningful local guarantees in terms of function value in sub-exponential time,\neven when all near-stationary points are global minima. This sharply contrasts\nwith the smooth setting, for which it is well-known that standard gradient\nmethods can do so in a dimension-independent rate. Our result complements the\nrich body of work in the theoretical computer science literature that provide\nhardness results conditional on conjectures such as $\\mathsf{P}\\neq\\mathsf{NP}$\nor cryptographic assumptions, in that ours holds unconditional of any such\nassumptions.\n","authors":["Guy Kornowski","Swati Padmanabhan","Ohad Shamir"],"pdf_url":"https://arxiv.org/pdf/2409.10323v1.pdf","comment":"27 pages; comments welcome!"},{"id":"http://arxiv.org/abs/2409.10320v1","updated":"2024-09-16T14:33:21Z","published":"2024-09-16T14:33:21Z","title":"SEAL: Towards Safe Autonomous Driving via Skill-Enabled Adversary\n  Learning for Closed-Loop Scenario Generation","summary":"  Verification and validation of autonomous driving (AD) systems and components\nis of increasing importance, as such technology increases in real-world\nprevalence. Safety-critical scenario generation is a key approach to robustify\nAD policies through closed-loop training. However, existing approaches for\nscenario generation rely on simplistic objectives, resulting in\noverly-aggressive or non-reactive adversarial behaviors. To generate diverse\nadversarial yet realistic scenarios, we propose SEAL, a scenario perturbation\napproach which leverages learned scoring functions and adversarial, human-like\nskills. SEAL-perturbed scenarios are more realistic than SOTA baselines,\nleading to improved ego task success across real-world, in-distribution, and\nout-of-distribution scenarios, of more than 20%. To facilitate future research,\nwe release our code and tools: https://github.com/cmubig/SEAL\n","authors":["Benjamin Stoler","Ingrid Navarro","Jonathan Francis","Jean Oh"],"pdf_url":"https://arxiv.org/pdf/2409.10320v1.pdf","comment":"8 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.04405v3","updated":"2024-09-16T14:30:14Z","published":"2024-08-08T12:14:17Z","title":"Probabilistic energy forecasting through quantile regression in\n  reproducing kernel Hilbert spaces","summary":"  Accurate energy demand forecasting is crucial for sustainable and resilient\nenergy development. To meet the Net Zero Representative Concentration Pathways\n(RCP) $4.5$ scenario in the DACH countries, increased renewable energy\nproduction, energy storage, and reduced commercial building consumption are\nneeded. This scenario's success depends on hydroelectric capacity and climatic\nfactors. Informed decisions require quantifying uncertainty in forecasts. This\nstudy explores a non-parametric method based on \\emph{reproducing kernel\nHilbert spaces (RKHS)}, known as kernel quantile regression, for energy\nprediction. Our experiments demonstrate its reliability and sharpness, and we\nbenchmark it against state-of-the-art methods in load and price forecasting for\nthe DACH region. We offer our implementation in conjunction with additional\nscripts to ensure the reproducibility of our research.\n","authors":["Luca Pernigo","Rohan Sen","Davide Baroli"],"pdf_url":"https://arxiv.org/pdf/2408.04405v3.pdf","comment":"12 pages, {Owner/Author | ACM} {2024}. This is the author's version\n  of the work. It is posted here for your personal use. Not for redistribution.\n  The definitive Version of Record will published in https://energy.acm.org/eir"},{"id":"http://arxiv.org/abs/2310.10461v3","updated":"2024-09-16T14:24:48Z","published":"2023-10-16T14:42:22Z","title":"Model Selection of Anomaly Detectors in the Absence of Labeled\n  Validation Data","summary":"  Anomaly detection is the task of identifying abnormal samples in large\nunlabeled datasets. While the advent of foundation models has produced powerful\nzero-shot anomaly detection methods, their deployment in practice is often\nhindered by the absence of labeled validation data -- without it, their\ndetection performance cannot be evaluated reliably. In this work, we propose\nSWSA (Selection With Synthetic Anomalies): a general-purpose framework to\nselect image-based anomaly detectors without labeled validation data. Instead\nof collecting labeled validation data, we generate synthetic anomalies without\nany training or fine-tuning, using only a small support set of normal images.\nOur synthetic anomalies are used to create detection tasks that compose a\nvalidation framework for model selection. In an empirical study, we evaluate\nSWSA with three types of synthetic anomalies and on two selection tasks: model\nselection of image-based anomaly detectors and prompt selection for CLIP-based\nanomaly detection. SWSA often selects models and prompts that match selections\nmade with a ground-truth validation set, outperforming baseline selection\nstrategies.\n","authors":["Clement Fung","Chen Qiu","Aodong Li","Maja Rudolph"],"pdf_url":"https://arxiv.org/pdf/2310.10461v3.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2403.10586v2","updated":"2024-09-16T14:19:39Z","published":"2024-03-15T17:03:45Z","title":"Reviewing AI's Role in Non-Muscle-Invasive Bladder Cancer Recurrence\n  Prediction","summary":"  Notorious for its 70-80% recurrence rate, Non-muscle-invasive Bladder Cancer\n(NMIBC) imposes a significant human burden and is one of the costliest cancers\nto manage. Current tools for predicting NMIBC recurrence rely on scoring\nsystems that often overestimate risk and have poor accuracy. This is where\nMachine learning (ML)-based techniques have emerged as a promising approach for\npredicting NMIBC recurrence by leveraging molecular and clinical data. This\ncomprehensive review paper critically analyses ML-based frameworks for\npredicting NMIBC recurrence, focusing on their statistical robustness and\nalgorithmic efficacy. We meticulously examine the strengths and weaknesses of\neach study, by focusing on various prediction tasks, data modalities, and ML\nmodels, highlighting their remarkable performance alongside inherent\nlimitations. A diverse array of ML algorithms that leverage multimodal data\nspanning radiomics, clinical, histopathological, and genomic data, exhibit\nsignificant promise in accurately predicting NMIBC recurrence. However, the\npath to widespread adoption faces challenges concerning the generalisability\nand interpretability of models, emphasising the need for collaborative efforts,\nrobust datasets, and the incorporation of cost-effectiveness. Our detailed\ncategorisation and in-depth analysis illuminate the nuances, complexities, and\ncontexts that influence real-world advancement and adoption of these AI-based\ntechniques. This rigorous analysis equips researchers with a deeper\nunderstanding of the intricacies of the ML algorithms employed. Researchers can\nuse these insights to refine approaches, address limitations, and boost\ngeneralisability of their ML models, ultimately leading to reduced healthcare\ncosts and improved patient outcomes.\n","authors":["Saram Abbas","Rishad Shafik","Naeem Soomro","Rakesh Heer","Kabita Adhikari"],"pdf_url":"https://arxiv.org/pdf/2403.10586v2.pdf","comment":"14 pages, 3 Figures"},{"id":"http://arxiv.org/abs/2305.07316v2","updated":"2024-09-16T14:13:03Z","published":"2023-05-12T08:43:28Z","title":"Parameterized Approximation for Robust Clustering in Discrete Geometric\n  Spaces","summary":"  We consider the well-studied Robust $(k, z)$-Clustering problem, which\ngeneralizes the classic $k$-Median, $k$-Means, and $k$-Center problems. Given a\nconstant $z\\ge 1$, the input to Robust $(k, z)$-Clustering is a set $P$ of $n$\nweighted points in a metric space $(M,\\delta)$ and a positive integer $k$.\nFurther, each point belongs to one (or more) of the $m$ many different groups\n$S_1,S_2,\\ldots,S_m$. Our goal is to find a set $X$ of $k$ centers such that\n$\\max_{i \\in [m]} \\sum_{p \\in S_i} w(p) \\delta(p,X)^z$ is minimized.\n  This problem arises in the domains of robust optimization [Anthony, Goyal,\nGupta, Nagarajan, Math. Oper. Res. 2010] and in algorithmic fairness. For\npolynomial time computation, an approximation factor of $O(\\log m/\\log\\log m)$\nis known [Makarychev, Vakilian, COLT $2021$], which is tight under a plausible\ncomplexity assumption even in the line metrics. For FPT time, there is a\n$(3^z+\\epsilon)$-approximation algorithm, which is tight under GAP-ETH [Goyal,\nJaiswal, Inf. Proc. Letters, 2023].\n  Motivated by the tight lower bounds for general discrete metrics, we focus on\n\\emph{geometric} spaces such as the (discrete) high-dimensional Euclidean\nsetting and metrics of low doubling dimension, which play an important role in\ndata analysis applications. First, for a universal constant $\\eta_0 >0.0006$,\nwe devise a $3^z(1-\\eta_{0})$-factor FPT approximation algorithm for discrete\nhigh-dimensional Euclidean spaces thereby bypassing the lower bound for general\nmetrics. We complement this result by showing that even the special case of\n$k$-Center in dimension $\\Theta(\\log n)$ is $(\\sqrt{3/2}- o(1))$-hard to\napproximate for FPT algorithms. Finally, we complete the FPT approximation\nlandscape by designing an FPT $(1+\\epsilon)$-approximation scheme (EPAS) for\nthe metric of sub-logarithmic doubling dimension.\n","authors":["Fateme Abbasi","Sandip Banerjee","Jaros≈Çaw Byrka","Parinya Chalermsook","Ameet Gadekar","Kamyar Khodamoradi","D√°niel Marx","Roohani Sharma","Joachim Spoerhase"],"pdf_url":"https://arxiv.org/pdf/2305.07316v2.pdf","comment":"26 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.10304v1","updated":"2024-09-16T14:10:38Z","published":"2024-09-16T14:10:38Z","title":"How to do impactful research in artificial intelligence for chemistry\n  and materials science","summary":"  Machine learning has been pervasively touching many fields of science.\nChemistry and materials science are no exception. While machine learning has\nbeen making a great impact, it is still not reaching its full potential or\nmaturity. In this perspective, we first outline current applications across a\ndiversity of problems in chemistry. Then, we discuss how machine learning\nresearchers view and approach problems in the field. Finally, we provide our\nconsiderations for maximizing impact when researching machine learning for\nchemistry.\n","authors":["Austin Cheng","Cher Tian Ser","Marta Skreta","Andr√©s Guzm√°n-Cordero","Luca Thiede","Andreas Burger","Abdulrahman Aldossary","Shi Xuan Leong","Sergio Pablo-Garc√≠a","Felix Strieth-Kalthoff","Al√°n Aspuru-Guzik"],"pdf_url":"https://arxiv.org/pdf/2409.10304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10289v1","updated":"2024-09-16T13:56:17Z","published":"2024-09-16T13:56:17Z","title":"ReflectDiffu: Reflect between Emotion-intent Contagion and Mimicry for\n  Empathetic Response Generation via a RL-Diffusion Framework","summary":"  Empathetic response generation necessitates the integration of emotional and\nintentional dynamics to foster meaningful interactions. Existing research\neither neglects the intricate interplay between emotion and intent, leading to\nsuboptimal controllability of empathy, or resorts to large language models\n(LLMs), which incur significant computational overhead. In this paper, we\nintroduce ReflectDiffu, a lightweight and comprehensive framework for\nempathetic response generation. This framework incorporates emotion contagion\nto augment emotional expressiveness and employs an emotion-reasoning mask to\npinpoint critical emotional elements. Additionally, it integrates intent\nmimicry within reinforcement learning for refinement during diffusion. By\nharnessing an intent twice reflect the mechanism of\nExploring-Sampling-Correcting, ReflectDiffu adeptly translates emotional\ndecision-making into precise intent actions, thereby addressing empathetic\nresponse misalignments stemming from emotional misrecognition. Through\nreflection, the framework maps emotional states to intents, markedly enhancing\nboth response empathy and flexibility. Comprehensive experiments reveal that\nReflectDiffu outperforms existing models regarding relevance, controllability,\nand informativeness, achieving state-of-the-art results in both automatic and\nhuman evaluations.\n","authors":["Jiahao Yuan","Zixiang Di","Zhiqing Cui","Guisong Yang","Usman Naseem"],"pdf_url":"https://arxiv.org/pdf/2409.10289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04525v2","updated":"2024-09-16T13:49:32Z","published":"2024-07-05T14:11:28Z","title":"Enhancing learning in spiking neural networks through neuronal\n  heterogeneity and neuromodulatory signaling","summary":"  Recent progress in artificial intelligence (AI) has been driven by insights\nfrom neuroscience, particularly with the development of artificial neural\nnetworks (ANNs). This has significantly enhanced the replication of complex\ncognitive tasks such as vision and natural language processing. Despite these\nadvances, ANNs struggle with continual learning, adaptable knowledge transfer,\nrobustness, and resource efficiency - capabilities that biological systems\nhandle seamlessly. Specifically, ANNs often overlook the functional and\nmorphological diversity of the brain, hindering their computational\ncapabilities. Furthermore, incorporating cell-type specific neuromodulatory\neffects into ANNs with neuronal heterogeneity could enable learning at two\nspatial scales: spiking behavior at the neuronal level, and synaptic plasticity\nat the circuit level, thereby potentially enhancing their learning abilities.\nIn this article, we summarize recent bio-inspired models, learning rules and\narchitectures and propose a biologically-informed framework for enhancing ANNs.\nOur proposed dual-framework approach highlights the potential of spiking neural\nnetworks (SNNs) for emulating diverse spiking behaviors and dendritic\ncompartments to simulate morphological and functional diversity of neuronal\ncomputations. Finally, we outline how the proposed approach integrates\nbrain-inspired compartmental models and task-driven SNNs, balances\nbioinspiration and complexity, and provides scalable solutions for pressing AI\nchallenges, such as continual learning, adaptability, robustness, and\nresource-efficiency.\n","authors":["Alejandro Rodriguez-Garcia","Jie Mei","Srikanth Ramaswamy"],"pdf_url":"https://arxiv.org/pdf/2407.04525v2.pdf","comment":"32 pages, 4 figures, 3 boxes"},{"id":"http://arxiv.org/abs/2205.04641v2","updated":"2024-09-16T13:48:17Z","published":"2022-05-10T03:18:48Z","title":"On Causality in Domain Adaptation and Semi-Supervised Learning: an\n  Information-Theoretic Analysis for Parametric Models","summary":"  Recent advancements in unsupervised domain adaptation (UDA) and\nsemi-supervised learning (SSL), particularly incorporating causality, have led\nto significant methodological improvements in these learning problems. However,\na formal theory that explains the role of causality in the generalization\nperformance of UDA/SSL is still lacking. In this paper, we consider the UDA/SSL\nscenarios where we access $m$ labelled source data and $n$ unlabelled target\ndata as training instances under different causal settings with a parametric\nprobabilistic model. We study the learning performance (e.g., excess risk) of\nprediction in the target domain from an information-theoretic perspective.\nSpecifically, we distinguish two scenarios: the learning problem is called\ncausal learning if the feature is the cause and the label is the effect, and is\ncalled anti-causal learning otherwise. We show that in causal learning, the\nexcess risk depends on the size of the source sample at a rate of\n$O(\\frac{1}{m})$ only if the labelling distribution between the source and\ntarget domains remains unchanged. In anti-causal learning, we show that the\nunlabelled data dominate the performance at a rate of typically\n$O(\\frac{1}{n})$. These results bring out the relationship between the data\nsample size and the hardness of the learning problem with different causal\nmechanisms.\n","authors":["Xuetong Wu","Mingming Gong","Jonathan H. Manton","Uwe Aickelin","Jingge Zhu"],"pdf_url":"https://arxiv.org/pdf/2205.04641v2.pdf","comment":"56 pages Including Appendix"},{"id":"http://arxiv.org/abs/2409.10286v1","updated":"2024-09-16T13:47:52Z","published":"2024-09-16T13:47:52Z","title":"Enhancing Image Classification in Small and Unbalanced Datasets through\n  Synthetic Data Augmentation","summary":"  Accurate and robust medical image classification is a challenging task,\nespecially in application domains where available annotated datasets are small\nand present high imbalance between target classes. Considering that data\nacquisition is not always feasible, especially for underrepresented classes,\nour approach introduces a novel synthetic augmentation strategy using\nclass-specific Variational Autoencoders (VAEs) and latent space interpolation\nto improve discrimination capabilities.\n  By generating realistic, varied synthetic data that fills feature space gaps,\nwe address issues of data scarcity and class imbalance. The method presented in\nthis paper relies on the interpolation of latent representations within each\nclass, thus enriching the training set and improving the model's\ngeneralizability and diagnostic accuracy. The proposed strategy was tested in a\nsmall dataset of 321 images created to train and validate an automatic method\nfor assessing the quality of cleanliness of esophagogastroduodenoscopy images.\nBy combining real and synthetic data, an increase of over 18\\% in the accuracy\nof the most challenging underrepresented class was observed. The proposed\nstrategy not only benefited the underrepresented class but also led to a\ngeneral improvement in other metrics, including a 6\\% increase in global\naccuracy and precision.\n","authors":["Neil De La Fuente","Mireia Maj√≥","Irina Luzko","Henry C√≥rdova","Gloria Fern√°ndez-Esparrach","Jorge Bernal"],"pdf_url":"https://arxiv.org/pdf/2409.10286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.12706v3","updated":"2024-09-16T13:36:34Z","published":"2022-05-25T12:02:59Z","title":"Maximum Mean Discrepancy on Exponential Windows for Online Change\n  Detection","summary":"  Detecting changes is of fundamental importance when analyzing data streams\nand has many applications, e.g., in predictive maintenance, fraud detection, or\nmedicine. A principled approach to detect changes is to compare the\ndistributions of observations within the stream to each other via hypothesis\ntesting. Maximum mean discrepancy (MMD), a (semi-)metric on the space of\nprobability distributions, provides powerful non-parametric two-sample tests on\nkernel-enriched domains. In particular, MMD is able to detect any disparity\nbetween distributions under mild conditions. However, classical MMD estimators\nsuffer from a quadratic runtime complexity, which renders their direct use for\nchange detection in data streams impractical. In this article, we propose a new\nchange detection algorithm, called Maximum Mean Discrepancy on Exponential\nWindows (MMDEW), that combines the benefits of MMD with an efficient\ncomputation based on exponential windows. We prove that MMDEW enjoys\npolylogarithmic runtime and logarithmic memory complexity and show empirically\nthat it outperforms the state of the art on benchmark data streams.\n","authors":["Florian Kalinke","Marco Heyden","Georg Gntuni","Edouard Fouch√©","Klemens B√∂hm"],"pdf_url":"https://arxiv.org/pdf/2205.12706v3.pdf","comment":"Add experiments regarding impact of test level"},{"id":"http://arxiv.org/abs/2409.10269v1","updated":"2024-09-16T13:25:42Z","published":"2024-09-16T13:25:42Z","title":"BAFNet: Bilateral Attention Fusion Network for Lightweight Semantic\n  Segmentation of Urban Remote Sensing Images","summary":"  Large-scale semantic segmentation networks often achieve high performance,\nwhile their application can be challenging when faced with limited sample sizes\nand computational resources. In scenarios with restricted network size and\ncomputational complexity, models encounter significant challenges in capturing\nlong-range dependencies and recovering detailed information in images. We\npropose a lightweight bilateral semantic segmentation network called bilateral\nattention fusion network (BAFNet) to efficiently segment high-resolution urban\nremote sensing images. The model consists of two paths, namely dependency path\nand remote-local path. The dependency path utilizes large kernel attention to\nacquire long-range dependencies in the image. Besides, multi-scale local\nattention and efficient remote attention are designed to construct remote-local\npath. Finally, a feature aggregation module is designed to effectively utilize\nthe different features of the two paths. Our proposed method was tested on\npublic high-resolution urban remote sensing datasets Vaihingen and Potsdam,\nwith mIoU reaching 83.20% and 86.53%, respectively. As a lightweight semantic\nsegmentation model, BAFNet not only outperforms advanced lightweight models in\naccuracy but also demonstrates comparable performance to non-lightweight\nstate-of-the-art methods on two datasets, despite a tenfold variance in\nfloating-point operations and a fifteenfold difference in network parameters.\n","authors":["Wentao Wang","Xili Wang"],"pdf_url":"https://arxiv.org/pdf/2409.10269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06509v2","updated":"2024-09-16T13:22:16Z","published":"2024-09-10T13:41:08Z","title":"Aligning Machine and Human Visual Representations across Abstraction\n  Levels","summary":"  Deep neural networks have achieved success across a wide range of\napplications, including as models of human behavior in vision tasks. However,\nneural network training and human learning differ in fundamental ways, and\nneural networks often fail to generalize as robustly as humans do, raising\nquestions regarding the similarity of their underlying representations. What is\nmissing for modern learning systems to exhibit more human-like behavior? We\nhighlight a key misalignment between vision models and humans: whereas human\nconceptual knowledge is hierarchically organized from fine- to coarse-scale\ndistinctions, model representations do not accurately capture all these levels\nof abstraction. To address this misalignment, we first train a teacher model to\nimitate human judgments, then transfer human-like structure from its\nrepresentations into pretrained state-of-the-art vision foundation models.\nThese human-aligned models more accurately approximate human behavior and\nuncertainty across a wide range of similarity tasks, including a new dataset of\nhuman judgments spanning multiple levels of semantic abstractions. They also\nperform better on a diverse set of machine learning tasks, increasing\ngeneralization and out-of-distribution robustness. Thus, infusing neural\nnetworks with additional human knowledge yields a best-of-both-worlds\nrepresentation that is both more consistent with human cognition and more\npractically useful, thus paving the way toward more robust, interpretable, and\nhuman-like artificial intelligence systems.\n","authors":["Lukas Muttenthaler","Klaus Greff","Frieda Born","Bernhard Spitzer","Simon Kornblith","Michael C. Mozer","Klaus-Robert M√ºller","Thomas Unterthiner","Andrew K. Lampinen"],"pdf_url":"https://arxiv.org/pdf/2409.06509v2.pdf","comment":"51 pages"},{"id":"http://arxiv.org/abs/2409.10267v1","updated":"2024-09-16T13:21:09Z","published":"2024-09-16T13:21:09Z","title":"Enhancing Personalized Recipe Recommendation Through Multi-Class\n  Classification","summary":"  This paper intends to address the challenge of personalized recipe\nrecommendation in the realm of diverse culinary preferences. The problem domain\ninvolves recipe recommendations, utilizing techniques such as association\nanalysis and classification. Association analysis explores the relationships\nand connections between different ingredients to enhance the user experience.\nMeanwhile, the classification aspect involves categorizing recipes based on\nuser-defined ingredients and preferences. A unique aspect of the paper is the\nconsideration of recipes and ingredients belonging to multiple classes,\nrecognizing the complexity of culinary combinations. This necessitates a\nsophisticated approach to classification and recommendation, ensuring the\nsystem accommodates the nature of recipe categorization. The paper seeks not\nonly to recommend recipes but also to explore the process involved in achieving\naccurate and personalized recommendations.\n","authors":["Harish Neelam","Koushik Sai Veerella"],"pdf_url":"https://arxiv.org/pdf/2409.10267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10263v1","updated":"2024-09-16T13:13:15Z","published":"2024-09-16T13:13:15Z","title":"Hierarchical Graph Pooling Based on Minimum Description Length","summary":"  Graph pooling is an essential part of deep graph representation learning. We\nintroduce MapEqPool, a principled pooling operator that takes the inherent\nhierarchical structure of real-world graphs into account. MapEqPool builds on\nthe map equation, an information-theoretic objective function for community\ndetection based on the minimum description length principle which naturally\nimplements Occam's razor and balances between model complexity and fit. We\ndemonstrate MapEqPool's competitive performance with an empirical comparison\nagainst various baselines across standard graph classification datasets.\n","authors":["Jan von Pichowski","Christopher Bl√∂cker","Ingo Scholtes"],"pdf_url":"https://arxiv.org/pdf/2409.10263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10259v1","updated":"2024-09-16T13:10:58Z","published":"2024-09-16T13:10:58Z","title":"Self-Updating Vehicle Monitoring Framework Employing Distributed\n  Acoustic Sensing towards Real-World Settings","summary":"  The recent emergence of Distributed Acoustic Sensing (DAS) technology has\nfacilitated the effective capture of traffic-induced seismic data. The\ntraffic-induced seismic wave is a prominent contributor to urban vibrations and\ncontain crucial information to advance urban exploration and governance.\nHowever, identifying vehicular movements within massive noisy data poses a\nsignificant challenge. In this study, we introduce a real-time semi-supervised\nvehicle monitoring framework tailored to urban settings. It requires only a\nsmall fraction of manual labels for initial training and exploits unlabeled\ndata for model improvement. Additionally, the framework can autonomously adapt\nto newly collected unlabeled data. Before DAS data undergo object detection as\ntwo-dimensional images to preserve spatial information, we leveraged\ncomprehensive one-dimensional signal preprocessing to mitigate noise.\nFurthermore, we propose a novel prior loss that incorporates the shapes of\nvehicular traces to track a single vehicle with varying speeds. To evaluate our\nmodel, we conducted experiments with seismic data from the Stanford 2 DAS\nArray. The results showed that our model outperformed the baseline model\nEfficient Teacher and its supervised counterpart, YOLO (You Only Look Once), in\nboth accuracy and robustness. With only 35 labeled images, our model surpassed\nYOLO's mAP 0.5:0.95 criterion by 18% and showed a 7% increase over Efficient\nTeacher. We conducted comparative experiments with multiple update strategies\nfor self-updating and identified an optimal approach. This approach surpasses\nthe performance of non-overfitting training conducted with all data in a single\npass.\n","authors":["Xi Wang","Xin Liu","Songming Zhu","Zhanwen Li","Lina Gao"],"pdf_url":"https://arxiv.org/pdf/2409.10259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04778v2","updated":"2024-09-16T12:48:34Z","published":"2024-01-09T19:07:59Z","title":"Generative neural networks for characteristic functions","summary":"  We provide a simulation algorithm to simulate from a (multivariate)\ncharacteristic function, which is only accessible in a black-box format. The\nmethod is based on a generative neural network, whose loss function exploits a\nspecific representation of the Maximum-Mean-Discrepancy metric to directly\nincorporate the targeted characteristic function. The algorithm is universal in\nthe sense that it is independent of the dimension and that it does not require\nany assumptions on the given characteristic function. Furthermore, finite\nsample guarantees on the approximation quality in terms of the Maximum-Mean\nDiscrepancy metric are derived. The method is illustrated in a simulation\nstudy.\n","authors":["Florian Br√ºck"],"pdf_url":"https://arxiv.org/pdf/2401.04778v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07606v2","updated":"2024-09-16T12:45:07Z","published":"2024-09-11T20:35:29Z","title":"The Role of Deep Learning Regularizations on Actors in Offline RL","summary":"  Deep learning regularization techniques, such as dropout, layer\nnormalization, or weight decay, are widely adopted in the construction of\nmodern artificial neural networks, often resulting in more robust training\nprocesses and improved generalization capabilities. However, in the domain of\nReinforcement Learning (RL), the application of these techniques has been\nlimited, usually applied to value function estimators, and may result in\ndetrimental effects. This issue is even more pronounced in offline RL settings,\nwhich bear greater similarity to supervised learning but have received less\nattention. Recent work in continuous offline RL has demonstrated that while we\ncan build sufficiently powerful critic networks, the generalization of actor\nnetworks remains a bottleneck. In this study, we empirically show that applying\nstandard regularization techniques to actor networks in offline RL actor-critic\nalgorithms yields improvements of 6% on average across two algorithms and three\ndifferent continuous D4RL domains.\n","authors":["Denis Tarasov","Anja Surina","Caglar Gulcehre"],"pdf_url":"https://arxiv.org/pdf/2409.07606v2.pdf","comment":"https://github.com/DT6A/ActoReg"},{"id":"http://arxiv.org/abs/2409.10242v1","updated":"2024-09-16T12:45:03Z","published":"2024-09-16T12:45:03Z","title":"Hedging Is Not All You Need: A Simple Baseline for Online Learning Under\n  Haphazard Inputs","summary":"  Handling haphazard streaming data, such as data from edge devices, presents a\nchallenging problem. Over time, the incoming data becomes inconsistent, with\nmissing, faulty, or new inputs reappearing. Therefore, it requires models that\nare reliable. Recent methods to solve this problem depend on a hedging-based\nsolution and require specialized elements like auxiliary dropouts, forked\narchitectures, and intricate network design. We observed that hedging can be\nreduced to a special case of weighted residual connection; this motivated us to\napproximate it with plain self-attention. In this work, we propose HapNet, a\nsimple baseline that is scalable, does not require online backpropagation, and\nis adaptable to varying input types. All present methods are restricted to\nscaling with a fixed window; however, we introduce a more complex problem of\nscaling with a variable window where the data becomes positionally\nuncorrelated, and cannot be addressed by present methods. We demonstrate that a\nvariant of the proposed approach can work even for this complex scenario. We\nextensively evaluated the proposed approach on five benchmarks and found\ncompetitive performance.\n","authors":["Himanshu Buckchash","Momojit Biswas","Rohit Agarwal","Dilip K. Prasad"],"pdf_url":"https://arxiv.org/pdf/2409.10242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10164v2","updated":"2024-09-16T12:42:47Z","published":"2024-03-15T10:18:06Z","title":"CoReEcho: Continuous Representation Learning for 2D+time\n  Echocardiography Analysis","summary":"  Deep learning (DL) models have been advancing automatic medical image\nanalysis on various modalities, including echocardiography, by offering a\ncomprehensive end-to-end training pipeline. This approach enables DL models to\nregress ejection fraction (EF) directly from 2D+time echocardiograms, resulting\nin superior performance. However, the end-to-end training pipeline makes the\nlearned representations less explainable. The representations may also fail to\ncapture the continuous relation among echocardiogram clips, indicating the\nexistence of spurious correlations, which can negatively affect the\ngeneralization. To mitigate this issue, we propose CoReEcho, a novel training\nframework emphasizing continuous representations tailored for direct EF\nregression. Our extensive experiments demonstrate that CoReEcho: 1) outperforms\nthe current state-of-the-art (SOTA) on the largest echocardiography dataset\n(EchoNet-Dynamic) with MAE of 3.90 & R2 of 82.44, and 2) provides robust and\ngeneralizable features that transfer more effectively in related downstream\ntasks. The code is publicly available at https://github.com/fadamsyah/CoReEcho.\n","authors":["Fadillah Adamsyah Maani","Numan Saeed","Aleksandr Matsun","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2403.10164v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13291v2","updated":"2024-09-16T12:34:52Z","published":"2024-07-18T08:45:14Z","title":"Scikit-fingerprints: easy and efficient computation of molecular\n  fingerprints in Python","summary":"  In this work, we present \\skfp, a Python package for computation of molecular\nfingerprints for applications in chemoinformatics. Our library offers an\nindustry-standard scikit-learn interface, allowing intuitive usage and easy\nintegration with machine learning pipelines. It is also highly optimized,\nfeaturing parallel computation that enables efficient processing of large\nmolecular datasets. Currently, \\skfp~stands as the most feature-rich library in\nthe open source Python ecosystem, offering over 30 molecular fingerprints. Our\nlibrary simplifies chemoinformatics tasks based on molecular fingerprints,\nincluding molecular property prediction and virtual screening. It is also\nflexible, highly efficient, and fully open source.\n","authors":["Jakub Adamczyk","Piotr Ludynia"],"pdf_url":"https://arxiv.org/pdf/2407.13291v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15189v2","updated":"2024-09-16T12:29:26Z","published":"2024-06-21T14:31:45Z","title":"Causal Learning in Biomedical Applications: A Benchmark","summary":"  Learning causal relationships between a set of variables is a challenging\nproblem in computer science. Many existing artificial benchmark datasets are\nbased on sampling from causal models and thus contain residual information that\nthe ${R} ^2$-sortability can identify. Here, we present a benchmark for methods\nin causal learning using time series. The presented dataset is not\n${R}^2$-sortable and is based on a real-world scenario of the Krebs cycle that\nis used in cells to release energy. We provide four scenarios of learning,\nincluding short and long time series, and provide guidance so that testing is\nunified between possible users.\n","authors":["Petr Ry≈°av√Ω","Xiaoyu He","Jakub Mareƒçek"],"pdf_url":"https://arxiv.org/pdf/2406.15189v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10218v1","updated":"2024-09-16T12:13:41Z","published":"2024-09-16T12:13:41Z","title":"Safety-Oriented Pruning and Interpretation of Reinforcement Learning\n  Policies","summary":"  Pruning neural networks (NNs) can streamline them but risks removing vital\nparameters from safe reinforcement learning (RL) policies. We introduce an\ninterpretable RL method called VERINTER, which combines NN pruning with model\nchecking to ensure interpretable RL safety. VERINTER exactly quantifies the\neffects of pruning and the impact of neural connections on complex safety\nproperties by analyzing changes in safety measurements. This method maintains\nsafety in pruned RL policies and enhances understanding of their safety\ndynamics, which has proven effective in multiple RL settings.\n","authors":["Dennis Gross","Helge Spieker"],"pdf_url":"https://arxiv.org/pdf/2409.10218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10204v1","updated":"2024-09-16T11:55:06Z","published":"2024-09-16T11:55:06Z","title":"Embedded Image-to-Image Translation for Efficient Sim-to-Real Transfer\n  in Learning-based Robot-Assisted Soft Manipulation","summary":"  Recent advances in robotic learning in simulation have shown impressive\nresults in accelerating learning complex manipulation skills. However, the\nsim-to-real gap, caused by discrepancies between simulation and reality, poses\nsignificant challenges for the effective deployment of autonomous surgical\nsystems. We propose a novel approach utilizing image translation models to\nmitigate domain mismatches and facilitate efficient robot skill learning in a\nsimulated environment. Our method involves the use of contrastive unpaired\nImage-to-image translation, allowing for the acquisition of embedded\nrepresentations from these transformed images. Subsequently, these embeddings\nare used to improve the efficiency of training surgical manipulation models. We\nconducted experiments to evaluate the performance of our approach,\ndemonstrating that it significantly enhances task success rates and reduces the\nsteps required for task completion compared to traditional methods. The results\nindicate that our proposed system effectively bridges the sim-to-real gap,\nproviding a robust framework for advancing the autonomy of surgical robots in\nminimally invasive procedures.\n","authors":["Jacinto Colan","Keisuke Sugita","Ana Davila","Yutaro Yamada","Yasuhisa Hasegawa"],"pdf_url":"https://arxiv.org/pdf/2409.10204v1.pdf","comment":"Accepted at 2024 IEEE International Symposium on\n  Micro-NanoMechatronics and Human Science"},{"id":"http://arxiv.org/abs/2409.10203v1","updated":"2024-09-16T11:52:17Z","published":"2024-09-16T11:52:17Z","title":"Efficient Milling Quality Prediction with Explainable Machine Learning","summary":"  This paper presents an explainable machine learning (ML) approach for\npredicting surface roughness in milling. Utilizing a dataset from milling\naluminum alloy 2017A, the study employs random forest regression models and\nfeature importance techniques. The key contributions include developing ML\nmodels that accurately predict various roughness values and identifying\nredundant sensors, particularly those for measuring normal cutting force. Our\nexperiments show that removing certain sensors can reduce costs without\nsacrificing predictive accuracy, highlighting the potential of explainable\nmachine learning to improve cost-effectiveness in machining.\n","authors":["Dennis Gross","Helge Spieker","Arnaud Gotlieb","Ricardo Knoblauch","Mohamed Elmansori"],"pdf_url":"https://arxiv.org/pdf/2409.10203v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2403.18731"},{"id":"http://arxiv.org/abs/2409.10188v1","updated":"2024-09-16T11:30:39Z","published":"2024-09-16T11:30:39Z","title":"Enhancing RL Safety with Counterfactual LLM Reasoning","summary":"  Reinforcement learning (RL) policies may exhibit unsafe behavior and are hard\nto explain. We use counterfactual large language model reasoning to enhance RL\npolicy safety post-training. We show that our approach improves and helps to\nexplain the RL policy safety.\n","authors":["Dennis Gross","Helge Spieker"],"pdf_url":"https://arxiv.org/pdf/2409.10188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17940v2","updated":"2024-09-16T11:29:25Z","published":"2024-04-27T15:44:21Z","title":"CBMAP: Clustering-based manifold approximation and projection for\n  dimensionality reduction","summary":"  Dimensionality reduction methods are employed to decrease data\ndimensionality, either to enhance machine learning performance or to facilitate\ndata visualization in two or three-dimensional spaces. These methods typically\nfall into two categories: feature selection and feature transformation. Feature\nselection retains significant features, while feature transformation projects\ndata into a lower-dimensional space, with linear and nonlinear methods. While\nnonlinear methods excel in preserving local structures and capturing nonlinear\nrelationships, they may struggle with interpreting global structures and can be\ncomputationally intensive. Recent algorithms, such as the t-SNE, UMAP, TriMap,\nand PaCMAP prioritize preserving local structures, often at the expense of\naccurately representing global structures, leading to clusters being spread out\nmore in lower-dimensional spaces. Moreover, these methods heavily rely on\nhyperparameters, making their results sensitive to parameter settings. To\naddress these limitations, this study introduces a clustering-based approach,\nnamely CBMAP (Clustering-Based Manifold Approximation and Projection), for\ndimensionality reduction. CBMAP aims to preserve both global and local\nstructures, ensuring that clusters in lower-dimensional spaces closely resemble\nthose in high-dimensional spaces. Experimental evaluations on benchmark\ndatasets demonstrate CBMAP's efficacy, offering speed, scalability, and minimal\nreliance on hyperparameters. Importantly, CBMAP enables low-dimensional\nprojection of test data, addressing a critical need in machine learning\napplications. CBMAP is made freely available at\nhttps://github.com/doganlab/cbmap and can be installed from the Python Package\nDirectory (PyPI) software repository with the command pip install cbmap.\n","authors":["Berat Dogan"],"pdf_url":"https://arxiv.org/pdf/2404.17940v2.pdf","comment":"22 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2409.10176v1","updated":"2024-09-16T11:10:54Z","published":"2024-09-16T11:10:54Z","title":"TCDformer-based Momentum Transfer Model for Long-term Sports Prediction","summary":"  Accurate sports prediction is a crucial skill for professional coaches, which\ncan assist in developing effective training strategies and scientific\ncompetition tactics. Traditional methods often use complex mathematical\nstatistical techniques to boost predictability, but this often is limited by\ndataset scale and has difficulty handling long-term predictions with variable\ndistributions, notably underperforming when predicting point-set-game\nmulti-level matches. To deal with this challenge, this paper proposes TM2, a\nTCDformer-based Momentum Transfer Model for long-term sports prediction, which\nencompasses a momentum encoding module and a prediction module based on\nmomentum transfer. TM2 initially encodes momentum in large-scale unstructured\ntime series using the local linear scaling approximation (LLSA) module. Then it\ndecomposes the reconstructed time series with momentum transfer into trend and\nseasonal components. The final prediction results are derived from the additive\ncombination of a multilayer perceptron (MLP) for predicting trend components\nand wavelet attention mechanisms for seasonal components. Comprehensive\nexperimental results show that on the 2023 Wimbledon men's tournament datasets,\nTM2 significantly surpasses existing sports prediction models in terms of\nperformance, reducing MSE by 61.64% and MAE by 63.64%.\n","authors":["Hui Liu","Jiacheng Gu","Xiyuan Huang","Junjie Shi","Tongtong Feng","Ning He"],"pdf_url":"https://arxiv.org/pdf/2409.10176v1.pdf","comment":"Under reviewing"},{"id":"http://arxiv.org/abs/2409.10171v1","updated":"2024-09-16T11:03:58Z","published":"2024-09-16T11:03:58Z","title":"Safe and Stable Closed-Loop Learning for Neural-Network-Supported Model\n  Predictive Control","summary":"  Safe learning of control policies remains challenging, both in optimal\ncontrol and reinforcement learning. In this article, we consider safe learning\nof parametrized predictive controllers that operate with incomplete information\nabout the underlying process. To this end, we employ Bayesian optimization for\nlearning the best parameters from closed-loop data. Our method focuses on the\nsystem's overall long-term performance in closed-loop while keeping it safe and\nstable. Specifically, we parametrize the stage cost function of an MPC using a\nfeedforward neural network. This allows for a high degree of flexibility,\nenabling the system to achieve a better closed-loop performance with respect to\na superordinate measure. However, this flexibility also necessitates safety\nmeasures, especially with respect to closed-loop stability. To this end, we\nexplicitly incorporated stability information in the\nBayesian-optimization-based learning procedure, thereby achieving rigorous\nprobabilistic safety guarantees. The proposed approach is illustrated using a\nnumeric example.\n","authors":["Sebastian Hirt","Maik Pfefferkorn","Rolf Findeisen"],"pdf_url":"https://arxiv.org/pdf/2409.10171v1.pdf","comment":"7 pages, 2 figures, accepted for CDC 2024"},{"id":"http://arxiv.org/abs/2409.10164v1","updated":"2024-09-16T10:54:04Z","published":"2024-09-16T10:54:04Z","title":"Quantile Regression for Distributional Reward Models in RLHF","summary":"  Reinforcement learning from human feedback (RLHF) has become a key method for\naligning large language models (LLMs) with human preferences through the use of\nreward models. However, traditional reward models typically generate point\nestimates, which oversimplify the diversity and complexity of human values and\npreferences. In this paper, we introduce Quantile Reward Models (QRMs), a novel\napproach to reward modeling that learns a distribution over rewards instead of\na single scalar value. Our method uses quantile regression to estimate a full,\npotentially multimodal distribution over preferences, providing a more powerful\nand nuanced representation of preferences. This distributional approach can\nbetter capture the diversity of human values, addresses label noise, and\naccommodates conflicting preferences by modeling them as distinct modes in the\ndistribution. Our experimental results show that QRM outperforms comparable\ntraditional point-estimate models on RewardBench. Furthermore, we demonstrate\nthat the additional information provided by the distributional estimates can be\nutilized in downstream applications, such as risk-aware reinforcement learning,\nresulting in LLM policies that generate fewer extremely negative responses. Our\ncode and model are released at https://github.com/Nicolinho/QRM.\n","authors":["Nicolai Dorka"],"pdf_url":"https://arxiv.org/pdf/2409.10164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10161v1","updated":"2024-09-16T10:52:16Z","published":"2024-09-16T10:52:16Z","title":"SplatSim: Zero-Shot Sim2Real Transfer of RGB Manipulation Policies Using\n  Gaussian Splatting","summary":"  Sim2Real transfer, particularly for manipulation policies relying on RGB\nimages, remains a critical challenge in robotics due to the significant domain\nshift between synthetic and real-world visual data. In this paper, we propose\nSplatSim, a novel framework that leverages Gaussian Splatting as the primary\nrendering primitive to reduce the Sim2Real gap for RGB-based manipulation\npolicies. By replacing traditional mesh representations with Gaussian Splats in\nsimulators, SplatSim produces highly photorealistic synthetic data while\nmaintaining the scalability and cost-efficiency of simulation. We demonstrate\nthe effectiveness of our framework by training manipulation policies within\nSplatSim}and deploying them in the real world in a zero-shot manner, achieving\nan average success rate of 86.25%, compared to 97.5% for policies trained on\nreal-world data.\n","authors":["Mohammad Nomaan Qureshi","Sparsh Garg","Francisco Yandun","David Held","George Kantor","Abhishesh Silwal"],"pdf_url":"https://arxiv.org/pdf/2409.10161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10160v1","updated":"2024-09-16T10:51:24Z","published":"2024-09-16T10:51:24Z","title":"Efficient Network Embedding by Approximate Equitable Partitions","summary":"  Structural network embedding is a crucial step in enabling effective\ndownstream tasks for complex systems that aims to project a network into a\nlower-dimensional space while preserving similarities among nodes. We introduce\na simple and efficient embedding technique based on approximate variants of\nequitable partitions. The approximation consists in introducing a user-tunable\ntolerance parameter relaxing the otherwise strict condition for exact equitable\npartitions that can be hardly found in real-world networks. We exploit a\nrelationship between equitable partitions and equivalence relations for Markov\nchains and ordinary differential equations to develop a partition refinement\nalgorithm for computing an approximate equitable partition in polynomial time.\nWe compare our method against state-of-the-art embedding techniques on\nbenchmark networks. We report comparable -- when not superior -- performance\nfor visualization, classification, and regression tasks at a cost between one\nand three orders of magnitude smaller using a prototype implementation,\nenabling the embedding of large-scale networks which could not be efficiently\nhandled by most of the competing techniques.\n","authors":["Giuseppe Squillace","Mirco Tribastone","Max Tschaikowski","Andrea Vandin"],"pdf_url":"https://arxiv.org/pdf/2409.10160v1.pdf","comment":"Accepted at ICDM 2024"},{"id":"http://arxiv.org/abs/2409.10156v1","updated":"2024-09-16T10:41:29Z","published":"2024-09-16T10:41:29Z","title":"Contrastive Learning for Character Detection in Ancient Greek Papyri","summary":"  This thesis investigates the effectiveness of SimCLR, a contrastive learning\ntechnique, in Greek letter recognition, focusing on the impact of various\naugmentation techniques. We pretrain the SimCLR backbone using the Alpub\ndataset (pretraining dataset) and fine-tune it on a smaller ICDAR dataset\n(finetuning dataset) to compare SimCLR's performance against traditional\nbaseline models, which use cross-entropy and triplet loss functions.\nAdditionally, we explore the role of different data augmentation strategies,\nessential for the SimCLR training process. Methodologically, we examine three\nprimary approaches: (1) a baseline model using cross-entropy loss, (2) a\ntriplet embedding model with a classification layer, and (3) a SimCLR\npretrained model with a classification layer. Initially, we train the baseline,\ntriplet, and SimCLR models using 93 augmentations on ResNet-18 and ResNet-50\nnetworks with the ICDAR dataset. From these, the top four augmentations are\nselected using a statistical t-test. Pretraining of SimCLR is conducted on the\nAlpub dataset, followed by fine-tuning on the ICDAR dataset. The triplet loss\nmodel undergoes a similar process, being pretrained on the top four\naugmentations before fine-tuning on ICDAR. Our experiments show that SimCLR\ndoes not outperform the baselines in letter recognition tasks. The baseline\nmodel with cross-entropy loss demonstrates better performance than both SimCLR\nand the triplet loss model. This study provides a detailed evaluation of\ncontrastive learning for letter recognition, highlighting SimCLR's limitations\nwhile emphasizing the strengths of traditional supervised learning models in\nthis task. We believe SimCLR's cropping strategies may cause a semantic shift\nin the input image, reducing training effectiveness despite the large\npretraining dataset. Our code is available at\nhttps://github.com/DIVA-DIA/MT_augmentation_and_contrastive_learning/.\n","authors":["Vedasri Nakka","Andreas Fischer","Rolf Ingold","Lars Vogtlin"],"pdf_url":"https://arxiv.org/pdf/2409.10156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15018v2","updated":"2024-09-16T10:32:28Z","published":"2024-04-23T13:23:27Z","title":"Conformal Predictive Systems Under Covariate Shift","summary":"  Conformal Predictive Systems (CPS) offer a versatile framework for\nconstructing predictive distributions, allowing for calibrated inference and\ninformative decision-making. However, their applicability has been limited to\nscenarios adhering to the Independent and Identically Distributed (IID) model\nassumption. This paper extends CPS to accommodate scenarios characterized by\ncovariate shifts. We therefore propose Weighted CPS (WCPS), akin to Weighted\nConformal Prediction (WCP), leveraging likelihood ratios between training and\ntesting covariate distributions. This extension enables the construction of\nnonparametric predictive distributions capable of handling covariate shifts. We\npresent theoretical underpinnings and conjectures regarding the validity and\nefficacy of WCPS and demonstrate its utility through empirical evaluations on\nboth synthetic and real-world datasets. Our simulation experiments indicate\nthat WCPS are probabilistically calibrated under covariate shift.\n","authors":["Jef Jonkers","Glenn Van Wallendael","Luc Duchateau","Sofie Van Hoecke"],"pdf_url":"https://arxiv.org/pdf/2404.15018v2.pdf","comment":"Accepted at the 13th Symposium on Conformal and Probabilistic\n  Prediction with Applications (COPA), 9-11 September 2024"},{"id":"http://arxiv.org/abs/2409.10142v1","updated":"2024-09-16T10:13:09Z","published":"2024-09-16T10:13:09Z","title":"AALF: Almost Always Linear Forecasting","summary":"  Recent works for time-series forecasting more and more leverage the high\npredictive power of Deep Learning models. With this increase in model\ncomplexity, however, comes a lack in understanding of the underlying model\ndecision process, which is problematic for high-stakes decision making. At the\nsame time, simple, interpretable forecasting methods such as Linear Models can\nstill perform very well, sometimes on-par, with Deep Learning approaches. We\nargue that simple models are good enough most of the time, and forecasting\nperformance can be improved by choosing a Deep Learning method only for certain\npredictions, increasing the overall interpretability of the forecasting\nprocess. In this context, we propose a novel online model selection framework\nwhich uses meta-learning to identify these predictions and only rarely uses a\nnon-interpretable, large model. An extensive empirical study on various\nreal-world datasets shows that our selection methodology outperforms\nstate-of-the-art online model selections methods in most cases. We find that\nalmost always choosing a simple Linear Model for forecasting results in\ncompetitive performance, suggesting that the need for opaque black-box models\nin time-series forecasting is smaller than recent works would suggest.\n","authors":["Matthias Jakobs","Thomas Liebig"],"pdf_url":"https://arxiv.org/pdf/2409.10142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.11777v4","updated":"2024-09-16T10:04:23Z","published":"2022-11-18T20:39:51Z","title":"Dataset of Pathloss and ToA Radio Maps With Localization Application","summary":"  In this article, we present a collection of radio map datasets in dense urban\nsetting, which we generated and made publicly available. The datasets include\nsimulated pathloss/received signal strength (RSS) and time of arrival (ToA)\nradio maps over a large collection of realistic dense urban setting in real\ncity maps. The two main applications of the presented dataset are 1) learning\nmethods that predict the pathloss from input city maps (namely, deep\nlearning-based simulations), and, 2) wireless localization. The fact that the\nRSS and ToA maps are computed by the same simulations over the same city maps\nallows for a fair comparison of the RSS and ToA-based localization methods.\n","authors":["√áaƒükan Yapar","Ron Levie","Gitta Kutyniok","Giuseppe Caire"],"pdf_url":"https://arxiv.org/pdf/2212.11777v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07187v3","updated":"2024-09-16T09:57:35Z","published":"2024-01-14T02:30:19Z","title":"A Survey on Statistical Theory of Deep Learning: Approximation, Training\n  Dynamics, and Generative Models","summary":"  In this article, we review the literature on statistical theories of neural\nnetworks from three perspectives: approximation, training dynamics and\ngenerative models. In the first part, results on excess risks for neural\nnetworks are reviewed in the nonparametric framework of regression (and\nclassification in Appendix~{\\color{blue}B}). These results rely on explicit\nconstructions of neural networks, leading to fast convergence rates of excess\nrisks. Nonetheless, their underlying analysis only applies to the global\nminimizer in the highly non-convex landscape of deep neural networks. This\nmotivates us to review the training dynamics of neural networks in the second\npart. Specifically, we review papers that attempt to answer ``how the neural\nnetwork trained via gradient-based methods finds the solution that can\ngeneralize well on unseen data.'' In particular, two well-known paradigms are\nreviewed: the Neural Tangent Kernel (NTK) paradigm, and Mean-Field (MF)\nparadigm. Last but not least, we review the most recent theoretical\nadvancements in generative models including Generative Adversarial Networks\n(GANs), diffusion models, and in-context learning (ICL) in the Large Language\nModels (LLMs) from two perpsectives reviewed previously, i.e., approximation\nand training dynamics.\n","authors":["Namjoon Suh","Guang Cheng"],"pdf_url":"https://arxiv.org/pdf/2401.07187v3.pdf","comment":"38 pages, 2 figures. Invited for review in Annual Review of\n  Statistics and Its Application"},{"id":"http://arxiv.org/abs/2405.10024v2","updated":"2024-09-16T09:30:40Z","published":"2024-05-16T12:04:55Z","title":"$Œî\\text{-}{\\rm OPE}$: Off-Policy Estimation with Pairs of Policies","summary":"  The off-policy paradigm casts recommendation as a counterfactual\ndecision-making task, allowing practitioners to unbiasedly estimate online\nmetrics using offline data. This leads to effective evaluation metrics, as well\nas learning procedures that directly optimise online success. Nevertheless, the\nhigh variance that comes with unbiasedness is typically the crux that\ncomplicates practical applications. An important insight is that the difference\nbetween policy values can often be estimated with significantly reduced\nvariance, if said policies have positive covariance. This allows us to\nformulate a pairwise off-policy estimation task: $\\Delta\\text{-}{\\rm OPE}$.\n  $\\Delta\\text{-}{\\rm OPE}$ subsumes the common use-case of estimating\nimprovements of a learnt policy over a production policy, using data collected\nby a stochastic logging policy. We introduce $\\Delta\\text{-}{\\rm OPE}$ methods\nbased on the widely used Inverse Propensity Scoring estimator and its\nextensions. Moreover, we characterise a variance-optimal additive control\nvariate that further enhances efficiency. Simulated, offline, and online\nexperiments show that our methods significantly improve performance for both\nevaluation and learning tasks.\n","authors":["Olivier Jeunen","Aleksei Ustimenko"],"pdf_url":"https://arxiv.org/pdf/2405.10024v2.pdf","comment":"Accepted as a short paper in the 2024 ACM Conference on Recommender\n  Systems (RecSys '24)"},{"id":"http://arxiv.org/abs/2402.04620v2","updated":"2024-09-16T09:22:20Z","published":"2024-02-07T07:07:02Z","title":"CataractBot: An LLM-Powered Expert-in-the-Loop Chatbot for Cataract\n  Patients","summary":"  The healthcare landscape is evolving, with patients seeking reliable\ninformation about their health conditions and available treatment options.\nDespite the abundance of information sources, the digital age overwhelms\nindividuals with excess, often inaccurate information. Patients primarily trust\nmedical professionals, highlighting the need for expert-endorsed health\ninformation. However, increased patient loads on experts has led to reduced\ncommunication time, impacting information sharing. To address this gap, we\ndevelop CataractBot, an experts-in-the-loop chatbot powered by LLMs, in\ncollaboration with an eye hospital in India. CataractBot answers cataract\nsurgery related questions instantly by querying a curated knowledge base, and\nprovides expert-verified responses asynchronously. It has multimodal and\nmultilingual capabilities. In an in-the-wild deployment study with 55\nparticipants, CataractBot proved valuable, providing anytime accessibility,\nsaving time, accommodating diverse literacy levels, alleviating power\ndifferences, and adding a privacy layer between patients and doctors. Users\nreported that their trust in the system was established through expert\nverification. Broadly, our results could inform future work on designing\nexpert-mediated LLM bots.\n","authors":["Pragnya Ramjee","Bhuvan Sachdeva","Satvik Golechha","Shreyas Kulkarni","Geeta Fulari","Kaushik Murali","Mohit Jain"],"pdf_url":"https://arxiv.org/pdf/2402.04620v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02141v2","updated":"2024-09-16T09:21:15Z","published":"2024-05-03T14:44:04Z","title":"Multi-Objective Recommendation via Multivariate Policy Learning","summary":"  Real-world recommender systems often need to balance multiple objectives when\ndeciding which recommendations to present to users. These include behavioural\nsignals (e.g. clicks, shares, dwell time), as well as broader objectives (e.g.\ndiversity, fairness). Scalarisation methods are commonly used to handle this\nbalancing task, where a weighted average of per-objective reward signals\ndetermines the final score used for ranking. Naturally, how these weights are\ncomputed exactly, is key to success for any online platform. We frame this as a\ndecision-making task, where the scalarisation weights are actions taken to\nmaximise an overall North Star reward (e.g. long-term user retention or\ngrowth). We extend existing policy learning methods to the continuous\nmultivariate action domain, proposing to maximise a pessimistic lower bound on\nthe North Star reward that the learnt policy will yield. Typical lower bounds\nbased on normal approximations suffer from insufficient coverage, and we\npropose an efficient and effective policy-dependent correction for this. We\nprovide guidance to design stochastic data collection policies, as well as\nhighly sensitive reward signals. Empirical observations from simulations,\noffline and online experiments highlight the efficacy of our deployed approach.\n","authors":["Olivier Jeunen","Jatin Mandav","Ivan Potapov","Nakul Agarwal","Sourabh Vaid","Wenzhe Shi","Aleksei Ustimenko"],"pdf_url":"https://arxiv.org/pdf/2405.02141v2.pdf","comment":"Accepted as a full paper in the 2024 ACM Conference on Recommender\n  Systems (RecSys '24)"},{"id":"http://arxiv.org/abs/2409.10111v1","updated":"2024-09-16T09:20:01Z","published":"2024-09-16T09:20:01Z","title":"Evaluating the Efficacy of Instance Incremental vs. Batch Learning in\n  Delayed Label Environments: An Empirical Study on Tabular Data Streaming for\n  Fraud Detection","summary":"  Real-world tabular learning production scenarios typically involve evolving\ndata streams, where data arrives continuously and its distribution may change\nover time. In such a setting, most studies in the literature regarding\nsupervised learning favor the use of instance incremental algorithms due to\ntheir ability to adapt to changes in the data distribution. Another significant\nreason for choosing these algorithms is \\textit{avoid storing observations in\nmemory} as commonly done in batch incremental settings. However, the design of\ninstance incremental algorithms often assumes immediate availability of labels,\nwhich is an optimistic assumption. In many real-world scenarios, such as fraud\ndetection or credit scoring, labels may be delayed. Consequently, batch\nincremental algorithms are widely used in many real-world tasks. This raises an\nimportant question: \"In delayed settings, is instance incremental learning the\nbest option regarding predictive performance and computational efficiency?\"\nUnfortunately, this question has not been studied in depth, probably due to the\nscarcity of real datasets containing delayed information. In this study, we\nconduct a comprehensive empirical evaluation and analysis of this question\nusing a real-world fraud detection problem and commonly used generated\ndatasets. Our findings indicate that instance incremental learning is not the\nsuperior option, considering on one side state-of-the-art models such as\nAdaptive Random Forest (ARF) and other side batch learning models such as\nXGBoost. Additionally, when considering the interpretability of the learning\nsystems, batch incremental solutions tend to be favored. Code:\n\\url{https://github.com/anselmeamekoe/DelayedLabelStream}\n","authors":["Kodjo Mawuena Amekoe","Mustapha Lebbah","Gregoire Jaffre","Hanene Azzag","Zaineb Chelly Dagdia"],"pdf_url":"https://arxiv.org/pdf/2409.10111v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2406.02313v3","updated":"2024-09-16T09:17:21Z","published":"2024-06-04T13:42:42Z","title":"Neural Thermodynamic Integration: Free Energies from Energy-based\n  Diffusion Models","summary":"  Thermodynamic integration (TI) offers a rigorous method for estimating\nfree-energy differences by integrating over a sequence of interpolating\nconformational ensembles. However, TI calculations are computationally\nexpensive and typically limited to coupling a small number of degrees of\nfreedom due to the need to sample numerous intermediate ensembles with\nsufficient conformational-space overlap. In this work, we propose to perform TI\nalong an alchemical pathway represented by a trainable neural network, which we\nterm Neural TI. Critically, we parametrize a time-dependent Hamiltonian\ninterpolating between the interacting and non-interacting systems, and optimize\nits gradient using a score matching objective. The ability of the resulting\nenergy-based diffusion model to sample all intermediate ensembles allows us to\nperform TI from a single reference calculation. We apply our method to\nLennard-Jones fluids, where we report accurate calculations of the excess\nchemical potential, demonstrating that Neural TI reproduces the underlying\nchanges in free energy without the need for simulations at interpolating\nHamiltonians.\n","authors":["B√°lint M√°t√©","Fran√ßois Fleuret","Tristan Bereau"],"pdf_url":"https://arxiv.org/pdf/2406.02313v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09071v5","updated":"2024-09-16T09:09:34Z","published":"2024-01-17T09:12:31Z","title":"Rethinking Spectral Graph Neural Networks with Spatially Adaptive\n  Filtering","summary":"  Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded\nin the spectral domain, their practical reliance on polynomial approximation\nimplies a profound linkage to the spatial domain. As previous studies rarely\nexamine spectral GNNs from the spatial perspective, their spatial-domain\ninterpretability remains elusive, e.g., what information is essentially encoded\nby spectral GNNs in the spatial domain? In this paper, to answer this question,\nwe establish a theoretical connection between spectral filtering and spatial\naggregation, unveiling an intrinsic interaction that spectral filtering\nimplicitly leads the original graph to an adapted new graph, explicitly\ncomputed for spatial aggregation. Both theoretical and empirical investigations\nreveal that the adapted new graph not only exhibits non-locality but also\naccommodates signed edge weights to reflect label consistency among nodes.\nThese findings thus highlight the interpretable role of spectral GNNs in the\nspatial domain and inspire us to rethink graph spectral filters beyond the\nfixed-order polynomials, which neglect global information. Built upon the\ntheoretical findings, we revisit the state-of-the-art spectral GNNs and propose\na novel Spatially Adaptive Filtering (SAF) framework, which leverages the\nadapted new graph by spectral filtering for an auxiliary non-local aggregation.\nNotably, our proposed SAF comprehensively models both node similarity and\ndissimilarity from a global perspective, therefore alleviating persistent\ndeficiencies of GNNs related to long-range dependencies and graph heterophily.\nExtensive experiments over 13 node classification benchmarks demonstrate the\nsuperiority of our proposed framework to the state-of-the-art models.\n","authors":["Jingwei Guo","Kaizhu Huang","Xinping Yi","Zixian Su","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.09071v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10104v1","updated":"2024-09-16T09:07:31Z","published":"2024-09-16T09:07:31Z","title":"A Comparative Study of Open Source Computer Vision Models for\n  Application on Small Data: The Case of CFRP Tape Laying","summary":"  In the realm of industrial manufacturing, Artificial Intelligence (AI) is\nplaying an increasing role, from automating existing processes to aiding in the\ndevelopment of new materials and techniques. However, a significant challenge\narises in smaller, experimental processes characterized by limited training\ndata availability, questioning the possibility to train AI models in such small\ndata contexts. In this work, we explore the potential of Transfer Learning to\naddress this challenge, specifically investigating the minimum amount of data\nrequired to develop a functional AI model. For this purpose, we consider the\nuse case of quality control of Carbon Fiber Reinforced Polymer (CFRP) tape\nlaying in aerospace manufacturing using optical sensors. We investigate the\nbehavior of different open-source computer vision models with a continuous\nreduction of the training data. Our results show that the amount of data\nrequired to successfully train an AI model can be drastically reduced, and the\nuse of smaller models does not necessarily lead to a loss of performance.\n","authors":["Thomas Fraunholz","Dennis Rall","Tim K√∂hler","Alfons Schuster","Monika Mayer","Lars Larsen"],"pdf_url":"https://arxiv.org/pdf/2409.10104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10096v1","updated":"2024-09-16T08:54:59Z","published":"2024-09-16T08:54:59Z","title":"Robust Reinforcement Learning with Dynamic Distortion Risk Measures","summary":"  In a reinforcement learning (RL) setting, the agent's optimal strategy\nheavily depends on her risk preferences and the underlying model dynamics of\nthe training environment. These two aspects influence the agent's ability to\nmake well-informed and time-consistent decisions when facing testing\nenvironments. In this work, we devise a framework to solve robust risk-aware RL\nproblems where we simultaneously account for environmental uncertainty and risk\nwith a class of dynamic robust distortion risk measures. Robustness is\nintroduced by considering all models within a Wasserstein ball around a\nreference model. We estimate such dynamic robust risk measures using neural\nnetworks by making use of strictly consistent scoring functions, derive policy\ngradient formulae using the quantile representation of distortion risk\nmeasures, and construct an actor-critic algorithm to solve this class of robust\nrisk-aware RL problems. We demonstrate the performance of our algorithm on a\nportfolio allocation example.\n","authors":["Anthony Coache","Sebastian Jaimungal"],"pdf_url":"https://arxiv.org/pdf/2409.10096v1.pdf","comment":"29 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.10094v1","updated":"2024-09-16T08:50:47Z","published":"2024-09-16T08:50:47Z","title":"DDoS: Diffusion Distribution Similarity for Out-of-Distribution\n  Detection","summary":"  Out-of-Distribution (OoD) detection determines whether the given samples are\nfrom the training distribution of the classifier-under-protection, i.e., the\nIn-Distribution (InD), or from a different OoD. Latest researches introduce\ndiffusion models pre-trained on InD data to advocate OoD detection by\ntransferring an OoD image into a generated one that is close to InD, so that\none could capture the distribution disparities between original and generated\nimages to detect OoD data. Existing diffusion-based detectors adopt perceptual\nmetrics on the two images to measure such disparities, but ignore a fundamental\nfact: Perceptual metrics are devised essentially for human-perceived\nsimilarities of low-level image patterns, e.g., textures and colors, and are\nnot advisable in evaluating distribution disparities, since images with\ndifferent low-level patterns could possibly come from the same distribution. To\naddress this issue, we formulate a diffusion-based detection framework that\nconsiders the distribution similarity between a tested image and its generated\ncounterpart via a novel proper similarity metric in the informative feature\nspace and probability space learned by the classifier-under-protection. An\nanomaly-removal strategy is further presented to enlarge such distribution\ndisparities by removing abnormal OoD information in the feature space to\nfacilitate the detection. Extensive empirical results unveil the insufficiency\nof perceptual metrics and the effectiveness of our distribution similarity\nframework with new state-of-the-art detection performance.\n","authors":["Kun Fang","Qinghua Tao","Zuopeng Yang","Xiaolin Huang","Jie Yang"],"pdf_url":"https://arxiv.org/pdf/2409.10094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03703v2","updated":"2024-09-16T08:43:13Z","published":"2024-04-04T07:49:39Z","title":"Mitigating analytical variability in fMRI results with style transfer","summary":"  We propose a novel approach to improve the reproducibility of neuroimaging\nresults by converting statistic maps across different functional MRI pipelines.\nWe make the assumption that pipelines used to compute fMRI statistic maps can\nbe considered as a style component and we propose to use different generative\nmodels, among which, Generative Adversarial Networks (GAN) and Diffusion Models\n(DM) to convert statistic maps across different pipelines. We explore the\nperformance of multiple GAN frameworks, and design a new DM framework for\nunsupervised multi-domain styletransfer. We constrain the generation of 3D fMRI\nstatistic maps using the latent space of an auxiliary classifier that\ndistinguishes statistic maps from different pipelines and extend traditional\nsampling techniques used in DM to improve the transition performance. Our\nexperiments demonstrate that our proposed methods aresuccessful: pipelines can\nindeed be transferred as a style component, providing animportant source of\ndata augmentation for future medical studies.\n","authors":["Elodie Germani","Camille Maumet","Elisa Fromont"],"pdf_url":"https://arxiv.org/pdf/2404.03703v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10085v1","updated":"2024-09-16T08:42:56Z","published":"2024-09-16T08:42:56Z","title":"A Riemannian Approach to Ground Metric Learning for Optimal Transport","summary":"  Optimal transport (OT) theory has attracted much attention in machine\nlearning and signal processing applications. OT defines a notion of distance\nbetween probability distributions of source and target data points. A crucial\nfactor that influences OT-based distances is the ground metric of the embedding\nspace in which the source and target data points lie. In this work, we propose\nto learn a suitable latent ground metric parameterized by a symmetric positive\ndefinite matrix. We use the rich Riemannian geometry of symmetric positive\ndefinite matrices to jointly learn the OT distance along with the ground\nmetric. Empirical results illustrate the efficacy of the learned metric in\nOT-based domain adaptation.\n","authors":["Pratik Jawanpuria","Dai Shi","Bamdev Mishra","Junbin Gao"],"pdf_url":"https://arxiv.org/pdf/2409.10085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10075v1","updated":"2024-09-16T08:26:06Z","published":"2024-09-16T08:26:06Z","title":"Steinmetz Neural Networks for Complex-Valued Data","summary":"  In this work, we introduce a new approach to processing complex-valued data\nusing DNNs consisting of parallel real-valued subnetworks with coupled outputs.\nOur proposed class of architectures, referred to as Steinmetz Neural Networks,\nleverages multi-view learning to construct more interpretable representations\nwithin the latent space. Subsequently, we present the Analytic Neural Network,\nwhich implements a consistency penalty that encourages analytic signal\nrepresentations in the Steinmetz neural network's latent space. This penalty\nenforces a deterministic and orthogonal relationship between the real and\nimaginary components. Utilizing an information-theoretic construction, we\ndemonstrate that the upper bound on the generalization error posited by the\nanalytic neural network is lower than that of the general class of Steinmetz\nneural networks. Our numerical experiments demonstrate the improved performance\nand robustness to additive noise, afforded by our proposed networks on\nbenchmark datasets and synthetic examples.\n","authors":["Shyam Venkatasubramanian","Ali Pezeshki","Vahid Tarokh"],"pdf_url":"https://arxiv.org/pdf/2409.10075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06955v2","updated":"2024-09-16T08:23:09Z","published":"2024-09-11T02:36:36Z","title":"Privacy-Preserving Federated Learning with Consistency via Knowledge\n  Distillation Using Conditional Generator","summary":"  Federated Learning (FL) is gaining popularity as a distributed learning\nframework that only shares model parameters or gradient updates and keeps\nprivate data locally. However, FL is at risk of privacy leakage caused by\nprivacy inference attacks. And most existing privacy-preserving mechanisms in\nFL conflict with achieving high performance and efficiency. Therefore, we\npropose FedMD-CG, a novel FL method with highly competitive performance and\nhigh-level privacy preservation, which decouples each client's local model into\na feature extractor and a classifier, and utilizes a conditional generator\ninstead of the feature extractor to perform server-side model aggregation. To\nensure the consistency of local generators and classifiers, FedMD-CG leverages\nknowledge distillation to train local models and generators at both the latent\nfeature level and the logit level. Also, we construct additional classification\nlosses and design new diversity losses to enhance client-side training.\nFedMD-CG is robust to data heterogeneity and does not require training extra\ndiscriminators (like cGAN). We conduct extensive experiments on various image\nclassification tasks to validate the superiority of FedMD-CG.\n","authors":["Kangyang Luo","Shuai Wang","Xiang Li","Yunshi Lan","Ming Gao","Jinlong Shu"],"pdf_url":"https://arxiv.org/pdf/2409.06955v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07734v2","updated":"2024-09-16T08:18:59Z","published":"2024-09-12T03:44:30Z","title":"DFDG: Data-Free Dual-Generator Adversarial Distillation for One-Shot\n  Federated Learning","summary":"  Federated Learning (FL) is a distributed machine learning scheme in which\nclients jointly participate in the collaborative training of a global model by\nsharing model information rather than their private datasets. In light of\nconcerns associated with communication and privacy, one-shot FL with a single\ncommunication round has emerged as a de facto promising solution. However,\nexisting one-shot FL methods either require public datasets, focus on model\nhomogeneous settings, or distill limited knowledge from local models, making it\ndifficult or even impractical to train a robust global model. To address these\nlimitations, we propose a new data-free dual-generator adversarial distillation\nmethod (namely DFDG) for one-shot FL, which can explore a broader local models'\ntraining space via training dual generators. DFDG is executed in an adversarial\nmanner and comprises two parts: dual-generator training and dual-model\ndistillation. In dual-generator training, we delve into each generator\nconcerning fidelity, transferability and diversity to ensure its utility, and\nadditionally tailor the cross-divergence loss to lessen the overlap of dual\ngenerators' output spaces. In dual-model distillation, the trained dual\ngenerators work together to provide the training data for updates of the global\nmodel. At last, our extensive experiments on various image classification tasks\nshow that DFDG achieves significant performance gains in accuracy compared to\nSOTA baselines.\n","authors":["Kangyang Luo","Shuai Wang","Yexuan Fu","Renrong Shao","Xiang Li","Yunshi Lan","Ming Gao","Jinlong Shu"],"pdf_url":"https://arxiv.org/pdf/2409.07734v2.pdf","comment":"Accepted by ICDM2024 main conference (long paper). arXiv admin note:\n  substantial text overlap with arXiv:2309.13546"},{"id":"http://arxiv.org/abs/2409.10069v1","updated":"2024-09-16T08:15:23Z","published":"2024-09-16T08:15:23Z","title":"Enhancing Anomaly Detection via Generating Diversified and\n  Hard-to-distinguish Synthetic Anomalies","summary":"  Unsupervised anomaly detection is a daunting task, as it relies solely on\nnormality patterns from the training data to identify unseen anomalies during\ntesting. Recent approaches have focused on leveraging domain-specific\ntransformations or perturbations to generate synthetic anomalies from normal\nsamples. The objective here is to acquire insights into normality patterns by\nlearning to differentiate between normal samples and these crafted anomalies.\nHowever, these approaches often encounter limitations when domain-specific\ntransformations are not well-specified such as in tabular data, or when it\nbecomes trivial to distinguish between them. To address these issues, we\nintroduce a novel domain-agnostic method that employs a set of conditional\nperturbators and a discriminator. The perturbators are trained to generate\ninput-dependent perturbations, which are subsequently utilized to construct\nsynthetic anomalies, and the discriminator is trained to distinguish normal\nsamples from them. We ensure that the generated anomalies are both diverse and\nhard to distinguish through two key strategies: i) directing perturbations to\nbe orthogonal to each other and ii) constraining perturbations to remain in\nproximity to normal samples. Throughout experiments on real-world datasets, we\ndemonstrate the superiority of our method over state-of-the-art benchmarks,\nwhich is evident not only in image data but also in tabular data, where\ndomain-specific transformation is not readily accessible. Additionally, we\nempirically confirm the adaptability of our method to semi-supervised settings,\ndemonstrating its capacity to incorporate supervised signals to enhance anomaly\ndetection performance even further.\n","authors":["Hyuntae Kim","Changhee Lee"],"pdf_url":"https://arxiv.org/pdf/2409.10069v1.pdf","comment":"Accepted at CIKM 2024"},{"id":"http://arxiv.org/abs/2409.10068v1","updated":"2024-09-16T08:05:58Z","published":"2024-09-16T08:05:58Z","title":"Spatiotemporal Covariance Neural Networks","summary":"  Modeling spatiotemporal interactions in multivariate time series is key to\ntheir effective processing, but challenging because of their irregular and\noften unknown structure. Statistical properties of the data provide useful\nbiases to model interdependencies and are leveraged by correlation and\ncovariance-based networks as well as by processing pipelines relying on\nprincipal component analysis (PCA). However, PCA and its temporal extensions\nsuffer instabilities in the covariance eigenvectors when the corresponding\neigenvalues are close to each other, making their application to dynamic and\nstreaming data settings challenging. To address these issues, we exploit the\nanalogy between PCA and graph convolutional filters to introduce the\nSpatioTemporal coVariance Neural Network (STVNN), a relational learning model\nthat operates on the sample covariance matrix of the time series and leverages\njoint spatiotemporal convolutions to model the data. To account for the\nstreaming and non-stationary setting, we consider an online update of the\nparameters and sample covariance matrix. We prove the STVNN is stable to the\nuncertainties introduced by these online estimations, thus improving over\ntemporal PCA-based methods. Experimental results corroborate our theoretical\nfindings and show that STVNN is competitive for multivariate time series\nprocessing, it adapts to changes in the data distribution, and it is orders of\nmagnitude more stable than online temporal PCA.\n","authors":["Andrea Cavallo","Mohammad Sabbaqi","Elvin Isufi"],"pdf_url":"https://arxiv.org/pdf/2409.10068v1.pdf","comment":"Joint European Conference on Machine Learning and Knowledge Discovery\n  in Databases (ECML PKDD) 2024"},{"id":"http://arxiv.org/abs/2409.10046v1","updated":"2024-09-16T07:19:08Z","published":"2024-09-16T07:19:08Z","title":"Global Lightning-Ignited Wildfires Prediction and Climate Change\n  Projections based on Explainable Machine Learning Models","summary":"  Wildfires pose a significant natural disaster risk to populations and\ncontribute to accelerated climate change. As wildfires are also affected by\nclimate change, extreme wildfires are becoming increasingly frequent. Although\nthey occur less frequently globally than those sparked by human activities,\nlightning-ignited wildfires play a substantial role in carbon emissions and\naccount for the majority of burned areas in certain regions. While existing\ncomputational models, especially those based on machine learning, aim to\npredict lightning-ignited wildfires, they are typically tailored to specific\nregions with unique characteristics, limiting their global applicability. In\nthis study, we present machine learning models designed to characterize and\npredict lightning-ignited wildfires on a global scale. Our approach involves\nclassifying lightning-ignited versus anthropogenic wildfires, and estimating\nwith high accuracy the probability of lightning to ignite a fire based on a\nwide spectrum of factors such as meteorological conditions and vegetation.\nUtilizing these models, we analyze seasonal and spatial trends in\nlightning-ignited wildfires shedding light on the impact of climate change on\nthis phenomenon. We analyze the influence of various features on the models\nusing eXplainable Artificial Intelligence (XAI) frameworks. Our findings\nhighlight significant global differences between anthropogenic and\nlightning-ignited wildfires. Moreover, we demonstrate that, even over a short\ntime span of less than a decade, climate changes have steadily increased the\nglobal risk of lightning-ignited wildfires. This distinction underscores the\nimperative need for dedicated predictive models and fire weather indices\ntailored specifically to each type of wildfire.\n","authors":["Assaf Shmuel","Teddy Lazebnik","Oren Glickman","Eyal Heifetz","Colin Price"],"pdf_url":"https://arxiv.org/pdf/2409.10046v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10045v1","updated":"2024-09-16T07:15:46Z","published":"2024-09-16T07:15:46Z","title":"Learning Latent Wireless Dynamics from Channel State Information","summary":"  In this work, we propose a novel data-driven machine learning (ML) technique\nto model and predict the dynamics of the wireless propagation environment in\nlatent space. Leveraging the idea of channel charting, which learns compressed\nrepresentations of high-dimensional channel state information (CSI), we\nincorporate a predictive component to capture the dynamics of the wireless\nsystem. Hence, we jointly learn a channel encoder that maps the estimated CSI\nto an appropriate latent space, and a predictor that models the relationships\nbetween such representations. Accordingly, our problem boils down to training a\njoint-embedding predictive architecture (JEPA) that simulates the latent\ndynamics of a wireless network from CSI. We present numerical evaluations on\nmeasured data and show that the proposed JEPA displays a two-fold increase in\naccuracy over benchmarks, for longer look-ahead prediction tasks.\n","authors":["Charbel Bou Chaaya","Abanoub M. Girgis","Mehdi Bennis"],"pdf_url":"https://arxiv.org/pdf/2409.10045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10044v1","updated":"2024-09-16T07:13:30Z","published":"2024-09-16T07:13:30Z","title":"Benchmarking Large Language Model Uncertainty for Prompt Optimization","summary":"  Prompt optimization algorithms for Large Language Models (LLMs) excel in\nmulti-step reasoning but still lack effective uncertainty estimation. This\npaper introduces a benchmark dataset to evaluate uncertainty metrics, focusing\non Answer, Correctness, Aleatoric, and Epistemic Uncertainty. Through analysis\nof models like GPT-3.5-Turbo and Meta-Llama-3.1-8B-Instruct, we show that\ncurrent metrics align more with Answer Uncertainty, which reflects output\nconfidence and diversity, rather than Correctness Uncertainty, highlighting the\nneed for improved metrics that are optimization-objective-aware to better guide\nprompt optimization. Our code and dataset are available at\nhttps://github.com/0Frett/PO-Uncertainty-Benchmarking.\n","authors":["Pei-Fu Guo","Yun-Da Tsai","Shou-De Lin"],"pdf_url":"https://arxiv.org/pdf/2409.10044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10918v4","updated":"2024-09-16T07:12:12Z","published":"2024-06-16T12:46:40Z","title":"Central Answer Modeling for an Embodied Multi-LLM System","summary":"  Embodied Question Answering (EQA) is an important problem, which involves an\nagent exploring the environment to answer user queries. In the existing\nliterature, EQA has exclusively been studied in single-agent scenarios, where\nexploration can be time-consuming and costly. In this work, we consider EQA in\na multi-agent framework involving multiple large language models (LLM) based\nagents independently answering queries about a household environment. To\ngenerate one answer for each query, we use the individual responses to train a\nCentral Answer Model (CAM) that aggregates responses for a robust answer. While\nprior Question Answering (QA) work has used a central module based on answers\nfrom multiple LLM-based experts, we specifically look at applying this\nframework to embodied LLM-based agents that must physically explore the\nenvironment first to become experts on their given environment to answer\nquestions. Our work is the first to utilize a central answer model framework\nwith embodied agents that must rely on exploring an unknown environment. We set\nup a variation of EQA where instead of the agents exploring the environment\nafter the question is asked, the agents first explore the environment for a set\namount of time and then answer a set of queries. Using CAM, we observe a $46\\%$\nhigher EQA accuracy when compared against aggregation methods for ensemble LLM,\nsuch as voting schemes and debates. CAM does not require any form of agent\ncommunication, alleviating it from the associated costs. We ablate CAM with\nvarious nonlinear (neural network, random forest, decision tree, XGBoost) and\nlinear (logistic regression classifier, SVM) algorithms. We experiment in\nvarious topological graph environments and examine the case where one of the\nagents is malicious and purposes contribute responses it believes to be wrong.\n","authors":["Bhrij Patel","Vishnu Sashank Dorbala","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2406.10918v4.pdf","comment":"15 pages, 11 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2409.10038v1","updated":"2024-09-16T07:01:41Z","published":"2024-09-16T07:01:41Z","title":"On the Diagram of Thought","summary":"  We introduce Diagram of Thought (DoT), a framework that models iterative\nreasoning in large language models (LLMs) as the construction of a directed\nacyclic graph (DAG) within a single model. Unlike traditional approaches that\nrepresent reasoning as linear chains or trees, DoT organizes propositions,\ncritiques, refinements, and verifications into a cohesive DAG structure,\nallowing the model to explore complex reasoning pathways while maintaining\nlogical consistency. Each node in the diagram corresponds to a proposition that\nhas been proposed, critiqued, refined, or verified, enabling the LLM to\niteratively improve its reasoning through natural language feedback. By\nleveraging auto-regressive next-token prediction with role-specific tokens, DoT\nfacilitates seamless transitions between proposing ideas and critically\nevaluating them, providing richer feedback than binary signals. Furthermore, we\nformalize the DoT framework using Topos Theory, providing a mathematical\nfoundation that ensures logical consistency and soundness in the reasoning\nprocess. This approach enhances both the training and inference processes\nwithin a single LLM, eliminating the need for multiple models or external\ncontrol mechanisms. DoT offers a conceptual framework for designing\nnext-generation reasoning-specialized models, emphasizing training efficiency,\nrobust reasoning capabilities, and theoretical grounding. The code is available\nat https://github.com/diagram-of-thought/diagram-of-thought.\n","authors":["Yifan Zhang","Yang Yuan","Andrew Chi-Chih Yao"],"pdf_url":"https://arxiv.org/pdf/2409.10038v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07884v3","updated":"2024-09-16T07:00:15Z","published":"2024-09-12T09:44:13Z","title":"Graph Neural Networks for Parkinsons Disease Detection","summary":"  Despite the promising performance of state of the art approaches for\nParkinsons Disease (PD) detection, these approaches often analyze individual\nspeech segments in isolation, which can lead to suboptimal results. Dysarthric\ncues that characterize speech impairments from PD patients are expected to be\nrelated across segments from different speakers. Isolated segment analysis\nfails to exploit these inter segment relationships. Additionally, not all\nspeech segments from PD patients exhibit clear dysarthric symptoms, introducing\nlabel noise that can negatively affect the performance and generalizability of\ncurrent approaches. To address these challenges, we propose a novel PD\ndetection framework utilizing Graph Convolutional Networks (GCNs). By\nrepresenting speech segments as nodes and capturing the similarity between\nsegments through edges, our GCN model facilitates the aggregation of dysarthric\ncues across the graph, effectively exploiting segment relationships and\nmitigating the impact of label noise. Experimental results demonstrate\ntheadvantages of the proposed GCN model for PD detection and provide insights\ninto its underlying mechanisms\n","authors":["Shakeel A. Sheikh","Yacouba Kaloga","Md Sahidullah","Ina Kodrasi"],"pdf_url":"https://arxiv.org/pdf/2409.07884v3.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2409.00979v2","updated":"2024-09-16T06:46:32Z","published":"2024-09-02T06:49:29Z","title":"Regret Analysis for Randomized Gaussian Process Upper Confidence Bound","summary":"  Gaussian process upper confidence bound (GP-UCB) is a theoretically\nestablished algorithm for Bayesian optimization (BO), where we assume the\nobjective function $f$ follows GP. One notable drawback of GP-UCB is that the\ntheoretical confidence parameter $\\beta$ increased along with the iterations is\ntoo large. To alleviate this drawback, this paper analyzes the randomized\nvariant of GP-UCB called improved randomized GP-UCB (IRGP-UCB), which uses the\nconfidence parameter generated from the shifted exponential distribution. We\nanalyze the expected regret and conditional expected regret, where the\nexpectation and the probability are taken respectively with $f$ and noises and\nwith the randomness of the BO algorithm. In both regret analyses, IRGP-UCB\nachieves a sub-linear regret upper bound without increasing the confidence\nparameter if the input domain is finite. Finally, we show numerical experiments\nusing synthetic and benchmark functions and real-world emulators.\n","authors":["Shion Takeno","Yu Inatsu","Masayuki Karasuyama"],"pdf_url":"https://arxiv.org/pdf/2409.00979v2.pdf","comment":"31 pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:2302.01511"},{"id":"http://arxiv.org/abs/2409.10023v1","updated":"2024-09-16T06:21:21Z","published":"2024-09-16T06:21:21Z","title":"Reinforcement learning-based statistical search strategy for an axion\n  model from flavor","summary":"  We propose a reinforcement learning-based search strategy to explore new\nphysics beyond the Standard Model. The reinforcement learning, which is one of\nmachine learning methods, is a powerful approach to find model parameters with\nphenomenological constraints. As a concrete example, we focus on a minimal\naxion model with a global $U(1)$ flavor symmetry. Agents of the learning\nsucceed in finding $U(1)$ charge assignments of quarks and leptons solving the\nflavor and cosmological puzzles in the Standard Model, and find more than 150\nrealistic solutions for the quark sector taking renormalization effects into\naccount. For the solutions found by the reinforcement learning-based analysis,\nwe discuss the sensitivity of future experiments for the detection of an axion\nwhich is a Nambu-Goldstone boson of the spontaneously broken $U(1)$. We also\nexamine how fast the reinforcement learning-based searching method finds the\nbest discrete parameters in comparison with conventional optimization methods.\nIn conclusion, the efficient parameter search based on the reinforcement\nlearning-based strategy enables us to perform a statistical analysis of the\nvast parameter space associated with the axion model from flavor.\n","authors":["Satsuki Nishimura","Coh Miyao","Hajime Otsuka"],"pdf_url":"https://arxiv.org/pdf/2409.10023v1.pdf","comment":"39 pages, 4 figures"},{"id":"http://arxiv.org/abs/2306.06192v7","updated":"2024-09-16T05:34:01Z","published":"2023-06-09T18:45:15Z","title":"CCE: Sample Efficient Sparse Reward Policy Learning for Robotic\n  Navigation via Confidence-Controlled Exploration","summary":"  We introduce Confidence-Controlled Exploration (CCE), a novel exploration\nscheme designed to enhance the training sample efficiency of reinforcement\nlearning (RL) algorithms for sparse reward settings such as robot navigation.\nSparse rewards are common in RL and convenient to design and implement, but\ntypically hard to deal with due to the challenges of exploration. Existing\nmethods deploy regularization-based methods to deal with the exploration\nchallenges. However, it is hard to characterize the balance between exploration\nand exploitation because regularization modifies the reward function itself,\nhence changing the objective we are optimizing for. In contrast to\nregularization-based approaches in the existing literature, our approach, CCE,\nis based on a novel relationship we provide between gradient estimation and\npolicy entropy. CCE dynamically adjusts the number of samples of the gradient\nupdate used during training to control exploration. Interestingly, CCE can be\napplied to both existing on-policy and off-policy RL methods, which we\ndemonstrate by empirically validating its efficacy on three popular RL methods:\nREINFORCE, Proximal Policy Optimization (PPO), and Soft Actor-Critic (SAC) for\ngoal-reaching robotic navigation tasks. We demonstrate through simulated and\nreal-world experiments that CCE outperforms conventional methods that employ\nconstant trajectory lengths and entropy regularization when constraining the\nsample budget. For a fixed sample budget, CCE achieves an 18\\% increase in\nnavigation success rate, a 20-38\\% reduction in navigation path length, and a\n9.32\\% decrease in elevation costs. Furthermore, we showcase the versatility of\nCCE by integrating it with the Clearpath Husky robot, illustrating its\napplicability in complex outdoor environments.\n","authors":["Bhrij Patel","Kasun Weerakoon","Wesley A. Suttle","Alec Koppel","Brian M. Sadler","Tianyi Zhou","Amrit Singh Bedi","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2306.06192v7.pdf","comment":"11 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.09251v2","updated":"2024-09-16T05:23:07Z","published":"2024-08-17T16:42:13Z","title":"V2X-VLM: End-to-End V2X Cooperative Autonomous Driving Through Large\n  Vision-Language Models","summary":"  Advancements in autonomous driving have increasingly focused on end-to-end\n(E2E) systems that manage the full spectrum of driving tasks, from\nenvironmental perception to vehicle navigation and control. This paper\nintroduces V2X-VLM, an innovative E2E vehicle-infrastructure cooperative\nautonomous driving (VICAD) framework with Vehicle-to-Everything (V2X) systems\nand large vision-language models (VLMs). V2X-VLM is designed to enhance\nsituational awareness, decision-making, and ultimate trajectory planning by\nintegrating multimodel data from vehicle-mounted cameras, infrastructure\nsensors, and textual information. The contrastive learning method is further\nemployed to complement VLM by refining feature discrimination, assisting the\nmodel to learn robust representations of the driving environment. Evaluations\non the DAIR-V2X dataset show that V2X-VLM outperforms state-of-the-art\ncooperative autonomous driving methods, while additional tests on corner cases\nvalidate its robustness in real-world driving conditions.\n","authors":["Junwei You","Haotian Shi","Zhuoyu Jiang","Zilin Huang","Rui Gan","Keshu Wu","Xi Cheng","Xiaopeng Li","Bin Ran"],"pdf_url":"https://arxiv.org/pdf/2408.09251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10825v2","updated":"2024-09-16T05:09:57Z","published":"2024-05-17T14:46:13Z","title":"Large Language Model (LLM) for Telecommunications: A Comprehensive\n  Survey on Principles, Key Techniques, and Opportunities","summary":"  Large language models (LLMs) have received considerable attention recently\ndue to their outstanding comprehension and reasoning capabilities, leading to\ngreat progress in many fields. The advancement of LLM techniques also offers\npromising opportunities to automate many tasks in the telecommunication\n(telecom) field. After pre-training and fine-tuning, LLMs can perform diverse\ndownstream tasks based on human instructions, paving the way to artificial\ngeneral intelligence (AGI)-enabled 6G. Given the great potential of LLM\ntechnologies, this work aims to provide a comprehensive overview of LLM-enabled\ntelecom networks. In particular, we first present LLM fundamentals, including\nmodel architecture, pre-training, fine-tuning, inference and utilization, model\nevaluation, and telecom deployment. Then, we introduce LLM-enabled key\ntechniques and telecom applications in terms of generation, classification,\noptimization, and prediction problems. Specifically, the LLM-enabled generation\napplications include telecom domain knowledge, code, and network configuration\ngeneration. After that, the LLM-based classification applications involve\nnetwork security, text, image, and traffic classification problems. Moreover,\nmultiple LLM-enabled optimization techniques are introduced, such as automated\nreward function design for reinforcement learning and verbal reinforcement\nlearning. Furthermore, for LLM-aided prediction problems, we discussed\ntime-series prediction models and multi-modality prediction problems for\ntelecom. Finally, we highlight the challenges and identify the future\ndirections of LLM-enabled telecom networks.\n","authors":["Hao Zhou","Chengming Hu","Ye Yuan","Yufei Cui","Yili Jin","Can Chen","Haolun Wu","Dun Yuan","Li Jiang","Di Wu","Xue Liu","Charlie Zhang","Xianbin Wang","Jiangchuan Liu"],"pdf_url":"https://arxiv.org/pdf/2405.10825v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09996v1","updated":"2024-09-16T05:05:03Z","published":"2024-09-16T05:05:03Z","title":"FreeMark: A Non-Invasive White-Box Watermarking for Deep Neural Networks","summary":"  Deep neural networks (DNNs) have achieved significant success in real-world\napplications. However, safeguarding their intellectual property (IP) remains\nextremely challenging. Existing DNN watermarking for IP protection often\nrequire modifying DNN models, which reduces model performance and limits their\npracticality.\n  This paper introduces FreeMark, a novel DNN watermarking framework that\nleverages cryptographic principles without altering the original host DNN\nmodel, thereby avoiding any reduction in model performance. Unlike traditional\nDNN watermarking methods, FreeMark innovatively generates secret keys from a\npre-generated watermark vector and the host model using gradient descent. These\nsecret keys, used to extract watermark from the model's activation values, are\nsecurely stored with a trusted third party, enabling reliable watermark\nextraction from suspect models. Extensive experiments demonstrate that FreeMark\neffectively resists various watermark removal attacks while maintaining high\nwatermark capacity.\n","authors":["Yuzhang Chen","Jiangnan Zhu","Yujie Gu","Minoru Kuribayashi","Kouichi Sakurai"],"pdf_url":"https://arxiv.org/pdf/2409.09996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09990v1","updated":"2024-09-16T04:46:22Z","published":"2024-09-16T04:46:22Z","title":"SHIRE: Enhancing Sample Efficiency using Human Intuition in\n  REinforcement Learning","summary":"  The ability of neural networks to perform robotic perception and control\ntasks such as depth and optical flow estimation, simultaneous localization and\nmapping (SLAM), and automatic control has led to their widespread adoption in\nrecent years. Deep Reinforcement Learning has been used extensively in these\nsettings, as it does not have the unsustainable training costs associated with\nsupervised learning. However, DeepRL suffers from poor sample efficiency, i.e.,\nit requires a large number of environmental interactions to converge to an\nacceptable solution. Modern RL algorithms such as Deep Q Learning and Soft\nActor-Critic attempt to remedy this shortcoming but can not provide the\nexplainability required in applications such as autonomous robotics. Humans\nintuitively understand the long-time-horizon sequential tasks common in\nrobotics. Properly using such intuition can make RL policies more explainable\nwhile enhancing their sample efficiency. In this work, we propose SHIRE, a\nnovel framework for encoding human intuition using Probabilistic Graphical\nModels (PGMs) and using it in the Deep RL training pipeline to enhance sample\nefficiency. Our framework achieves 25-78% sample efficiency gains across the\nenvironments we evaluate at negligible overhead cost. Additionally, by teaching\nRL agents the encoded elementary behavior, SHIRE enhances policy\nexplainability. A real-world demonstration further highlights the efficacy of\npolicies trained using our framework.\n","authors":["Amogh Joshi","Adarsh Kumar Kosta","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2409.09990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08395v2","updated":"2024-09-16T04:42:10Z","published":"2024-09-12T20:48:28Z","title":"Graphical Structural Learning of rs-fMRI data in Heavy Smokers","summary":"  Recent studies revealed structural and functional brain changes in heavy\nsmokers. However, the specific changes in topological brain connections are not\nwell understood. We used Gaussian Undirected Graphs with the graphical lasso\nalgorithm on rs-fMRI data from smokers and non-smokers to identify significant\nchanges in brain connections. Our results indicate high stability in the\nestimated graphs and identify several brain regions significantly affected by\nsmoking, providing valuable insights for future clinical research.\n","authors":["Yiru Gong","Qimin Zhang","Huili Zheng","Zheyan Liu","Shaohan Chen"],"pdf_url":"https://arxiv.org/pdf/2409.08395v2.pdf","comment":"Accepted by IEEE CCSB 2024 conference"},{"id":"http://arxiv.org/abs/2409.09984v1","updated":"2024-09-16T04:27:11Z","published":"2024-09-16T04:27:11Z","title":"Convergence of Sharpness-Aware Minimization Algorithms using Increasing\n  Batch Size and Decaying Learning Rate","summary":"  The sharpness-aware minimization (SAM) algorithm and its variants, including\ngap guided SAM (GSAM), have been successful at improving the generalization\ncapability of deep neural network models by finding flat local minima of the\nempirical loss in training. Meanwhile, it has been shown theoretically and\npractically that increasing the batch size or decaying the learning rate avoids\nsharp local minima of the empirical loss. In this paper, we consider the GSAM\nalgorithm with increasing batch sizes or decaying learning rates, such as\ncosine annealing or linear learning rate, and theoretically show its\nconvergence. Moreover, we numerically compare SAM (GSAM) with and without an\nincreasing batch size and conclude that using an increasing batch size or\ndecaying learning rate finds flatter local minima than using a constant batch\nsize and learning rate.\n","authors":["Hinata Harada","Hideaki Iiduka"],"pdf_url":"https://arxiv.org/pdf/2409.09984v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09980v1","updated":"2024-09-16T04:23:06Z","published":"2024-09-16T04:23:06Z","title":"From Bytes to Bites: Using Country Specific Machine Learning Models to\n  Predict Famine","summary":"  Hunger crises are critical global issues affecting millions, particularly in\nlow-income and developing countries. This research investigates how machine\nlearning can be utilized to predict and inform decisions regarding famine and\nhunger crises. By leveraging a diverse set of variables (natural, economic, and\nconflict-related), three machine learning models (Linear Regression, XGBoost,\nand RandomForestRegressor) were employed to predict food consumption scores, a\nkey indicator of household nutrition. The RandomForestRegressor emerged as the\nmost accurate model, with an average prediction error of 10.6%, though accuracy\nvaried significantly across countries, ranging from 2% to over 30%. Notably,\neconomic indicators were consistently the most significant predictors of\naverage household nutrition, while no single feature dominated across all\nregions, underscoring the necessity for comprehensive data collection and\ntailored, country-specific models. These findings highlight the potential of\nmachine learning, particularly Random Forests, to enhance famine prediction,\nsuggesting that continued research and improved data gathering are essential\nfor more effective global hunger forecasting.\n","authors":["Salloni Kapoor","Simeon Sayer"],"pdf_url":"https://arxiv.org/pdf/2409.09980v1.pdf","comment":"17 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2406.10719v4","updated":"2024-09-16T04:16:35Z","published":"2024-06-15T19:12:00Z","title":"Trading Devil: Robust backdoor attack via Stochastic investment models\n  and Bayesian approach","summary":"  With the growing use of voice-activated systems and speech recognition\ntechnologies, the danger of backdoor attacks on audio data has grown\nsignificantly. This research looks at a specific type of attack, known as a\nStochastic investment-based backdoor attack (MarketBack), in which adversaries\nstrategically manipulate the stylistic properties of audio to fool speech\nrecognition systems. The security and integrity of machine learning models are\nseriously threatened by backdoor attacks, in order to maintain the reliability\nof audio applications and systems, the identification of such attacks becomes\ncrucial in the context of audio data. Experimental results demonstrated that\nMarketBack is feasible to achieve an average attack success rate close to 100%\nin seven victim models when poisoning less than 1% of the training data.\n","authors":["Orson Mengara"],"pdf_url":"https://arxiv.org/pdf/2406.10719v4.pdf","comment":"(Last update!, a constructive comment from arxiv led to this latest\n  update ) Stochastic investment models and a Bayesian approach to better\n  modeling of uncertainty : adversarial machine learning or Stochastic market.\n  arXiv admin note: substantial text overlap with arXiv:2402.05967 (see this\n  link to the paper by : Orson Mengara)"},{"id":"http://arxiv.org/abs/2409.09978v1","updated":"2024-09-16T04:15:36Z","published":"2024-09-16T04:15:36Z","title":"Context-Conditioned Spatio-Temporal Predictive Learning for Reliable V2V\n  Channel Prediction","summary":"  Achieving reliable multidimensional Vehicle-to-Vehicle (V2V) channel state\ninformation (CSI) prediction is both challenging and crucial for optimizing\ndownstream tasks that depend on instantaneous CSI. This work extends\ntraditional prediction approaches by focusing on four-dimensional (4D) CSI,\nwhich includes predictions over time, bandwidth, and antenna (TX and RX) space.\nSuch a comprehensive framework is essential for addressing the dynamic nature\nof mobility environments within intelligent transportation systems,\nnecessitating the capture of both temporal and spatial dependencies across\ndiverse domains. To address this complexity, we propose a novel\ncontext-conditioned spatiotemporal predictive learning method. This method\nleverages causal convolutional long short-term memory (CA-ConvLSTM) to\neffectively capture dependencies within 4D CSI data, and incorporates\ncontext-conditioned attention mechanisms to enhance the efficiency of\nspatiotemporal memory updates. Additionally, we introduce an adaptive\nmeta-learning scheme tailored for recurrent networks to mitigate the issue of\naccumulative prediction errors. We validate the proposed method through\nempirical studies conducted across three different geometric configurations and\nmobility scenarios. Our results demonstrate that the proposed approach\noutperforms existing state-of-the-art predictive models, achieving superior\nperformance across various geometries. Moreover, we show that the meta-learning\nframework significantly enhances the performance of recurrent-based predictive\nmodels in highly challenging cross-geometry settings, thus highlighting its\nrobustness and adaptability.\n","authors":["Lei Chu","Daoud Burghal","Michael Neuman","Andreas F. Molisch"],"pdf_url":"https://arxiv.org/pdf/2409.09978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09958v1","updated":"2024-09-16T03:08:09Z","published":"2024-09-16T03:08:09Z","title":"An Offline Adaptation Framework for Constrained Multi-Objective\n  Reinforcement Learning","summary":"  In recent years, significant progress has been made in multi-objective\nreinforcement learning (RL) research, which aims to balance multiple objectives\nby incorporating preferences for each objective. In most existing studies,\nspecific preferences must be provided during deployment to indicate the desired\npolicies explicitly. However, designing these preferences depends heavily on\nhuman prior knowledge, which is typically obtained through extensive\nobservation of high-performing demonstrations with expected behaviors. In this\nwork, we propose a simple yet effective offline adaptation framework for\nmulti-objective RL problems without assuming handcrafted target preferences,\nbut only given several demonstrations to implicitly indicate the preferences of\nexpected policies. Additionally, we demonstrate that our framework can\nnaturally be extended to meet constraints on safety-critical objectives by\nutilizing safe demonstrations, even when the safety thresholds are unknown.\nEmpirical results on offline multi-objective and safe tasks demonstrate the\ncapability of our framework to infer policies that align with real preferences\nwhile meeting the constraints implied by the provided demonstrations.\n","authors":["Qian Lin","Zongkai Liu","Danying Mo","Chao Yu"],"pdf_url":"https://arxiv.org/pdf/2409.09958v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09957v1","updated":"2024-09-16T03:05:11Z","published":"2024-09-16T03:05:11Z","title":"Deep Graph Anomaly Detection: A Survey and New Perspectives","summary":"  Graph anomaly detection (GAD), which aims to identify unusual graph instances\n(nodes, edges, subgraphs, or graphs), has attracted increasing attention in\nrecent years due to its significance in a wide range of applications. Deep\nlearning approaches, graph neural networks (GNNs) in particular, have been\nemerging as a promising paradigm for GAD, owing to its strong capability in\ncapturing complex structure and/or node attributes in graph data. Considering\nthe large number of methods proposed for GNN-based GAD, it is of paramount\nimportance to summarize the methodologies and findings in the existing GAD\nstudies, so that we can pinpoint effective model designs for tackling open GAD\nproblems. To this end, in this work we aim to present a comprehensive review of\ndeep learning approaches for GAD. Existing GAD surveys are focused on\ntask-specific discussions, making it difficult to understand the technical\ninsights of existing methods and their limitations in addressing some unique\nchallenges in GAD. To fill this gap, we first discuss the problem complexities\nand their resulting challenges in GAD, and then provide a systematic review of\ncurrent deep GAD methods from three novel perspectives of methodology,\nincluding GNN backbone design, proxy task design for GAD, and graph anomaly\nmeasures. To deepen the discussions, we further propose a taxonomy of 13\nfine-grained method categories under these three perspectives to provide more\nin-depth insights into the model designs and their capabilities. To facilitate\nthe experiments and validation, we also summarize a collection of widely-used\nGAD datasets and empirical comparison. We further discuss multiple open\nproblems to inspire more future high-quality research. A continuously updated\nrepository for datasets, links to the codes of algorithms, and empirical\ncomparison is available at\nhttps://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.\n","authors":["Hezhe Qiao","Hanghang Tong","Bo An","Irwin King","Charu Aggarwal","Guansong Pang"],"pdf_url":"https://arxiv.org/pdf/2409.09957v1.pdf","comment":"24 pages, 6 figures, and 7 tables"},{"id":"http://arxiv.org/abs/2403.14608v7","updated":"2024-09-16T02:54:50Z","published":"2024-03-21T17:55:50Z","title":"Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey","summary":"  Large models represent a groundbreaking advancement in multiple application\nfields, enabling remarkable achievements across various tasks. However, their\nunprecedented scale comes with significant computational costs. These models,\noften consisting of billions of parameters, require vast amounts of\ncomputational resources for execution. Especially, the expansive scale and\ncomputational demands pose considerable challenges when customizing them for\nparticular downstream tasks, particularly over the hardware platforms\nconstrained by computational capabilities. Parameter Efficient Fine-Tuning\n(PEFT) provides a practical solution by efficiently adjusting the large models\nover the various downstream tasks. In particular, PEFT refers to the process of\nadjusting the parameters of a pre-trained large model to adapt it to a specific\ntask or domain while minimizing the number of additional parameters introduced\nor computational resources required. This approach is particularly important\nwhen dealing with large-scale language models with high parameter counts, as\nfine-tuning these models from scratch can be computationally expensive and\nresource-intensive, posing considerable challenges in the supporting system\nplatform design. In this survey, we present comprehensive studies of various\nPEFT algorithms, examining their performance and computational overhead.\nMoreover, we provide an overview of applications developed using different PEFT\nalgorithms and discuss common techniques employed to mitigate computation costs\nfor PEFT. In addition to providing an extensive survey from an algorithmic\nstandpoint, we also examine various real-world system designs to investigate\nthe implementation costs associated with different PEFT approaches. This survey\nserves as a valuable resource for researchers aiming to understand both the\nPEFT algorithm and its system implementation, offering detailed ......\n","authors":["Zeyu Han","Chao Gao","Jinyang Liu","Jeff Zhang","Sai Qian Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.14608v7.pdf","comment":"25 pages, 12 figures. Due to word limit, the abstract here is\n  truncated. The full abstract is available in the PDF"},{"id":"http://arxiv.org/abs/2409.09951v1","updated":"2024-09-16T02:45:54Z","published":"2024-09-16T02:45:54Z","title":"Optimal ablation for interpretability","summary":"  Interpretability studies often involve tracing the flow of information\nthrough machine learning models to identify specific model components that\nperform relevant computations for tasks of interest. Prior work quantifies the\nimportance of a model component on a particular task by measuring the impact of\nperforming ablation on that component, or simulating model inference with the\ncomponent disabled. We propose a new method, optimal ablation (OA), and show\nthat OA-based component importance has theoretical and empirical advantages\nover measuring importance via other ablation methods. We also show that\nOA-based component importance can benefit several downstream interpretability\ntasks, including circuit discovery, localization of factual recall, and latent\nprediction.\n","authors":["Maximilian Li","Lucas Janson"],"pdf_url":"https://arxiv.org/pdf/2409.09951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09944v1","updated":"2024-09-16T02:37:07Z","published":"2024-09-16T02:37:07Z","title":"Fault Analysis And Predictive Maintenance Of Induction Motor Using\n  Machine Learning","summary":"  Induction motors are one of the most crucial electrical equipment and are\nextensively used in industries in a wide range of applications. This paper\npresents a machine learning model for the fault detection and classification of\ninduction motor faults by using three phase voltages and currents as inputs.\nThe aim of this work is to protect vital electrical components and to prevent\nabnormal event progression through early detection and diagnosis. This work\npresents a fast forward artificial neural network model to detect some of the\ncommonly occurring electrical faults like overvoltage, under voltage, single\nphasing, unbalanced voltage, overload, ground fault. A separate model free\nmonitoring system wherein the motor itself acts like a sensor is presented and\nthe only monitored signals are the input given to the motor. Limits for current\nand voltage values are set for the faulty and healthy conditions, which is done\nby a classifier. Real time data from a 0.33 HP induction motor is used to train\nand test the neural network. The model so developed analyses the voltage and\ncurrent values given at a particular instant and classifies the data into no\nfault or the specific fault. The model is then interfaced with a real motor to\naccurately detect and classify the faults so that further necessary action can\nbe taken.\n","authors":["Kavana Venkatesh","Neethi M"],"pdf_url":"https://arxiv.org/pdf/2409.09944v1.pdf","comment":"Presented at ICEECCOT-2018, Published in IEEE Xplore, 6 pages, 3\n  figures"},{"id":"http://arxiv.org/abs/2304.01409v2","updated":"2024-09-16T02:35:01Z","published":"2023-04-03T22:56:08Z","title":"An Efficient Learning-Based Solver for Two-Stage DC Optimal Power Flow\n  with Feasibility Guarantees","summary":"  In this paper, we consider the scenario-based two-stage stochastic DC optimal\npower flow (OPF) problem for optimal and reliable dispatch when the load is\nfacing uncertainty. Although this problem is a linear program, it remains\ncomputationally challenging to solve due to the large number of scenarios\nneeded to accurately represent the uncertainties. To mitigate the computational\nissues, many techniques have been proposed to approximate the second-stage\ndecisions so they can be dealt more efficiently. The challenge of finding good\npolicies to approximate the second-stage decisions is that these solutions need\nto be feasible, which has been difficult to achieve with existing policies.\n  To address these challenges, this paper proposes a learning method to solve\nthe two-stage problem in a more efficient and optimal way. A technique called\nthe gauge map is incorporated into the learning architecture design to\nguarantee the learned solutions' feasibility to the network constraints.\nNamely, we can design policies that are feed forward functions and only output\nfeasible solutions. Simulation results on standard IEEE systems show that,\ncompared to iterative solvers and the widely used affine policy, our proposed\nmethod not only learns solutions of good quality but also accelerates the\ncomputation by orders of magnitude.\n","authors":["Ling Zhang","Daniel Tabas","Baosen Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.01409v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09931v1","updated":"2024-09-16T02:14:26Z","published":"2024-09-16T02:14:26Z","title":"Generalizability of Graph Neural Network Force Fields for Predicting\n  Solid-State Properties","summary":"  Machine-learned force fields (MLFFs) promise to offer a computationally\nefficient alternative to ab initio simulations for complex molecular systems.\nHowever, ensuring their generalizability beyond training data is crucial for\ntheir wide application in studying solid materials. This work investigates the\nability of a graph neural network (GNN)-based MLFF, trained on Lennard-Jones\nArgon, to describe solid-state phenomena not explicitly included during\ntraining. We assess the MLFF's performance in predicting phonon density of\nstates (PDOS) for a perfect face-centered cubic (FCC) crystal structure at both\nzero and finite temperatures. Additionally, we evaluate vacancy migration rates\nand energy barriers in an imperfect crystal using direct molecular dynamics\n(MD) simulations and the string method. Notably, vacancy configurations were\nabsent from the training data. Our results demonstrate the MLFF's capability to\ncapture essential solid-state properties with good agreement to reference data,\neven for unseen configurations. We further discuss data engineering strategies\nto enhance the generalizability of MLFFs. The proposed set of benchmark tests\nand workflow for evaluating MLFF performance in describing perfect and\nimperfect crystals pave the way for reliable application of MLFFs in studying\ncomplex solid-state materials.\n","authors":["Shaswat Mohanty","Yifan Wang","Wei Cai"],"pdf_url":"https://arxiv.org/pdf/2409.09931v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2409.09930v1","updated":"2024-09-16T02:08:33Z","published":"2024-09-16T02:08:33Z","title":"Mining of Switching Sparse Networks for Missing Value Imputation in\n  Multivariate Time Series","summary":"  Multivariate time series data suffer from the problem of missing values,\nwhich hinders the application of many analytical methods. To achieve the\naccurate imputation of these missing values, exploiting inter-correlation by\nemploying the relationships between sequences (i.e., a network) is as important\nas the use of temporal dependency, since a sequence normally correlates with\nother sequences. Moreover, exploiting an adequate network depending on time is\nalso necessary since the network varies over time. However, in real-world\nscenarios, we normally know neither the network structure nor when the network\nchanges beforehand. Here, we propose a missing value imputation method for\nmultivariate time series, namely MissNet, that is designed to exploit temporal\ndependency with a state-space model and inter-correlation by switching sparse\nnetworks. The network encodes conditional independence between features, which\nhelps us understand the important relationships for imputation visually. Our\nalgorithm, which scales linearly with reference to the length of the data,\nalternatively infers networks and fills in missing values using the networks\nwhile discovering the switching of the networks. Extensive experiments\ndemonstrate that MissNet outperforms the state-of-the-art algorithms for\nmultivariate time series imputation and provides interpretable results.\n","authors":["Kohei Obata","Koki Kawabata","Yasuko Matsubara","Yasushi Sakurai"],"pdf_url":"https://arxiv.org/pdf/2409.09930v1.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2409.00276v2","updated":"2024-09-16T01:41:17Z","published":"2024-08-30T22:12:57Z","title":"Exact Recovery Guarantees for Parameterized Non-linear System\n  Identification Problem under Adversarial Attacks","summary":"  In this work, we study the system identification problem for parameterized\nnon-linear systems using basis functions under adversarial attacks. Motivated\nby the LASSO-type estimators, we analyze the exact recovery property of a\nnon-smooth estimator, which is generated by solving an embedded $\\ell_1$-loss\nminimization problem. First, we derive necessary and sufficient conditions for\nthe well-specifiedness of the estimator and the uniqueness of global solutions\nto the underlying optimization problem. Next, we provide exact recovery\nguarantees for the estimator under two different scenarios of boundedness and\nLipschitz continuity of the basis functions. The non-asymptotic exact recovery\nis guaranteed with high probability, even when there are more severely\ncorrupted data than clean data. Finally, we numerically illustrate the validity\nof our theory. This is the first study on the sample complexity analysis of a\nnon-smooth estimator for the non-linear system identification problem.\n","authors":["Haixiang Zhang","Baturalp Yalcin","Javad Lavaei","Eduardo D. Sontag"],"pdf_url":"https://arxiv.org/pdf/2409.00276v2.pdf","comment":"33 pages"},{"id":"http://arxiv.org/abs/2409.09920v1","updated":"2024-09-16T01:35:34Z","published":"2024-09-16T01:35:34Z","title":"Multi-Step Embed to Control: A Novel Deep Learning-based Approach for\n  Surrogate Modelling in Reservoir Simulation","summary":"  Reduced-order models, also known as proxy model or surrogate model, are\napproximate models that are less computational expensive as opposed to fully\ndescriptive models. With the integration of machine learning, these models have\ngarnered increasing research interests recently. However, many existing\nreduced-order modeling methods, such as embed to control (E2C) and embed to\ncontrol and observe (E2CO), fall short in long-term predictions due to the\naccumulation of prediction errors over time. This issue arises partly from the\none-step prediction framework inherent in E2C and E2CO architectures. This\npaper introduces a deep learning-based surrogate model, referred as multi-step\nembed-to-control model, for the construction of proxy models with improved\nlong-term prediction performance. Unlike E2C and E2CO, the proposed network\nconsiders multiple forward transitions in the latent space at a time using\nKoopman operator, allowing the model to incorporate a sequence of state\nsnapshots during training phrases. Additionally, the loss function of this\nnovel approach has been redesigned to accommodate these multiple transitions\nand to respect the underlying physical principles. To validate the efficacy of\nthe proposed method, the developed framework was implemented within two-phase\n(oil and water) reservoir model under a waterflooding scheme. Comparative\nanalysis demonstrate that the proposed model significantly outperforms the\nconventional E2C model in long-term simulation scenarios. Notably, there was a\nsubstantial reduction in temporal errors in the prediction of saturation\nprofiles and a decent improvement in pressure forecasting accuracy.\n","authors":["Jungang Chen","Eduardo Gildin","John Killough"],"pdf_url":"https://arxiv.org/pdf/2409.09920v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.16341v2","updated":"2024-09-16T01:33:08Z","published":"2023-12-26T21:44:09Z","title":"Harnessing the Power of Federated Learning in Federated Contextual\n  Bandits","summary":"  Federated learning (FL) has demonstrated great potential in revolutionizing\ndistributed machine learning, and tremendous efforts have been made to extend\nit beyond the original focus on supervised learning. Among many directions,\nfederated contextual bandits (FCB), a pivotal integration of FL and sequential\ndecision-making, has garnered significant attention in recent years. Despite\nsubstantial progress, existing FCB approaches have largely employed their\ntailored FL components, often deviating from the canonical FL framework.\nConsequently, even renowned algorithms like FedAvg remain under-utilized in\nFCB, let alone other FL advancements. Motivated by this disconnection, this\nwork takes one step towards building a tighter relationship between the\ncanonical FL study and the investigations on FCB. In particular, a novel FCB\ndesign, termed FedIGW, is proposed to leverage a regression-based CB algorithm,\ni.e., inverse gap weighting. Compared with existing FCB approaches, the\nproposed FedIGW design can better harness the entire spectrum of FL\ninnovations, which is concretely reflected as (1) flexible incorporation of\n(both existing and forthcoming) FL protocols; (2) modularized plug-in of FL\nanalyses in performance guarantees; (3) seamless integration of FL appendages\n(such as personalization, robustness, and privacy). We substantiate these\nclaims through rigorous theoretical analyses and empirical evaluations.\n","authors":["Chengshuai Shi","Ruida Zhou","Kun Yang","Cong Shen"],"pdf_url":"https://arxiv.org/pdf/2312.16341v2.pdf","comment":"Accepted to Transactions on Machine Learning Research (07/2024); a\n  preliminary version appeared in the Multi-Agent Security Workshop at NeurIPS\n  2023"},{"id":"http://arxiv.org/abs/2406.04755v2","updated":"2024-09-16T01:23:27Z","published":"2024-06-07T08:54:55Z","title":"LLM Whisperer: An Inconspicuous Attack to Bias LLM Responses","summary":"  Writing effective prompts for large language models (LLM) can be unintuitive\nand burdensome. In response, services that optimize or suggest prompts have\nemerged. While such services can reduce user effort, they also introduce a\nrisk: the prompt provider can subtly manipulate prompts to produce heavily\nbiased LLM responses. In this work, we show that subtle synonym replacements in\nprompts can increase the likelihood (by a difference up to 78%) that LLMs\nmention a target concept (e.g., a brand, political party, nation). We\nsubstantiate our observations through a user study, showing our adversarially\nperturbed prompts 1) are indistinguishable from unaltered prompts by humans, 2)\npush LLMs to recommend target concepts more often, and 3) make users more\nlikely to notice target concepts, all without arousing suspicion. The\npracticality of this attack has the potential to undermine user autonomy. Among\nother measures, we recommend implementing warnings against using prompts from\nuntrusted parties.\n","authors":["Weiran Lin","Anna Gerchanovsky","Omer Akgul","Lujo Bauer","Matt Fredrikson","Zifan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.04755v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11569v3","updated":"2024-09-16T00:35:15Z","published":"2024-06-17T14:06:13Z","title":"Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated\n  Meta-Learning: Convergence-Generalization Trade-Offs","summary":"  For modern artificial intelligence (AI) applications such as large language\nmodels (LLMs), the training paradigm has recently shifted to pre-training\nfollowed by fine-tuning. Furthermore, owing to dwindling open repositories of\ndata and thanks to efforts to democratize access to AI models, pre-training is\nexpected to increasingly migrate from the current centralized deployments to\nfederated learning (FL) implementations. Meta-learning provides a general\nframework in which pre-training and fine-tuning can be formalized.\nMeta-learning-based personalized FL (meta-pFL) moves beyond basic\npersonalization by targeting generalization to new agents and tasks. This paper\nstudies the generalization performance of meta-pFL for a wireless setting in\nwhich the agents participating in the pre-training phase, i.e., meta-learning,\nare connected via a shared wireless channel to the server. Adopting\nover-the-air computing, we study the trade-off between generalization to new\nagents and tasks, on the one hand, and convergence, on the other hand. The\ntrade-off arises from the fact that channel impairments may enhance\ngeneralization, while degrading convergence. Extensive numerical results\nvalidate the theory.\n","authors":["Haifeng Wen","Hong Xing","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2406.11569v3.pdf","comment":"39 pages, 7 figures, submitted for possible journal publication"},{"id":"http://arxiv.org/abs/2405.03329v2","updated":"2024-09-16T00:19:16Z","published":"2024-05-06T10:09:35Z","title":"Policy Learning for Balancing Short-Term and Long-Term Rewards","summary":"  Empirical researchers and decision-makers spanning various domains frequently\nseek profound insights into the long-term impacts of interventions. While the\nsignificance of long-term outcomes is undeniable, an overemphasis on them may\ninadvertently overshadow short-term gains. Motivated by this, this paper\nformalizes a new framework for learning the optimal policy that effectively\nbalances both long-term and short-term rewards, where some long-term outcomes\nare allowed to be missing. In particular, we first present the identifiability\nof both rewards under mild assumptions. Next, we deduce the semiparametric\nefficiency bounds, along with the consistency and asymptotic normality of their\nestimators. We also reveal that short-term outcomes, if associated, contribute\nto improving the estimator of the long-term reward. Based on the proposed\nestimators, we develop a principled policy learning approach and further derive\nthe convergence rates of regret and estimation errors associated with the\nlearned policy. Extensive experiments are conducted to validate the\neffectiveness of the proposed method, demonstrating its practical\napplicability.\n","authors":["Peng Wu","Ziyu Shen","Feng Xie","Zhongyao Wang","Chunchen Liu","Yan Zeng"],"pdf_url":"https://arxiv.org/pdf/2405.03329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09903v1","updated":"2024-09-16T00:14:48Z","published":"2024-09-16T00:14:48Z","title":"Learning large softmax mixtures with warm start EM","summary":"  Mixed multinomial logits are discrete mixtures introduced several decades ago\nto model the probability of choosing an attribute from $p$ possible candidates,\nin heterogeneous populations. The model has recently attracted attention in the\nAI literature, under the name softmax mixtures, where it is routinely used in\nthe final layer of a neural network to map a large number $p$ of vectors in\n$\\mathbb{R}^L$ to a probability vector. Despite its wide applicability and\nempirical success, statistically optimal estimators of the mixture parameters,\nobtained via algorithms whose running time scales polynomially in $L$, are not\nknown. This paper provides a solution to this problem for contemporary\napplications, such as large language models, in which the mixture has a large\nnumber $p$ of support points, and the size $N$ of the sample observed from the\nmixture is also large. Our proposed estimator combines two classical\nestimators, obtained respectively via a method of moments (MoM) and the\nexpectation-minimization (EM) algorithm. Although both estimator types have\nbeen studied, from a theoretical perspective, for Gaussian mixtures, no similar\nresults exist for softmax mixtures for either procedure. We develop a new MoM\nparameter estimator based on latent moment estimation that is tailored to our\nmodel, and provide the first theoretical analysis for a MoM-based procedure in\nsoftmax mixtures. Although consistent, MoM for softmax mixtures can exhibit\npoor numerical performance, as observed other mixture models. Nevertheless, as\nMoM is provably in a neighborhood of the target, it can be used as warm start\nfor any iterative algorithm. We study in detail the EM algorithm, and provide\nits first theoretical analysis for softmax mixtures. Our final proposal for\nparameter estimation is the EM algorithm with a MoM warm start.\n","authors":["Xin Bing","Florentina Bunea","Jonathan Niles-Weed","Marten Wegkamp"],"pdf_url":"https://arxiv.org/pdf/2409.09903v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.10281v1","updated":"2024-09-16T13:44:20Z","published":"2024-09-16T13:44:20Z","title":"DreamHead: Learning Spatial-Temporal Correspondence via Hierarchical\n  Diffusion for Audio-driven Talking Head Synthesis","summary":"  Audio-driven talking head synthesis strives to generate lifelike video\nportraits from provided audio. The diffusion model, recognized for its superior\nquality and robust generalization, has been explored for this task. However,\nestablishing a robust correspondence between temporal audio cues and\ncorresponding spatial facial expressions with diffusion models remains a\nsignificant challenge in talking head generation. To bridge this gap, we\npresent DreamHead, a hierarchical diffusion framework that learns\nspatial-temporal correspondences in talking head synthesis without compromising\nthe model's intrinsic quality and adaptability.~DreamHead learns to predict\ndense facial landmarks from audios as intermediate signals to model the spatial\nand temporal correspondences.~Specifically, a first hierarchy of\naudio-to-landmark diffusion is first designed to predict temporally smooth and\naccurate landmark sequences given audio sequence signals. Then, a second\nhierarchy of landmark-to-image diffusion is further proposed to produce\nspatially consistent facial portrait videos, by modeling spatial\ncorrespondences between the dense facial landmark and appearance. Extensive\nexperiments show that proposed DreamHead can effectively learn spatial-temporal\nconsistency with the designed hierarchical diffusion and produce high-fidelity\naudio-driven talking head videos for multiple identities.\n","authors":["Fa-Ting Hong","Yunfei Liu","Yu Li","Changyin Zhou","Fei Yu","Dan Xu"],"pdf_url":"https://arxiv.org/pdf/2409.10281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08795v2","updated":"2024-09-16T12:58:16Z","published":"2024-09-13T12:59:39Z","title":"LLaQo: Towards a Query-Based Coach in Expressive Music Performance\n  Assessment","summary":"  Research in music understanding has extensively explored composition-level\nattributes such as key, genre, and instrumentation through advanced\nrepresentations, leading to cross-modal applications using large language\nmodels. However, aspects of musical performance such as stylistic expression\nand technique remain underexplored, along with the potential of using large\nlanguage models to enhance educational outcomes with customized feedback. To\nbridge this gap, we introduce LLaQo, a Large Language Query-based music coach\nthat leverages audio language modeling to provide detailed and formative\nassessments of music performances. We also introduce instruction-tuned\nquery-response datasets that cover a variety of performance dimensions from\npitch accuracy to articulation, as well as contextual performance understanding\n(such as difficulty and performance techniques). Utilizing AudioMAE encoder and\nVicuna-7b LLM backend, our model achieved state-of-the-art (SOTA) results in\npredicting teachers' performance ratings, as well as in identifying piece\ndifficulty and playing techniques. Textual responses from LLaQo was moreover\nrated significantly higher compared to other baseline models in a user study\nusing audio-text matching. Our proposed model can thus provide informative\nanswers to open-ended questions related to musical performance from audio data.\n","authors":["Huan Zhang","Vincent Cheung","Hayato Nishioka","Simon Dixon","Shinichi Furuya"],"pdf_url":"https://arxiv.org/pdf/2409.08795v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10197v1","updated":"2024-09-16T11:43:19Z","published":"2024-09-16T11:43:19Z","title":"Fit and Prune: Fast and Training-free Visual Token Pruning for\n  Multi-modal Large Language Models","summary":"  Recent progress in Multimodal Large Language Models(MLLMs) often use large\nimage tokens to compensate the visual shortcoming of MLLMs, which not only\nexhibits obvious redundancy but also greatly exacerbates the already high\ncomputation. Token pruning is an effective solution for speeding up MLLMs, but\nwhen and how to drop tokens still remains a challenge. In this paper, we\npropose a novel and training-free approach for the effective visual token\npruning of MLLMs, termed FitPrune, which can quickly produce a complete pruning\nrecipe for MLLMs according to a pre-defined budget. Specifically, FitPrune\nconsiders token pruning as a statistical problem of MLLM and its objective is\nto find out an optimal pruning scheme that can minimize the divergence of the\nattention distributions before and after pruning. In practice, FitPrune can be\nquickly accomplished based on the attention statistics from a small batch of\ninference data, avoiding the expensive trials of MLLMs. According to the\npruning recipe, an MLLM can directly remove the redundant visual tokens of\ndifferent examples during inference. To validate FitPrune, we apply it to a set\nof recent MLLMs, including LLaVA-1.5, LLaVA-HR and LLaVA-NEXT, and conduct\nextensive experiments on a set of benchmarks. The experimental results show\nthat our FitPrune can not only reduce the computational complexity to a large\nextent, while retaining high performance, e.g., -54.9% FLOPs for LLaVA-NEXT\nwith only 0.5% accuracy drop. Notably, the pruning recipe can be obtained in\nabout 5 minutes. Our code is available at https://github.com/ywh187/FitPrune.\n","authors":["Weihao Ye","Qiong Wu","Wenhao Lin","Yiyi Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.10197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10042v1","updated":"2024-09-16T07:12:04Z","published":"2024-09-16T07:12:04Z","title":"Cross: A Delay Based Congestion Control Method for RTP Media","summary":"  After more than a decade of development, real time communication (RTC) for\nvideo telephony has made significantly progress. However, emerging high-quality\nRTC applications with high definition and high frame rate requires sufficient\nbandwidth. The default congestion control mechanism specifically tuned for\nvideo telephony leaves plenty of room for optimization under high-rate\nscenarios. It is necessary to develop new rate control solutions to utilize\nbandwidth efficiently and to provide better experience for such services. A\ndelay-based congestion control method called Cross is proposed, which regulates\nrate based on queue load with a multiplicative increase and multiplicative\ndecrease fashion. A simulation module is developed to validate the\neffectiveness of these congestion control algorithms for RTC services. The\nmodule is released with the hope to provide convenience for RTC research\ncommunity. Simulation results demonstrate that Cross can achieve low queuing\ndelay and maintain high channel utilization under random loss environments.\nOnline deployment shows that Cross can reduce the video freezing ratio by up to\n58.45\\% on average when compared with a benchmark algorithm.\n","authors":["Songyang Zhang","Changpeng Yang"],"pdf_url":"https://arxiv.org/pdf/2409.10042v1.pdf","comment":"12 pages"}]},"2024-09-17T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2406.08818v3","updated":"2024-09-17T05:29:50Z","published":"2024-06-13T05:20:42Z","title":"Linguistic Bias in ChatGPT: Language Models Reinforce Dialect\n  Discrimination","summary":"  We present a large-scale study of linguistic bias exhibited by ChatGPT\ncovering ten dialects of English (Standard American English, Standard British\nEnglish, and eight widely spoken non-\"standard\" varieties from around the\nworld). We prompted GPT-3.5 Turbo and GPT-4 with text by native speakers of\neach variety and analyzed the responses via detailed linguistic feature\nannotation and native speaker evaluation. We find that the models default to\n\"standard\" varieties of English; based on evaluation by native speakers, we\nalso find that model responses to non-\"standard\" varieties consistently exhibit\na range of issues: stereotyping (19% worse than for \"standard\" varieties),\ndemeaning content (25% worse), lack of comprehension (9% worse), and\ncondescending responses (15% worse). We also find that if these models are\nasked to imitate the writing style of prompts in non-\"standard\" varieties, they\nproduce text that exhibits lower comprehension of the input and is especially\nprone to stereotyping. GPT-4 improves on GPT-3.5 in terms of comprehension,\nwarmth, and friendliness, but also exhibits a marked increase in stereotyping\n(+18%). The results indicate that GPT-3.5 Turbo and GPT-4 can perpetuate\nlinguistic discrimination toward speakers of non-\"standard\" varieties.\n","authors":["Eve Fleisig","Genevieve Smith","Madeline Bossi","Ishita Rustagi","Xavier Yin","Dan Klein"],"pdf_url":"https://arxiv.org/pdf/2406.08818v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10482v2","updated":"2024-09-17T12:10:49Z","published":"2024-09-16T17:18:11Z","title":"Schrodinger's Memory: Large Language Models","summary":"  Memory is the foundation of all human activities; without memory, it would be\nnearly impossible for people to perform any task in daily life. With the\ndevelopment of Large Language Models (LLMs), their language capabilities are\nbecoming increasingly comparable to those of humans. But do LLMs have memory?\nBased on current performance, LLMs do appear to exhibit memory. So, what is the\nunderlying mechanism of this memory? Previous research has lacked a deep\nexploration of LLMs' memory capabilities and the underlying theory. In this\npaper, we use Universal Approximation Theorem (UAT) to explain the memory\nmechanism in LLMs. We also conduct experiments to verify the memory\ncapabilities of various LLMs, proposing a new method to assess their abilities\nbased on these memory ability. We argue that LLM memory operates like\nSchr\\\"odinger's memory, meaning that it only becomes observable when a specific\nmemory is queried. We can only determine if the model retains a memory based on\nits output in response to the query; otherwise, it remains indeterminate.\nFinally, we expand on this concept by comparing the memory capabilities of the\nhuman brain and LLMs, highlighting the similarities and differences in their\noperational mechanisms.\n","authors":["Wei Wang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2409.10482v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10177v2","updated":"2024-09-17T06:30:03Z","published":"2024-09-16T11:13:14Z","title":"Augmenting Automatic Speech Recognition Models with Disfluency Detection","summary":"  Speech disfluency commonly occurs in conversational and spontaneous speech.\nHowever, standard Automatic Speech Recognition (ASR) models struggle to\naccurately recognize these disfluencies because they are typically trained on\nfluent transcripts. Current research mainly focuses on detecting disfluencies\nwithin transcripts, overlooking their exact location and duration in the\nspeech. Additionally, previous work often requires model fine-tuning and\naddresses limited types of disfluencies.\n  In this work, we present an inference-only approach to augment any ASR model\nwith the ability to detect open-set disfluencies. We first demonstrate that ASR\nmodels have difficulty transcribing speech disfluencies. Next, this work\nproposes a modified Connectionist Temporal Classification(CTC)-based forced\nalignment algorithm from \\cite{kurzinger2020ctc} to predict word-level\ntimestamps while effectively capturing disfluent speech. Additionally, we\ndevelop a model to classify alignment gaps between timestamps as either\ncontaining disfluent speech or silence. This model achieves an accuracy of\n81.62% and an F1-score of 80.07%. We test the augmentation pipeline of\nalignment gap detection and classification on a disfluent dataset. Our results\nshow that we captured 74.13% of the words that were initially missed by the\ntranscription, demonstrating the potential of this pipeline for downstream\ntasks.\n","authors":["Robin Amann","Zhaolin Li","Barbara Bruno","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2409.10177v2.pdf","comment":"Accepted by SLT2024"},{"id":"http://arxiv.org/abs/2409.10173v2","updated":"2024-09-17T06:42:20Z","published":"2024-09-16T11:10:29Z","title":"jina-embeddings-v3: Multilingual Embeddings With Task LoRA","summary":"  We introduce jina-embeddings-v3, a novel text embedding model with 570\nmillion parameters, achieves state-of-the-art performance on multilingual data\nand long-context retrieval tasks, supporting context lengths of up to 8192\ntokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)\nadapters to generate high-quality embeddings for query-document retrieval,\nclustering, classification, and text matching. Additionally, Matryoshka\nRepresentation Learning is integrated into the training process, allowing\nflexible truncation of embedding dimensions without compromising performance.\nEvaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the\nlatest proprietary embeddings from OpenAI and Cohere on English tasks, while\nachieving superior performance compared to multilingual-e5-large-instruct\nacross all multilingual tasks.\n","authors":["Saba Sturua","Isabelle Mohr","Mohammad Kalim Akram","Michael G√ºnther","Bo Wang","Markus Krimmel","Feng Wang","Georgios Mastrapas","Andreas Koukounas","Andreas Koukounas","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2409.10173v2.pdf","comment":"20 pages, pp11-13 references, pp14-20 appendix and experiment tables"},{"id":"http://arxiv.org/abs/2404.07066v4","updated":"2024-09-17T01:37:18Z","published":"2024-04-10T14:56:40Z","title":"Exploring Concept Depth: How Large Language Models Acquire Knowledge at\n  Different Layers?","summary":"  Large language models (LLMs) have shown remarkable performances across a wide\nrange of tasks. However, the mechanisms by which these models encode tasks of\nvarying complexities remain poorly understood. In this paper, we explore the\nhypothesis that LLMs process concepts of varying complexities in different\nlayers, introducing the idea of ``Concept Depth'' to suggest that more complex\nconcepts are typically acquired in deeper layers. Specifically, we categorize\nconcepts based on their level of abstraction, defining them in the order of\nincreasing complexity within factual, emotional, and inferential tasks. We\nconduct extensive probing experiments using layer-wise representations across\nvarious LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the\nthree domains of tasks. Our findings reveal that models could efficiently\nconduct probing for simpler tasks in shallow layers, and more complex tasks\ntypically necessitate deeper layers for accurate understanding. Additionally,\nwe examine how external factors, such as adding noise to the input and\nquantizing the model weights, might affect layer-wise representations. Our\nfindings suggest that these factors can impede the development of a conceptual\nunderstanding of LLMs until deeper layers are explored. We hope that our\nproposed concept and experimental insights will enhance the understanding of\nthe mechanisms underlying LLMs. Our codes are available at\n\\url{https://github.com/Luckfort/CD}.\n","authors":["Mingyu Jin","Qinkai Yu","Jingyuan Huang","Qingcheng Zeng","Zhenting Wang","Wenyue Hua","Haiyan Zhao","Kai Mei","Yanda Meng","Kaize Ding","Fan Yang","Mengnan Du","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.07066v4.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2409.09831v2","updated":"2024-09-17T11:18:37Z","published":"2024-09-15T19:11:01Z","title":"Generating Synthetic Free-text Medical Records with Low\n  Re-identification Risk using Masked Language Modeling","summary":"  In this paper, we present a system that generates synthetic free-text medical\nrecords, such as discharge summaries, admission notes and doctor\ncorrespondences, using Masked Language Modeling (MLM). Our system is designed\nto preserve the critical information of the records while introducing\nsignificant diversity and minimizing re-identification risk. The system\nincorporates a de-identification component that uses Philter to mask Protected\nHealth Information (PHI), followed by a Medical Entity Recognition (NER) model\nto retain key medical information. We explore various masking ratios and\nmask-filling techniques to balance the trade-off between diversity and fidelity\nin the synthetic outputs without affecting overall readability. Our results\ndemonstrate that the system can produce high-quality synthetic data with\nsignificant diversity while achieving a HIPAA-compliant PHI recall rate of 0.96\nand a low re-identification risk of 0.035. Furthermore, downstream evaluations\nusing a NER task reveal that the synthetic data can be effectively used to\ntrain models with performance comparable to those trained on real data. The\nflexibility of the system allows it to be adapted for specific use cases,\nmaking it a valuable tool for privacy-preserving data generation in medical\nresearch and healthcare applications.\n","authors":["Samuel Belkadi","Libo Ren","Nicolo Micheletti","Lifeng Han","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2409.09831v2.pdf","comment":"Added references and rephrased some sentences"},{"id":"http://arxiv.org/abs/2409.09785v2","updated":"2024-09-17T09:32:04Z","published":"2024-09-15T16:32:49Z","title":"Large Language Model Based Generative Error Correction: A Challenge and\n  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition","summary":"  Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations.\n","authors":["Chao-Han Huck Yang","Taejin Park","Yuan Gong","Yuanchao Li","Zhehuai Chen","Yen-Ting Lin","Chen Chen","Yuchen Hu","Kunal Dhawan","Piotr ≈ªelasko","Chao Zhang","Yun-Nung Chen","Yu Tsao","Jagadeesh Balam","Boris Ginsburg","Sabato Marco Siniscalchi","Eng Siong Chng","Peter Bell","Catherine Lai","Shinji Watanabe","Andreas Stolcke"],"pdf_url":"https://arxiv.org/pdf/2409.09785v2.pdf","comment":"IEEE SLT 2024. The initial draft version has been done in December\n  2023. Post-ASR Text Processing and Understanding Community:\n  https://huggingface.co/GenSEC-LLM"},{"id":"http://arxiv.org/abs/2409.09662v2","updated":"2024-09-17T14:44:34Z","published":"2024-09-15T08:25:24Z","title":"ExploreSelf: Fostering User-driven Exploration and Reflection on\n  Personal Challenges with Adaptive Guidance by Large Language Models","summary":"  Expressing stressful experiences in words is proven to improve mental and\nphysical health, but individuals often disengage with writing interventions as\nthey struggle to organize their thoughts and emotions. Reflective prompts have\nbeen used to provide direction, and large language models (LLMs) have\ndemonstrated the potential to provide tailored guidance. Current systems often\nlimit users' flexibility to direct their reflections. We thus present\nExploreSelf, an LLM-driven application designed to empower users to control\ntheir reflective journey. ExploreSelf allows users to receive adaptive support\nthrough dynamically generated questions. Through an exploratory study with 19\nparticipants, we examine how participants explore and reflect on personal\nchallenges using ExploreSelf. Our findings demonstrate that participants valued\nthe balance between guided support and freedom to control their reflective\njourney, leading to deeper engagement and insight. Building on our findings, we\ndiscuss implications for designing LLM-driven tools that promote user\nempowerment through effective reflective practices.\n","authors":["Inhwa Song","SoHyun Park","Sachin R. Pendse","Jessica Lee Schleider","Munmun De Choudhury","Young-Ho Kim"],"pdf_url":"https://arxiv.org/pdf/2409.09662v2.pdf","comment":"17 pages excluding reference and appendix"},{"id":"http://arxiv.org/abs/2409.09383v2","updated":"2024-09-17T01:35:25Z","published":"2024-09-14T09:21:46Z","title":"LLM-Powered Ensemble Learning for Paper Source Tracing: A GPU-Free\n  Approach","summary":"  We participated in the KDD CUP 2024 paper source tracing competition and\nachieved the 3rd place. This competition tasked participants with identifying\nthe reference sources (i.e., ref-sources, as referred to by the organizers of\nthe competition) of given academic papers. Unlike most teams that addressed\nthis challenge by fine-tuning pre-trained neural language models such as BERT\nor ChatGLM, our primary approach utilized closed-source large language models\n(LLMs). With recent advancements in LLM technology, closed-source LLMs have\ndemonstrated the capability to tackle complex reasoning tasks in zero-shot or\nfew-shot scenarios. Consequently, in the absence of GPUs, we employed\nclosed-source LLMs to directly generate predicted reference sources from the\nprovided papers. We further refined these predictions through ensemble\nlearning. Notably, our method was the only one among the award-winning\napproaches that did not require the use of GPUs for model training. Code\navailable at https://github.com/Cklwanfifa/KDDCUP2024-PST.\n","authors":["Kunlong Chen","Junjun Wang","Zhaoqun Chen","Kunjin Chen","Yitian Chen"],"pdf_url":"https://arxiv.org/pdf/2409.09383v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15796v3","updated":"2024-09-17T12:00:10Z","published":"2024-06-22T09:40:07Z","title":"Unveiling Entity-Level Unlearning for Large Language Models: A\n  Comprehensive Analysis","summary":"  Large language model unlearning has garnered increasing attention due to its\npotential to address security and privacy concerns, leading to extensive\nresearch in the field. However, much of this research has concentrated on\ninstance-level unlearning, specifically targeting the removal of predefined\ninstances containing sensitive content. This focus has left a significant gap\nin the exploration of full entity-level unlearning, which is critical in\nreal-world scenarios such as copyright protection. To this end, we propose a\nnovel task of Entity-level unlearning, which aims to erase entity-related\nknowledge from the target model completely. To thoroughly investigate this\ntask, we systematically evaluate trending unlearning algorithms, revealing that\ncurrent methods struggle to achieve effective entity-level unlearning. Then, we\nfurther explore the factors that influence the performance of the unlearning\nalgorithms, identifying that knowledge coverage and the size of the forget set\nplay pivotal roles. Notably, our analysis also uncovers that entities\nintroduced through fine-tuning are more vulnerable to unlearning than\npre-trained entities. These findings collectively offer valuable insights for\nadvancing entity-level unlearning for LLMs.\n","authors":["Weitao Ma","Xiaocheng Feng","Weihong Zhong","Lei Huang","Yangfan Ye","Xiachong Feng","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2406.15796v3.pdf","comment":"Work in progress"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2409.10328v2","updated":"2024-09-17T02:35:24Z","published":"2024-09-16T14:39:04Z","title":"Fuse4Seg: Image-Level Fusion Based Multi-Modality Medical Image\n  Segmentation","summary":"  Although multi-modality medical image segmentation holds significant\npotential for enhancing the diagnosis and understanding of complex diseases by\nintegrating diverse imaging modalities, existing methods predominantly rely on\nfeature-level fusion strategies. We argue the current feature-level fusion\nstrategy is prone to semantic inconsistencies and misalignments across various\nimaging modalities because it merges features at intermediate layers in a\nneural network without evaluative control. To mitigate this, we introduce a\nnovel image-level fusion based multi-modality medical image segmentation\nmethod, Fuse4Seg, which is a bi-level learning framework designed to model the\nintertwined dependencies between medical image segmentation and medical image\nfusion. The image-level fusion process is seamlessly employed to guide and\nenhance the segmentation results through a layered optimization approach.\nBesides, the knowledge gained from the segmentation module can effectively\nenhance the fusion module. This ensures that the resultant fused image is a\ncoherent representation that accurately amalgamates information from all\nmodalities. Moreover, we construct a BraTS-Fuse benchmark based on BraTS\ndataset, which includes 2040 paired original images, multi-modal fusion images,\nand ground truth. This benchmark not only serves image-level medical\nsegmentation but is also the largest dataset for medical image fusion to date.\nExtensive experiments on several public datasets and our benchmark demonstrate\nthe superiority of our approach over prior state-of-the-art (SOTA)\nmethodologies.\n","authors":["Yuchen Guo","Weifeng Su"],"pdf_url":"https://arxiv.org/pdf/2409.10328v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10063v2","updated":"2024-09-17T06:46:21Z","published":"2024-09-16T07:56:41Z","title":"GlobalMapNet: An Online Framework for Vectorized Global HD Map\n  Construction","summary":"  High-definition (HD) maps are essential for autonomous driving systems.\nTraditionally, an expensive and labor-intensive pipeline is implemented to\nconstruct HD maps, which is limited in scalability. In recent years,\ncrowdsourcing and online mapping have emerged as two alternative methods, but\nthey have limitations respectively. In this paper, we provide a novel\nmethodology, namely global map construction, to perform direct generation of\nvectorized global maps, combining the benefits of crowdsourcing and online\nmapping. We introduce GlobalMapNet, the first online framework for vectorized\nglobal HD map construction, which updates and utilizes a global map on the ego\nvehicle. To generate the global map from scratch, we propose GlobalMapBuilder\nto match and merge local maps continuously. We design a new algorithm, Map NMS,\nto remove duplicate map elements and produce a clean map. We also propose\nGlobalMapFusion to aggregate historical map information, improving consistency\nof prediction. We examine GlobalMapNet on two widely recognized datasets,\nArgoverse2 and nuScenes, showing that our framework is capable of generating\nglobally consistent results.\n","authors":["Anqi Shi","Yuze Cai","Xiangyu Chen","Jian Pu","Zeyu Fu","Hong Lu"],"pdf_url":"https://arxiv.org/pdf/2409.10063v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09877v2","updated":"2024-09-17T01:30:22Z","published":"2024-09-15T22:04:33Z","title":"REG: Refined Generalized Focal Loss for Road Asset Detection on Thai\n  Highways Using Vision-Based Detection and Segmentation Models","summary":"  This paper introduces a novel framework for detecting and segmenting critical\nroad assets on Thai highways using an advanced Refined Generalized Focal Loss\n(REG) formulation. Integrated into state-of-the-art vision-based detection and\nsegmentation models, the proposed method effectively addresses class imbalance\nand the challenges of localizing small, underrepresented road elements,\nincluding pavilions, pedestrian bridges, information signs, single-arm poles,\nbus stops, warning signs, and concrete guardrails. To improve both detection\nand segmentation accuracy, a multi-task learning strategy is adopted,\noptimizing REG across multiple tasks. REG is further enhanced by incorporating\na spatial-contextual adjustment term, which accounts for the spatial\ndistribution of road assets, and a probabilistic refinement that captures\nprediction uncertainty in complex environments, such as varying lighting\nconditions and cluttered backgrounds. Our rigorous mathematical formulation\ndemonstrates that REG minimizes localization and classification errors by\napplying adaptive weighting to hard-to-detect instances while down-weighting\neasier examples. Experimental results show a substantial performance\nimprovement, achieving a mAP50 of 80.34 and an F1-score of 77.87, significantly\noutperforming conventional methods. This research underscores the capability of\nadvanced loss function refinements to enhance the robustness and accuracy of\nroad asset detection and segmentation, thereby contributing to improved road\nsafety and infrastructure management. For an in-depth discussion of the\nmathematical background and related methods, please refer to previous work\navailable at \\url{https://github.com/kaopanboonyuen/REG}.\n","authors":["Teerapong Panboonyuen"],"pdf_url":"https://arxiv.org/pdf/2409.09877v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2409.09740v2","updated":"2024-09-17T08:00:50Z","published":"2024-09-15T14:10:31Z","title":"VGG-Tex: A Vivid Geometry-Guided Facial Texture Estimation Model for\n  High Fidelity Monocular 3D Face Reconstruction","summary":"  3D face reconstruction from monocular images has promoted the development of\nvarious applications such as augmented reality. Though existing methods have\nmade remarkable progress, most of them emphasize geometric reconstruction,\nwhile overlooking the importance of texture prediction. To address this issue,\nwe propose VGG-Tex, a novel Vivid Geometry-Guided Facial Texture Estimation\nmodel designed for High Fidelity Monocular 3D Face Reconstruction. The core of\nthis approach is leveraging 3D parametric priors to enhance the outcomes of 2D\nUV texture estimation. Specifically, VGG-Tex includes a Facial Attributes\nEncoding Module, a Geometry-Guided Texture Generator, and a Visibility-Enhanced\nTexture Completion Module. These components are responsible for extracting\nparametric priors, generating initial textures, and refining texture details,\nrespectively. Based on the geometry-texture complementarity principle, VGG-Tex\nalso introduces a Texture-guided Geometry Refinement Module to further balance\nthe overall fidelity of the reconstructed 3D faces, along with corresponding\nlosses. Comprehensive experiments demonstrate that our method significantly\nimproves texture reconstruction performance compared to existing\nstate-of-the-art methods.\n","authors":["Haoyu Wu","Ziqiao Peng","Xukun Zhou","Yunfei Cheng","Jun He","Hongyan Liu","Zhaoxin Fan"],"pdf_url":"https://arxiv.org/pdf/2409.09740v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.05423v5","updated":"2024-09-17T02:48:38Z","published":"2021-03-09T13:58:35Z","title":"Deep Learning Based 3D Segmentation: A Survey","summary":"  3D segmentation is a fundamental and challenging problem in computer vision\nwith applications in autonomous driving and robotics. It has received\nsignificant attention from the computer vision, graphics and machine learning\ncommunities. Conventional methods for 3D segmentation, based on hand-crafted\nfeatures and machine learning classifiers, lack generalization ability. Driven\nby their success in 2D computer vision, deep learning techniques have recently\nbecome the tool of choice for 3D segmentation tasks. This has led to an influx\nof many methods in the literature that have been evaluated on different\nbenchmark datasets. Whereas survey papers on RGB-D and point cloud segmentation\nexist, there is a lack of a recent in-depth survey that covers all 3D data\nmodalities and application domains. This paper fills the gap and\ncomprehensively surveys the recent progress in deep learning-based 3D\nsegmentation techniques. We cover over 220 works from the last six years,\nanalyze their strengths and limitations, and discuss their competitive results\non benchmark datasets. The survey provides a summary of the most commonly used\npipelines and finally highlights promising research directions for the future.\n","authors":["Yong He","Hongshan Yu","Xiaoyan Liu","Zhengeng Yang","Wei Sun","Saeed Anwar","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2103.05423v5.pdf","comment":"30 pages, 10 tables, 8 figures, update the segmentation method to\n  2024, add the segmentation application in semantic map construction and\n  cultural heritage preservation, change the paper format"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.10173v2","updated":"2024-09-17T06:42:20Z","published":"2024-09-16T11:10:29Z","title":"jina-embeddings-v3: Multilingual Embeddings With Task LoRA","summary":"  We introduce jina-embeddings-v3, a novel text embedding model with 570\nmillion parameters, achieves state-of-the-art performance on multilingual data\nand long-context retrieval tasks, supporting context lengths of up to 8192\ntokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)\nadapters to generate high-quality embeddings for query-document retrieval,\nclustering, classification, and text matching. Additionally, Matryoshka\nRepresentation Learning is integrated into the training process, allowing\nflexible truncation of embedding dimensions without compromising performance.\nEvaluation on the MTEB benchmark shows that jina-embeddings-v3 outperforms the\nlatest proprietary embeddings from OpenAI and Cohere on English tasks, while\nachieving superior performance compared to multilingual-e5-large-instruct\nacross all multilingual tasks.\n","authors":["Saba Sturua","Isabelle Mohr","Mohammad Kalim Akram","Michael G√ºnther","Bo Wang","Markus Krimmel","Feng Wang","Georgios Mastrapas","Andreas Koukounas","Andreas Koukounas","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2409.10173v2.pdf","comment":"20 pages, pp11-13 references, pp14-20 appendix and experiment tables"},{"id":"http://arxiv.org/abs/2409.07730v2","updated":"2024-09-17T00:48:38Z","published":"2024-09-12T03:33:19Z","title":"Music auto-tagging in the long tail: A few-shot approach","summary":"  In the realm of digital music, using tags to efficiently organize and\nretrieve music from extensive databases is crucial for music catalog owners.\nHuman tagging by experts is labor-intensive but mostly accurate, whereas\nautomatic tagging through supervised learning has approached satisfying\naccuracy but is restricted to a predefined set of training tags. Few-shot\nlearning offers a viable solution to expand beyond this small set of predefined\ntags by enabling models to learn from only a few human-provided examples to\nunderstand tag meanings and subsequently apply these tags autonomously. We\npropose to integrate few-shot learning methodology into multi-label music\nauto-tagging by using features from pre-trained models as inputs to a\nlightweight linear classifier, also known as a linear probe. We investigate\ndifferent popular pre-trained features, as well as different few-shot\nparametrizations with varying numbers of classes and samples per class. Our\nexperiments demonstrate that a simple model with pre-trained features can\nachieve performance close to state-of-the-art models while using significantly\nless training data, such as 20 samples per tag. Additionally, our linear probe\nperforms competitively with leading models when trained on the entire training\ndataset. The results show that this transfer learning-based few-shot approach\ncould effectively address the issue of automatically assigning long-tail tags\nwith only limited labeled data.\n","authors":["T. Aleksandra Ma","Alexander Lerch"],"pdf_url":"https://arxiv.org/pdf/2409.07730v2.pdf","comment":"Published in Audio Engineering Society NY Show 2024 as a Peer\n  Reviewed (Category 1) paper; typos corrected"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2409.10489v2","updated":"2024-09-17T12:01:14Z","published":"2024-09-16T17:22:34Z","title":"Flash STU: Fast Spectral Transform Units","summary":"  This paper describes an efficient, open source PyTorch implementation of the\nSpectral Transform Unit. We investigate sequence prediction tasks over several\nmodalities including language, robotics, and simulated dynamical systems. We\nfind that for the same parameter count, the STU and its variants outperform the\nTransformer as well as other leading state space models across various\nmodalities.\n","authors":["Y. Isabel Liu","Windsor Nguyen","Yagiz Devre","Evan Dogariu","Anirudha Majumdar","Elad Hazan"],"pdf_url":"https://arxiv.org/pdf/2409.10489v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.11295v3","updated":"2024-09-17T09:44:27Z","published":"2023-08-22T09:17:45Z","title":"Uncertainty Estimation of Transformers' Predictions via Topological\n  Analysis of the Attention Matrices","summary":"  Transformer-based language models have set new benchmarks across a wide range\nof NLP tasks, yet reliably estimating the uncertainty of their predictions\nremains a significant challenge. Existing uncertainty estimation (UE)\ntechniques often fall short in classification tasks, either offering minimal\nimprovements over basic heuristics or relying on costly ensemble models.\nMoreover, attempts to leverage common embeddings for UE in linear probing\nscenarios have yielded only modest gains, indicating that alternative model\ncomponents should be explored.\n  We tackle these limitations by harnessing the geometry of attention maps\nacross multiple heads and layers to assess model confidence. Our approach\nextracts topological features from attention matrices, providing a\nlow-dimensional, interpretable representation of the model's internal dynamics.\nAdditionally, we introduce topological features to compare attention patterns\nacross heads and layers. Our method significantly outperforms existing UE\ntechniques on benchmarks for acceptability judgments and artificial text\ndetection, offering a more efficient and interpretable solution for uncertainty\nestimation in large-scale language models.\n","authors":["Elizaveta Kostenok","Daniil Cherniavskii","Alexey Zaytsev"],"pdf_url":"https://arxiv.org/pdf/2308.11295v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09945v2","updated":"2024-09-17T03:18:25Z","published":"2024-09-16T02:37:51Z","title":"Mobility-GNN: a human mobility-based graph neural network for tracking\n  and analyzing the spatial dynamics of the synthetic opioid crisis in the USA,\n  2013-2020","summary":"  Synthetic opioids are the most common drugs involved in drug-involved\noverdose mortalities in the U.S. The Center for Disease Control and Prevention\nreported that in 2018, about 70% of all drug overdose deaths involved opioids\nand 67% of all opioid-involved deaths were accounted for by synthetic opioids.\nIn this study, we investigated the spread of synthetic opioids between 2013 and\n2020 in the U.S., and analyzed the relationship between the spatiotemporal\npattern of synthetic opioid-involved deaths and another key opioid, heroin, and\ncompared patterns of deaths involving these two types of drugs during this time\nperiod. Spatial connections between counties were incorporated into a graph\nconvolutional neural network model to represent and analyze the spread of\nsynthetic opioid-involved deaths, and in the context of heroin-involved deaths.\n","authors":["Zhiyue Xia","Kathleen Stewart"],"pdf_url":"https://arxiv.org/pdf/2409.09945v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07066v4","updated":"2024-09-17T01:37:18Z","published":"2024-04-10T14:56:40Z","title":"Exploring Concept Depth: How Large Language Models Acquire Knowledge at\n  Different Layers?","summary":"  Large language models (LLMs) have shown remarkable performances across a wide\nrange of tasks. However, the mechanisms by which these models encode tasks of\nvarying complexities remain poorly understood. In this paper, we explore the\nhypothesis that LLMs process concepts of varying complexities in different\nlayers, introducing the idea of ``Concept Depth'' to suggest that more complex\nconcepts are typically acquired in deeper layers. Specifically, we categorize\nconcepts based on their level of abstraction, defining them in the order of\nincreasing complexity within factual, emotional, and inferential tasks. We\nconduct extensive probing experiments using layer-wise representations across\nvarious LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the\nthree domains of tasks. Our findings reveal that models could efficiently\nconduct probing for simpler tasks in shallow layers, and more complex tasks\ntypically necessitate deeper layers for accurate understanding. Additionally,\nwe examine how external factors, such as adding noise to the input and\nquantizing the model weights, might affect layer-wise representations. Our\nfindings suggest that these factors can impede the development of a conceptual\nunderstanding of LLMs until deeper layers are explored. We hope that our\nproposed concept and experimental insights will enhance the understanding of\nthe mechanisms underlying LLMs. Our codes are available at\n\\url{https://github.com/Luckfort/CD}.\n","authors":["Mingyu Jin","Qinkai Yu","Jingyuan Huang","Qingcheng Zeng","Zhenting Wang","Wenyue Hua","Haiyan Zhao","Kai Mei","Yanda Meng","Kaize Ding","Fan Yang","Mengnan Du","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.07066v4.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2409.09906v2","updated":"2024-09-17T03:30:44Z","published":"2024-09-16T00:26:42Z","title":"Variance-reduced first-order methods for deterministically constrained\n  stochastic nonconvex optimization with strong convergence guarantees","summary":"  In this paper, we study a class of deterministically constrained stochastic\noptimization problems. Existing methods typically aim to find an\n$\\epsilon$-stochastic stationary point, where the expected violations of both\nconstraints and first-order stationarity are within a prescribed accuracy\n$\\epsilon$. However, in many practical applications, it is crucial that the\nconstraints be nearly satisfied with certainty, making such an\n$\\epsilon$-stochastic stationary point potentially undesirable due to the risk\nof significant constraint violations. To address this issue, we propose\nsingle-loop variance-reduced stochastic first-order methods, where the\nstochastic gradient of the stochastic component is computed using either a\ntruncated recursive momentum scheme or a truncated Polyak momentum scheme for\nvariance reduction, while the gradient of the deterministic component is\ncomputed exactly. Under the error bound condition with a parameter $\\theta \\geq\n1$ and other suitable assumptions, we establish that the proposed methods\nachieve a sample complexity and first-order operation complexity of $\\widetilde\nO(\\epsilon^{-\\max\\{4, 2\\theta\\}})$ for finding a stronger $\\epsilon$-stochastic\nstationary point, where the constraint violation is within $\\epsilon$ with\ncertainty, and the expected violation of first-order stationarity is within\n$\\epsilon$. To the best of our knowledge, this is the first work to develop\nmethods with provable complexity guarantees for finding an approximate\nstochastic stationary point of such problems that nearly satisfies all\nconstraints with certainty.\n","authors":["Zhaosong Lu","Sanyou Mei","Yifeng Xiao"],"pdf_url":"https://arxiv.org/pdf/2409.09906v2.pdf","comment":"Fixed several typos"},{"id":"http://arxiv.org/abs/2409.09831v2","updated":"2024-09-17T11:18:37Z","published":"2024-09-15T19:11:01Z","title":"Generating Synthetic Free-text Medical Records with Low\n  Re-identification Risk using Masked Language Modeling","summary":"  In this paper, we present a system that generates synthetic free-text medical\nrecords, such as discharge summaries, admission notes and doctor\ncorrespondences, using Masked Language Modeling (MLM). Our system is designed\nto preserve the critical information of the records while introducing\nsignificant diversity and minimizing re-identification risk. The system\nincorporates a de-identification component that uses Philter to mask Protected\nHealth Information (PHI), followed by a Medical Entity Recognition (NER) model\nto retain key medical information. We explore various masking ratios and\nmask-filling techniques to balance the trade-off between diversity and fidelity\nin the synthetic outputs without affecting overall readability. Our results\ndemonstrate that the system can produce high-quality synthetic data with\nsignificant diversity while achieving a HIPAA-compliant PHI recall rate of 0.96\nand a low re-identification risk of 0.035. Furthermore, downstream evaluations\nusing a NER task reveal that the synthetic data can be effectively used to\ntrain models with performance comparable to those trained on real data. The\nflexibility of the system allows it to be adapted for specific use cases,\nmaking it a valuable tool for privacy-preserving data generation in medical\nresearch and healthcare applications.\n","authors":["Samuel Belkadi","Libo Ren","Nicolo Micheletti","Lifeng Han","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2409.09831v2.pdf","comment":"Added references and rephrased some sentences"},{"id":"http://arxiv.org/abs/2409.09785v2","updated":"2024-09-17T09:32:04Z","published":"2024-09-15T16:32:49Z","title":"Large Language Model Based Generative Error Correction: A Challenge and\n  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition","summary":"  Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations.\n","authors":["Chao-Han Huck Yang","Taejin Park","Yuan Gong","Yuanchao Li","Zhehuai Chen","Yen-Ting Lin","Chen Chen","Yuchen Hu","Kunal Dhawan","Piotr ≈ªelasko","Chao Zhang","Yun-Nung Chen","Yu Tsao","Jagadeesh Balam","Boris Ginsburg","Sabato Marco Siniscalchi","Eng Siong Chng","Peter Bell","Catherine Lai","Shinji Watanabe","Andreas Stolcke"],"pdf_url":"https://arxiv.org/pdf/2409.09785v2.pdf","comment":"IEEE SLT 2024. The initial draft version has been done in December\n  2023. Post-ASR Text Processing and Understanding Community:\n  https://huggingface.co/GenSEC-LLM"},{"id":"http://arxiv.org/abs/2409.07841v3","updated":"2024-09-17T01:41:32Z","published":"2024-09-12T08:41:07Z","title":"TSELM: Target Speaker Extraction using Discrete Tokens and Language\n  Models","summary":"  We propose TSELM, a novel target speaker extraction network that leverages\ndiscrete tokens and language models. TSELM utilizes multiple discretized layers\nfrom WavLM as input tokens and incorporates cross-attention mechanisms to\nintegrate target speaker information. Language models are employed to capture\nthe sequence dependencies, while a scalable HiFi-GAN is used to reconstruct the\naudio from the tokens. By applying a cross-entropy loss, TSELM models the\nprobability distribution of output tokens, thus converting the complex\nregression problem of audio generation into a classification task. Experimental\nresults show that TSELM achieves excellent results in speech quality and\ncomparable results in speech intelligibility.\n","authors":["Beilong Tang","Bang Zeng","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2409.07841v3.pdf","comment":"Submitted to ICASSP 2025"}]},"2024-09-15T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2409.09891v1","updated":"2024-09-15T23:00:54Z","published":"2024-09-15T23:00:54Z","title":"Acquiring Pronunciation Knowledge from Transcribed Speech Audio via\n  Multi-task Learning","summary":"  Recent work has shown the feasibility and benefit of bootstrapping an\nintegrated sequence-to-sequence (Seq2Seq) linguistic frontend from a\ntraditional pipeline-based frontend for text-to-speech (TTS). To overcome the\nfixed lexical coverage of bootstrapping training data, previous work has\nproposed to leverage easily accessible transcribed speech audio as an\nadditional training source for acquiring novel pronunciation knowledge for\nuncovered words, which relies on an auxiliary ASR model as part of a cumbersome\nimplementation flow. In this work, we propose an alternative method to leverage\ntranscribed speech audio as an additional training source, based on multi-task\nlearning (MTL). Experiments show that, compared to a baseline Seq2Seq frontend,\nthe proposed MTL-based method reduces PER from 2.5% to 1.6% for those word\ntypes covered exclusively in transcribed speech audio, achieving a similar\nperformance to the previous method but with a much simpler implementation flow.\n","authors":["Siqi Sun","Korin Richmond"],"pdf_url":"https://arxiv.org/pdf/2409.09891v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2201.12191v6","updated":"2024-09-15T21:37:58Z","published":"2022-01-28T15:45:13Z","title":"Kernelized Concept Erasure","summary":"  The representation space of neural models for textual data emerges in an\nunsupervised manner during training. Understanding how those representations\nencode human-interpretable concepts is a fundamental problem. One prominent\napproach for the identification of concepts in neural representations is\nsearching for a linear subspace whose erasure prevents the prediction of the\nconcept from the representations. However, while many linear erasure algorithms\nare tractable and interpretable, neural networks do not necessarily represent\nconcepts in a linear manner. To identify non-linearly encoded concepts, we\npropose a kernelization of a linear minimax game for concept erasure. We\ndemonstrate that it is possible to prevent specific non-linear adversaries from\npredicting the concept. However, the protection does not transfer to different\nnonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded\nconcept remains an open problem.\n","authors":["Shauli Ravfogel","Francisco Vargas","Yoav Goldberg","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2201.12191v6.pdf","comment":"Accepted as a long paper in EMNLP22"},{"id":"http://arxiv.org/abs/2409.09866v1","updated":"2024-09-15T21:19:24Z","published":"2024-09-15T21:19:24Z","title":"Constructing a Singing Style Caption Dataset","summary":"  Singing voice synthesis and conversion have emerged as significant subdomains\nof voice generation, leading to much demands on prompt-conditioned generation.\nUnlike common voice data, generating a singing voice requires an understanding\nof various associated vocal and musical characteristics, such as the vocal tone\nof the singer or emotional expressions. However, existing open-source\naudio-text datasets for voice generation tend to capture only a very limited\nrange of attributes, often missing musical characteristics of the audio. To\nfill this gap, we introduce S2Cap, an audio-text pair dataset with a diverse\nset of attributes. S2Cap consists of pairs of textual prompts and music audio\nsamples with a wide range of vocal and musical attributes, including pitch,\nvolume, tempo, mood, singer's gender and age, and musical genre and emotional\nexpression. Utilizing S2Cap, we suggest an effective novel baseline algorithm\nfor singing style captioning. Singing style captioning is a relative task to\nvoice generation that generates text descriptions of vocal characteristics,\nwhich we first suggested. First, to mitigate the misalignment between the audio\nencoder and the text decoder, we present a novel mechanism called CRESCENDO,\nwhich utilizes positive-pair similarity learning to synchronize the embedding\nspaces of a pretrained audio encoder to get similar embeddings with a text\nencoder. We additionally supervise the model using the singer's voice, which is\ndemixed by the accompaniment. This supervision allows the model to more\naccurately capture vocal characteristics, leading to improved singing style\ncaptions that better reflect the style of the singer. The dataset and the codes\nare available at \\bulurl{https://github.com/HJ-Ok/S2cap}.\n","authors":["Hyunjong Ok","Jaeho Lee"],"pdf_url":"https://arxiv.org/pdf/2409.09866v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2409.09844v1","updated":"2024-09-15T19:50:00Z","published":"2024-09-15T19:50:00Z","title":"A Benchmark Dataset with Larger Context for Non-Factoid Question\n  Answering over Islamic Text","summary":"  Accessing and comprehending religious texts, particularly the Quran (the\nsacred scripture of Islam) and Ahadith (the corpus of the sayings or traditions\nof the Prophet Muhammad), in today's digital era necessitates efficient and\naccurate Question-Answering (QA) systems. Yet, the scarcity of QA systems\ntailored specifically to the detailed nature of inquiries about the Quranic\nTafsir (explanation, interpretation, context of Quran for clarity) and Ahadith\nposes significant challenges. To address this gap, we introduce a comprehensive\ndataset meticulously crafted for QA purposes within the domain of Quranic\nTafsir and Ahadith. This dataset comprises a robust collection of over 73,000\nquestion-answer pairs, standing as the largest reported dataset in this\nspecialized domain. Importantly, both questions and answers within the dataset\nare meticulously enriched with contextual information, serving as invaluable\nresources for training and evaluating tailored QA systems. However, while this\npaper highlights the dataset's contributions and establishes a benchmark for\nevaluating QA performance in the Quran and Ahadith domains, our subsequent\nhuman evaluation uncovered critical insights regarding the limitations of\nexisting automatic evaluation techniques. The discrepancy between automatic\nevaluation metrics, such as ROUGE scores, and human assessments became\napparent. The human evaluation indicated significant disparities: the model's\nverdict consistency with expert scholars ranged between 11% to 20%, while its\ncontextual understanding spanned a broader spectrum of 50% to 90%. These\nfindings underscore the necessity for evaluation techniques that capture the\nnuances and complexities inherent in understanding religious texts, surpassing\nthe limitations of traditional automatic metrics.\n","authors":["Faiza Qamar","Seemab Latif","Rabia Latif"],"pdf_url":"https://arxiv.org/pdf/2409.09844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09825v1","updated":"2024-09-15T18:56:20Z","published":"2024-09-15T18:56:20Z","title":"GP-GPT: Large Language Model for Gene-Phenotype Mapping","summary":"  Pre-trained large language models(LLMs) have attracted increasing attention\nin biomedical domains due to their success in natural language processing.\nHowever, the complex traits and heterogeneity of multi-sources genomics data\npose significant challenges when adapting these models to the bioinformatics\nand biomedical field. To address these challenges, we present GP-GPT, the first\nspecialized large language model for genetic-phenotype knowledge representation\nand genomics relation analysis. Our model is fine-tuned in two stages on a\ncomprehensive corpus composed of over 3,000,000 terms in genomics, proteomics,\nand medical genetics, derived from multiple large-scale validated datasets and\nscientific publications. GP-GPT demonstrates proficiency in accurately\nretrieving medical genetics information and performing common genomics analysis\ntasks, such as genomics information retrieval and relationship determination.\nComparative experiments across domain-specific tasks reveal that GP-GPT\noutperforms state-of-the-art LLMs, including Llama2, Llama3 and GPT-4. These\nresults highlight GP-GPT's potential to enhance genetic disease relation\nresearch and facilitate accurate and efficient analysis in the fields of\ngenomics and medical genetics. Our investigation demonstrated the subtle\nchanges of bio-factor entities' representations in the GP-GPT, which suggested\nthe opportunities for the application of LLMs to advancing gene-phenotype\nresearch.\n","authors":["Yanjun Lyu","Zihao Wu","Lu Zhang","Jing Zhang","Yiwei Li","Wei Ruan","Zhengliang Liu","Xiaowei Yu","Chao Cao","Tong Chen","Minheng Chen","Yan Zhuang","Xiang Li","Rongjie Liu","Chao Huang","Wentao Li","Tianming Liu","Dajiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2409.09825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09822v1","updated":"2024-09-15T18:43:11Z","published":"2024-09-15T18:43:11Z","title":"Causal Inference with Large Language Model: A Survey","summary":"  Causal inference has been a pivotal challenge across diverse domains such as\nmedicine and economics, demanding a complicated integration of human knowledge,\nmathematical reasoning, and data mining capabilities. Recent advancements in\nnatural language processing (NLP), particularly with the advent of large\nlanguage models (LLMs), have introduced promising opportunities for traditional\ncausal inference tasks. This paper reviews recent progress in applying LLMs to\ncausal inference, encompassing various tasks spanning different levels of\ncausation. We summarize the main causal problems and approaches, and present a\ncomparison of their evaluation results in different causal scenarios.\nFurthermore, we discuss key findings and outline directions for future\nresearch, underscoring the potential implications of integrating LLMs in\nadvancing causal inference methodologies.\n","authors":["Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2409.09822v1.pdf","comment":"15 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2405.19544v2","updated":"2024-09-15T17:42:20Z","published":"2024-05-29T22:12:52Z","title":"One-Shot Safety Alignment for Large Language Models via Optimal\n  Dualization","summary":"  The growing safety concerns surrounding Large Language Models (LLMs) raise an\nurgent need to align them with diverse human preferences to simultaneously\nenhance their helpfulness and safety. A promising approach is to enforce safety\nconstraints through Reinforcement Learning from Human Feedback (RLHF). For such\nconstrained RLHF, common Lagrangian-based primal-dual policy optimization\nmethods are computationally expensive and often unstable. This paper presents a\ndualization perspective that reduces constrained alignment to an equivalent\nunconstrained alignment problem. We do so by pre-optimizing a smooth and convex\ndual function that has a closed form. This shortcut eliminates the need for\ncumbersome primal-dual policy iterations, thus greatly reducing the\ncomputational burden and improving training stability. Our strategy leads to\ntwo practical algorithms in model-based and preference-based scenarios (MoCAN\nand PeCAN, respectively). A broad range of experiments demonstrate the\neffectiveness of our methods.\n","authors":["Xinmeng Huang","Shuo Li","Edgar Dobriban","Osbert Bastani","Hamed Hassani","Dongsheng Ding"],"pdf_url":"https://arxiv.org/pdf/2405.19544v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09788v1","updated":"2024-09-15T16:45:42Z","published":"2024-09-15T16:45:42Z","title":"Reasoning Paths with Reference Objects Elicit Quantitative Spatial\n  Reasoning in Large Vision-Language Models","summary":"  Despite recent advances demonstrating vision-language models' (VLMs)\nabilities to describe complex relationships in images using natural language,\ntheir capability to quantitatively reason about object sizes and distances\nremains underexplored. In this work, we introduce a manually annotated\nbenchmark, Q-Spatial Bench, with 271 questions across five categories designed\nfor quantitative spatial reasoning and systematically investigate the\nperformance of state-of-the-art VLMs on this task. Our analysis reveals that\nreasoning about distances between objects is particularly challenging for SoTA\nVLMs; however, some VLMs significantly outperform others, with an over 40-point\ngap between the two best performing models. We also make the surprising\nobservation that the success rate of the top-performing VLM increases by 19\npoints when a reasoning path using a reference object emerges naturally in the\nresponse. Inspired by this observation, we develop a zero-shot prompting\ntechnique, SpatialPrompt, that encourages VLMs to answer quantitative spatial\nquestions using reference objects as visual cues. By instructing VLMs to use\nreference objects in their reasoning paths via SpatialPrompt, Gemini 1.5 Pro,\nGemini 1.5 Flash, and GPT-4V improve their success rates by over 40, 20, and 30\npoints, respectively. We emphasize that these significant improvements are\nobtained without needing more data, model architectural modifications, or\nfine-tuning.\n","authors":["Yuan-Hong Liao","Rafid Mahmood","Sanja Fidler","David Acuna"],"pdf_url":"https://arxiv.org/pdf/2409.09788v1.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2205.07208v3","updated":"2024-09-15T16:11:57Z","published":"2022-05-15T07:48:13Z","title":"Fine-tuning Pre-trained Language Models for Few-shot Intent Detection:\n  Supervised Pre-training and Isotropization","summary":"  It is challenging to train a good intent classifier for a task-oriented\ndialogue system with only a few annotations. Recent studies have shown that\nfine-tuning pre-trained language models with a small amount of labeled\nutterances from public benchmarks in a supervised manner is extremely helpful.\nHowever, we find that supervised pre-training yields an anisotropic feature\nspace, which may suppress the expressive power of the semantic representations.\nInspired by recent research in isotropization, we propose to improve supervised\npre-training by regularizing the feature space towards isotropy. We propose two\nregularizers based on contrastive learning and correlation matrix respectively,\nand demonstrate their effectiveness through extensive experiments. Our main\nfinding is that it is promising to regularize supervised pre-training with\nisotropization to further improve the performance of few-shot intent detection.\nThe source code can be found at https://github.com/fanolabs/isoIntentBert-main.\n","authors":["Haode Zhang","Haowen Liang","Yuwei Zhang","Liming Zhan","Xiaolei Lu","Albert Y. S. Lam","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2205.07208v3.pdf","comment":"NAACL 2022, oral"},{"id":"http://arxiv.org/abs/2306.05278v2","updated":"2024-09-15T16:07:55Z","published":"2023-06-08T15:26:52Z","title":"Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs.\n  Continual Pre-training","summary":"  We consider the task of few-shot intent detection, which involves training a\ndeep learning model to classify utterances based on their underlying intents\nusing only a small amount of labeled data. The current approach to address this\nproblem is through continual pre-training, i.e., fine-tuning pre-trained\nlanguage models (PLMs) on external resources (e.g., conversational corpora,\npublic intent detection datasets, or natural language understanding datasets)\nbefore using them as utterance encoders for training an intent classifier. In\nthis paper, we show that continual pre-training may not be essential, since the\noverfitting problem of PLMs on this task may not be as serious as expected.\nSpecifically, we find that directly fine-tuning PLMs on only a handful of\nlabeled examples already yields decent results compared to methods that employ\ncontinual pre-training, and the performance gap diminishes rapidly as the\nnumber of labeled data increases. To maximize the utilization of the limited\navailable data, we propose a context augmentation method and leverage\nsequential self-distillation to boost performance. Comprehensive experiments on\nreal-world benchmarks show that given only two or more labeled samples per\nclass, direct fine-tuning outperforms many strong baselines that utilize\nexternal data sources for continual pre-training. The code can be found at\nhttps://github.com/hdzhang-code/DFTPlus.\n","authors":["Haode Zhang","Haowen Liang","Liming Zhan","Albert Y. S. Lam","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2306.05278v2.pdf","comment":"ACL 2023, Findings"},{"id":"http://arxiv.org/abs/2402.10453v2","updated":"2024-09-15T15:58:45Z","published":"2024-02-16T05:03:01Z","title":"Steering Conversational Large Language Models for Long Emotional Support\n  Conversations","summary":"  In this study, we address the challenge of enabling large language models\n(LLMs) to consistently adhere to emotional support strategies in extended\nconversations. We focus on the steerability of the Llama-2 and Llama-3 suite of\nmodels, examining their ability to maintain these strategies throughout\ninteractions. To assess this, we introduce the Strategy Relevant Attention\n(SRA) metric, which quantifies the model's adherence to the prompted strategy\nthrough attention maps. To facilitate our study, we create a\nstrategy-conditioned synthetic conversational dataset derived from the ESConv\ndataset. We also propose various baselines informed by our proposed SRA metric\nto address the challenge and propose a fine-tuned model that significantly\nenhances the steerability of the base model in following the strategy\nthroughout the conversation. The code and data are publicly available on our\nGitHub.\n","authors":["Navid Madani","Sougata Saha","Rohini Srihari"],"pdf_url":"https://arxiv.org/pdf/2402.10453v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09943v2","updated":"2024-09-15T15:45:50Z","published":"2024-07-13T16:47:20Z","title":"Minimizing PLM-Based Few-Shot Intent Detectors","summary":"  Recent research has demonstrated the feasibility of training efficient intent\ndetectors based on pre-trained language model~(PLM) with limited labeled data.\nHowever, deploying these detectors in resource-constrained environments such as\nmobile devices poses challenges due to their large sizes. In this work, we aim\nto address this issue by exploring techniques to minimize the size of PLM-based\nintent detectors trained with few-shot data. Specifically, we utilize large\nlanguage models (LLMs) for data augmentation, employ a cutting-edge model\ncompression method for knowledge distillation, and devise a vocabulary pruning\nmechanism called V-Prune. Through these approaches, we successfully achieve a\ncompression ratio of 21 in model memory usage, including both Transformer and\nthe vocabulary, while maintaining almost identical performance levels on four\nreal-world benchmarks.\n","authors":["Haode Zhang","Albert Y. S. Lam","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2407.09943v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13084v2","updated":"2024-09-15T15:03:59Z","published":"2024-05-21T07:35:21Z","title":"The 2nd FutureDial Challenge: Dialog Systems with Retrieval Augmented\n  Generation (FutureDial-RAG)","summary":"  Recently, increasing research interests have focused on retrieval augmented\ngeneration (RAG) to mitigate hallucination for large language models (LLMs).\nFollowing this trend, we launch the FutureDial-RAG challenge at SLT 2024, which\naims at promoting the study of RAG for dialog systems. The challenge builds\nupon the MobileCS2 dataset, a real-life customer service datasets with nearly\n3000 high-quality dialogs containing annotations for knowledge base query and\ncorresponding results. Over the dataset, we define two tasks, track 1 for\nknowledge retrieval and track 2 for response generation, which are core\nresearch questions in dialog systems with RAG. We build baseline systems for\nthe two tracks and design metrics to measure whether the systems can perform\naccurate retrieval and generate informative and coherent response. The baseline\nresults show that it is very challenging to perform well on the two tasks,\nwhich encourages the participating teams and the community to study how to make\nbetter use of RAG for real-life dialog systems.\n","authors":["Yucheng Cai","Si Chen","Yuxuan Wu","Yi Huang","Junlan Feng","Zhijian Ou"],"pdf_url":"https://arxiv.org/pdf/2405.13084v2.pdf","comment":"Accepted by SLT 2024"},{"id":"http://arxiv.org/abs/2409.09760v1","updated":"2024-09-15T15:01:00Z","published":"2024-09-15T15:01:00Z","title":"ELMI: Interactive and Intelligent Sign Language Translation of Lyrics\n  for Song Signing","summary":"  d/Deaf and hearing song-signers become prevalent on video-sharing platforms,\nbut translating songs into sign language remains cumbersome and inaccessible.\nOur formative study revealed the challenges song-signers face, including\nsemantic, syntactic, expressive, and rhythmic considerations in translations.\nWe present ELMI, an accessible song-signing tool that assists in translating\nlyrics into sign language. ELMI enables users to edit glosses line-by-line,\nwith real-time synced lyric highlighting and music video snippets. Users can\nalso chat with a large language model-driven AI to discuss meaning, glossing,\nemoting, and timing. Through an exploratory study with 13 song-signers, we\nexamined how ELMI facilitates their workflows and how song-signers leverage and\nreceive an LLM-driven chat for translation. Participants successfully adopted\nELMI to song-signing, with active discussions on the fly. They also reported\nimproved confidence and independence in their translations, finding ELMI\nencouraging, constructive, and informative. We discuss design implications for\nleveraging LLMs in culturally sensitive song-signing translations.\n","authors":["Suhyeon Yoo","Khai N. Truong","Young-Ho Kim"],"pdf_url":"https://arxiv.org/pdf/2409.09760v1.pdf","comment":"18 pages excluding reference and appendix"},{"id":"http://arxiv.org/abs/2409.08234v2","updated":"2024-09-15T14:41:42Z","published":"2024-09-12T17:33:06Z","title":"LLM Honeypot: Leveraging Large Language Models as Advanced Interactive\n  Honeypot Systems","summary":"  The rapid evolution of cyber threats necessitates innovative solutions for\ndetecting and analyzing malicious activity. Honeypots, which are decoy systems\ndesigned to lure and interact with attackers, have emerged as a critical\ncomponent in cybersecurity. In this paper, we present a novel approach to\ncreating realistic and interactive honeypot systems using Large Language Models\n(LLMs). By fine-tuning a pre-trained open-source language model on a diverse\ndataset of attacker-generated commands and responses, we developed a honeypot\ncapable of sophisticated engagement with attackers. Our methodology involved\nseveral key steps: data collection and processing, prompt engineering, model\nselection, and supervised fine-tuning to optimize the model's performance.\nEvaluation through similarity metrics and live deployment demonstrated that our\napproach effectively generates accurate and informative responses. The results\nhighlight the potential of LLMs to revolutionize honeypot technology, providing\ncybersecurity professionals with a powerful tool to detect and analyze\nmalicious activity, thereby enhancing overall security infrastructure.\n","authors":["Hakan T. Otal","M. Abdullah Canbaz"],"pdf_url":"https://arxiv.org/pdf/2409.08234v2.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.09741v1","updated":"2024-09-15T14:11:24Z","published":"2024-09-15T14:11:24Z","title":"Benchmarking LLMs in Political Content Text-Annotation: Proof-of-Concept\n  with Toxicity and Incivility Data","summary":"  This article benchmarked the ability of OpenAI's GPTs and a number of\nopen-source LLMs to perform annotation tasks on political content. We used a\nnovel protest event dataset comprising more than three million digital\ninteractions and created a gold standard that includes ground-truth labels\nannotated by human coders about toxicity and incivility on social media. We\nincluded in our benchmark Google's Perspective algorithm, which, along with\nGPTs, was employed throughout their respective APIs while the open-source LLMs\nwere deployed locally. The findings show that Perspective API using a laxer\nthreshold, GPT-4o, and Nous Hermes 2 Mixtral outperform other LLM's zero-shot\nclassification annotations. In addition, Nous Hermes 2 and Mistral OpenOrca,\nwith a smaller number of parameters, are able to perform the task with high\nperformance, being attractive options that could offer good trade-offs between\nperformance, implementing costs and computing time. Ancillary findings using\nexperiments setting different temperature levels show that although GPTs tend\nto show not only excellent computing time but also overall good levels of\nreliability, only open-source LLMs ensure full reproducibility in the\nannotation.\n","authors":["Basti√°n Gonz√°lez-Bustamante"],"pdf_url":"https://arxiv.org/pdf/2409.09741v1.pdf","comment":"Paper prepared for delivery at the 8th Monash-Warwick-Zurich\n  Text-as-Data Workshop, September 16-17, 2024: 11 pages, 3 tables, 3 figures"},{"id":"http://arxiv.org/abs/2409.09739v1","updated":"2024-09-15T14:10:01Z","published":"2024-09-15T14:10:01Z","title":"PersonaMark: Personalized LLM watermarking for model protection and user\n  attribution","summary":"  The rapid development of LLMs brings both convenience and potential threats.\nAs costumed and private LLMs are widely applied, model copyright protection has\nbecome important. Text watermarking is emerging as a promising solution to\nAI-generated text detection and model protection issues. However, current text\nwatermarks have largely ignored the critical need for injecting different\nwatermarks for different users, which could help attribute the watermark to a\nspecific individual. In this paper, we explore the personalized text\nwatermarking scheme for LLM copyright protection and other scenarios, ensuring\naccountability and traceability in content generation. Specifically, we propose\na novel text watermarking method PersonaMark that utilizes sentence structure\nas the hidden medium for the watermark information and optimizes the\nsentence-level generation algorithm to minimize disruption to the model's\nnatural generation process. By employing a personalized hashing function to\ninject unique watermark signals for different users, personalized watermarked\ntext can be obtained. Since our approach performs on sentence level instead of\ntoken probability, the text quality is highly preserved. The injection process\nof unique watermark signals for different users is time-efficient for a large\nnumber of users with the designed multi-user hashing function. As far as we\nknow, we achieved personalized text watermarking for the first time through\nthis. We conduct an extensive evaluation of four different LLMs in terms of\nperplexity, sentiment polarity, alignment, readability, etc. The results\ndemonstrate that our method maintains performance with minimal perturbation to\nthe model's behavior, allows for unbiased insertion of watermark information,\nand exhibits strong watermark recognition capabilities.\n","authors":["Yuehan Zhang","Peizhuo Lv","Yinpeng Liu","Yongqiang Ma","Wei Lu","Xiaofeng Wang","Xiaozhong Liu","Jiawei Liu"],"pdf_url":"https://arxiv.org/pdf/2409.09739v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.02076v4","updated":"2024-09-15T13:40:01Z","published":"2024-09-03T17:25:54Z","title":"LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs","summary":"  In evaluating the long-context capabilities of large language models (LLMs),\nbenchmarks such as \"Needle-in-a-Haystack\" (NIAH), Ruler, and Needlebench are\ncommonly used. While these benchmarks measure how well models understand\nlong-context input sequences, they do not effectively gauge the quality of\nlong-form text generation--a critical aspect for applications such as design\nproposals and creative writing. To address this gap, we have introduced a new\nlong-form text evaluation benchmark, LongGenBench, which tests models' ability\nto identify specific events within generated long text sequences. In this\nbenchmark, we prompt long-context LMs to create long-form text that must\ninclude particular events or constraints and evaluate their ability to\nincorporate these elements. We evaluated ten long-context LMs across four\ndistinct scenarios, three types of prompt instructions, and two different\ngeneration-length settings (16K and 32K). Although these models perform well on\nNIAH benchmarks, none demonstrated satisfactory performance on the\nLongGenBench, raising concerns about their ability to generate coherent\nlong-form text that follows instructions. Additionally, as the length of the\ngenerated text increases, all models exhibit a significant drop in performance.\n","authors":["Yuhao Wu","Ming Shan Hee","Zhiqing Hu","Roy Ka-Wei Lee"],"pdf_url":"https://arxiv.org/pdf/2409.02076v4.pdf","comment":"work in progress; Github: https://github.com/mozhu621/LongGenBench/"},{"id":"http://arxiv.org/abs/2409.05840v2","updated":"2024-09-15T13:32:08Z","published":"2024-09-09T17:44:00Z","title":"MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct","summary":"  The development of Multimodal Large Language Models (MLLMs) has seen\nsignificant advancements with increasing demands in various fields (e.g.,\nmultimodal agents, embodied intelligence). While model-driven approaches\nattempt to enhance MLLMs capabilities through diverse architectures, the gains\nhave become increasingly marginal. Conversely, data-driven methods, which scale\nup image-text instruction data, are more effective but face limited data\ndiversity and complexity challenges. The absence of high-quality data\nconstitutes a significant development barrier for MLLMs. To address the data\nquality bottleneck, we propose MMEvol, a novel multimodal instruction data\nevolution framework. This framework iteratively improve data quality through a\nrefined combination of fine-grained perception, cognitive reasoning, and\ninteraction evolution, generating a more complex and diverse image-text\ninstruction dataset that empowers MLLMs with enhanced capabilities. Beginning\nwith an initial set of instructions, SEED-163K, we utilize MMEvol to\nsystematically broaden the diversity of instruction types, extend visual\nreasoning steps to improve cognitive reasoning abilities, and thoroughly\nexplore fine-grained information within images to enhance visual understanding\nand robustness. To comprehensively evaluate the effectiveness of our approach,\nwe conduct extensive qualitative analysis and quantitative experiments across\n13 vision-language tasks. Compared to baseline models trained with the initial\nseed data, the results demonstrate that our method achieves an average accuracy\nimprovement of 3.1 percentage points. Furthermore, our approach reaches\nstate-of-the-art (SOTA) performance in nine tasks using significantly less data\ncompared to state-of-the-art models.\n","authors":["Run Luo","Haonan Zhang","Longze Chen","Ting-En Lin","Xiong Liu","Yuchuan Wu","Min Yang","Minzheng Wang","Pengpeng Zeng","Lianli Gao","Heng Tao Shen","Yunshui Li","Xiaobo Xia","Fei Huang","Jingkuan Song","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2409.05840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09717v1","updated":"2024-09-15T12:49:05Z","published":"2024-09-15T12:49:05Z","title":"Automatic Control With Human-Like Reasoning: Exploring Language Model\n  Embodied Air Traffic Agents","summary":"  Recent developments in language models have created new opportunities in air\ntraffic control studies. The current focus is primarily on text and\nlanguage-based use cases. However, these language models may offer a higher\npotential impact in the air traffic control domain, thanks to their ability to\ninteract with air traffic environments in an embodied agent form. They also\nprovide a language-like reasoning capability to explain their decisions, which\nhas been a significant roadblock for the implementation of automatic air\ntraffic control.\n  This paper investigates the application of a language model-based agent with\nfunction-calling and learning capabilities to resolve air traffic conflicts\nwithout human intervention. The main components of this research are\nfoundational large language models, tools that allow the agent to interact with\nthe simulator, and a new concept, the experience library. An innovative part of\nthis research, the experience library, is a vector database that stores\nsynthesized knowledge that agents have learned from interactions with the\nsimulations and language models.\n  To evaluate the performance of our language model-based agent, both\nopen-source and closed-source models were tested. The results of our study\nreveal significant differences in performance across various configurations of\nthe language model-based agents. The best-performing configuration was able to\nsolve almost all 120 but one imminent conflict scenarios, including up to four\naircraft at the same time. Most importantly, the agents are able to provide\nhuman-level text explanations on traffic situations and conflict resolution\nstrategies.\n","authors":["Justas Andriu≈°keviƒçius","Junzi Sun"],"pdf_url":"https://arxiv.org/pdf/2409.09717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09704v1","updated":"2024-09-15T11:53:24Z","published":"2024-09-15T11:53:24Z","title":"AlpaPICO: Extraction of PICO Frames from Clinical Trial Documents Using\n  LLMs","summary":"  In recent years, there has been a surge in the publication of clinical trial\nreports, making it challenging to conduct systematic reviews. Automatically\nextracting Population, Intervention, Comparator, and Outcome (PICO) from\nclinical trial studies can alleviate the traditionally time-consuming process\nof manually scrutinizing systematic reviews. Existing approaches of PICO frame\nextraction involves supervised approach that relies on the existence of\nmanually annotated data points in the form of BIO label tagging. Recent\napproaches, such as In-Context Learning (ICL), which has been shown to be\neffective for a number of downstream NLP tasks, require the use of labeled\nexamples. In this work, we adopt ICL strategy by employing the pretrained\nknowledge of Large Language Models (LLMs), gathered during the pretraining\nphase of an LLM, to automatically extract the PICO-related terminologies from\nclinical trial documents in unsupervised set up to bypass the availability of\nlarge number of annotated data instances. Additionally, to showcase the highest\neffectiveness of LLM in oracle scenario where large number of annotated samples\nare available, we adopt the instruction tuning strategy by employing Low Rank\nAdaptation (LORA) to conduct the training of gigantic model in low resource\nenvironment for the PICO frame extraction task. Our empirical results show that\nour proposed ICL-based framework produces comparable results on all the version\nof EBM-NLP datasets and the proposed instruction tuned version of our framework\nproduces state-of-the-art results on all the different EBM-NLP datasets. Our\nproject is available at \\url{https://github.com/shrimonmuke0202/AlpaPICO.git}.\n","authors":["Madhusudan Ghosh","Shrimon Mukherjee","Asmit Ganguly","Partha Basuchowdhuri","Sudip Kumar Naskar","Debasis Ganguly"],"pdf_url":"https://arxiv.org/pdf/2409.09704v1.pdf","comment":"Accepted at Methods"},{"id":"http://arxiv.org/abs/2402.08403v5","updated":"2024-09-15T09:10:55Z","published":"2024-02-13T12:04:43Z","title":"LLMs and the Human Condition","summary":"  Theory based AI research has had a hard time recently and the aim here is to\npropose a model of what LLMs are actually doing when they impress us with their\nlanguage skills. The model integrates three established theories of human\ndecision-making from philosophy, sociology, and computer science. The paper\nstarts with the collective understanding of reasoning from the early days of AI\nresearch - primarily because that model is how we humans think we think, and is\nthe most accessible. It then describes what is commonly thought of as \"reactive\nsystems\" which is the position taken by many philosophers and indeed many\ncontemporary AI researchers. The third component to the proposed model is from\nsociology and, although not flattering to our modern ego, provides an\nexplanation to a puzzle that for many years has occupied those of us working on\nconversational user interfaces.\n","authors":["Peter Wallis"],"pdf_url":"https://arxiv.org/pdf/2402.08403v5.pdf","comment":"Significant edits mainly to give the paper a single purpose - removed\n  discussion of the mechanism - but just generally tighter"},{"id":"http://arxiv.org/abs/2407.13301v2","updated":"2024-09-15T08:43:17Z","published":"2024-07-18T09:06:27Z","title":"CoD, Towards an Interpretable Medical Agent using Chain of Diagnosis","summary":"  The field of medical diagnosis has undergone a significant transformation\nwith the advent of large language models (LLMs), yet the challenges of\ninterpretability within these models remain largely unaddressed. This study\nintroduces Chain-of-Diagnosis (CoD) to enhance the interpretability of\nLLM-based medical diagnostics. CoD transforms the diagnostic process into a\ndiagnostic chain that mirrors a physician's thought process, providing a\ntransparent reasoning pathway. Additionally, CoD outputs the disease confidence\ndistribution to ensure transparency in decision-making. This interpretability\nmakes model diagnostics controllable and aids in identifying critical symptoms\nfor inquiry through the entropy reduction of confidences. With CoD, we\ndeveloped DiagnosisGPT, capable of diagnosing 9604 diseases. Experimental\nresults demonstrate that DiagnosisGPT outperforms other LLMs on diagnostic\nbenchmarks. Moreover, DiagnosisGPT provides interpretability while ensuring\ncontrollability in diagnostic rigor.\n","authors":["Junying Chen","Chi Gui","Anningzhe Gao","Ke Ji","Xidong Wang","Xiang Wan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09774v2","updated":"2024-09-15T08:41:01Z","published":"2023-11-16T10:56:24Z","title":"HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs","summary":"  Adapting a language model into a specific domain, a.k.a `domain adaption', is\na common practice when specialized knowledge, e.g. medicine, is not\nencapsulated in a general language model like Llama2. The challenge lies in the\nheterogeneity of data across the two training stages, as it varies in\nlanguages, genres, or formats. To tackle this and simplify the learning\nprotocol, we propose to transform heterogeneous data, from the both\npre-training and supervised stages, into a unified, simple input-output pair\nformat. We validate the new protocol in the domains where proprietary LLMs like\nChatGPT perform relatively poorly, such as Traditional Chinese Medicine. The\ndeveloped model, HuatuoGPT-II, has shown state-of-the-art performance in\nChinese medicine domain on a number of benchmarks, e.g. medical licensing\nexams. It even outperforms proprietary models like ChatGPT and GPT-4 in some\naspects, especially in Traditional Chinese Medicine. Expert manual evaluations\nfurther validate HuatuoGPT-II's advantages over existing LLMs. Notably,\nHuatuoGPT-II was benchmarked in a fresh Chinese National Medical Licensing\nExamination where it achieved the best performance, showcasing not only its\neffectiveness but also its generalization capabilities.\n","authors":["Junying Chen","Xidong Wang","Ke Ji","Anningzhe Gao","Feng Jiang","Shunian Chen","Hongbo Zhang","Dingjie Song","Wenya Xie","Chuyi Kong","Jianquan Li","Xiang Wan","Haizhou Li","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2311.09774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09659v1","updated":"2024-09-15T08:14:18Z","published":"2024-09-15T08:14:18Z","title":"Leveraging Open-Source Large Language Models for Native Language\n  Identification","summary":"  Native Language Identification (NLI) - the task of identifying the native\nlanguage (L1) of a person based on their writing in the second language (L2) -\nhas applications in forensics, marketing, and second language acquisition.\nHistorically, conventional machine learning approaches that heavily rely on\nextensive feature engineering have outperformed transformer-based language\nmodels on this task. Recently, closed-source generative large language models\n(LLMs), e.g., GPT-4, have demonstrated remarkable performance on NLI in a\nzero-shot setting, including promising results in open-set classification.\nHowever, closed-source LLMs have many disadvantages, such as high costs and\nundisclosed nature of training data. This study explores the potential of using\nopen-source LLMs for NLI. Our results indicate that open-source LLMs do not\nreach the accuracy levels of closed-source LLMs when used out-of-the-box.\nHowever, when fine-tuned on labeled training data, open-source LLMs can achieve\nperformance comparable to that of commercial LLMs.\n","authors":["Yee Man Ng","Ilia Markov"],"pdf_url":"https://arxiv.org/pdf/2409.09659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09652v1","updated":"2024-09-15T07:50:33Z","published":"2024-09-15T07:50:33Z","title":"Unveiling Gender Bias in Large Language Models: Using Teacher's\n  Evaluation in Higher Education As an Example","summary":"  This paper investigates gender bias in Large Language Model (LLM)-generated\nteacher evaluations in higher education setting, focusing on evaluations\nproduced by GPT-4 across six academic subjects. By applying a comprehensive\nanalytical framework that includes Odds Ratio (OR) analysis, Word Embedding\nAssociation Test (WEAT), sentiment analysis, and contextual analysis, this\npaper identified patterns of gender-associated language reflecting societal\nstereotypes. Specifically, words related to approachability and support were\nused more frequently for female instructors, while words related to\nentertainment were predominantly used for male instructors, aligning with the\nconcepts of communal and agentic behaviors. The study also found moderate to\nstrong associations between male salient adjectives and male names, though\ncareer and family words did not distinctly capture gender biases. These\nfindings align with prior research on societal norms and stereotypes,\nreinforcing the notion that LLM-generated text reflects existing biases.\n","authors":["Yuanning Huang"],"pdf_url":"https://arxiv.org/pdf/2409.09652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.09841v4","updated":"2024-09-15T07:49:32Z","published":"2023-06-16T13:39:35Z","title":"Are Large Language Models Really Good Logical Reasoners? A Comprehensive\n  Evaluation and Beyond","summary":"  Logical reasoning consistently plays a fundamental and significant role in\nthe domains of knowledge engineering and artificial intelligence. Recently,\nLarge Language Models (LLMs) have emerged as a noteworthy innovation in natural\nlanguage processing (NLP). However, the question of whether LLMs can\neffectively address the task of logical reasoning, which requires gradual\ncognitive inference similar to human intelligence, remains unanswered. To this\nend, we aim to bridge this gap and provide comprehensive evaluations in this\npaper. Firstly, to offer systematic evaluations, we select fifteen typical\nlogical reasoning datasets and organize them into deductive, inductive,\nabductive and mixed-form reasoning settings. Considering the comprehensiveness\nof evaluations, we include 3 early-era representative LLMs and 4 trending LLMs.\nSecondly, different from previous evaluations relying only on simple metrics\n(e.g., \\emph{accuracy}), we propose fine-level evaluations in objective and\nsubjective manners, covering both answers and explanations, including\n\\emph{answer correctness}, \\emph{explain correctness}, \\emph{explain\ncompleteness} and \\emph{explain redundancy}. Additionally, to uncover the\nlogical flaws of LLMs, problematic cases will be attributed to five error types\nfrom two dimensions, i.e., \\emph{evidence selection process} and\n\\emph{reasoning process}. Thirdly, to avoid the influences of knowledge bias\nand concentrate purely on benchmarking the logical reasoning capability of\nLLMs, we propose a new dataset with neutral content. Based on the in-depth\nevaluations, this paper finally forms a general evaluation scheme of logical\nreasoning capability from six dimensions (i.e., \\emph{Correct},\n\\emph{Rigorous}, \\emph{Self-aware}, \\emph{Active}, \\emph{Oriented} and \\emph{No\nhallucination}). It reflects the pros and cons of LLMs and gives guiding\ndirections for future works.\n","authors":["Fangzhi Xu","Qika Lin","Jiawei Han","Tianzhe Zhao","Jun Liu","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2306.09841v4.pdf","comment":"37 pages, 16 figures"},{"id":"http://arxiv.org/abs/2409.09646v1","updated":"2024-09-15T07:44:23Z","published":"2024-09-15T07:44:23Z","title":"A Simple HMM with Self-Supervised Representations for Phone Segmentation","summary":"  Despite the recent advance in self-supervised representations, unsupervised\nphonetic segmentation remains challenging. Most approaches focus on improving\nphonetic representations with self-supervised learning, with the hope that the\nimprovement can transfer to phonetic segmentation. In this paper, contrary to\nrecent approaches, we show that peak detection on Mel spectrograms is a strong\nbaseline, better than many self-supervised approaches. Based on this finding,\nwe propose a simple hidden Markov model that uses self-supervised\nrepresentations and features at the boundaries for phone segmentation. Our\nresults demonstrate consistent improvements over previous approaches, with a\ngeneralized formulation allowing versatile design adaptations.\n","authors":["Gene-Ping Yang","Hao Tang"],"pdf_url":"https://arxiv.org/pdf/2409.09646v1.pdf","comment":"Accepted to SLT 2024"},{"id":"http://arxiv.org/abs/2406.19280v2","updated":"2024-09-15T07:25:49Z","published":"2024-06-27T15:50:41Z","title":"HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale","summary":"  The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.\n","authors":["Junying Chen","Chi Gui","Ruyi Ouyang","Anningzhe Gao","Shunian Chen","Guiming Hardy Chen","Xidong Wang","Ruifei Zhang","Zhenyang Cai","Ke Ji","Guangjun Yu","Xiang Wan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20578v2","updated":"2024-09-15T07:23:10Z","published":"2024-07-30T06:23:59Z","title":"Comparison of Large Language Models for Generating Contextually Relevant\n  Questions","summary":"  This study explores the effectiveness of Large Language Models (LLMs) for\nAutomatic Question Generation in educational settings. Three LLMs are compared\nin their ability to create questions from university slide text without\nfine-tuning. Questions were obtained in a two-step pipeline: first, answer\nphrases were extracted from slides using Llama 2-Chat 13B; then, the three\nmodels generated questions for each answer. To analyze whether the questions\nwould be suitable in educational applications for students, a survey was\nconducted with 46 students who evaluated a total of 246 questions across five\nmetrics: clarity, relevance, difficulty, slide relation, and question-answer\nalignment. Results indicate that GPT-3.5 and Llama 2-Chat 13B outperform Flan\nT5 XXL by a small margin, particularly in terms of clarity and question-answer\nalignment. GPT-3.5 especially excels at tailoring questions to match the input\nanswers. The contribution of this research is the analysis of the capacity of\nLLMs for Automatic Question Generation in education.\n","authors":["Ivo Lodovico Molina","Valdemar ≈†v√°bensk√Ω","Tsubasa Minematsu","Li Chen","Fumiya Okubo","Atsushi Shimada"],"pdf_url":"https://arxiv.org/pdf/2407.20578v2.pdf","comment":"Published in Springer ECTEL 2024 conference proceedings, see\n  https://doi.org/10.1007/978-3-031-72312-4_18"},{"id":"http://arxiv.org/abs/2409.09636v1","updated":"2024-09-15T07:15:05Z","published":"2024-09-15T07:15:05Z","title":"Towards understanding evolution of science through language model series","summary":"  We introduce AnnualBERT, a series of language models designed specifically to\ncapture the temporal evolution of scientific text. Deviating from the\nprevailing paradigms of subword tokenizations and \"one model to rule them all\",\nAnnualBERT adopts whole words as tokens and is composed of a base RoBERTa model\npretrained from scratch on the full-text of 1.7 million arXiv papers published\nuntil 2008 and a collection of progressively trained models on arXiv papers at\nan annual basis. We demonstrate the effectiveness of AnnualBERT models by\nshowing that they not only have comparable performances in standard tasks but\nalso achieve state-of-the-art performances on domain-specific NLP tasks as well\nas link prediction tasks in the arXiv citation network. We then utilize probing\ntasks to quantify the models' behavior in terms of representation learning and\nforgetting as time progresses. Our approach enables the pretrained models to\nnot only improve performances on scientific text processing tasks but also to\nprovide insights into the development of scientific discourse over time. The\nseries of the models is available at https://huggingface.co/jd445/AnnualBERTs.\n","authors":["Junjie Dong","Zhuoqi Lyu","Qing Ke"],"pdf_url":"https://arxiv.org/pdf/2409.09636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09629v1","updated":"2024-09-15T06:44:26Z","published":"2024-09-15T06:44:26Z","title":"Confidence Estimation for LLM-Based Dialogue State Tracking","summary":"  Estimation of a model's confidence on its outputs is critical for\nConversational AI systems based on large language models (LLMs), especially for\nreducing hallucination and preventing over-reliance. In this work, we provide\nan exhaustive exploration of methods, including approaches proposed for open-\nand closed-weight LLMs, aimed at quantifying and leveraging model uncertainty\nto improve the reliability of LLM-generated responses, specifically focusing on\ndialogue state tracking (DST) in task-oriented dialogue systems (TODS).\nRegardless of the model type, well-calibrated confidence scores are essential\nto handle uncertainties, thereby improving model performance. We evaluate four\nmethods for estimating confidence scores based on softmax, raw token scores,\nverbalized confidences, and a combination of these methods, using the area\nunder the curve (AUC) metric to assess calibration, with higher AUC indicating\nbetter calibration. We also enhance these with a self-probing mechanism,\nproposed for closed models. Furthermore, we assess these methods using an\nopen-weight model fine-tuned for the task of DST, achieving superior joint goal\naccuracy (JGA). Our findings also suggest that fine-tuning open-weight LLMs can\nresult in enhanced AUC performance, indicating better confidence score\ncalibration.\n","authors":["Yi-Jyun Sun","Suvodip Dey","Dilek Hakkani-Tur","Gokhan Tur"],"pdf_url":"https://arxiv.org/pdf/2409.09629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15297v3","updated":"2024-09-15T06:20:19Z","published":"2024-08-27T11:31:12Z","title":"YOLO-Stutter: End-to-end Region-Wise Speech Dysfluency Detection","summary":"  Dysfluent speech detection is the bottleneck for disordered speech analysis\nand spoken language learning. Current state-of-the-art models are governed by\nrule-based systems which lack efficiency and robustness, and are sensitive to\ntemplate design. In this paper, we propose YOLO-Stutter: a first end-to-end\nmethod that detects dysfluencies in a time-accurate manner. YOLO-Stutter takes\nimperfect speech-text alignment as input, followed by a spatial feature\naggregator, and a temporal dependency extractor to perform region-wise boundary\nand class predictions. We also introduce two dysfluency corpus, VCTK-Stutter\nand VCTK-TTS, that simulate natural spoken dysfluencies including repetition,\nblock, missing, replacement, and prolongation. Our end-to-end method achieves\nstate-of-the-art performance with a minimum number of trainable parameters for\non both simulated data and real aphasia speech. Code and datasets are\nopen-sourced at https://github.com/rorizzz/YOLO-Stutter\n","authors":["Xuanru Zhou","Anshul Kashyap","Steve Li","Ayati Sharma","Brittany Morin","David Baquirin","Jet Vonk","Zoe Ezzes","Zachary Miller","Maria Luisa Gorno Tempini","Jiachen Lian","Gopala Krishna Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2408.15297v3.pdf","comment":"Interspeech 2024"},{"id":"http://arxiv.org/abs/2403.04307v3","updated":"2024-09-15T05:37:56Z","published":"2024-03-07T08:25:46Z","title":"HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild","summary":"  Hallucinations pose a significant challenge to the reliability of large\nlanguage models (LLMs) in critical domains. Recent benchmarks designed to\nassess LLM hallucinations within conventional NLP tasks, such as\nknowledge-intensive question answering (QA) and summarization, are insufficient\nfor capturing the complexities of user-LLM interactions in dynamic, real-world\nsettings. To address this gap, we introduce HaluEval-Wild, the first benchmark\nspecifically designed to evaluate LLM hallucinations in the wild. We\nmeticulously collect challenging (adversarially filtered by Alpaca) user\nqueries from ShareGPT, an existing real-world user-LLM interaction datasets, to\nevaluate the hallucination rates of various LLMs. Upon analyzing the collected\nqueries, we categorize them into five distinct types, which enables a\nfine-grained analysis of the types of hallucinations LLMs exhibit, and\nsynthesize the reference answers with the powerful GPT-4 model and\nretrieval-augmented generation (RAG). Our benchmark offers a novel approach\ntowards enhancing our comprehension of and improving LLM reliability in\nscenarios reflective of real-world interactions. Our benchmark is available at\nhttps://github.com/HaluEval-Wild/HaluEval-Wild.\n","authors":["Zhiying Zhu","Yiming Yang","Zhiqing Sun"],"pdf_url":"https://arxiv.org/pdf/2403.04307v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09615v1","updated":"2024-09-15T05:32:21Z","published":"2024-09-15T05:32:21Z","title":"Enhancing Text Annotation through Rationale-Driven Collaborative\n  Few-Shot Prompting","summary":"  The traditional data annotation process is often labor-intensive,\ntime-consuming, and susceptible to human bias, which complicates the management\nof increasingly complex datasets. This study explores the potential of large\nlanguage models (LLMs) as automated data annotators to improve efficiency and\nconsistency in annotation tasks. By employing rationale-driven collaborative\nfew-shot prompting techniques, we aim to improve the performance of LLMs in\ntext annotation. We conduct a rigorous evaluation of six LLMs across four\nbenchmark datasets, comparing seven distinct methodologies. Our results\ndemonstrate that collaborative methods consistently outperform traditional\nfew-shot techniques and other baseline approaches, particularly in complex\nannotation tasks. Our work provides valuable insights and a robust framework\nfor leveraging collaborative learning methods to tackle challenging text\nannotation tasks.\n","authors":["Jianfei Wu","Xubin Wang","Weijia Jia"],"pdf_url":"https://arxiv.org/pdf/2409.09615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09613v1","updated":"2024-09-15T05:27:56Z","published":"2024-09-15T05:27:56Z","title":"Rethinking KenLM: Good and Bad Model Ensembles for Efficient Text\n  Quality Filtering in Large Web Corpora","summary":"  With the increasing demand for substantial amounts of high-quality data to\ntrain large language models (LLMs), efficiently filtering large web corpora has\nbecome a critical challenge. For this purpose, KenLM, a lightweight\nn-gram-based language model that operates on CPUs, is widely used. However, the\ntraditional method of training KenLM utilizes only high-quality data and,\nconsequently, does not explicitly learn the linguistic patterns of low-quality\ndata. To address this issue, we propose an ensemble approach that leverages two\ncontrasting KenLMs: (i) Good KenLM, trained on high-quality data; and (ii) Bad\nKenLM, trained on low-quality data. Experimental results demonstrate that our\napproach significantly reduces noisy content while preserving high-quality\ncontent compared to the traditional KenLM training method. This indicates that\nour method can be a practical solution with minimal computational overhead for\nresource-constrained environments.\n","authors":["Yungi Kim","Hyunsoo Ha","Sukyung Lee","Jihoo Kim","Seonghoon Yang","Chanjun Park"],"pdf_url":"https://arxiv.org/pdf/2409.09613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19816v2","updated":"2024-09-15T05:02:37Z","published":"2024-07-29T09:08:40Z","title":"Comparative Analysis of Encoder-Based NER and Large Language Models for\n  Skill Extraction from Russian Job Vacancies","summary":"  The labor market is undergoing rapid changes, with increasing demands on job\nseekers and a surge in job openings. Identifying essential skills and\ncompetencies from job descriptions is challenging due to varying employer\nrequirements and the omission of key skills. This study addresses these\nchallenges by comparing traditional Named Entity Recognition (NER) methods\nbased on encoders with Large Language Models (LLMs) for extracting skills from\nRussian job vacancies. Using a labeled dataset of 4,000 job vacancies for\ntraining and 1,472 for testing, the performance of both approaches is\nevaluated. Results indicate that traditional NER models, especially DeepPavlov\nRuBERT NER tuned, outperform LLMs across various metrics including accuracy,\nprecision, recall, and inference time. The findings suggest that traditional\nNER models provide more effective and efficient solutions for skill extraction,\nenhancing job requirement clarity and aiding job seekers in aligning their\nqualifications with employer expectations. This research contributes to the\nfield of natural language processing (NLP) and its application in the labor\nmarket, particularly in non-English contexts.\n","authors":["Nikita Matkin","Aleksei Smirnov","Mikhail Usanin","Egor Ivanov","Kirill Sobyanin","Sofiia Paklina","Petr Parshakov"],"pdf_url":"https://arxiv.org/pdf/2407.19816v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09603v1","updated":"2024-09-15T03:55:03Z","published":"2024-09-15T03:55:03Z","title":"Towards Data-Centric RLHF: Simple Metrics for Preference Dataset\n  Comparison","summary":"  The goal of aligning language models to human preferences requires data that\nreveal these preferences. Ideally, time and money can be spent carefully\ncollecting and tailoring bespoke preference data to each downstream\napplication. However, in practice, a select few publicly available preference\ndatasets are often used to train reward models for reinforcement learning from\nhuman feedback (RLHF). While new preference datasets are being introduced with\nincreasing frequency, there are currently no existing efforts to measure and\ncompare these datasets. In this paper, we systematically study preference\ndatasets through three perspectives: scale, label noise, and information\ncontent. We propose specific metrics for each of these perspectives and uncover\ndifferent axes of comparison for a better understanding of preference datasets.\nOur work is a first step towards a data-centric approach to alignment by\nproviding perspectives that aid in training efficiency and iterative data\ncollection for RLHF.\n","authors":["Judy Hanwen Shen","Archit Sharma","Jun Qin"],"pdf_url":"https://arxiv.org/pdf/2409.09603v1.pdf","comment":"Working Paper"},{"id":"http://arxiv.org/abs/2409.09598v1","updated":"2024-09-15T03:25:55Z","published":"2024-09-15T03:25:55Z","title":"Improving Statistical Significance in Human Evaluation of Automatic\n  Metrics via Soft Pairwise Accuracy","summary":"  Selecting an automatic metric that best emulates human judgments is often\nnon-trivial, because there is no clear definition of \"best emulates.\" A\nmeta-metric is required to compare the human judgments to the automatic metric\njudgments, and metric rankings depend on the choice of meta-metric. We propose\nSoft Pairwise Accuracy (SPA), a new meta-metric that builds on Pairwise\nAccuracy (PA) but incorporates the statistical significance of both the human\njudgments and the metric judgments. SPA allows for more fine-grained\ncomparisons between systems than a simplistic binary win/loss, and addresses a\nnumber of shortcomings with PA: it is more stable with respect to both the\nnumber of systems and segments used for evaluation, it mitigates the issue of\nmetric ties due to quantization, and it produces more statistically significant\nresults. SPA was selected as the official system-level metric for the 2024 WMT\nmetric shared task.\n","authors":["Brian Thompson","Nitika Mathur","Daniel Deutsch","Huda Khayrallah"],"pdf_url":"https://arxiv.org/pdf/2409.09598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09586v1","updated":"2024-09-15T02:13:03Z","published":"2024-09-15T02:13:03Z","title":"ValueCompass: A Framework of Fundamental Values for Human-AI Alignment","summary":"  As AI systems become more advanced, ensuring their alignment with a diverse\nrange of individuals and societal values becomes increasingly critical. But how\ncan we capture fundamental human values and assess the degree to which AI\nsystems align with them? We introduce ValueCompass, a framework of fundamental\nvalues, grounded in psychological theory and a systematic review, to identify\nand evaluate human-AI alignment. We apply ValueCompass to measure the value\nalignment of humans and language models (LMs) across four real-world vignettes:\ncollaborative writing, education, public sectors, and healthcare. Our findings\nuncover risky misalignment between humans and LMs, such as LMs agreeing with\nvalues like \"Choose Own Goals\", which are largely disagreed by humans. We also\nobserve values vary across vignettes, underscoring the necessity for\ncontext-aware AI alignment strategies. This work provides insights into the\ndesign space of human-AI alignment, offering foundations for developing AI that\nresponsibly reflects societal values and ethics.\n","authors":["Hua Shen","Tiffany Knearem","Reshmi Ghosh","Yu-Ju Yang","Tanushree Mitra","Yun Huang"],"pdf_url":"https://arxiv.org/pdf/2409.09586v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09584v1","updated":"2024-09-15T02:07:28Z","published":"2024-09-15T02:07:28Z","title":"RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for\n  Code Generation","summary":"  LLM agents enhanced by tree search algorithms have yielded notable\nperformances in code generation. However, current search algorithms in this\ndomain suffer from low search quality due to several reasons: 1) Ineffective\ndesign of the search space for the high-reasoning demands of code generation\ntasks, 2) Inadequate integration of code feedback with the search algorithm,\nand 3) Poor handling of negative feedback during the search, leading to reduced\nsearch efficiency and quality. To address these challenges, we propose to\nsearch for the reasoning process of the code and use the detailed feedback of\ncode execution to refine erroneous thoughts during the search. In this paper,\nwe introduce RethinkMCTS, which employs the Monte Carlo Tree Search (MCTS)\nalgorithm to conduct thought-level searches before generating code, thereby\nexploring a wider range of strategies. More importantly, we construct verbal\nfeedback from fine-grained code execution feedback to refine erroneous thoughts\nduring the search. This ensures that the search progresses along the correct\nreasoning paths, thus improving the overall search quality of the tree by\nleveraging execution feedback. Through extensive experiments, we demonstrate\nthat RethinkMCTS outperforms previous search-based and feedback-based code\ngeneration baselines. On the HumanEval dataset, it improves the pass@1 of\nGPT-3.5-turbo from 70.12 to 89.02 and GPT-4o-mini from 87.20 to 94.51. It\neffectively conducts more thorough exploration through thought-level searches\nand enhances the search quality of the entire tree by incorporating rethink\noperation.\n","authors":["Qingyao Li","Wei Xia","Kounianhua Du","Xinyi Dai","Ruiming Tang","Yasheng Wang","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.09584v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2311.02578v2","updated":"2024-09-15T01:16:57Z","published":"2023-11-05T06:51:04Z","title":"Temporal Sequencing of Documents","summary":"  We outline an unsupervised method for temporal rank ordering of sets of\nhistorical documents, namely American State of the Union Addresses and DEEDS, a\ncorpus of medieval English property transfer documents. Our method relies upon\neffectively capturing the gradual change in word usage via a bandwidth estimate\nfor the non-parametric Generalized Linear Models (Fan, Heckman, and Wand,\n1995). The number of possible rank orders needed to search through for cost\nfunctions related to the bandwidth can be quite large, even for a small set of\ndocuments. We tackle this problem of combinatorial optimization using the\nSimulated Annealing algorithm, which allows us to obtain the optimal document\ntemporal orders. Our rank ordering method significantly improved the temporal\nsequencing of both corpora compared to a randomly sequenced baseline. This\nunsupervised approach should enable the temporal ordering of undated document\nsets.\n","authors":["Michael Gervers","Gelila Tilahun"],"pdf_url":"https://arxiv.org/pdf/2311.02578v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09568v1","updated":"2024-09-15T01:06:07Z","published":"2024-09-15T01:06:07Z","title":"Thesis proposal: Are We Losing Textual Diversity to Natural Language\n  Processing?","summary":"  This thesis argues that the currently widely used Natural Language Processing\nalgorithms possibly have various limitations related to the properties of the\ntexts they handle and produce. With the wide adoption of these tools in rapid\nprogress, we must ask what these limitations are and what are the possible\nimplications of integrating such tools even more deeply into our daily lives.\n  As a testbed, we have chosen the task of Neural Machine Translation (NMT).\nNevertheless, we aim for general insights and outcomes, applicable even to\ncurrent Large Language Models (LLMs). We ask whether the algorithms used in NMT\nhave inherent inductive biases that are beneficial for most types of inputs but\nmight harm the processing of untypical texts. To explore this hypothesis, we\ndefine a set of measures to quantify text diversity based on its statistical\nproperties, like uniformity or rhythmicity of word-level surprisal, on multiple\nscales (sentence, discourse, language). We then conduct a series of experiments\nto investigate whether NMT systems struggle with maintaining the diversity of\nsuch texts, potentially reducing the richness of the language generated by\nthese systems, compared to human translators.\n  We search for potential causes of these limitations rooted in training\nobjectives and decoding algorithms. Our ultimate goal is to develop\nalternatives that do not enforce uniformity in the distribution of statistical\nproperties in the output and that allow for better global planning of the\ntranslation, taking into account the intrinsic ambiguity of the translation\ntask.\n","authors":["Josef Jon"],"pdf_url":"https://arxiv.org/pdf/2409.09568v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2409.09896v1","updated":"2024-09-15T23:32:04Z","published":"2024-09-15T23:32:04Z","title":"GRIN: Zero-Shot Metric Depth with Pixel-Level Diffusion","summary":"  3D reconstruction from a single image is a long-standing problem in computer\nvision. Learning-based methods address its inherent scale ambiguity by\nleveraging increasingly large labeled and unlabeled datasets, to produce\ngeometric priors capable of generating accurate predictions across domains. As\na result, state of the art approaches show impressive performance in zero-shot\nrelative and metric depth estimation. Recently, diffusion models have exhibited\nremarkable scalability and generalizable properties in their learned\nrepresentations. However, because these models repurpose tools originally\ndesigned for image generation, they can only operate on dense ground-truth,\nwhich is not available for most depth labels, especially in real-world\nsettings. In this paper we present GRIN, an efficient diffusion model designed\nto ingest sparse unstructured training data. We use image features with 3D\ngeometric positional encodings to condition the diffusion process both globally\nand locally, generating depth predictions at a pixel-level. With comprehensive\nexperiments across eight indoor and outdoor datasets, we show that GRIN\nestablishes a new state of the art in zero-shot metric monocular depth\nestimation even when trained from scratch.\n","authors":["Vitor Guizilini","Pavel Tokmakov","Achal Dave","Rares Ambrus"],"pdf_url":"https://arxiv.org/pdf/2409.09896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.04743v4","updated":"2024-09-15T23:26:01Z","published":"2023-05-01T02:58:48Z","title":"MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car\n  Damage Instance Segmentation","summary":"  Evaluating car damages from misfortune is critical to the car insurance\nindustry. However, the accuracy is still insufficient for real-world\napplications since the deep learning network is not designed for car damage\nimages as inputs, and its segmented masks are still very coarse. This paper\npresents MARS (Mask Attention Refinement with Sequential quadtree nodes) for\ncar damage instance segmentation. Our MARS represents self-attention mechanisms\nto draw global dependencies between the sequential quadtree nodes layer and\nquadtree transformer to recalibrate channel weights and predict highly accurate\ninstance masks. Our extensive experiments demonstrate that MARS outperforms\nstate-of-the-art (SOTA) instance segmentation methods on three popular\nbenchmarks such as Mask R-CNN [9], PointRend [13], and Mask Transfiner [12], by\na large margin of +1.3 maskAP-based R50-FPN backbone and +2.3 maskAP-based\nR101-FPN backbone on Thai car-damage dataset. Our demos are available at\nhttps://github.com/kaopanboonyuen/MARS.\n","authors":["Teerapong Panboonyuen","Naphat Nithisopa","Panin Pienroj","Laphonchai Jirachuphun","Chaiwasut Watthanasirikrit","Naruepon Pornwiriyakul"],"pdf_url":"https://arxiv.org/pdf/2305.04743v4.pdf","comment":"14 pages. arXiv admin note: substantial text overlap with\n  arXiv:2111.13673 by other authors"},{"id":"http://arxiv.org/abs/2409.09893v1","updated":"2024-09-15T23:18:33Z","published":"2024-09-15T23:18:33Z","title":"Resolving Inconsistent Semantics in Multi-Dataset Image Segmentation","summary":"  Leveraging multiple training datasets to scale up image segmentation models\nis beneficial for increasing robustness and semantic understanding. Individual\ndatasets have well-defined ground truth with non-overlapping mask layouts and\nmutually exclusive semantics. However, merging them for multi-dataset training\ndisrupts this harmony and leads to semantic inconsistencies; for example, the\nclass \"person\" in one dataset and class \"face\" in another will require\nmultilabel handling for certain pixels. Existing methods struggle with this\nsetting, particularly when evaluated on label spaces mixed from the individual\ntraining sets. To overcome these issues, we introduce a simple yet effective\nmulti-dataset training approach by integrating language-based embeddings of\nclass names and label space-specific query embeddings. Our method maintains\nhigh performance regardless of the underlying inconsistencies between training\ndatasets. Notably, on four benchmark datasets with label space inconsistencies\nduring inference, we outperform previous methods by 1.6% mIoU for semantic\nsegmentation, 9.1% PQ for panoptic segmentation, 12.1% AP for instance\nsegmentation, and 3.0% in the newly proposed PIQ metric.\n","authors":["Qilong Zhangli","Di Liu","Abhishek Aich","Dimitris Metaxas","Samuel Schulter"],"pdf_url":"https://arxiv.org/pdf/2409.09893v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01062v5","updated":"2024-09-15T21:46:02Z","published":"2024-06-03T07:20:34Z","title":"Layout Agnostic Scene Text Image Synthesis with Diffusion Models","summary":"  While diffusion models have significantly advanced the quality of image\ngeneration their capability to accurately and coherently render text within\nthese images remains a substantial challenge. Conventional diffusion-based\nmethods for scene text generation are typically limited by their reliance on an\nintermediate layout output. This dependency often results in a constrained\ndiversity of text styles and fonts an inherent limitation stemming from the\ndeterministic nature of the layout generation phase. To address these\nchallenges this paper introduces SceneTextGen a novel diffusion-based model\nspecifically designed to circumvent the need for a predefined layout stage. By\ndoing so SceneTextGen facilitates a more natural and varied representation of\ntext. The novelty of SceneTextGen lies in its integration of three key\ncomponents: a character-level encoder for capturing detailed typographic\nproperties coupled with a character-level instance segmentation model and a\nword-level spotting model to address the issues of unwanted text generation and\nminor character inaccuracies. We validate the performance of our method by\ndemonstrating improved character recognition rates on generated images across\ndifferent public visual text datasets in comparison to both standard diffusion\nbased methods and text specific methods.\n","authors":["Qilong Zhangli","Jindong Jiang","Di Liu","Licheng Yu","Xiaoliang Dai","Ankit Ramchandani","Guan Pang","Dimitris N. Metaxas","Praveen Krishnan"],"pdf_url":"https://arxiv.org/pdf/2406.01062v5.pdf","comment":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2024, pp. 7496-7506"},{"id":"http://arxiv.org/abs/2304.10727v3","updated":"2024-09-15T21:38:21Z","published":"2023-04-21T03:45:59Z","title":"RoCOCO: Robustness Benchmark of MS-COCO to Stress-test Image-Text\n  Matching Models","summary":"  With the extensive use of vision-language models in various downstream tasks,\nevaluating their robustness is crucial. In this paper, we propose a benchmark\nfor assessing the robustness of vision-language models. We believe that a\nrobust model should properly understand both linguistic and visual semantics\nand be resilient to explicit variations. In pursuit of this goal, we create new\nvariants of texts and images in the MS-COCO test set and re-evaluate the\nstate-of-the-art (SOTA) models with the new data. Specifically, we alter the\nmeaning of text by replacing a word, and generate visually altered images that\nmaintain some visual context while introducing noticeable pixel changes through\nimage mixing techniques.Our evaluations on the proposed benchmark reveal\nsubstantial performance degradation in many SOTA models (e.g., Image-to-Text\nRecall@1: 81.9\\% $\\rightarrow$ 48.4\\% in BLIP, 66.1\\% $\\rightarrow$ 37.6\\% in\nVSE$\\infty$), with the models often favoring the altered texts/images over the\noriginal ones. This indicates the current vision-language models struggle with\nsubtle changes and often fail to understand the overall context of texts and\nimages. Based on these findings, we propose semantic contrastive loss and\nvisual contrastive loss to learn more robust embedding. Datasets and code are\navailable at {\\url{https://github.com/pseulki/rococo}}.\n","authors":["Seulki Park","Daeho Um","Hajung Yoon","Sanghyuk Chun","Sangdoo Yun"],"pdf_url":"https://arxiv.org/pdf/2304.10727v3.pdf","comment":"Accepted to ECCV Synthetic Data for Computer Vision Workshop (Oral)"},{"id":"http://arxiv.org/abs/2409.09867v1","updated":"2024-09-15T21:24:51Z","published":"2024-09-15T21:24:51Z","title":"Towards Kinetic Manipulation of the Latent Space","summary":"  The latent space of many generative models are rich in unexplored valleys and\nmountains. The majority of tools used for exploring them are so far limited to\nGraphical User Interfaces (GUIs). While specialized hardware can be used for\nthis task, we show that a simple feature extraction of pre-trained\nConvolutional Neural Networks (CNNs) from a live RGB camera feed does a very\ngood job at manipulating the latent space with simple changes in the scene,\nwith vast room for improvement. We name this new paradigm Visual-reactive\nInterpolation, and the full code can be found at\nhttps://github.com/PDillis/stylegan3-fun.\n","authors":["Diego Porres"],"pdf_url":"https://arxiv.org/pdf/2409.09867v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21652v2","updated":"2024-09-15T21:04:23Z","published":"2024-07-31T14:53:41Z","title":"Spatial Transformer Network YOLO Model for Agricultural Object Detection","summary":"  Object detection plays a crucial role in the field of computer vision by\nautonomously locating and identifying objects of interest. The You Only Look\nOnce (YOLO) model is an effective single-shot detector. However, YOLO faces\nchallenges in cluttered or partially occluded scenes and can struggle with\nsmall, low-contrast objects. We propose a new method that integrates spatial\ntransformer networks (STNs) into YOLO to improve performance. The proposed\nSTN-YOLO aims to enhance the model's effectiveness by focusing on important\nareas of the image and improving the spatial invariance of the model before the\ndetection process. Our proposed method improved object detection performance\nboth qualitatively and quantitatively. We explore the impact of different\nlocalization networks within the STN module as well as the robustness of the\nmodel across different spatial transformations. We apply the STN-YOLO on\nbenchmark datasets for Agricultural object detection as well as a new dataset\nfrom a state-of-the-art plant phenotyping greenhouse facility. Our code and\ndataset are publicly available.\n","authors":["Yash Zambre","Ekdev Rajkitkul","Akshatha Mohan","Joshua Peeples"],"pdf_url":"https://arxiv.org/pdf/2407.21652v2.pdf","comment":"7 pages, 5 figures, accepted to 2024 IEEE International Conference on\n  Machine Learning and Applications"},{"id":"http://arxiv.org/abs/2409.00606v2","updated":"2024-09-15T20:50:43Z","published":"2024-09-01T04:07:03Z","title":"Style Transfer: From Stitching to Neural Networks","summary":"  This article compares two style transfer methods in image processing: the\ntraditional method, which synthesizes new images by stitching together small\npatches from existing images, and a modern machine learning-based approach that\nuses a segmentation network to isolate foreground objects and apply style\ntransfer solely to the background. The traditional method excels in creating\nartistic abstractions but can struggle with seamlessness, whereas the machine\nlearning method preserves the integrity of foreground elements while enhancing\nthe background, offering improved aesthetic quality and computational\nefficiency. Our study indicates that machine learning-based methods are more\nsuited for real-world applications where detail preservation in foreground\nelements is essential.\n","authors":["Xinhe Xu","Zhuoer Wang","Yihan Zhang","Yizhou Liu","Zhaoyue Wang","Zhihao Xu","Muhan Zhao","Huaiying Luo"],"pdf_url":"https://arxiv.org/pdf/2409.00606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09860v1","updated":"2024-09-15T20:48:47Z","published":"2024-09-15T20:48:47Z","title":"Revisiting Physical-World Adversarial Attack on Traffic Sign\n  Recognition: A Commercial Systems Perspective","summary":"  Traffic Sign Recognition (TSR) is crucial for safe and correct driving\nautomation. Recent works revealed a general vulnerability of TSR models to\nphysical-world adversarial attacks, which can be low-cost, highly deployable,\nand capable of causing severe attack effects such as hiding a critical traffic\nsign or spoofing a fake one. However, so far existing works generally only\nconsidered evaluating the attack effects on academic TSR models, leaving the\nimpacts of such attacks on real-world commercial TSR systems largely unclear.\nIn this paper, we conduct the first large-scale measurement of physical-world\nadversarial attacks against commercial TSR systems. Our testing results reveal\nthat it is possible for existing attack works from academia to have highly\nreliable (100\\%) attack success against certain commercial TSR system\nfunctionality, but such attack capabilities are not generalizable, leading to\nmuch lower-than-expected attack success rates overall. We find that one\npotential major factor is a spatial memorization design that commonly exists in\ntoday's commercial TSR systems. We design new attack success metrics that can\nmathematically model the impacts of such design on the TSR system-level attack\nsuccess, and use them to revisit existing attacks. Through these efforts, we\nuncover 7 novel observations, some of which directly challenge the observations\nor claims in prior works due to the introduction of the new metrics.\n","authors":["Ningfei Wang","Shaoyuan Xie","Takami Sato","Yunpeng Luo","Kaidi Xu","Qi Alfred Chen"],"pdf_url":"https://arxiv.org/pdf/2409.09860v1.pdf","comment":"Accepted by NDSS 2025"},{"id":"http://arxiv.org/abs/2311.14875v3","updated":"2024-09-15T20:36:12Z","published":"2023-11-24T23:54:33Z","title":"Bayesian Neural Networks for 2D MRI Segmentation","summary":"  Uncertainty quantification is vital for safety-critical Deep Learning\napplications like medical image segmentation. We introduce BA U-Net, an\nuncertainty-aware model for MRI segmentation that integrates Bayesian Neural\nNetworks with Attention Mechanisms. BA U-Net delivers accurate, interpretable\nresults, crucial for reliable pathology screening. Evaluated on BraTS 2020,\nthis model addresses the critical need for confidence estimation in deep\nlearning-based medical imaging.\n","authors":["Lohith Konathala"],"pdf_url":"https://arxiv.org/pdf/2311.14875v3.pdf","comment":"9 pages, conference-paper style"},{"id":"http://arxiv.org/abs/2407.14279v2","updated":"2024-09-15T20:14:42Z","published":"2024-07-19T13:01:12Z","title":"OpenSU3D: Open World 3D Scene Understanding using Foundation Models","summary":"  In this paper, we present a novel, scalable approach for constructing open\nset, instance-level 3D scene representations, advancing open world\nunderstanding of 3D environments. Existing methods require pre-constructed 3D\nscenes and face scalability issues due to per-point feature vector learning,\nlimiting their efficacy with complex queries. Our method overcomes these\nlimitations by incrementally building instance-level 3D scene representations\nusing 2D foundation models, efficiently aggregating instance-level details such\nas masks, feature vectors, names, and captions. We introduce fusion schemes for\nfeature vectors to enhance their contextual knowledge and performance on\ncomplex queries. Additionally, we explore large language models for robust\nautomatic annotation and spatial reasoning tasks. We evaluate our proposed\napproach on multiple scenes from ScanNet and Replica datasets demonstrating\nzero-shot generalization capabilities, exceeding current state-of-the-art\nmethods in open world 3D scene understanding.\n","authors":["Rafay Mohiuddin","Sai Manoj Prakhya","Fiona Collins","Ziyuan Liu","Andr√© Borrmann"],"pdf_url":"https://arxiv.org/pdf/2407.14279v2.pdf","comment":"Project Page: https://opensu3d.github.io/"},{"id":"http://arxiv.org/abs/2409.09841v1","updated":"2024-09-15T19:37:37Z","published":"2024-09-15T19:37:37Z","title":"Tracking Virtual Meetings in the Wild: Re-identification in\n  Multi-Participant Virtual Meetings","summary":"  In recent years, workplaces and educational institutes have widely adopted\nvirtual meeting platforms. This has led to a growing interest in analyzing and\nextracting insights from these meetings, which requires effective detection and\ntracking of unique individuals. In practice, there is no standardization in\nvideo meetings recording layout, and how they are captured across the different\nplatforms and services. This, in turn, creates a challenge in acquiring this\ndata stream and analyzing it in a uniform fashion. Our approach provides a\nsolution to the most general form of video recording, usually consisting of a\ngrid of participants (\\cref{fig:videomeeting}) from a single video source with\nno metadata on participant locations, while using the least amount of\nconstraints and assumptions as to how the data was acquired. Conventional\napproaches often use YOLO models coupled with tracking algorithms, assuming\nlinear motion trajectories akin to that observed in CCTV footage. However, such\nassumptions fall short in virtual meetings, where participant video feed window\ncan abruptly change location across the grid. In an organic video meeting\nsetting, participants frequently join and leave, leading to sudden, non-linear\nmovements on the video grid. This disrupts optical flow-based tracking methods\nthat depend on linear motion. Consequently, standard object detection and\ntracking methods might mistakenly assign multiple participants to the same\ntracker. In this paper, we introduce a novel approach to track and re-identify\nparticipants in remote video meetings, by utilizing the spatio-temporal priors\narising from the data in our domain. This, in turn, increases tracking\ncapabilities compared to the use of general object tracking. Our approach\nreduces the error rate by 95% on average compared to YOLO-based tracking\nmethods as a baseline.\n","authors":["Oriel Perl","Ido Leshem","Uria Franko","Yuval Goldman"],"pdf_url":"https://arxiv.org/pdf/2409.09841v1.pdf","comment":"Accepted to ECCV 2024 workshop"},{"id":"http://arxiv.org/abs/2409.09832v1","updated":"2024-09-15T19:12:52Z","published":"2024-09-15T19:12:52Z","title":"Template-based Multi-Domain Face Recognition","summary":"  Despite the remarkable performance of deep neural networks for face detection\nand recognition tasks in the visible spectrum, their performance on more\nchallenging non-visible domains is comparatively still lacking. While\nsignificant research has been done in the fields of domain adaptation and\ndomain generalization, in this paper we tackle scenarios in which these methods\nhave limited applicability owing to the lack of training data from target\ndomains. We focus on the problem of single-source (visible) and multi-target\n(SWIR, long-range/remote, surveillance, and body-worn) face recognition task.\nWe show through experiments that a good template generation algorithm becomes\ncrucial as the complexity of the target domain increases. In this context, we\nintroduce a template generation algorithm called Norm Pooling (and a variant\nknown as Sparse Pooling) and show that it outperforms average pooling across\ndifferent domains and networks, on the IARPA JANUS Benchmark Multi-domain Face\n(IJB-MDF) dataset.\n","authors":["Anirudh Nanduri","Rama Chellappa"],"pdf_url":"https://arxiv.org/pdf/2409.09832v1.pdf","comment":"IJCB 2024 - Special Session on Recognition at Long Range and from\n  High Altitude"},{"id":"http://arxiv.org/abs/2409.09829v1","updated":"2024-09-15T19:06:46Z","published":"2024-09-15T19:06:46Z","title":"NARF24: Estimating Articulated Object Structure for Implicit Rendering","summary":"  Articulated objects and their representations pose a difficult problem for\nrobots. These objects require not only representations of geometry and texture,\nbut also of the various connections and joint parameters that make up each\narticulation. We propose a method that learns a common Neural Radiance Field\n(NeRF) representation across a small number of collected scenes. This\nrepresentation is combined with a parts-based image segmentation to produce an\nimplicit space part localization, from which the connectivity and joint\nparameters of the articulated object can be estimated, thus enabling\nconfiguration-conditioned rendering.\n","authors":["Stanley Lewis","Tom Gao","Odest Chadwicke Jenkins"],"pdf_url":"https://arxiv.org/pdf/2409.09829v1.pdf","comment":"extended abstract as submitted to ICRA@40 anniversary conference"},{"id":"http://arxiv.org/abs/2408.16445v2","updated":"2024-09-15T19:02:12Z","published":"2024-08-29T11:16:34Z","title":"Mismatched: Evaluating the Limits of Image Matching Approaches and\n  Benchmarks","summary":"  Three-dimensional (3D) reconstruction from two-dimensional images is an\nactive research field in computer vision, with applications ranging from\nnavigation and object tracking to segmentation and three-dimensional modeling.\nTraditionally, parametric techniques have been employed for this task. However,\nrecent advancements have seen a shift towards learning-based methods. Given the\nrapid pace of research and the frequent introduction of new image matching\nmethods, it is essential to evaluate them. In this paper, we present a\ncomprehensive evaluation of various image matching methods using a\nstructure-from-motion pipeline. We assess the performance of these methods on\nboth in-domain and out-of-domain datasets, identifying key limitations in both\nthe methods and benchmarks. We also investigate the impact of edge detection as\na pre-processing step. Our analysis reveals that image matching for 3D\nreconstruction remains an open challenge, necessitating careful selection and\ntuning of models for specific scenarios, while also highlighting mismatches in\nhow metrics currently represent method performance.\n","authors":["Sierra Bonilla","Chiara Di Vece","Rema Daher","Xinwei Ju","Danail Stoyanov","Francisco Vasconcelos","Sophia Bano"],"pdf_url":"https://arxiv.org/pdf/2408.16445v2.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.16471v3","updated":"2024-09-15T18:47:19Z","published":"2024-04-25T09:55:35Z","title":"COBRA -- COnfidence score Based on shape Regression Analysis for\n  method-independent quality assessment of object pose estimation from single\n  images","summary":"  We present a generic algorithm for scoring pose estimation methods that rely\non single image semantic analysis. The algorithm employs a lightweight putative\nshape representation using a combination of multiple Gaussian Processes. Each\nGaussian Process (GP) yields distance normal distributions from multiple\nreference points in the object's coordinate system to its surface, thus\nproviding a geometric evaluation framework for scoring predicted poses. Our\nconfidence measure comprises the average mixture probability of pixel\nback-projections onto the shape template. In the reported experiments, we\ncompare the accuracy of our GP based representation of objects versus the\nactual geometric models and demonstrate the ability of our method to capture\nthe influence of outliers as opposed to the corresponding intrinsic measures\nthat ship with the segmentation and pose estimation methods.\n","authors":["Panagiotis Sapoutzoglou","George Giapitzakis","George Terzakis","Maria Pateraki"],"pdf_url":"https://arxiv.org/pdf/2404.16471v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16803v2","updated":"2024-09-15T18:34:34Z","published":"2024-03-25T14:21:49Z","title":"Exploiting Priors from 3D Diffusion Models for RGB-Based One-Shot View\n  Planning","summary":"  Object reconstruction is relevant for many autonomous robotic tasks that\nrequire interaction with the environment. A key challenge in such scenarios is\nplanning view configurations to collect informative measurements for\nreconstructing an initially unknown object. One-shot view planning enables\nefficient data collection by predicting view configurations and planning the\nglobally shortest path connecting all views at once. However, prior knowledge\nabout the object is required to conduct one-shot view planning. In this work,\nwe propose a novel one-shot view planning approach that utilizes the powerful\n3D generation capabilities of diffusion models as priors. By incorporating such\ngeometric priors into our pipeline, we achieve effective one-shot view planning\nstarting with only a single RGB image of the object to be reconstructed. Our\nplanning experiments in simulation and real-world setups indicate that our\napproach balances well between object reconstruction quality and movement cost.\n","authors":["Sicong Pan","Liren Jin","Xuying Huang","Cyrill Stachniss","Marija Popoviƒá","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2403.16803v2.pdf","comment":"Sicong Pan and Liren Jin have equal contribution. Publication to\n  appear in IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS), 2024"},{"id":"http://arxiv.org/abs/2409.09808v1","updated":"2024-09-15T18:02:26Z","published":"2024-09-15T18:02:26Z","title":"Famba-V: Fast Vision Mamba with Cross-Layer Token Fusion","summary":"  Mamba and Vision Mamba (Vim) models have shown their potential as an\nalternative to methods based on Transformer architecture. This work introduces\nFast Mamba for Vision (Famba-V), a cross-layer token fusion technique to\nenhance the training efficiency of Vim models. The key idea of Famba-V is to\nidentify and fuse similar tokens across different Vim layers based on a suit of\ncross-layer strategies instead of simply applying token fusion uniformly across\nall the layers that existing works propose. We evaluate the performance of\nFamba-V on CIFAR-100. Our results show that Famba-V is able to enhance the\ntraining efficiency of Vim models by reducing both training time and peak\nmemory usage during training. Moreover, the proposed cross-layer strategies\nallow Famba-V to deliver superior accuracy-efficiency trade-offs. These results\nall together demonstrate Famba-V as a promising efficiency enhancement\ntechnique for Vim models.\n","authors":["Hui Shen","Zhongwei Wan","Xin Wang","Mi Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.09808v1.pdf","comment":"Camera ready version of ECCV 2024 The Fourth Workshop on\n  Computational Aspects of Deep Learning"},{"id":"http://arxiv.org/abs/2409.04218v2","updated":"2024-09-15T17:52:07Z","published":"2024-09-06T12:17:23Z","title":"MpoxMamba: A Grouped Mamba-based Lightweight Hybrid Network for Mpox\n  Detection","summary":"  Due to the lack of effective mpox detection tools, the mpox virus continues\nto spread worldwide and has once again been declared a public health emergency\nof international concern by the World Health Organization. Lightweight deep\nlearning model-based detection systems are crucial to alleviate mpox outbreaks\nsince they are suitable for widespread deployment, especially in\nresource-limited scenarios. However, the key to its successful application\ndepends on ensuring that the model can effectively model local features and\nlong-range dependencies in mpox lesions while maintaining lightweight. Inspired\nby the success of Mamba in modeling long-range dependencies and its linear\ncomplexity, we proposed a lightweight hybrid architecture called MpoxMamba for\nefficient mpox detection. MpoxMamba utilizes depth-wise separable convolutions\nto extract local feature representations in mpox skin lesions and greatly\nenhances the model's ability to model the global contextual information by\ngrouped Mamba modules. Notably, MpoxMamba's parameter size and FLOPs are 0.77M\nand 0.53G, respectively. Experimental results on two widely recognized\nbenchmark datasets demonstrate that MpoxMamba outperforms state-of-the-art\nlightweight models and existing mpox detection methods. Importantly, we\ndeveloped a web-based online application to provide free mpox detection\n(http://5227i971s5.goho.co:30290). The source codes of MpoxMamba are available\nat https://github.com/YubiaoYue/MpoxMamba.\n","authors":["Yubiao Yue","Jun Xue","Haihuang Liang","Zhenzhang Li","Yufeng Wang"],"pdf_url":"https://arxiv.org/pdf/2409.04218v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09804v1","updated":"2024-09-15T17:44:51Z","published":"2024-09-15T17:44:51Z","title":"Abnormal Event Detection In Videos Using Deep Embedding","summary":"  Abnormal event detection or anomaly detection in surveillance videos is\ncurrently a challenge because of the diversity of possible events. Due to the\nlack of anomalous events at training time, anomaly detection requires the\ndesign of learning methods without supervision. In this work we propose an\nunsupervised approach for video anomaly detection with the aim to jointly\noptimize the objectives of the deep neural network and the anomaly detection\ntask using a hybrid architecture. Initially, a convolutional autoencoder is\npre-trained in an unsupervised manner with a fusion of depth, motion and\nappearance features. In the second step, we utilize the encoder part of the\npre-trained autoencoder and extract the embeddings of the fused input. Now, we\njointly train/ fine tune the encoder to map the embeddings to a hypercenter.\nThus, embeddings of normal data fall near the hypercenter, whereas embeddings\nof anomalous data fall far away from the hypercenter.\n","authors":["Darshan Venkatrayappa"],"pdf_url":"https://arxiv.org/pdf/2409.09804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09797v1","updated":"2024-09-15T17:08:34Z","published":"2024-09-15T17:08:34Z","title":"Domain and Content Adaptive Convolutions for Cross-Domain Adenocarcinoma\n  Segmentation","summary":"  Recent advances in computer-aided diagnosis for histopathology have been\nlargely driven by the use of deep learning models for automated image analysis.\nWhile these networks can perform on par with medical experts, their performance\ncan be impeded by out-of-distribution data. The Cross-Organ and Cross-Scanner\nAdenocarcinoma Segmentation (COSAS) challenge aimed to address the task of\ncross-domain adenocarcinoma segmentation in the presence of morphological and\nscanner-induced domain shifts. In this paper, we present a U-Net-based\nsegmentation framework designed to tackle this challenge. Our approach achieved\nsegmentation scores of 0.8020 for the cross-organ track and 0.8527 for the\ncross-scanner track on the final challenge test sets, ranking it the\nbest-performing submission.\n","authors":["Frauke Wilm","Mathias √ñttl","Marc Aubreville","Katharina Breininger"],"pdf_url":"https://arxiv.org/pdf/2409.09797v1.pdf","comment":"5 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2409.09796v1","updated":"2024-09-15T17:07:58Z","published":"2024-09-15T17:07:58Z","title":"Universal Topology Refinement for Medical Image Segmentation with\n  Polynomial Feature Synthesis","summary":"  Although existing medical image segmentation methods provide impressive\npixel-wise accuracy, they often neglect topological correctness, making their\nsegmentations unusable for many downstream tasks. One option is to retrain such\nmodels whilst including a topology-driven loss component. However, this is\ncomputationally expensive and often impractical. A better solution would be to\nhave a versatile plug-and-play topology refinement method that is compatible\nwith any domain-specific segmentation pipeline. Directly training a\npost-processing model to mitigate topological errors often fails as such models\ntend to be biased towards the topological errors of a target segmentation\nnetwork. The diversity of these errors is confined to the information provided\nby a labelled training set, which is especially problematic for small datasets.\nOur method solves this problem by training a model-agnostic topology refinement\nnetwork with synthetic segmentations that cover a wide variety of topological\nerrors. Inspired by the Stone-Weierstrass theorem, we synthesize\ntopology-perturbation masks with randomly sampled coefficients of orthogonal\npolynomial bases, which ensures a complete and unbiased representation.\nPractically, we verified the efficiency and effectiveness of our methods as\nbeing compatible with multiple families of polynomial bases, and show evidence\nthat our universal plug-and-play topology refinement network outperforms both\nexisting topology-driven learning-based and post-processing methods. We also\nshow that combining our method with learning-based models provides an\neffortless add-on, which can further improve the performance of existing\napproaches.\n","authors":["Liu Li","Hanchun Wang","Matthew Baugh","Qiang Ma","Weitong Zhang","Cheng Ouyang","Daniel Rueckert","Bernhard Kainz"],"pdf_url":"https://arxiv.org/pdf/2409.09796v1.pdf","comment":"Accepted by the 27th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI 2024)"},{"id":"http://arxiv.org/abs/2409.09790v1","updated":"2024-09-15T16:50:27Z","published":"2024-09-15T16:50:27Z","title":"Multiple Rotation Averaging with Constrained Reweighting Deep Matrix\n  Factorization","summary":"  Multiple rotation averaging plays a crucial role in computer vision and\nrobotics domains. The conventional optimization-based methods optimize a\nnonlinear cost function based on certain noise assumptions, while most previous\nlearning-based methods require ground truth labels in the supervised training\nprocess. Recognizing the handcrafted noise assumption may not be reasonable in\nall real-world scenarios, this paper proposes an effective rotation averaging\nmethod for mining data patterns in a learning manner while avoiding the\nrequirement of labels. Specifically, we apply deep matrix factorization to\ndirectly solve the multiple rotation averaging problem in unconstrained linear\nspace. For deep matrix factorization, we design a neural network model, which\nis explicitly low-rank and symmetric to better suit the background of multiple\nrotation averaging. Meanwhile, we utilize a spanning tree-based edge filtering\nto suppress the influence of rotation outliers. What's more, we also adopt a\nreweighting scheme and dynamic depth selection strategy to further improve the\nrobustness. Our method synthesizes the merit of both optimization-based and\nlearning-based methods. Experimental results on various datasets validate the\neffectiveness of our proposed method.\n","authors":["Shiqi Li","Jihua Zhu","Yifan Xie","Naiwen Hu","Mingchen Zhu","Zhongyu Li","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09788v1","updated":"2024-09-15T16:45:42Z","published":"2024-09-15T16:45:42Z","title":"Reasoning Paths with Reference Objects Elicit Quantitative Spatial\n  Reasoning in Large Vision-Language Models","summary":"  Despite recent advances demonstrating vision-language models' (VLMs)\nabilities to describe complex relationships in images using natural language,\ntheir capability to quantitatively reason about object sizes and distances\nremains underexplored. In this work, we introduce a manually annotated\nbenchmark, Q-Spatial Bench, with 271 questions across five categories designed\nfor quantitative spatial reasoning and systematically investigate the\nperformance of state-of-the-art VLMs on this task. Our analysis reveals that\nreasoning about distances between objects is particularly challenging for SoTA\nVLMs; however, some VLMs significantly outperform others, with an over 40-point\ngap between the two best performing models. We also make the surprising\nobservation that the success rate of the top-performing VLM increases by 19\npoints when a reasoning path using a reference object emerges naturally in the\nresponse. Inspired by this observation, we develop a zero-shot prompting\ntechnique, SpatialPrompt, that encourages VLMs to answer quantitative spatial\nquestions using reference objects as visual cues. By instructing VLMs to use\nreference objects in their reasoning paths via SpatialPrompt, Gemini 1.5 Pro,\nGemini 1.5 Flash, and GPT-4V improve their success rates by over 40, 20, and 30\npoints, respectively. We emphasize that these significant improvements are\nobtained without needing more data, model architectural modifications, or\nfine-tuning.\n","authors":["Yuan-Hong Liao","Rafid Mahmood","Sanja Fidler","David Acuna"],"pdf_url":"https://arxiv.org/pdf/2409.09788v1.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2409.09784v1","updated":"2024-09-15T16:27:34Z","published":"2024-09-15T16:27:34Z","title":"Enhancing Lesion Segmentation in PET/CT Imaging with Deep Learning and\n  Advanced Data Preprocessing Techniques","summary":"  The escalating global cancer burden underscores the critical need for precise\ndiagnostic tools in oncology. This research employs deep learning to enhance\nlesion segmentation in PET/CT imaging, utilizing a dataset of 900 whole-body\nFDG-PET/CT and 600 PSMA-PET/CT studies from the AutoPET challenge III. Our\nmethodical approach includes robust preprocessing and data augmentation\ntechniques to ensure model robustness and generalizability. We investigate the\ninfluence of non-zero normalization and modifications to the data augmentation\npipeline, such as the introduction of RandGaussianSharpen and adjustments to\nthe Gamma transform parameter. This study aims to contribute to the\nstandardization of preprocessing and augmentation strategies in PET/CT imaging,\npotentially improving the diagnostic accuracy and the personalized management\nof cancer patients. Our code will be open-sourced and available at\nhttps://github.com/jiayiliu-pku/DC2024.\n","authors":["Jiayi Liu","Qiaoyi Xue","Youdan Feng","Tianming Xu","Kaixin Shen","Chuyun Shen","Yuhang Shi"],"pdf_url":"https://arxiv.org/pdf/2409.09784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09779v1","updated":"2024-09-15T15:58:20Z","published":"2024-09-15T15:58:20Z","title":"Underwater Image Enhancement via Dehazing and Color Restoration","summary":"  With the rapid development of marine engineering projects such as marine\nresource extraction and oceanic surveys, underwater visual imaging and analysis\nhas become a critical technology. Unfortunately, due to the inevitable\nnon-linear attenuation of light in underwater environments, underwater images\nand videos often suffer from low contrast, blurriness, and color degradation,\nwhich significantly complicate the subsequent research. Existing underwater\nimage enhancement methods often treat the haze and color cast as a unified\ndegradation process and disregard their independence and interdependence, which\nlimits the performance improvement. Here, we propose a Vision Transformer\n(ViT)-based network (referred to as WaterFormer) to improve the underwater\nimage quality. WaterFormer contains three major components: a dehazing block\n(DehazeFormer Block) to capture the self-correlated haze features and extract\ndeep-level features, a Color Restoration Block (CRB) to capture self-correlated\ncolor cast features, and a Channel Fusion Block (CFB) to capture fusion\nfeatures within the network. To ensure authenticity, a soft reconstruction\nlayer based on the underwater imaging physics model is included. To improve the\nquality of the enhanced images, we introduce the Chromatic Consistency Loss and\nSobel Color Loss to train the network. Comprehensive experimental results\ndemonstrate that WaterFormer outperforms other state-of-the-art methods in\nenhancing underwater images.\n","authors":["Chengqin Wu","Shuai Yu","Qingson Hu","Jingxiang Xu","Lijun Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.09779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09777v1","updated":"2024-09-15T15:55:24Z","published":"2024-09-15T15:55:24Z","title":"DiFSD: Ego-Centric Fully Sparse Paradigm with Uncertainty Denoising and\n  Iterative Refinement for Efficient End-to-End Autonomous Driving","summary":"  Current end-to-end autonomous driving methods resort to unifying modular\ndesigns for various tasks (e.g. perception, prediction and planning). Although\noptimized in a planning-oriented spirit with a fully differentiable framework,\nexisting end-to-end driving systems without ego-centric designs still suffer\nfrom unsatisfactory performance and inferior efficiency, owing to the\nrasterized scene representation learning and redundant information\ntransmission. In this paper, we revisit the human driving behavior and propose\nan ego-centric fully sparse paradigm, named DiFSD, for end-to-end self-driving.\nSpecifically, DiFSD mainly consists of sparse perception, hierarchical\ninteraction and iterative motion planner. The sparse perception module performs\ndetection, tracking and online mapping based on sparse representation of the\ndriving scene. The hierarchical interaction module aims to select the Closest\nIn-Path Vehicle / Stationary (CIPV / CIPS) from coarse to fine, benefiting from\nan additional geometric prior. As for the iterative motion planner, both\nselected interactive agents and ego-vehicle are considered for joint motion\nprediction, where the output multi-modal ego-trajectories are optimized in an\niterative fashion. Besides, both position-level motion diffusion and\ntrajectory-level planning denoising are introduced for uncertainty modeling,\nthus facilitating the training stability and convergence of the whole\nframework. Extensive experiments conducted on nuScenes dataset demonstrate the\nsuperior planning performance and great efficiency of DiFSD, which\nsignificantly reduces the average L2 error by \\textbf{66\\%} and collision rate\nby \\textbf{77\\%} than UniAD while achieves \\textbf{8.2$\\times$} faster running\nefficiency.\n","authors":["Haisheng Su","Wei Wu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2409.09777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15503v2","updated":"2024-09-15T15:51:44Z","published":"2024-08-28T03:17:40Z","title":"RoboSense: Large-scale Dataset and Benchmark for Multi-sensor Low-speed\n  Autonomous Driving","summary":"  Robust object detection and tracking under arbitrary sight of view is\nchallenging yet essential for the development of Autonomous Vehicle technology.\nWith the growing demand of unmanned function vehicles, near-field scene\nunderstanding becomes an important research topic in the areas of low-speed\nautonomous driving. Due to the complexity of driving conditions and diversity\nof near obstacles such as blind spots and high occlusion, the perception\ncapability of near-field environment is still inferior than its farther\ncounterpart. To further enhance the intelligent ability of unmanned vehicles,\nin this paper, we construct a multimodal data collection platform based on 3\nmain types of sensors (Camera, LiDAR and Fisheye), which supports flexible\nsensor configurations to enable dynamic sight of view for ego vehicle, either\nglobal view or local view. Meanwhile, a large-scale multi-sensor dataset is\nbuilt, named RoboSense, to facilitate near-field scene understanding. RoboSense\ncontains more than 133K synchronized data with 1.4M 3D bounding box and IDs\nannotated in the full $360^{\\circ}$ view, forming 216K trajectories across 7.6K\ntemporal sequences. It has $270\\times$ and $18\\times$ as many annotations of\nnear-field obstacles within 5$m$ as the previous single-vehicle datasets such\nas KITTI and nuScenes. Moreover, we define a novel matching criterion for\nnear-field 3D perception and prediction metrics. Based on RoboSense, we\nformulate 6 popular tasks to facilitate the future development of related\nresearch, where the detailed data analysis as well as benchmarks are also\nprovided accordingly.\n","authors":["Haisheng Su","Feixiang Song","Cong Ma","Wei Wu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2408.15503v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16662v3","updated":"2024-09-15T15:51:00Z","published":"2024-08-29T16:05:22Z","title":"Space3D-Bench: Spatial 3D Question Answering Benchmark","summary":"  Answering questions about the spatial properties of the environment poses\nchallenges for existing language and vision foundation models due to a lack of\nunderstanding of the 3D world notably in terms of relationships between\nobjects. To push the field forward, multiple 3D Q&A datasets were proposed\nwhich, overall, provide a variety of questions, but they individually focus on\nparticular aspects of 3D reasoning or are limited in terms of data modalities.\nTo address this, we present Space3D-Bench - a collection of 1000 general\nspatial questions and answers related to scenes of the Replica dataset which\noffers a variety of data modalities: point clouds, posed RGB-D images,\nnavigation meshes and 3D object detections. To ensure that the questions cover\na wide range of 3D objectives, we propose an indoor spatial questions taxonomy\ninspired by geographic information systems and use it to balance the dataset\naccordingly. Moreover, we provide an assessment system that grades natural\nlanguage responses based on predefined ground-truth answers by leveraging a\nVision Language Model's comprehension of both text and images to compare the\nresponses with ground-truth textual information or relevant visual data.\nFinally, we introduce a baseline called RAG3D-Chat integrating the world\nunderstanding of foundation models with rich context retrieval, achieving an\naccuracy of 67% on the proposed dataset.\n","authors":["Emilia Szymanska","Mihai Dusmanu","Jan-Willem Buurlage","Mahdi Rad","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2408.16662v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09774v1","updated":"2024-09-15T15:46:03Z","published":"2024-09-15T15:46:03Z","title":"Generalizing Alignment Paradigm of Text-to-Image Generation with\n  Preferences through $f$-divergence Minimization","summary":"  Direct Preference Optimization (DPO) has recently expanded its successful\napplication from aligning large language models (LLMs) to aligning\ntext-to-image models with human preferences, which has generated considerable\ninterest within the community. However, we have observed that these approaches\nrely solely on minimizing the reverse Kullback-Leibler divergence during\nalignment process between the fine-tuned model and the reference model,\nneglecting the incorporation of other divergence constraints. In this study, we\nfocus on extending reverse Kullback-Leibler divergence in the alignment\nparadigm of text-to-image models to $f$-divergence, which aims to garner better\nalignment performance as well as good generation diversity. We provide the\ngeneralized formula of the alignment paradigm under the $f$-divergence\ncondition and thoroughly analyze the impact of different divergence constraints\non alignment process from the perspective of gradient fields. We conduct\ncomprehensive evaluation on image-text alignment performance, human value\nalignment performance and generation diversity performance under different\ndivergence constraints, and the results indicate that alignment based on\nJensen-Shannon divergence achieves the best trade-off among them. The option of\ndivergence employed for aligning text-to-image models significantly impacts the\ntrade-off between alignment performance (especially human value alignment) and\ngeneration diversity, which highlights the necessity of selecting an\nappropriate divergence for practical applications.\n","authors":["Haoyuan Sun","Bo Xia","Yongzhe Chang","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09774v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2409.09766v1","updated":"2024-09-15T15:32:29Z","published":"2024-09-15T15:32:29Z","title":"Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer\n  setting","summary":"  This study explores a workflow for automated segmentation of lesions in FDG\nand PSMA PET/CT images. Due to the substantial differences in image\ncharacteristics between FDG and PSMA, specialized preprocessing steps are\nrequired. Utilizing YOLOv8 for data classification, the FDG and PSMA images are\npreprocessed separately before feeding them into the segmentation models,\naiming to improve lesion segmentation accuracy. The study focuses on evaluating\nthe performance of automated segmentation workflow for multitracer PET images.\nThe findings are expected to provide critical insights for enhancing diagnostic\nworkflows and patient-specific treatment plans. Our code will be open-sourced\nand available at https://github.com/jiayiliu-pku/AP2024.\n","authors":["Qiaoyi Xue","Youdan Feng","Jiayi Liu","Tianming Xu","Kaixin Shen","Chuyun Shen","Yuhang Shi"],"pdf_url":"https://arxiv.org/pdf/2409.09766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04823v2","updated":"2024-09-15T15:28:01Z","published":"2024-08-09T02:36:56Z","title":"One Shot is Enough for Sequential Infrared Small Target Segmentation","summary":"  Infrared small target sequences exhibit strong similarities between frames\nand contain rich contextual information, which motivates us to achieve\nsequential infrared small target segmentation (IRSTS) with minimal data.\nInspired by the success of Segment Anything Model (SAM) across various\ndownstream tasks, we propose a one-shot and training-free method that perfectly\nadapts SAM's zero-shot generalization capability to sequential IRSTS.\nSpecifically, we first obtain a confidence map through local feature matching\n(LFM). The highest point in the confidence map is used as the prompt to replace\nthe manual prompt. Then, to address the over-segmentation issue caused by the\ndomain gap, we design the point prompt-centric focusing (PPCF) module.\nSubsequently, to prevent miss and false detections, we introduce the\ntriple-level ensemble (TLE) module to produce the final mask. Experiments\ndemonstrate that our method requires only one shot to achieve comparable\nperformance to state-of-the-art IRSTS methods and significantly outperforms\nother one-shot segmentation methods. Moreover, ablation studies confirm the\nrobustness of our method in the type of annotations and the selection of\nreference images.\n","authors":["Bingbing Dan","Meihui Li","Tao Tang","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.04823v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02815v3","updated":"2024-09-15T15:11:55Z","published":"2023-10-04T13:38:53Z","title":"CoBEV: Elevating Roadside 3D Object Detection with Depth and Height\n  Complementarity","summary":"  Roadside camera-driven 3D object detection is a crucial task in intelligent\ntransportation systems, which extends the perception range beyond the\nlimitations of vision-centric vehicles and enhances road safety. While previous\nstudies have limitations in using only depth or height information, we find\nboth depth and height matter and they are in fact complementary. The depth\nfeature encompasses precise geometric cues, whereas the height feature is\nprimarily focused on distinguishing between various categories of height\nintervals, essentially providing semantic context. This insight motivates the\ndevelopment of Complementary-BEV (CoBEV), a novel end-to-end monocular 3D\nobject detection framework that integrates depth and height to construct robust\nBEV representations. In essence, CoBEV estimates each pixel's depth and height\ndistribution and lifts the camera features into 3D space for lateral fusion\nusing the newly proposed two-stage complementary feature selection (CFS)\nmodule. A BEV feature distillation framework is also seamlessly integrated to\nfurther enhance the detection accuracy from the prior knowledge of the\nfusion-modal CoBEV teacher. We conduct extensive experiments on the public 3D\ndetection benchmarks of roadside camera-based DAIR-V2X-I and Rope3D, as well as\nthe private Supremind-Road dataset, demonstrating that CoBEV not only achieves\nthe accuracy of the new state-of-the-art, but also significantly advances the\nrobustness of previous methods in challenging long-distance scenarios and noisy\ncamera disturbance, and enhances generalization by a large margin in\nheterologous settings with drastic changes in scene and camera parameters. For\nthe first time, the vehicle AP score of a camera model reaches 80% on\nDAIR-V2X-I in terms of easy mode. The source code will be made publicly\navailable at https://github.com/MasterHow/CoBEV.\n","authors":["Hao Shi","Chengshan Pang","Jiaming Zhang","Kailun Yang","Yuhao Wu","Huajian Ni","Yining Lin","Rainer Stiefelhagen","Kaiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2310.02815v3.pdf","comment":"Accepted to IEEE Transactions on Image Processing (TIP). The source\n  code will be made publicly available at https://github.com/MasterHow/CoBEV"},{"id":"http://arxiv.org/abs/2409.09756v1","updated":"2024-09-15T14:58:20Z","published":"2024-09-15T14:58:20Z","title":"MesonGS: Post-training Compression of 3D Gaussians via Efficient\n  Attribute Transformation","summary":"  3D Gaussian Splatting demonstrates excellent quality and speed in novel view\nsynthesis. Nevertheless, the huge file size of the 3D Gaussians presents\nchallenges for transmission and storage. Current works design compact models to\nreplace the substantial volume and attributes of 3D Gaussians, along with\nintensive training to distill information. These endeavors demand considerable\ntraining time, presenting formidable hurdles for practical deployment. To this\nend, we propose MesonGS, a codec for post-training compression of 3D Gaussians.\nInitially, we introduce a measurement criterion that considers both\nview-dependent and view-independent factors to assess the impact of each\nGaussian point on the rendering output, enabling the removal of insignificant\npoints. Subsequently, we decrease the entropy of attributes through two\ntransformations that complement subsequent entropy coding techniques to enhance\nthe file compression rate. More specifically, we first replace rotation\nquaternions with Euler angles; then, we apply region adaptive hierarchical\ntransform to key attributes to reduce entropy. Lastly, we adopt finer-grained\nquantization to avoid excessive information loss. Moreover, a well-crafted\nfinetune scheme is devised to restore quality. Extensive experiments\ndemonstrate that MesonGS significantly reduces the size of 3D Gaussians while\npreserving competitive quality.\n","authors":["Shuzhao Xie","Weixiang Zhang","Chen Tang","Yunpeng Bai","Rongwei Lu","Shijia Ge","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09756v1.pdf","comment":"18 pages, 8 figures, ECCV 2024"},{"id":"http://arxiv.org/abs/2404.09942v2","updated":"2024-09-15T14:54:52Z","published":"2024-04-15T17:11:25Z","title":"Knowledge-enhanced Visual-Language Pretraining for Computational\n  Pathology","summary":"  In this paper, we consider the problem of visual representation learning for\ncomputational pathology, by exploiting large-scale image-text pairs gathered\nfrom public resources, along with the domain-specific knowledge in pathology.\nSpecifically, we make the following contributions: (i) We curate a pathology\nknowledge tree that consists of 50,470 informative attributes for 4,718\ndiseases requiring pathology diagnosis from 32 human tissues. To our knowledge,\nthis is the first comprehensive structured pathology knowledge base; (ii) We\ndevelop a knowledge-enhanced visual-language pretraining approach, where we\nfirst project pathology-specific knowledge into latent embedding space via a\nlanguage model, and use it to guide the visual representation learning; (iii)\nWe conduct thorough experiments to validate the effectiveness of our proposed\ncomponents, demonstrating significant performance improvement on various\ndownstream tasks, including cross-modal retrieval, zero-shot classification on\npathology patches, and zero-shot tumor subtyping on whole slide images (WSIs).\n","authors":["Xiao Zhou","Xiaoman Zhang","Chaoyi Wu","Ya Zhang","Weidi Xie","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2404.09942v2.pdf","comment":"ECCV2024(Oral)"},{"id":"http://arxiv.org/abs/2409.09754v1","updated":"2024-09-15T14:52:16Z","published":"2024-09-15T14:52:16Z","title":"Towards Single-Lens Controllable Depth-of-Field Imaging via All-in-Focus\n  Aberration Correction and Monocular Depth Estimation","summary":"  Controllable Depth-of-Field (DoF) imaging commonly produces amazing visual\neffects based on heavy and expensive high-end lenses. However, confronted with\nthe increasing demand for mobile scenarios, it is desirable to achieve a\nlightweight solution with Minimalist Optical Systems (MOS). This work centers\naround two major limitations of MOS, i.e., the severe optical aberrations and\nuncontrollable DoF, for achieving single-lens controllable DoF imaging via\ncomputational methods. A Depth-aware Controllable DoF Imaging (DCDI) framework\nis proposed equipped with All-in-Focus (AiF) aberration correction and\nmonocular depth estimation, where the recovered image and corresponding depth\nmap are utilized to produce imaging results under diverse DoFs of any high-end\nlens via patch-wise convolution. To address the depth-varying optical\ndegradation, we introduce a Depth-aware Degradation-adaptive Training (DA2T)\nscheme. At the dataset level, a Depth-aware Aberration MOS (DAMOS) dataset is\nestablished based on the simulation of Point Spread Functions (PSFs) under\ndifferent object distances. Additionally, we design two plug-and-play\ndepth-aware mechanisms to embed depth information into the aberration image\nrecovery for better tackling depth-aware degradation. Furthermore, we propose a\nstorage-efficient Omni-Lens-Field model to represent the 4D PSF library of\nvarious lenses. With the predicted depth map, recovered image, and depth-aware\nPSF map inferred by Omni-Lens-Field, single-lens controllable DoF imaging is\nachieved. Comprehensive experimental results demonstrate that the proposed\nframework enhances the recovery performance, and attains impressive single-lens\ncontrollable DoF imaging results, providing a seminal baseline for this field.\nThe source code and the established dataset will be publicly available at\nhttps://github.com/XiaolongQian/DCDI.\n","authors":["Xiaolong Qian","Qi Jiang","Yao Gao","Shaohua Gao","Zhonghua Yi","Lei Sun","Kai Wei","Haifeng Li","Kailun Yang","Kaiwei Wang","Jian Bai"],"pdf_url":"https://arxiv.org/pdf/2409.09754v1.pdf","comment":"The source code and the established dataset will be publicly\n  available at https://github.com/XiaolongQian/DCDI"},{"id":"http://arxiv.org/abs/2409.09753v1","updated":"2024-09-15T14:49:30Z","published":"2024-09-15T14:49:30Z","title":"DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation","summary":"  Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.\n","authors":["Shahriar Rifat","Jonathan Ashdown","Francesco Restuccia"],"pdf_url":"https://arxiv.org/pdf/2409.09753v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.11906v4","updated":"2024-09-15T14:42:27Z","published":"2023-04-24T08:29:45Z","title":"Transformer-based stereo-aware 3D object detection from binocular images","summary":"  Transformers have shown promising progress in various visual object detection\ntasks, including monocular 2D/3D detection and surround-view 3D detection. More\nimportantly, the attention mechanism in the Transformer model and the 3D\ninformation extraction in binocular stereo are both similarity-based. However,\ndirectly applying existing Transformer-based detectors to binocular stereo 3D\nobject detection leads to slow convergence and significant precision drops. We\nargue that a key cause of that defect is that existing Transformers ignore the\nbinocular-stereo-specific image correspondence information. In this paper, we\nexplore the model design of Transformers in binocular 3D object detection,\nfocusing particularly on extracting and encoding task-specific image\ncorrespondence information. To achieve this goal, we present TS3D, a\nTransformer-based Stereo-aware 3D object detector. In the TS3D, a\nDisparity-Aware Positional Encoding (DAPE) module is proposed to embed the\nimage correspondence information into stereo features. The correspondence is\nencoded as normalized sub-pixel-level disparity and is used in conjunction with\nsinusoidal 2D positional encoding to provide the 3D location information of the\nscene. To enrich multi-scale stereo features, we propose a Stereo Preserving\nFeature Pyramid Network (SPFPN). The SPFPN is designed to preserve the\ncorrespondence information while fusing intra-scale and aggregating cross-scale\nstereo features. Our proposed TS3D achieves a 41.29% Moderate Car detection\naverage precision on the KITTI test set and takes 88 ms to detect objects from\neach binocular image pair. It is competitive with advanced counterparts in\nterms of both precision and inference speed.\n","authors":["Hanqing Sun","Yanwei Pang","Jiale Cao","Jin Xie","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2304.11906v4.pdf","comment":"Accepted by IEEE T-ITS"},{"id":"http://arxiv.org/abs/2409.09748v1","updated":"2024-09-15T14:38:29Z","published":"2024-09-15T14:38:29Z","title":"Explore the Hallucination on Low-level Perception for MLLMs","summary":"  The rapid development of Multi-modality Large Language Models (MLLMs) has\nsignificantly influenced various aspects of industry and daily life, showcasing\nimpressive capabilities in visual perception and understanding. However, these\nmodels also exhibit hallucinations, which limit their reliability as AI\nsystems, especially in tasks involving low-level visual perception and\nunderstanding. We believe that hallucinations stem from a lack of explicit\nself-awareness in these models, which directly impacts their overall\nperformance. In this paper, we aim to define and evaluate the self-awareness of\nMLLMs in low-level visual perception and understanding tasks. To this end, we\npresent QL-Bench, a benchmark settings to simulate human responses to low-level\nvision, investigating self-awareness in low-level visual perception through\nvisual question answering related to low-level attributes such as clarity and\nlighting. Specifically, we construct the LLSAVisionQA dataset, comprising 2,990\nsingle images and 1,999 image pairs, each accompanied by an open-ended question\nabout its low-level features. Through the evaluation of 15 MLLMs, we\ndemonstrate that while some models exhibit robust low-level visual\ncapabilities, their self-awareness remains relatively underdeveloped. Notably,\nfor the same model, simpler questions are often answered more accurately than\ncomplex ones. However, self-awareness appears to improve when addressing more\nchallenging questions. We hope that our benchmark will motivate further\nresearch, particularly focused on enhancing the self-awareness of MLLMs in\ntasks involving low-level visual perception and understanding.\n","authors":["Yinan Sun","Zicheng Zhang","Haoning Wu","Xiaohong Liu","Weisi Lin","Guangtao Zhai","Xiongkuo Min"],"pdf_url":"https://arxiv.org/pdf/2409.09748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02003v3","updated":"2024-09-15T14:36:01Z","published":"2024-02-03T03:13:50Z","title":"GenFace: A Large-Scale Fine-Grained Face Forgery Benchmark and Cross\n  Appearance-Edge Learning","summary":"  The rapid advancement of photorealistic generators has reached a critical\njuncture where the discrepancy between authentic and manipulated images is\nincreasingly indistinguishable. Thus, benchmarking and advancing techniques\ndetecting digital manipulation become an urgent issue. Although there have been\na number of publicly available face forgery datasets, the forgery faces are\nmostly generated using GAN-based synthesis technology, which does not involve\nthe most recent technologies like diffusion. The diversity and quality of\nimages generated by diffusion models have been significantly improved and thus\na much more challenging face forgery dataset shall be used to evaluate SOTA\nforgery detection literature. In this paper, we propose a large-scale, diverse,\nand fine-grained high-fidelity dataset, namely GenFace, to facilitate the\nadvancement of deepfake detection, which contains a large number of forgery\nfaces generated by advanced generators such as the diffusion-based model and\nmore detailed labels about the manipulation approaches and adopted generators.\nIn addition to evaluating SOTA approaches on our benchmark, we design an\ninnovative cross appearance-edge learning (CAEL) detector to capture\nmulti-grained appearance and edge global representations, and detect\ndiscriminative and general forgery traces. Moreover, we devise an\nappearance-edge cross-attention (AECA) module to explore the various\nintegrations across two domains. Extensive experiment results and\nvisualizations show that our detection model outperforms the state of the arts\non different settings like cross-generator, cross-forgery, and cross-dataset\nevaluations. Code and datasets will be available at\n\\url{https://github.com/Jenine-321/GenFace\n","authors":["Yaning Zhang","Zitong Yu","Tianyi Wang","Xiaobin Huang","Linlin Shen","Zan Gao","Jianfeng Ren"],"pdf_url":"https://arxiv.org/pdf/2402.02003v3.pdf","comment":"Accepted by IEEE Transactions on Information Forensics and Security"},{"id":"http://arxiv.org/abs/2408.06899v3","updated":"2024-09-15T14:16:47Z","published":"2024-08-13T13:50:46Z","title":"EEPPR: Event-based Estimation of Periodic Phenomena Rate using\n  Correlation in 3D","summary":"  We present a novel method for measuring the rate of periodic phenomena (e.g.,\nrotation, flicker, and vibration), by an event camera, a device asynchronously\nreporting brightness changes at independently operating pixels with high\ntemporal resolution. The approach assumes that for a periodic phenomenon, a\nhighly similar set of events is generated within a spatio-temporal window at a\ntime difference corresponding to its period. The sets of similar events are\ndetected by a correlation in the spatio-temporal event stream space. The\nproposed method, EEPPR, is evaluated on a dataset of 12 sequences of periodic\nphenomena, i.e. flashing light and vibration, and periodic motion, e.g.,\nrotation, ranging from 3.2 Hz to 2 kHz (equivalent to 192 - 120 000 RPM). EEPPR\nsignificantly outperforms published methods on this dataset, achieving a mean\nrelative error of 0.1%, setting new state-of-the-art. The dataset and codes are\npublicly available on GitHub.\n","authors":["Jakub Kol√°≈ô","Radim ≈†petl√≠k","Ji≈ô√≠ Matas"],"pdf_url":"https://arxiv.org/pdf/2408.06899v3.pdf","comment":"8 pages, 2 figues, 3 tables"},{"id":"http://arxiv.org/abs/2409.09731v1","updated":"2024-09-15T13:32:24Z","published":"2024-09-15T13:32:24Z","title":"Learning Two-factor Representation for Magnetic Resonance Image\n  Super-resolution","summary":"  Magnetic Resonance Imaging (MRI) requires a trade-off between resolution,\nsignal-to-noise ratio, and scan time, making high-resolution (HR) acquisition\nchallenging. Therefore, super-resolution for MR image is a feasible solution.\nHowever, most existing methods face challenges in accurately learning a\ncontinuous volumetric representation from low-resolution image or require HR\nimage for supervision. To solve these challenges, we propose a novel method for\nMR image super-resolution based on two-factor representation. Specifically, we\nfactorize intensity signals into a linear combination of learnable basis and\ncoefficient factors, enabling efficient continuous volumetric representation\nfrom low-resolution MR image. Besides, we introduce a coordinate-based encoding\nto capture structural relationships between sparse voxels, facilitating smooth\ncompletion in unobserved regions. Experiments on BraTS 2019 and MSSEG 2016\ndatasets demonstrate that our method achieves state-of-the-art performance,\nproviding superior visual fidelity and robustness, particularly in large\nup-sampling scale MR image super-resolution.\n","authors":["Weifeng Wei","Heng Chen","Pengxiang Su"],"pdf_url":"https://arxiv.org/pdf/2409.09731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.13560v4","updated":"2024-09-15T13:26:45Z","published":"2024-01-24T16:17:23Z","title":"SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image\n  Segmentation","summary":"  The Transformer architecture has shown a remarkable ability in modeling\nglobal relationships. However, it poses a significant computational challenge\nwhen processing high-dimensional medical images. This hinders its development\nand widespread adoption in this task. Mamba, as a State Space Model (SSM),\nrecently emerged as a notable manner for long-range dependencies in sequential\nmodeling, excelling in natural language processing filed with its remarkable\nmemory efficiency and computational speed. Inspired by its success, we\nintroduce SegMamba, a novel 3D medical image \\textbf{Seg}mentation\n\\textbf{Mamba} model, designed to effectively capture long-range dependencies\nwithin whole volume features at every scale. Our SegMamba, in contrast to\nTransformer-based methods, excels in whole volume feature modeling from a state\nspace model standpoint, maintaining superior processing speed, even with volume\nfeatures at a resolution of {$64\\times 64\\times 64$}. Comprehensive experiments\non the BraTS2023 dataset demonstrate the effectiveness and efficiency of our\nSegMamba. The code for SegMamba is available at:\nhttps://github.com/ge-xing/SegMamba\n","authors":["Zhaohu Xing","Tian Ye","Yijun Yang","Guang Liu","Lei Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.13560v4.pdf","comment":"Code has released"},{"id":"http://arxiv.org/abs/2409.09725v1","updated":"2024-09-15T13:09:09Z","published":"2024-09-15T13:09:09Z","title":"Precise Pick-and-Place using Score-Based Diffusion Networks","summary":"  In this paper, we propose a novel coarse-to-fine continuous pose diffusion\nmethod to enhance the precision of pick-and-place operations within robotic\nmanipulation tasks. Leveraging the capabilities of diffusion networks, we\nfacilitate the accurate perception of object poses. This accurate perception\nenhances both pick-and-place success rates and overall manipulation precision.\nOur methodology utilizes a top-down RGB image projected from an RGB-D camera\nand adopts a coarse-to-fine architecture. This architecture enables efficient\nlearning of coarse and fine models. A distinguishing feature of our approach is\nits focus on continuous pose estimation, which enables more precise object\nmanipulation, particularly concerning rotational angles. In addition, we employ\npose and color augmentation techniques to enable effective training with\nlimited data. Through extensive experiments in simulated and real-world\nscenarios, as well as an ablation study, we comprehensively evaluate our\nproposed methodology. Taken together, the findings validate its effectiveness\nin achieving high-precision pick-and-place tasks.\n","authors":["Shih-Wei Guo","Tsu-Ching Hsiao","Yu-Lun Liu","Chun-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2409.09725v1.pdf","comment":"8 pages, 7 figures. Project webpage:\n  https://tony2guo.github.io/precise-pick-and-place/"},{"id":"http://arxiv.org/abs/2409.09724v1","updated":"2024-09-15T13:08:59Z","published":"2024-09-15T13:08:59Z","title":"MFCLIP: Multi-modal Fine-grained CLIP for Generalizable Diffusion Face\n  Forgery Detection","summary":"  The rapid development of photo-realistic face generation methods has raised\nsignificant concerns in society and academia, highlighting the urgent need for\nrobust and generalizable face forgery detection (FFD) techniques. Although\nexisting approaches mainly capture face forgery patterns using image modality,\nother modalities like fine-grained noises and texts are not fully explored,\nwhich limits the generalization capability of the model. In addition, most FFD\nmethods tend to identify facial images generated by GAN, but struggle to detect\nunseen diffusion-synthesized ones. To address the limitations, we aim to\nleverage the cutting-edge foundation model, contrastive language-image\npre-training (CLIP), to achieve generalizable diffusion face forgery detection\n(DFFD). In this paper, we propose a novel multi-modal fine-grained CLIP\n(MFCLIP) model, which mines comprehensive and fine-grained forgery traces\nacross image-noise modalities via language-guided face forgery representation\nlearning, to facilitate the advancement of DFFD. Specifically, we devise a\nfine-grained language encoder (FLE) that extracts fine global language features\nfrom hierarchical text prompts. We design a multi-modal vision encoder (MVE) to\ncapture global image forgery embeddings as well as fine-grained noise forgery\npatterns extracted from the richest patch, and integrate them to mine general\nvisual forgery traces. Moreover, we build an innovative plug-and-play sample\npair attention (SPA) method to emphasize relevant negative pairs and suppress\nirrelevant ones, allowing cross-modality sample pairs to conduct more flexible\nalignment. Extensive experiments and visualizations show that our model\noutperforms the state of the arts on different settings like cross-generator,\ncross-forgery, and cross-dataset evaluations.\n","authors":["Yaning Zhang","Tianyi Wang","Zitong Yu","Zan Gao","Linlin Shen","Shengyong Chen"],"pdf_url":"https://arxiv.org/pdf/2409.09724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09721v1","updated":"2024-09-15T13:02:14Z","published":"2024-09-15T13:02:14Z","title":"Finetuning CLIP to Reason about Pairwise Differences","summary":"  Vision-language models (VLMs) such as CLIP are trained via contrastive\nlearning between text and image pairs, resulting in aligned image and text\nembeddings that are useful for many downstream tasks. A notable drawback of\nCLIP, however, is that the resulting embedding space seems to lack some of the\nstructure of their purely text-based alternatives. For instance, while text\nembeddings have been long noted to satisfy \\emph{analogies} in embedding space\nusing vector arithmetic, CLIP has no such property. In this paper, we propose\nan approach to natively train CLIP in a contrastive manner to reason about\ndifferences in embedding space. We finetune CLIP so that the differences in\nimage embedding space correspond to \\emph{text descriptions of the image\ndifferences}, which we synthetically generate with large language models on\nimage-caption paired datasets. We first demonstrate that our approach yields\nsignificantly improved capabilities in ranking images by a certain attribute\n(e.g., elephants are larger than cats), which is useful in retrieval or\nconstructing attribute-based classifiers, and improved zeroshot classification\nperformance on many downstream image classification tasks. In addition, our\napproach enables a new mechanism for inference that we refer to as comparative\nprompting, where we leverage prior knowledge of text descriptions of\ndifferences between classes of interest, achieving even larger performance\ngains in classification. Finally, we illustrate that the resulting embeddings\nobey a larger degree of geometric properties in embedding space, such as in\ntext-to-image generation.\n","authors":["Dylan Sam","Devin Willmott","Joao D. Semedo","J. Zico Kolter"],"pdf_url":"https://arxiv.org/pdf/2409.09721v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2409.09716v1","updated":"2024-09-15T12:47:39Z","published":"2024-09-15T12:47:39Z","title":"Disentangling Visual Priors: Unsupervised Learning of Scene\n  Interpretations with Compositional Autoencoder","summary":"  Contemporary deep learning architectures lack principled means for capturing\nand handling fundamental visual concepts, like objects, shapes, geometric\ntransforms, and other higher-level structures. We propose a neurosymbolic\narchitecture that uses a domain-specific language to capture selected priors of\nimage formation, including object shape, appearance, categorization, and\ngeometric transforms. We express template programs in that language and learn\ntheir parameterization with features extracted from the scene by a\nconvolutional neural network. When executed, the parameterized program produces\ngeometric primitives which are rendered and assessed for correspondence with\nthe scene content and trained via auto-association with gradient. We confront\nour approach with a baseline method on a synthetic benchmark and demonstrate\nits capacity to disentangle selected aspects of the image formation process,\nlearn from small data, correct inference in the presence of noise, and\nout-of-sample generalization.\n","authors":["Krzysztof Krawiec","Antoni Nowinowski"],"pdf_url":"https://arxiv.org/pdf/2409.09716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09714v1","updated":"2024-09-15T12:45:15Z","published":"2024-09-15T12:45:15Z","title":"Pre-Training for 3D Hand Pose Estimation with Contrastive Learning on\n  Large-Scale Hand Images in the Wild","summary":"  We present a contrastive learning framework based on in-the-wild hand images\ntailored for pre-training 3D hand pose estimators, dubbed HandCLR. Pre-training\non large-scale images achieves promising results in various tasks, but prior 3D\nhand pose pre-training methods have not fully utilized the potential of diverse\nhand images accessible from in-the-wild videos. To facilitate scalable\npre-training, we first prepare an extensive pool of hand images from\nin-the-wild videos and design our method with contrastive learning.\nSpecifically, we collected over 2.0M hand images from recent human-centric\nvideos, such as 100DOH and Ego4D. To extract discriminative information from\nthese images, we focus on the similarity of hands; pairs of similar hand poses\noriginating from different samples, and propose a novel contrastive learning\nmethod that embeds similar hand pairs closer in the latent space. Our\nexperiments demonstrate that our method outperforms conventional contrastive\nlearning approaches that produce positive pairs sorely from a single image with\ndata augmentation. We achieve significant improvements over the\nstate-of-the-art method in various datasets, with gains of 15% on FreiHand, 10%\non DexYCB, and 4% on AssemblyHands.\n","authors":["Nie Lin","Takehiko Ohkawa","Mingfang Zhang","Yifei Huang","Ryosuke Furuta","Yoichi Sato"],"pdf_url":"https://arxiv.org/pdf/2409.09714v1.pdf","comment":"HANDS@ECCV24 (Extended Abstracts)"},{"id":"http://arxiv.org/abs/2409.01633v3","updated":"2024-09-15T12:17:03Z","published":"2024-09-03T06:04:39Z","title":"Dreaming is All You Need","summary":"  In classification tasks, achieving a harmonious balance between exploration\nand precision is of paramount importance. To this end, this research introduces\ntwo novel deep learning models, SleepNet and DreamNet, to strike this balance.\nSleepNet seamlessly integrates supervised learning with unsupervised ``sleep\"\nstages using pre-trained encoder models. Dedicated neurons within SleepNet are\nembedded in these unsupervised features, forming intermittent ``sleep\" blocks\nthat facilitate exploratory learning. Building upon the foundation of SleepNet,\nDreamNet employs full encoder-decoder frameworks to reconstruct the hidden\nstates, mimicking the human \"dreaming\" process. This reconstruction process\nenables further exploration and refinement of the learned representations.\nMoreover, the principle ideas of our SleepNet and DreamNet are generic and can\nbe applied to both computer vision and natural language processing downstream\ntasks. Through extensive empirical evaluations on diverse image and text\ndatasets, SleepNet and DreanNet have demonstrated superior performance compared\nto state-of-the-art models, showcasing the strengths of unsupervised\nexploration and supervised precision afforded by our innovative approaches.\n","authors":["Mingze Ni","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2409.01633v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09708v1","updated":"2024-09-15T12:14:24Z","published":"2024-09-15T12:14:24Z","title":"ELSA: Exploiting Layer-wise N:M Sparsity for Vision Transformer\n  Acceleration","summary":"  $N{:}M$ sparsity is an emerging model compression method supported by more\nand more accelerators to speed up sparse matrix multiplication in deep neural\nnetworks. Most existing $N{:}M$ sparsity methods compress neural networks with\na uniform setting for all layers in a network or heuristically determine the\nlayer-wise configuration by considering the number of parameters in each layer.\nHowever, very few methods have been designed for obtaining a layer-wise\ncustomized $N{:}M$ sparse configuration for vision transformers (ViTs), which\nusually consist of transformer blocks involving the same number of parameters.\nIn this work, to address the challenge of selecting suitable sparse\nconfiguration for ViTs on $N{:}M$ sparsity-supporting accelerators, we propose\nELSA, Exploiting Layer-wise $N{:}M$ Sparsity for ViTs. Considering not only all\n$N{:}M$ sparsity levels supported by a given accelerator but also the expected\nthroughput improvement, our methodology can reap the benefits of accelerators\nsupporting mixed sparsity by trading off negligible accuracy loss with both\nmemory usage and inference time reduction for ViT models. For instance, our\napproach achieves a noteworthy 2.9$\\times$ reduction in FLOPs for both Swin-B\nand DeiT-B with only a marginal degradation of accuracy on ImageNet. Our code\nwill be released upon paper acceptance.\n","authors":["Ning-Chi Huang","Chi-Chih Chang","Wei-Cheng Lin","Endri Taka","Diana Marculescu","Kai-Chiang Wu"],"pdf_url":"https://arxiv.org/pdf/2409.09708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09707v1","updated":"2024-09-15T12:14:19Z","published":"2024-09-15T12:14:19Z","title":"Synergistic Spotting and Recognition of Micro-Expression via Temporal\n  State Transition","summary":"  Micro-expressions are involuntary facial movements that cannot be consciously\ncontrolled, conveying subtle cues with substantial real-world applications. The\nanalysis of micro-expressions generally involves two main tasks: spotting\nmicro-expression intervals in long videos and recognizing the emotions\nassociated with these intervals. Previous deep learning methods have primarily\nrelied on classification networks utilizing sliding windows. However, fixed\nwindow sizes and window-level hard classification introduce numerous\nconstraints. Additionally, these methods have not fully exploited the potential\nof complementary pathways for spotting and recognition. In this paper, we\npresent a novel temporal state transition architecture grounded in the state\nspace model, which replaces conventional window-level classification with\nvideo-level regression. Furthermore, by leveraging the inherent connections\nbetween spotting and recognition tasks, we propose a synergistic strategy that\nenhances overall analysis performance. Extensive experiments demonstrate that\nour method achieves state-of-the-art performance. The codes and pre-trained\nmodels are available at https://github.com/zizheng-guo/ME-TST.\n","authors":["Bochao Zou","Zizheng Guo","Wenfeng Qin","Xin Li","Kangsheng Wang","Huimin Ma"],"pdf_url":"https://arxiv.org/pdf/2409.09707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.00436v2","updated":"2024-09-15T11:47:20Z","published":"2023-11-01T10:59:57Z","title":"Enhancing Traffic Object Detection in Variable Illumination with\n  RGB-Event Fusion","summary":"  Traffic object detection under variable illumination is challenging due to\nthe information loss caused by the limited dynamic range of conventional\nframe-based cameras. To address this issue, we introduce bio-inspired event\ncameras and propose a novel Structure-aware Fusion Network (SFNet) that\nextracts sharp and complete object structures from the event stream to\ncompensate for the lost information in images through cross-modality fusion,\nenabling the network to obtain illumination-robust representations for traffic\nobject detection. Specifically, to mitigate the sparsity or blurriness issues\narising from diverse motion states of traffic objects in fixed-interval event\nsampling methods, we propose the Reliable Structure Generation Network (RSGNet)\nto generate Speed Invariant Frames (SIF), ensuring the integrity and sharpness\nof object structures. Next, we design a novel Adaptive Feature Complement\nModule (AFCM) which guides the adaptive fusion of two modality features to\ncompensate for the information loss in the images by perceiving the global\nlightness distribution of the images, thereby generating illumination-robust\nrepresentations. Finally, considering the lack of large-scale and high-quality\nannotations in the existing event-based object detection datasets, we build a\nDSEC-Det dataset, which consists of 53 sequences with 63,931 images and more\nthan 208,000 labels for 8 classes. Extensive experimental results demonstrate\nthat our proposed SFNet can overcome the perceptual boundaries of conventional\ncameras and outperform the frame-based method by 8.0% in mAP50 and 5.9% in\nmAP50:95. Our code and dataset will be available at\nhttps://github.com/YN-Yang/SFNet.\n","authors":["Zhanwen Liu","Nan Yang","Yang Wang","Yuke Li","Xiangmo Zhao","Fei-Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2311.00436v2.pdf","comment":"Accepted by IEEE T-ITS"},{"id":"http://arxiv.org/abs/2409.05122v2","updated":"2024-09-15T10:32:46Z","published":"2024-09-08T15:02:25Z","title":"PMT: Progressive Mean Teacher via Exploring Temporal Consistency for\n  Semi-Supervised Medical Image Segmentation","summary":"  Semi-supervised learning has emerged as a widely adopted technique in the\nfield of medical image segmentation. The existing works either focuses on the\nconstruction of consistency constraints or the generation of pseudo labels to\nprovide high-quality supervisory signals, whose main challenge mainly comes\nfrom how to keep the continuous improvement of model capabilities. In this\npaper, we propose a simple yet effective semi-supervised learning framework,\ntermed Progressive Mean Teachers (PMT), for medical image segmentation, whose\ngoal is to generate high-fidelity pseudo labels by learning robust and diverse\nfeatures in the training process. Specifically, our PMT employs a standard mean\nteacher to penalize the consistency of the current state and utilizes two sets\nof MT architectures for co-training. The two sets of MT architectures are\nindividually updated for prolonged periods to maintain stable model diversity\nestablished through performance gaps generated by iteration differences.\nAdditionally, a difference-driven alignment regularizer is employed to expedite\nthe alignment of lagging models with the representation capabilities of leading\nmodels. Furthermore, a simple yet effective pseudo-label filtering algorithm is\nemployed for facile evaluation of models and selection of high-fidelity\npseudo-labels outputted when models are operating at high performance for\nco-training purposes. Experimental results on two datasets with different\nmodalities, i.e., CT and MRI, demonstrate that our method outperforms the\nstate-of-the-art medical image segmentation approaches across various\ndimensions. The code is available at https://github.com/Axi404/PMT.\n","authors":["Ning Gao","Sanping Zhou","Le Wang","Nanning Zheng"],"pdf_url":"https://arxiv.org/pdf/2409.05122v2.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2409.09681v1","updated":"2024-09-15T10:10:13Z","published":"2024-09-15T10:10:13Z","title":"E-Commerce Inpainting with Mask Guidance in Controlnet for Reducing\n  Overcompletion","summary":"  E-commerce image generation has always been one of the core demands in the\ne-commerce field. The goal is to restore the missing background that matches\nthe main product given. In the post-AIGC era, diffusion models are primarily\nused to generate product images, achieving impressive results. This paper\nsystematically analyzes and addresses a core pain point in diffusion model\ngeneration: overcompletion, which refers to the difficulty in maintaining\nproduct features. We propose two solutions: 1. Using an instance mask\nfine-tuned inpainting model to mitigate this phenomenon; 2. Adopting a\ntrain-free mask guidance approach, which incorporates refined product masks as\nconstraints when combining ControlNet and UNet to generate the main product,\nthereby avoiding overcompletion of the product. Our method has achieved\npromising results in practical applications and we hope it can serve as an\ninspiring technical report in this field.\n","authors":["Guandong Li"],"pdf_url":"https://arxiv.org/pdf/2409.09681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09680v1","updated":"2024-09-15T10:06:06Z","published":"2024-09-15T10:06:06Z","title":"Reliable Multi-View Learning with Conformal Prediction for Aortic\n  Stenosis Classification in Echocardiography","summary":"  The fundamental problem with ultrasound-guided diagnosis is that the acquired\nimages are often 2-D cross-sections of a 3-D anatomy, potentially missing\nimportant anatomical details. This limitation leads to challenges in ultrasound\nechocardiography, such as poor visualization of heart valves or foreshortening\nof ventricles. Clinicians must interpret these images with inherent\nuncertainty, a nuance absent in machine learning's one-hot labels. We propose\nRe-Training for Uncertainty (RT4U), a data-centric method to introduce\nuncertainty to weakly informative inputs in the training set. This simple\napproach can be incorporated to existing state-of-the-art aortic stenosis\nclassification methods to further improve their accuracy. When combined with\nconformal prediction techniques, RT4U can yield adaptively sized prediction\nsets which are guaranteed to contain the ground truth class to a high accuracy.\nWe validate the effectiveness of RT4U on three diverse datasets: a public\n(TMED-2) and a private AS dataset, along with a CIFAR-10-derived toy dataset.\nResults show improvement on all the datasets.\n","authors":["Ang Nan Gu","Michael Tsang","Hooman Vaseli","Teresa Tsang","Purang Abolmaesumi"],"pdf_url":"https://arxiv.org/pdf/2409.09680v1.pdf","comment":"This preprint has not undergone any post-submission improvements or\n  corrections. The Version of Record of this contribution is published in:\n  International Conference on Medical Image Computing and Computer-Assisted\n  Intervention (MICCAI), Springer (2024) under the same title"},{"id":"http://arxiv.org/abs/2409.09678v1","updated":"2024-09-15T10:04:44Z","published":"2024-09-15T10:04:44Z","title":"A Comprehensive Methodological Survey of Human Activity Recognition\n  Across Divers Data Modalities","summary":"  Human Activity Recognition (HAR) systems aim to understand human behaviour\nand assign a label to each action, attracting significant attention in computer\nvision due to their wide range of applications. HAR can leverage various data\nmodalities, such as RGB images and video, skeleton, depth, infrared, point\ncloud, event stream, audio, acceleration, and radar signals. Each modality\nprovides unique and complementary information suited to different application\nscenarios. Consequently, numerous studies have investigated diverse approaches\nfor HAR using these modalities. This paper presents a comprehensive survey of\nthe latest advancements in HAR from 2014 to 2024, focusing on machine learning\n(ML) and deep learning (DL) approaches categorized by input data modalities. We\nreview both single-modality and multi-modality techniques, highlighting\nfusion-based and co-learning frameworks. Additionally, we cover advancements in\nhand-crafted action features, methods for recognizing human-object\ninteractions, and activity detection. Our survey includes a detailed dataset\ndescription for each modality and a summary of the latest HAR systems, offering\ncomparative results on benchmark datasets. Finally, we provide insightful\nobservations and propose effective future research directions in HAR.\n","authors":["Jungpil Shin","Najmul Hassan","Abu Saleh Musa Miah1","Satoshi Nishimura"],"pdf_url":"https://arxiv.org/pdf/2409.09678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09673v1","updated":"2024-09-15T09:34:15Z","published":"2024-09-15T09:34:15Z","title":"SITSMamba for Crop Classification based on Satellite Image Time Series","summary":"  Satellite image time series (SITS) data provides continuous observations over\ntime, allowing for the tracking of vegetation changes and growth patterns\nthroughout the seasons and years. Numerous deep learning (DL) approaches using\nSITS for crop classification have emerged recently, with the latest approaches\nadopting Transformer for SITS classification. However, the quadratic complexity\nof self-attention in Transformer poses challenges for classifying long time\nseries. While the cutting-edge Mamba architecture has demonstrated strength in\nvarious domains, including remote sensing image interpretation, its capacity to\nlearn temporal representations in SITS data remains unexplored. Moreover, the\nexisting SITS classification methods often depend solely on crop labels as\nsupervision signals, which fails to fully exploit the temporal information. In\nthis paper, we proposed a Satellite Image Time Series Mamba (SITSMamba) method\nfor crop classification based on remote sensing time series data. The proposed\nSITSMamba contains a spatial encoder based on Convolutional Neural Networks\n(CNN) and a Mamba-based temporal encoder. To exploit richer temporal\ninformation from SITS, we design two branches of decoder used for different\ntasks. The first branch is a crop Classification Branch (CBranch), which\nincludes a ConvBlock to decode the feature to a crop map. The second branch is\na SITS Reconstruction Branch that uses a Linear layer to transform the encoded\nfeature to predict the original input values. Furthermore, we design a\nPositional Weight (PW) applied to the RBranch to help the model learn rich\nlatent knowledge from SITS. We also design two weighting factors to control the\nbalance of the two branches during training. The code of SITSMamba is available\nat: https://github.com/XiaoleiQinn/SITSMamba.\n","authors":["Xiaolei Qin","Xin Su","Liangpei Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.09673v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09670v1","updated":"2024-09-15T08:58:26Z","published":"2024-09-15T08:58:26Z","title":"Unsupervised Hyperspectral and Multispectral Image Blind Fusion Based on\n  Deep Tucker Decomposition Network with Spatial-Spectral Manifold Learning","summary":"  Hyperspectral and multispectral image fusion aims to generate high spectral\nand spatial resolution hyperspectral images (HR-HSI) by fusing high-resolution\nmultispectral images (HR-MSI) and low-resolution hyperspectral images (LR-HSI).\nHowever, existing fusion methods encounter challenges such as unknown\ndegradation parameters, incomplete exploitation of the correlation between\nhigh-dimensional structures and deep image features. To overcome these issues,\nin this article, an unsupervised blind fusion method for hyperspectral and\nmultispectral images based on Tucker decomposition and spatial spectral\nmanifold learning (DTDNML) is proposed. We design a novel deep Tucker\ndecomposition network that maps LR-HSI and HR-MSI into a consistent feature\nspace, achieving reconstruction through decoders with shared parameter. To\nbetter exploit and fuse spatial-spectral features in the data, we design a core\ntensor fusion network that incorporates a spatial spectral attention mechanism\nfor aligning and fusing features at different scales. Furthermore, to enhance\nthe capacity in capturing global information, a Laplacian-based\nspatial-spectral manifold constraints is introduced in shared-decoders.\nSufficient experiments have validated that this method enhances the accuracy\nand efficiency of hyperspectral and multispectral fusion on different remote\nsensing datasets. The source code is available at\nhttps://github.com/Shawn-H-Wang/DTDNML.\n","authors":["He Wang","Yang Xu","Zebin Wu","Zhihui Wei"],"pdf_url":"https://arxiv.org/pdf/2409.09670v1.pdf","comment":"Accepted by TNNLS 2024"},{"id":"http://arxiv.org/abs/2409.09668v1","updated":"2024-09-15T08:43:18Z","published":"2024-09-15T08:43:18Z","title":"EditBoard: Towards A Comprehensive Evaluation Benchmark for Text-based\n  Video Editing Models","summary":"  The rapid development of diffusion models has significantly advanced\nAI-generated content (AIGC), particularly in Text-to-Image (T2I) and\nText-to-Video (T2V) generation. Text-based video editing, leveraging these\ngenerative capabilities, has emerged as a promising field, enabling precise\nmodifications to videos based on text prompts. Despite the proliferation of\ninnovative video editing models, there is a conspicuous lack of comprehensive\nevaluation benchmarks that holistically assess these models' performance across\nvarious dimensions. Existing evaluations are limited and inconsistent,\ntypically summarizing overall performance with a single score, which obscures\nmodels' effectiveness on individual editing tasks. To address this gap, we\npropose EditBoard, the first comprehensive evaluation benchmark for text-based\nvideo editing models. EditBoard encompasses nine automatic metrics across four\ndimensions, evaluating models on four task categories and introducing three new\nmetrics to assess fidelity. This task-oriented benchmark facilitates objective\nevaluation by detailing model performance and providing insights into each\nmodel's strengths and weaknesses. By open-sourcing EditBoard, we aim to\nstandardize evaluation and advance the development of robust video editing\nmodels.\n","authors":["Yupeng Chen","Penglin Chen","Xiaoyu Zhang","Yixian Huang","Qian Xie"],"pdf_url":"https://arxiv.org/pdf/2409.09668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01380v2","updated":"2024-09-15T07:47:34Z","published":"2024-06-03T14:42:38Z","title":"Convolutional Unscented Kalman Filter for Multi-Object Tracking with\n  Outliers","summary":"  Multi-object tracking (MOT) is an essential technique for navigation in\nautonomous driving. In tracking-by-detection systems, biases, false positives,\nand misses, which are referred to as outliers, are inevitable due to complex\ntraffic scenarios. Recent tracking methods are based on filtering algorithms\nthat overlook these outliers, leading to reduced tracking accuracy or even loss\nof the objects trajectory. To handle this challenge, we adopt a probabilistic\nperspective, regarding the generation of outliers as misspecification between\nthe actual distribution of measurement data and the nominal measurement model\nused for filtering. We further demonstrate that, by designing a convolutional\noperation, we can mitigate this misspecification. Incorporating this operation\ninto the widely used unscented Kalman filter (UKF) in commonly adopted tracking\nalgorithms, we derive a variant of the UKF that is robust to outliers, called\nthe convolutional UKF (ConvUKF). We show that ConvUKF maintains the Gaussian\nconjugate property, thus allowing for real-time tracking. We also prove that\nConvUKF has a bounded tracking error in the presence of outliers, which implies\nrobust stability. The experimental results on the KITTI and nuScenes datasets\nshow improved accuracy compared to representative baseline algorithms for MOT\ntasks.\n","authors":["Shiqi Liu","Wenhan Cao","Chang Liu","Tianyi Zhang","Shengbo Eben Li"],"pdf_url":"https://arxiv.org/pdf/2406.01380v2.pdf","comment":"IEEE Transactions on Intelligent Vehicles"},{"id":"http://arxiv.org/abs/2409.09649v1","updated":"2024-09-15T07:46:18Z","published":"2024-09-15T07:46:18Z","title":"SparX: A Sparse Cross-Layer Connection Mechanism for Hierarchical Vision\n  Mamba and Transformer Networks","summary":"  Due to the capability of dynamic state space models (SSMs) in capturing\nlong-range dependencies with near-linear computational complexity, Mamba has\nshown notable performance in NLP tasks. This has inspired the rapid development\nof Mamba-based vision models, resulting in promising results in visual\nrecognition tasks. However, such models are not capable of distilling features\nacross layers through feature aggregation, interaction, and selection.\nMoreover, existing cross-layer feature aggregation methods designed for CNNs or\nViTs are not practical in Mamba-based models due to high computational costs.\nTherefore, this paper aims to introduce an efficient cross-layer feature\naggregation mechanism for Mamba-based vision backbone networks. Inspired by the\nRetinal Ganglion Cells (RGCs) in the human visual system, we propose a new\nsparse cross-layer connection mechanism termed SparX to effectively improve\ncross-layer feature interaction and reuse. Specifically, we build two different\ntypes of network layers: ganglion layers and normal layers. The former has\nhigher connectivity and complexity, enabling multi-layer feature aggregation\nand interaction in an input-dependent manner. In contrast, the latter has lower\nconnectivity and complexity. By interleaving these two types of layers, we\ndesign a new vision backbone network with sparsely cross-connected layers,\nachieving an excellent trade-off among model size, computational cost, memory\ncost, and accuracy in comparison to its counterparts. For instance, with fewer\nparameters, SparX-Mamba-T improves the top-1 accuracy of VMamba-T from 82.5% to\n83.5%, while SparX-Swin-T achieves a 1.3% increase in top-1 accuracy compared\nto Swin-T. Extensive experimental results demonstrate that our new connection\nmechanism possesses both superior performance and generalization capabilities\non various vision tasks.\n","authors":["Meng Lou","Yunxiang Fu","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2409.09649v1.pdf","comment":"Code will be publicly available at: https://github.com/LMMMEng/SparX"},{"id":"http://arxiv.org/abs/2406.19280v2","updated":"2024-09-15T07:25:49Z","published":"2024-06-27T15:50:41Z","title":"HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale","summary":"  The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.\n","authors":["Junying Chen","Chi Gui","Ruyi Ouyang","Anningzhe Gao","Shunian Chen","Guiming Hardy Chen","Xidong Wang","Ruifei Zhang","Zhenyang Cai","Ke Ji","Guangjun Yu","Xiang Wan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09635v1","updated":"2024-09-15T07:12:33Z","published":"2024-09-15T07:12:33Z","title":"A Novel Framework For Text Detection From Natural Scene Images With\n  Complex Background","summary":"  Recognizing texts from camera images is a known hard problem because of the\ndifficulties in text detection from the varied and complicated background. In\nthis paper we propose a novel and efficient method to detect text region from\nimages with complex background using Wavelet Transforms. The framework uses\nWavelet Transformation of the original image in its grayscale form followed by\nSub-band filtering. Then Region clustering technique is applied using centroids\nof the regions, further Bounding box is fitted to each region thus identifying\nthe text regions. This method is much sophisticated and efficient than the\nprevious methods as it doesn't stick to a particular font size of the text\nthus, making it generalized. The sample set used for experimental purpose\nconsists of 50 images with varying backgrounds. Images with edge prominence are\nconsidered. Furthermore, our method can be easily customized for applications\nwith different scopes.\n","authors":["Basavaraj Kaladagi","Jagadeesh Pujari"],"pdf_url":"https://arxiv.org/pdf/2409.09635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09628v1","updated":"2024-09-15T06:43:03Z","published":"2024-09-15T06:43:03Z","title":"Can Large Language Models Grasp Event Signals? Exploring Pure Zero-Shot\n  Event-based Recognition","summary":"  Recent advancements in event-based zero-shot object recognition have\ndemonstrated promising results. However, these methods heavily depend on\nextensive training and are inherently constrained by the characteristics of\nCLIP. To the best of our knowledge, this research is the first study to explore\nthe understanding capabilities of large language models (LLMs) for event-based\nvisual content. We demonstrate that LLMs can achieve event-based object\nrecognition without additional training or fine-tuning in conjunction with\nCLIP, effectively enabling pure zero-shot event-based recognition.\nParticularly, we evaluate the ability of GPT-4o / 4turbo and two other\nopen-source LLMs to directly recognize event-based visual content. Extensive\nexperiments are conducted across three benchmark datasets, systematically\nassessing the recognition accuracy of these models. The results show that LLMs,\nespecially when enhanced with well-designed prompts, significantly improve\nevent-based zero-shot recognition performance. Notably, GPT-4o outperforms the\ncompared models and exceeds the recognition accuracy of state-of-the-art\nevent-based zero-shot methods on N-ImageNet by five orders of magnitude. The\nimplementation of this paper is available at\n\\url{https://github.com/ChrisYu-Zz/Pure-event-based-recognition-based-LLM}.\n","authors":["Zongyou Yu","Qiang Qu","Xiaoming Chen","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05531v2","updated":"2024-09-15T06:37:55Z","published":"2024-09-09T11:43:35Z","title":"HMAFlow: Learning More Accurate Optical Flow via Hierarchical Motion\n  Field Alignment","summary":"  Optical flow estimation is a fundamental and long-standing visual task. In\nthis work, we present a novel method, dubbed HMAFlow, to improve optical flow\nestimation in challenging scenes, particularly those involving small objects.\nThe proposed model mainly consists of two core components: a Hierarchical\nMotion Field Alignment (HMA) module and a Correlation Self-Attention (CSA)\nmodule. In addition, we rebuild 4D cost volumes by employing a Multi-Scale\nCorrelation Search (MCS) layer and replacing average pooling in common cost\nvolumes with a search strategy utilizing multiple search ranges. Experimental\nresults demonstrate that our model achieves the best generalization performance\ncompared to other state-of-the-art methods. Specifically, compared with RAFT,\nour method achieves relative error reductions of 14.2% and 3.4% on the clean\npass and final pass of the Sintel online benchmark, respectively. On the KITTI\ntest benchmark, HMAFlow surpasses RAFT and GMA in the Fl-all metric by relative\nmargins of 6.8% and 7.7%, respectively. To facilitate future research, our code\nwill be made available at https://github.com/BooTurbo/HMAFlow.\n","authors":["Dianbo Ma","Kousuke Imamura","Ziyan Gao","Xiangjie Wang","Satoshi Yamane"],"pdf_url":"https://arxiv.org/pdf/2409.05531v2.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.09616v1","updated":"2024-09-15T05:32:48Z","published":"2024-09-15T05:32:48Z","title":"Enhancing Weakly-Supervised Object Detection on Static Images through\n  (Hallucinated) Motion","summary":"  While motion has garnered attention in various tasks, its potential as a\nmodality for weakly-supervised object detection (WSOD) in static images remains\nunexplored. Our study introduces an approach to enhance WSOD methods by\nintegrating motion information. This method involves leveraging hallucinated\nmotion from static images to improve WSOD on image datasets, utilizing a\nSiamese network for enhanced representation learning with motion, addressing\ncamera motion through motion normalization, and selectively training images\nbased on object motion. Experimental validation on the COCO and YouTube-BB\ndatasets demonstrates improvements over a state-of-the-art method.\n","authors":["Cagri Gungor","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2409.09616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15841v2","updated":"2024-09-15T05:00:18Z","published":"2024-07-22T17:58:04Z","title":"SlowFast-LLaVA: A Strong Training-Free Baseline for Video Large Language\n  Models","summary":"  We propose SlowFast-LLaVA (or SF-LLaVA for short), a training-free video\nlarge language model (LLM) that can jointly capture detailed spatial semantics\nand long-range temporal context without exceeding the token budget of commonly\nused LLMs. This is realized by using a two-stream SlowFast design of inputs for\nVideo LLMs to aggregate features from sampled frames in an effective way.\nSpecifically, the Slow pathway extracts features at a low frame rate while\nkeeping as much spatial detail as possible (e.g., with 12x24 tokens), and the\nFast pathway operates on a high frame rate but uses a larger spatial pooling\nstride (e.g., downsampling 6x) to focus on the motion cues. As a result, this\ndesign allows us to adequately capture both spatial and temporal features that\nare beneficial for detailed video understanding. Experimental results show that\nSF-LLaVA outperforms existing training-free methods on a wide range of video\ntasks. On some benchmarks, it achieves comparable or even better performance\ncompared to state-of-the-art Video LLMs that are fine-tuned on video datasets.\nCode has been made available at: https://github.com/apple/ml-slowfast-llava.\n","authors":["Mingze Xu","Mingfei Gao","Zhe Gan","Hong-You Chen","Zhengfeng Lai","Haiming Gang","Kai Kang","Afshin Dehghan"],"pdf_url":"https://arxiv.org/pdf/2407.15841v2.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2409.09611v1","updated":"2024-09-15T04:43:00Z","published":"2024-09-15T04:43:00Z","title":"Integrating Audio Narrations to Strengthen Domain Generalization in\n  Multimodal First-Person Action Recognition","summary":"  First-person activity recognition is rapidly growing due to the widespread\nuse of wearable cameras but faces challenges from domain shifts across\ndifferent environments, such as varying objects or background scenes. We\npropose a multimodal framework that improves domain generalization by\nintegrating motion, audio, and appearance features. Key contributions include\nanalyzing the resilience of audio and motion features to domain shifts, using\naudio narrations for enhanced audio-text alignment, and applying consistency\nratings between audio and visual narrations to optimize the impact of audio in\nrecognition during training. Our approach achieves state-of-the-art performance\non the ARGO1M dataset, effectively generalizing across unseen scenarios and\nlocations.\n","authors":["Cagri Gungor","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2409.09611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09610v1","updated":"2024-09-15T04:34:38Z","published":"2024-09-15T04:34:38Z","title":"TextureDiffusion: Target Prompt Disentangled Editing for Various Texture\n  Transfer","summary":"  Recently, text-guided image editing has achieved significant success.\nHowever, existing methods can only apply simple textures like wood or gold when\nchanging the texture of an object. Complex textures such as cloud or fire pose\na challenge. This limitation stems from that the target prompt needs to contain\nboth the input image content and <texture>, restricting the texture\nrepresentation. In this paper, we propose TextureDiffusion, a tuning-free image\nediting method applied to various texture transfer. Initially, the target\nprompt is directly set to \"<texture>\", making the texture disentangled from the\ninput image content to enhance texture representation. Subsequently, query\nfeatures in self-attention and features in residual blocks are utilized to\npreserve the structure of the input image. Finally, to maintain the background,\nwe introduce an edit localization technique which blends the self-attention\nresults and the intermediate latents. Comprehensive experiments demonstrate\nthat TextureDiffusion can harmoniously transfer various textures with excellent\nstructure and background preservation.\n","authors":["Zihan Su","Junhao Zhuang","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2409.09610v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17326v7","updated":"2024-09-15T04:15:20Z","published":"2023-05-27T02:04:25Z","title":"Matrix Information Theory for Self-Supervised Learning","summary":"  The maximum entropy encoding framework provides a unified perspective for\nmany non-contrastive learning methods like SimSiam, Barlow Twins, and MEC.\nInspired by this framework, we introduce Matrix-SSL, a novel approach that\nleverages matrix information theory to interpret the maximum entropy encoding\nloss as matrix uniformity loss. Furthermore, Matrix-SSL enhances the maximum\nentropy encoding method by seamlessly incorporating matrix alignment loss,\ndirectly aligning covariance matrices in different branches. Experimental\nresults reveal that Matrix-SSL outperforms state-of-the-art methods on the\nImageNet dataset under linear evaluation settings and on MS-COCO for transfer\nlearning tasks. Specifically, when performing transfer learning tasks on\nMS-COCO, our method outperforms previous SOTA methods such as MoCo v2 and BYOL\nup to 3.3% with only 400 epochs compared to 800 epochs pre-training. We also\ntry to introduce representation learning into the language modeling regime by\nfine-tuning a 7B model using matrix cross-entropy loss, with a margin of 3.1%\non the GSM8K dataset over the standard cross-entropy loss. Code available at\nhttps://github.com/yifanzhang-pro/Matrix-SSL.\n","authors":["Yifan Zhang","Zhiquan Tan","Jingqin Yang","Weiran Huang","Yang Yuan"],"pdf_url":"https://arxiv.org/pdf/2305.17326v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09605v1","updated":"2024-09-15T04:09:12Z","published":"2024-09-15T04:09:12Z","title":"DreamMover: Leveraging the Prior of Diffusion Models for Image\n  Interpolation with Large Motion","summary":"  We study the problem of generating intermediate images from image pairs with\nlarge motion while maintaining semantic consistency. Due to the large motion,\nthe intermediate semantic information may be absent in input images. Existing\nmethods either limit to small motion or focus on topologically similar objects,\nleading to artifacts and inconsistency in the interpolation results. To\novercome this challenge, we delve into pre-trained image diffusion models for\ntheir capabilities in semantic cognition and representations, ensuring\nconsistent expression of the absent intermediate semantic representations with\nthe input. To this end, we propose DreamMover, a novel image interpolation\nframework with three main components: 1) A natural flow estimator based on the\ndiffusion model that can implicitly reason about the semantic correspondence\nbetween two images. 2) To avoid the loss of detailed information during fusion,\nour key insight is to fuse information in two parts, high-level space and\nlow-level space. 3) To enhance the consistency between the generated images and\ninput, we propose the self-attention concatenation and replacement approach.\nLastly, we present a challenging benchmark dataset InterpBench to evaluate the\nsemantic consistency of generated results. Extensive experiments demonstrate\nthe effectiveness of our method. Our project is available at\nhttps://dreamm0ver.github.io .\n","authors":["Liao Shen","Tianqi Liu","Huiqiang Sun","Xinyi Ye","Baopu Li","Jianming Zhang","Zhiguo Cao"],"pdf_url":"https://arxiv.org/pdf/2409.09605v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2405.20606v2","updated":"2024-09-15T03:32:03Z","published":"2024-05-31T03:40:15Z","title":"Vision-Language Meets the Skeleton: Progressively Distillation with\n  Cross-Modal Knowledge for 3D Action Representation Learning","summary":"  Skeleton-based action representation learning aims to interpret and\nunderstand human behaviors by encoding the skeleton sequences, which can be\ncategorized into two primary training paradigms: supervised learning and\nself-supervised learning. However, the former one-hot classification requires\nlabor-intensive predefined action categories annotations, while the latter\ninvolves skeleton transformations (e.g., cropping) in the pretext tasks that\nmay impair the skeleton structure. To address these challenges, we introduce a\nnovel skeleton-based training framework (C$^2$VL) based on Cross-modal\nContrastive learning that uses the progressive distillation to learn\ntask-agnostic human skeleton action representation from the Vision-Language\nknowledge prompts. Specifically, we establish the vision-language action\nconcept space through vision-language knowledge prompts generated by\npre-trained large multimodal models (LMMs), which enrich the fine-grained\ndetails that the skeleton action space lacks. Moreover, we propose the\nintra-modal self-similarity and inter-modal cross-consistency softened targets\nin the cross-modal representation learning process to progressively control and\nguide the degree of pulling vision-language knowledge prompts and corresponding\nskeletons closer. These soft instance discrimination and self-knowledge\ndistillation strategies contribute to the learning of better skeleton-based\naction representations from the noisy skeleton-vision-language pairs. During\nthe inference phase, our method requires only the skeleton data as the input\nfor action recognition and no longer for vision-language prompts. Extensive\nexperiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets demonstrate\nthat our method outperforms the previous methods and achieves state-of-the-art\nresults. Code is available at: https://github.com/cseeyangchen/C2VL.\n","authors":["Yang Chen","Tian He","Junfeng Fu","Ling Wang","Jingcai Guo","Ting Hu","Hong Cheng"],"pdf_url":"https://arxiv.org/pdf/2405.20606v2.pdf","comment":"Accepted by IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2207.06817v3","updated":"2024-09-15T02:57:43Z","published":"2022-07-14T10:53:53Z","title":"Pseudo-Labeling Based Practical Semi-Supervised Meta-Training for\n  Few-Shot Learning","summary":"  Most existing few-shot learning (FSL) methods require a large amount of\nlabeled data in meta-training, which is a major limit. To reduce the\nrequirement of labels, a semi-supervised meta-training (SSMT) setting has been\nproposed for FSL, which includes only a few labeled samples and numbers of\nunlabeled samples in base classes. However, existing methods under this setting\nrequire class-aware sample selection from the unlabeled set, which violates the\nassumption of unlabeled set. In this paper, we propose a practical\nsemi-supervised meta-training setting with truly unlabeled data to facilitate\nthe applications of FSL in realistic scenarios. To better utilize both the\nlabeled and truly unlabeled data, we propose a simple and effective\nmeta-training framework, called pseudo-labeling based meta-learning (PLML).\nFirstly, we train a classifier via common semi-supervised learning (SSL) and\nuse it to obtain the pseudo-labels of unlabeled data. Then we build few-shot\ntasks from labeled and pseudo-labeled data and design a novel finetuning method\nwith feature smoothing and noise suppression to better learn the FSL model from\nnoise labels. Surprisingly, through extensive experiments across two FSL\ndatasets, we find that this simple meta-training framework effectively prevents\nthe performance degradation of various FSL models under limited labeled data,\nand also significantly outperforms the state-of-the-art SSMT models. Besides,\nbenefiting from meta-training, our method also improves two representative SSL\nalgorithms as well.\n","authors":["Xingping Dong","Tianran Ouyang","Shengcai Liao","Bo Du","Ling Shao"],"pdf_url":"https://arxiv.org/pdf/2207.06817v3.pdf","comment":"This paper has been accepted by IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2409.09593v1","updated":"2024-09-15T02:42:25Z","published":"2024-09-15T02:42:25Z","title":"One-Shot Learning for Pose-Guided Person Image Synthesis in the Wild","summary":"  Current Pose-Guided Person Image Synthesis (PGPIS) methods depend heavily on\nlarge amounts of labeled triplet data to train the generator in a supervised\nmanner. However, they often falter when applied to in-the-wild samples,\nprimarily due to the distribution gap between the training datasets and\nreal-world test samples. While some researchers aim to enhance model\ngeneralizability through sophisticated training procedures, advanced\narchitectures, or by creating more diverse datasets, we adopt the test-time\nfine-tuning paradigm to customize a pre-trained Text2Image (T2I) model.\nHowever, naively applying test-time tuning results in inconsistencies in facial\nidentities and appearance attributes. To address this, we introduce a Visual\nConsistency Module (VCM), which enhances appearance consistency by combining\nthe face, text, and image embedding. Our approach, named OnePoseTrans, requires\nonly a single source image to generate high-quality pose transfer results,\noffering greater stability than state-of-the-art data-driven methods. For each\ntest case, OnePoseTrans customizes a model in around 48 seconds with an NVIDIA\nV100 GPU.\n","authors":["Dongqi Fan","Tao Chen","Mingjie Wang","Rui Ma","Qiang Tang","Zili Yi","Qian Wang","Liang Chang"],"pdf_url":"https://arxiv.org/pdf/2409.09593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09588v1","updated":"2024-09-15T02:26:17Z","published":"2024-09-15T02:26:17Z","title":"GLCONet: Learning Multi-source Perception Representation for Camouflaged\n  Object Detection","summary":"  Recently, biological perception has been a powerful tool for handling the\ncamouflaged object detection (COD) task. However, most existing methods are\nheavily dependent on the local spatial information of diverse scales from\nconvolutional operations to optimize initial features. A commonly neglected\npoint in these methods is the long-range dependencies between feature pixels\nfrom different scale spaces that can help the model build a global structure of\nthe object, inducing a more precise image representation. In this paper, we\npropose a novel Global-Local Collaborative Optimization Network, called\nGLCONet. Technically, we first design a collaborative optimization strategy\nfrom the perspective of multi-source perception to simultaneously model the\nlocal details and global long-range relationships, which can provide features\nwith abundant discriminative information to boost the accuracy in detecting\ncamouflaged objects. Furthermore, we introduce an adjacent reverse decoder that\ncontains cross-layer aggregation and reverse optimization to integrate\ncomplementary information from different levels for generating high-quality\nrepresentations. Extensive experiments demonstrate that the proposed GLCONet\nmethod with different backbones can effectively activate potentially\nsignificant pixels in an image, outperforming twenty state-of-the-art methods\non three public COD datasets. The source code is available at:\n\\https://github.com/CSYSI/GLCONet.\n","authors":["Yanguang Sun","Hanyu Xuan","Jian Yang","Lei Luo"],"pdf_url":"https://arxiv.org/pdf/2409.09588v1.pdf","comment":"Accepted at TNNLS 2024"},{"id":"http://arxiv.org/abs/2409.09582v1","updated":"2024-09-15T01:54:17Z","published":"2024-09-15T01:54:17Z","title":"NEVLP: Noise-Robust Framework for Efficient Vision-Language Pre-training","summary":"  The success of Vision Language Models (VLMs) on various vision-language tasks\nheavily relies on pre-training with large scale web-crawled datasets. However,\nthe noisy and incomplete nature of web data makes dataset scale crucial for\nperformance, rendering end-to-end training increasingly prohibitive. In this\npaper, we propose NEVLP, a noise-robust framework for efficient vision-language\npre-training that requires less pre-training data. Specifically, we bridge the\nmodality gap between a frozen image encoder and a large language model with a\ntransformer and introduce two innovative learning strategies: noise-adaptive\nlearning and concept-enhanced learning to mitigate the impact of noise. In\nnoise-adaptive learning, we estimate the noise probability of each image-text\npair based on the transformer's memorization effect and employ noise-adaptive\nregularization on image-text contrastive learning to condition cross-modal\nalignment. In concept-enhanced learning, we enrich incomplete text by\nincorporating visual concepts (objects in the image) to provide prior\ninformation about existing objects for image-text matching and image-grounded\ntext generation, thereby mitigating text incompletion. Our framework\neffectively utilizes noisy web data and achieves state-of-the-art performance\nwith less pre-training data across a wide range of vision-language tasks,\nincluding image-text retrieval, image captioning, and visual question\nanswering.\n","authors":["Yiyi Tao","Zhuoyue Wang","Hang Zhang","Lun Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09569v1","updated":"2024-09-15T01:09:55Z","published":"2024-09-15T01:09:55Z","title":"Bias Begets Bias: The Impact of Biased Embeddings on Diffusion Models","summary":"  With the growing adoption of Text-to-Image (TTI) systems, the social biases\nof these models have come under increased scrutiny. Herein we conduct a\nsystematic investigation of one such source of bias for diffusion models:\nembedding spaces. First, because traditional classifier-based fairness\ndefinitions require true labels not present in generative modeling, we propose\nstatistical group fairness criteria based on a model's internal representation\nof the world. Using these definitions, we demonstrate theoretically and\nempirically that an unbiased text embedding space for input prompts is a\nnecessary condition for representationally balanced diffusion models, meaning\nthe distribution of generated images satisfy diversity requirements with\nrespect to protected attributes. Next, we investigate the impact of biased\nembeddings on evaluating the alignment between generated images and prompts, a\nprocess which is commonly used to assess diffusion models. We find that biased\nmultimodal embeddings like CLIP can result in lower alignment scores for\nrepresentationally balanced TTI models, thus rewarding unfair behavior.\nFinally, we develop a theoretical framework through which biases in alignment\nevaluation can be studied and propose bias mitigation methods. By specifically\nadapting the perspective of embedding spaces, we establish new fairness\nconditions for diffusion model development and evaluation.\n","authors":["Sahil Kuchlous","Marvin Li","Jeffrey G. Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09569v1.pdf","comment":"19 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.17961v2","updated":"2024-09-15T00:55:55Z","published":"2024-04-27T17:16:45Z","title":"Random Walk on Pixel Manifolds for Anomaly Segmentation of Complex\n  Driving Scenes","summary":"  In anomaly segmentation for complex driving scenes, state-of-the-art\napproaches utilize anomaly scoring functions to calculate anomaly scores. For\nthese functions, accurately predicting the logits of inlier classes for each\npixel is crucial for precisely inferring the anomaly score. However, in\nreal-world driving scenarios, the diversity of scenes often results in\ndistorted manifolds of pixel embeddings in the space. This effect is not\nconducive to directly using the pixel embeddings for the logit prediction\nduring inference, a concern overlooked by existing methods. To address this\nproblem, we propose a novel method called Random Walk on Pixel Manifolds\n(RWPM). RWPM utilizes random walks to reveal the intrinsic relationships among\npixels to refine the pixel embeddings. The refined pixel embeddings alleviate\nthe distortion of manifolds, improving the accuracy of anomaly scores. Our\nextensive experiments show that RWPM consistently improve the performance of\nthe existing anomaly segmentation methods and achieve the best results. Code is\navailable at: \\url{https://github.com/ZelongZeng/RWPM}.\n","authors":["Zelong Zeng","Kaname Tomite"],"pdf_url":"https://arxiv.org/pdf/2404.17961v2.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2409.09566v1","updated":"2024-09-15T00:53:44Z","published":"2024-09-15T00:53:44Z","title":"Learning Transferable Features for Implicit Neural Representations","summary":"  Implicit neural representations (INRs) have demonstrated success in a variety\nof applications, including inverse problems and neural rendering. An INR is\ntypically trained to capture one signal of interest, resulting in learned\nneural features that are highly attuned to that signal. Assumed to be less\ngeneralizable, we explore the aspect of transferability of such learned neural\nfeatures for fitting similar signals. We introduce a new INR training\nframework, STRAINER that learns transferrable features for fitting INRs to new\nsignals from a given distribution, faster and with better reconstruction\nquality. Owing to the sequential layer-wise affine operations in an INR, we\npropose to learn transferable representations by sharing initial encoder layers\nacross multiple INRs with independent decoder layers. At test time, the learned\nencoder representations are transferred as initialization for an otherwise\nrandomly initialized INR. We find STRAINER to yield extremely powerful\ninitialization for fitting images from the same domain and allow for $\\approx\n+10dB$ gain in signal quality early on compared to an untrained INR itself.\nSTRAINER also provides a simple way to encode data-driven priors in INRs. We\nevaluate STRAINER on multiple in-domain and out-of-domain signal fitting tasks\nand inverse problems and further provide detailed analysis and discussion on\nthe transferability of STRAINER's features. Our demo can be accessed at\nhttps://colab.research.google.com/drive/1fBZAwqE8C_lrRPAe-hQZJTWrMJuAKtG2?usp=sharing .\n","authors":["Kushal Vyas","Ahmed Imtiaz Humayun","Aniket Dashpute","Richard G. Baraniuk","Ashok Veeraraghavan","Guha Balakrishnan"],"pdf_url":"https://arxiv.org/pdf/2409.09566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09564v1","updated":"2024-09-15T00:38:34Z","published":"2024-09-15T00:38:34Z","title":"TG-LLaVA: Text Guided LLaVA via Learnable Latent Embeddings","summary":"  Currently, inspired by the success of vision-language models (VLMs), an\nincreasing number of researchers are focusing on improving VLMs and have\nachieved promising results. However, most existing methods concentrate on\noptimizing the connector and enhancing the language model component, while\nneglecting improvements to the vision encoder itself. In contrast, we propose\nText Guided LLaVA (TG-LLaVA) in this paper, which optimizes VLMs by guiding the\nvision encoder with text, offering a new and orthogonal optimization direction.\nSpecifically, inspired by the purpose-driven logic inherent in human behavior,\nwe use learnable latent embeddings as a bridge to analyze textual instruction\nand add the analysis results to the vision encoder as guidance, refining it.\nSubsequently, another set of latent embeddings extracts additional detailed\ntext-guided information from high-resolution local patches as auxiliary\ninformation. Finally, with the guidance of text, the vision encoder can extract\ntext-related features, similar to how humans focus on the most relevant parts\nof an image when considering a question. This results in generating better\nanswers. Experiments on various datasets validate the effectiveness of the\nproposed method. Remarkably, without the need for additional training data, our\npropsoed method can bring more benefits to the baseline (LLaVA-1.5) compared\nwith other concurrent methods. Furthermore, the proposed method consistently\nbrings improvement in different settings.\n","authors":["Dawei Yan","Pengcheng Li","Yang Li","Hao Chen","Qingguo Chen","Weihua Luo","Wei Dong","Qingsen Yan","Haokui Zhang","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2409.09564v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.09881v1","updated":"2024-09-15T22:22:27Z","published":"2024-09-15T22:22:27Z","title":"Proximal Ranking Policy Optimization for Practical Safety in\n  Counterfactual Learning to Rank","summary":"  Counterfactual learning to rank (CLTR) can be risky and, in various\ncircumstances, can produce sub-optimal models that hurt performance when\ndeployed. Safe CLTR was introduced to mitigate these risks when using inverse\npropensity scoring to correct for position bias. However, the existing safety\nmeasure for CLTR is not applicable to state-of-the-art CLTR methods, cannot\nhandle trust bias, and relies on specific assumptions about user behavior. We\npropose a novel approach, proximal ranking policy optimization (PRPO), that\nprovides safety in deployment without assumptions about user behavior. PRPO\nremoves incentives for learning ranking behavior that is too dissimilar to a\nsafe ranking model. Thereby, PRPO imposes a limit on how much learned models\ncan degrade performance metrics, without relying on any specific user\nassumptions. Our experiments show that PRPO provides higher performance than\nthe existing safe inverse propensity scoring approach. PRPO always maintains\nsafety, even in maximally adversarial situations. By avoiding assumptions, PRPO\nis the first method with unconditional safety in deployment that translates to\nrobust safety for real-world applications.\n","authors":["Shashank Gupta","Harrie Oosterhuis","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2409.09881v1.pdf","comment":"Accepted at the CONSEQUENCES 2024 workshop, co-located with ACM\n  RecSys 2024"},{"id":"http://arxiv.org/abs/2409.09795v1","updated":"2024-09-15T17:05:35Z","published":"2024-09-15T17:05:35Z","title":"CROSS-JEM: Accurate and Efficient Cross-encoders for Short-text Ranking\n  Tasks","summary":"  Ranking a set of items based on their relevance to a given query is a core\nproblem in search and recommendation. Transformer-based ranking models are the\nstate-of-the-art approaches for such tasks, but they score each query-item\nindependently, ignoring the joint context of other relevant items. This leads\nto sub-optimal ranking accuracy and high computational costs. In response, we\npropose Cross-encoders with Joint Efficient Modeling (CROSS-JEM), a novel\nranking approach that enables transformer-based models to jointly score\nmultiple items for a query, maximizing parameter utilization. CROSS-JEM\nleverages (a) redundancies and token overlaps to jointly score multiple items,\nthat are typically short-text phrases arising in search and recommendations,\nand (b) a novel training objective that models ranking probabilities. CROSS-JEM\nachieves state-of-the-art accuracy and over 4x lower ranking latency over\nstandard cross-encoders. Our contributions are threefold: (i) we highlight the\ngap between the ranking application's need for scoring thousands of items per\nquery and the limited capabilities of current cross-encoders; (ii) we introduce\nCROSS-JEM for joint efficient scoring of multiple items per query; and (iii) we\ndemonstrate state-of-the-art accuracy on standard public datasets and a\nproprietary dataset. CROSS-JEM opens up new directions for designing tailored\nearly-attention-based ranking models that incorporate strict production\nconstraints such as item multiplicity and latency.\n","authors":["Bhawna Paliwal","Deepak Saini","Mudit Dhawan","Siddarth Asokan","Nagarajan Natarajan","Surbhi Aggarwal","Pankaj Malhotra","Jian Jiao","Manik Varma"],"pdf_url":"https://arxiv.org/pdf/2409.09795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05033v2","updated":"2024-09-15T13:29:18Z","published":"2024-09-08T08:57:12Z","title":"A Survey on Diffusion Models for Recommender Systems","summary":"  While traditional recommendation techniques have made significant strides in\nthe past decades, they still suffer from limited generalization performance\ncaused by factors like inadequate collaborative signals, weak latent\nrepresentations, and noisy data. In response, diffusion models (DMs) have\nemerged as promising solutions for recommender systems due to their robust\ngenerative capabilities, solid theoretical foundations, and improved training\nstability. To this end, in this paper, we present the first comprehensive\nsurvey on diffusion models for recommendation, and draw a bird's-eye view from\nthe perspective of the whole pipeline in real-world recommender systems. We\nsystematically categorize existing research works into three primary domains:\n(1) diffusion for data engineering & encoding, focusing on data augmentation\nand representation enhancement; (2) diffusion as recommender models, employing\ndiffusion models to directly estimate user preferences and rank items; and (3)\ndiffusion for content presentation, utilizing diffusion models to generate\npersonalized content such as fashion and advertisement creatives. Our taxonomy\nhighlights the unique strengths of diffusion models in capturing complex data\ndistributions and generating high-quality, diverse samples that closely align\nwith user preferences. We also summarize the core characteristics of the\nadapting diffusion models for recommendation, and further identify key areas\nfor future exploration, which helps establish a roadmap for researchers and\npractitioners seeking to advance recommender systems through the innovative\napplication of diffusion models. To further facilitate the research community\nof recommender systems based on diffusion models, we actively maintain a GitHub\nrepository for papers and other related resources in this rising direction\nhttps://github.com/CHIANGEL/Awesome-Diffusion-for-RecSys.\n","authors":["Jianghao Lin","Jiaqi Liu","Jiachen Zhu","Yunjia Xi","Chengkai Liu","Yangtian Zhang","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.05033v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2409.09722v1","updated":"2024-09-15T13:02:50Z","published":"2024-09-15T13:02:50Z","title":"Measuring Recency Bias In Sequential Recommendation Systems","summary":"  Recency bias in a sequential recommendation system refers to the overly high\nemphasis placed on recent items within a user session. This bias can diminish\nthe serendipity of recommendations and hinder the system's ability to capture\nusers' long-term interests, leading to user disengagement. We propose a simple\nyet effective novel metric specifically designed to quantify recency bias. Our\nfindings also demonstrate that high recency bias measured in our proposed\nmetric adversely impacts recommendation performance too, and mitigating it\nresults in improved recommendation performances across all models evaluated in\nour experiments, thus highlighting the importance of measuring recency bias.\n","authors":["Jeonglyul Oh","Sungzoon Cho"],"pdf_url":"https://arxiv.org/pdf/2409.09722v1.pdf","comment":"Accepted at the CONSEQUENCES '24 workshop, co-located with ACM RecSys\n  '24"},{"id":"http://arxiv.org/abs/2409.09704v1","updated":"2024-09-15T11:53:24Z","published":"2024-09-15T11:53:24Z","title":"AlpaPICO: Extraction of PICO Frames from Clinical Trial Documents Using\n  LLMs","summary":"  In recent years, there has been a surge in the publication of clinical trial\nreports, making it challenging to conduct systematic reviews. Automatically\nextracting Population, Intervention, Comparator, and Outcome (PICO) from\nclinical trial studies can alleviate the traditionally time-consuming process\nof manually scrutinizing systematic reviews. Existing approaches of PICO frame\nextraction involves supervised approach that relies on the existence of\nmanually annotated data points in the form of BIO label tagging. Recent\napproaches, such as In-Context Learning (ICL), which has been shown to be\neffective for a number of downstream NLP tasks, require the use of labeled\nexamples. In this work, we adopt ICL strategy by employing the pretrained\nknowledge of Large Language Models (LLMs), gathered during the pretraining\nphase of an LLM, to automatically extract the PICO-related terminologies from\nclinical trial documents in unsupervised set up to bypass the availability of\nlarge number of annotated data instances. Additionally, to showcase the highest\neffectiveness of LLM in oracle scenario where large number of annotated samples\nare available, we adopt the instruction tuning strategy by employing Low Rank\nAdaptation (LORA) to conduct the training of gigantic model in low resource\nenvironment for the PICO frame extraction task. Our empirical results show that\nour proposed ICL-based framework produces comparable results on all the version\nof EBM-NLP datasets and the proposed instruction tuned version of our framework\nproduces state-of-the-art results on all the different EBM-NLP datasets. Our\nproject is available at \\url{https://github.com/shrimonmuke0202/AlpaPICO.git}.\n","authors":["Madhusudan Ghosh","Shrimon Mukherjee","Asmit Ganguly","Partha Basuchowdhuri","Sudip Kumar Naskar","Debasis Ganguly"],"pdf_url":"https://arxiv.org/pdf/2409.09704v1.pdf","comment":"Accepted at Methods"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2409.09896v1","updated":"2024-09-15T23:32:04Z","published":"2024-09-15T23:32:04Z","title":"GRIN: Zero-Shot Metric Depth with Pixel-Level Diffusion","summary":"  3D reconstruction from a single image is a long-standing problem in computer\nvision. Learning-based methods address its inherent scale ambiguity by\nleveraging increasingly large labeled and unlabeled datasets, to produce\ngeometric priors capable of generating accurate predictions across domains. As\na result, state of the art approaches show impressive performance in zero-shot\nrelative and metric depth estimation. Recently, diffusion models have exhibited\nremarkable scalability and generalizable properties in their learned\nrepresentations. However, because these models repurpose tools originally\ndesigned for image generation, they can only operate on dense ground-truth,\nwhich is not available for most depth labels, especially in real-world\nsettings. In this paper we present GRIN, an efficient diffusion model designed\nto ingest sparse unstructured training data. We use image features with 3D\ngeometric positional encodings to condition the diffusion process both globally\nand locally, generating depth predictions at a pixel-level. With comprehensive\nexperiments across eight indoor and outdoor datasets, we show that GRIN\nestablishes a new state of the art in zero-shot metric monocular depth\nestimation even when trained from scratch.\n","authors":["Vitor Guizilini","Pavel Tokmakov","Achal Dave","Rares Ambrus"],"pdf_url":"https://arxiv.org/pdf/2409.09896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09894v1","updated":"2024-09-15T23:22:21Z","published":"2024-09-15T23:22:21Z","title":"Estimating Wage Disparities Using Foundation Models","summary":"  One thread of empirical work in social science focuses on decomposing group\ndifferences in outcomes into unexplained components and components explained by\nobservable factors. In this paper, we study gender wage decompositions, which\nrequire estimating the portion of the gender wage gap explained by career\nhistories of workers. Classical methods for decomposing the wage gap employ\nsimple predictive models of wages which condition on a small set of simple\nsummaries of labor history. The problem is that these predictive models cannot\ntake advantage of the full complexity of a worker's history, and the resulting\ndecompositions thus suffer from omitted variable bias (OVB), where covariates\nthat are correlated with both gender and wages are not included in the model.\nHere we explore an alternative methodology for wage gap decomposition that\nemploys powerful foundation models, such as large language models, as the\npredictive engine. Foundation models excel at making accurate predictions from\ncomplex, high-dimensional inputs. We use a custom-built foundation model,\ndesigned to predict wages from full labor histories, to decompose the gender\nwage gap. We prove that the way such models are usually trained might still\nlead to OVB, but develop fine-tuning algorithms that empirically mitigate this\nissue. Our model captures a richer representation of career history than simple\nmodels and predicts wages more accurately. In detail, we first provide a novel\nset of conditions under which an estimator of the wage gap based on a\nfine-tuned foundation model is $\\sqrt{n}$-consistent. Building on the theory,\nwe then propose methods for fine-tuning foundation models that minimize OVB.\nUsing data from the Panel Study of Income Dynamics, we find that history\nexplains more of the gender wage gap than standard econometric models can\nmeasure, and we identify elements of history that are important for reducing\nOVB.\n","authors":["Keyon Vafa","Susan Athey","David M. Blei"],"pdf_url":"https://arxiv.org/pdf/2409.09894v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09892v1","updated":"2024-09-15T23:08:31Z","published":"2024-09-15T23:08:31Z","title":"Dynamic Fraud Detection: Integrating Reinforcement Learning into Graph\n  Neural Networks","summary":"  Financial fraud refers to the act of obtaining financial benefits through\ndishonest means. Such behavior not only disrupts the order of the financial\nmarket but also harms economic and social development and breeds other illegal\nand criminal activities. With the popularization of the internet and online\npayment methods, many fraudulent activities and money laundering behaviors in\nlife have shifted from offline to online, posing a great challenge to\nregulatory authorities. How to efficiently detect these financial fraud\nactivities has become an urgent issue that needs to be resolved. Graph neural\nnetworks are a type of deep learning model that can utilize the interactive\nrelationships within graph structures, and they have been widely applied in the\nfield of fraud detection. However, there are still some issues. First,\nfraudulent activities only account for a very small part of transaction\ntransfers, leading to an inevitable problem of label imbalance in fraud\ndetection. At the same time, fraudsters often disguise their behavior, which\ncan have a negative impact on the final prediction results. In addition,\nexisting research has overlooked the importance of balancing neighbor\ninformation and central node information. For example, when the central node\nhas too many neighbors, the features of the central node itself are often\nneglected. Finally, fraud activities and patterns are constantly changing over\ntime, so considering the dynamic evolution of graph edge relationships is also\nvery important.\n","authors":["Yuxin Dong","Jianhua Yao","Jiajing Wang","Yingbin Liang","Shuhan Liao","Minheng Xiao"],"pdf_url":"https://arxiv.org/pdf/2409.09892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.12169v5","updated":"2024-09-15T23:04:30Z","published":"2023-07-22T21:18:41Z","title":"Rail-only: A Low-Cost High-Performance Network for Training LLMs with\n  Trillion Parameters","summary":"  This paper presents a low-cost network architecture for training large\nlanguage models (LLMs) at hyperscale. We study the optimal parallelization\nstrategy of LLMs and propose a novel datacenter network design tailored to\nLLM's unique communication pattern. We show that LLM training generates sparse\ncommunication patterns in the network and, therefore, does not require\nany-to-any full-bisection network to complete efficiently. As a result, our\ndesign eliminates the spine layer in traditional GPU clusters. We name this\ndesign a Rail-only network and demonstrate that it achieves the same training\nperformance while reducing the network cost by 38% to 77% and network power\nconsumption by 37% to 75% compared to a conventional GPU datacenter. Our\narchitecture also supports Mixture-of-Expert (MoE) models with all-to-all\ncommunication through forwarding, with only 8.2% to 11.2% completion time\noverhead for all-to-all traffic. We study the failure robustness of Rail-only\nnetworks and provide insights into the performance impact of different network\nand training parameters.\n","authors":["Weiyang Wang","Manya Ghobadi","Kayvon Shakeri","Ying Zhang","Naader Hasani"],"pdf_url":"https://arxiv.org/pdf/2307.12169v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09888v1","updated":"2024-09-15T22:52:46Z","published":"2024-09-15T22:52:46Z","title":"Flexible Diffusion Scopes with Parameterized Laplacian for Heterophilic\n  Graph Learning","summary":"  The ability of Graph Neural Networks (GNNs) to capture long-range and global\ntopology information is limited by the scope of conventional graph Laplacian,\nleading to unsatisfactory performance on some datasets, particularly on\nheterophilic graphs. To address this limitation, we propose a new class of\nparameterized Laplacian matrices, which provably offers more flexibility in\ncontrolling the diffusion distance between nodes than the conventional graph\nLaplacian, allowing long-range information to be adaptively captured through\ndiffusion on graph. Specifically, we first prove that the diffusion distance\nand spectral distance on graph have an order-preserving relationship. With this\nresult, we demonstrate that the parameterized Laplacian can accelerate the\ndiffusion of long-range information, and the parameters in the Laplacian enable\nflexibility of the diffusion scopes. Based on the theoretical results, we\npropose topology-guided rewiring mechanism to capture helpful long-range\nneighborhood information for heterophilic graphs. With this mechanism and the\nnew Laplacian, we propose two GNNs with flexible diffusion scopes: namely the\nParameterized Diffusion based Graph Convolutional Networks (PD-GCN) and Graph\nAttention Networks (PD-GAT). Synthetic experiments reveal the high correlations\nbetween the parameters of the new Laplacian and the performance of\nparameterized GNNs under various graph homophily levels, which verifies that\nour new proposed GNNs indeed have the ability to adjust the parameters to\nadaptively capture the global information for different levels of heterophilic\ngraphs. They also outperform the state-of-the-art (SOTA) models on 6 out of 7\nreal-world benchmark datasets, which further confirms their superiority.\n","authors":["Qincheng Lu","Jiaqi Zhu","Sitao Luan","Xiao-Wen Chang"],"pdf_url":"https://arxiv.org/pdf/2409.09888v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2403.01475"},{"id":"http://arxiv.org/abs/2409.09887v1","updated":"2024-09-15T22:50:57Z","published":"2024-09-15T22:50:57Z","title":"Leiden-Fusion Partitioning Method for Effective Distributed Training of\n  Graph Embeddings","summary":"  In the area of large-scale training of graph embeddings, effective training\nframeworks and partitioning methods are critical for handling large networks.\nHowever, they face two major challenges: 1) existing synchronized distributed\nframeworks require continuous communication to access information from other\nmachines, and 2) the inability of current partitioning methods to ensure that\nsubgraphs remain connected components without isolated nodes, which is\nessential for effective training of GNNs since training relies on information\naggregation from neighboring nodes. To address these issues, we introduce a\nnovel partitioning method, named Leiden-Fusion, designed for large-scale\ntraining of graphs with minimal communication. Our method extends the Leiden\ncommunity detection algorithm with a greedy algorithm that merges the smallest\ncommunities with highly connected neighboring communities. Our method\nguarantees that, for an initially connected graph, each partition is a densely\nconnected subgraph with no isolated nodes. After obtaining the partitions, we\ntrain a GNN for each partition independently, and finally integrate all\nembeddings for node classification tasks, which significantly reduces the need\nfor network communication and enhances the efficiency of distributed graph\ntraining. We demonstrate the effectiveness of our method through extensive\nevaluations on several benchmark datasets, achieving high efficiency while\npreserving the quality of the graph embeddings for node classification tasks.\n","authors":["Yuhe Bai","Camelia Constantin","Hubert Naacke"],"pdf_url":"https://arxiv.org/pdf/2409.09887v1.pdf","comment":"Accepted at the 2024 European Conference on Machine Learning and\n  Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD 2024)"},{"id":"http://arxiv.org/abs/2407.18353v2","updated":"2024-09-15T22:27:16Z","published":"2024-07-25T19:39:03Z","title":"Privacy-Preserving Hierarchical Model-Distributed Inference","summary":"  This paper focuses on designing a privacy-preserving Machine Learning (ML)\ninference protocol for a hierarchical setup, where clients own/generate data,\nmodel owners (cloud servers) have a pre-trained ML model, and edge servers\nperform ML inference on clients' data using the cloud server's ML model. Our\ngoal is to speed up ML inference while providing privacy to both data and the\nML model. Our approach (i) uses model-distributed inference (model\nparallelization) at the edge servers and (ii) reduces the amount of\ncommunication to/from the cloud server. Our privacy-preserving hierarchical\nmodel-distributed inference, privateMDI design uses additive secret sharing and\nlinearly homomorphic encryption to handle linear calculations in the ML\ninference, and garbled circuit and a novel three-party oblivious transfer are\nused to handle non-linear functions. privateMDI consists of offline and online\nphases. We designed these phases in a way that most of the data exchange is\ndone in the offline phase while the communication overhead of the online phase\nis reduced. In particular, there is no communication to/from the cloud server\nin the online phase, and the amount of communication between the client and\nedge servers is minimized. The experimental results demonstrate that privateMDI\nsignificantly reduces the ML inference time as compared to the baselines.\n","authors":["Fatemeh Jafarian Dehkordi","Yasaman Keshtkarjahromi","Hulya Seferoglu"],"pdf_url":"https://arxiv.org/pdf/2407.18353v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09881v1","updated":"2024-09-15T22:22:27Z","published":"2024-09-15T22:22:27Z","title":"Proximal Ranking Policy Optimization for Practical Safety in\n  Counterfactual Learning to Rank","summary":"  Counterfactual learning to rank (CLTR) can be risky and, in various\ncircumstances, can produce sub-optimal models that hurt performance when\ndeployed. Safe CLTR was introduced to mitigate these risks when using inverse\npropensity scoring to correct for position bias. However, the existing safety\nmeasure for CLTR is not applicable to state-of-the-art CLTR methods, cannot\nhandle trust bias, and relies on specific assumptions about user behavior. We\npropose a novel approach, proximal ranking policy optimization (PRPO), that\nprovides safety in deployment without assumptions about user behavior. PRPO\nremoves incentives for learning ranking behavior that is too dissimilar to a\nsafe ranking model. Thereby, PRPO imposes a limit on how much learned models\ncan degrade performance metrics, without relying on any specific user\nassumptions. Our experiments show that PRPO provides higher performance than\nthe existing safe inverse propensity scoring approach. PRPO always maintains\nsafety, even in maximally adversarial situations. By avoiding assumptions, PRPO\nis the first method with unconditional safety in deployment that translates to\nrobust safety for real-world applications.\n","authors":["Shashank Gupta","Harrie Oosterhuis","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2409.09881v1.pdf","comment":"Accepted at the CONSEQUENCES 2024 workshop, co-located with ACM\n  RecSys 2024"},{"id":"http://arxiv.org/abs/2409.09875v1","updated":"2024-09-15T21:50:52Z","published":"2024-09-15T21:50:52Z","title":"Scaling Continuous Kernels with Sparse Fourier Domain Learning","summary":"  We address three key challenges in learning continuous kernel\nrepresentations: computational efficiency, parameter efficiency, and spectral\nbias. Continuous kernels have shown significant potential, but their practical\nadoption is often limited by high computational and memory demands.\nAdditionally, these methods are prone to spectral bias, which impedes their\nability to capture high-frequency details. To overcome these limitations, we\npropose a novel approach that leverages sparse learning in the Fourier domain.\nOur method enables the efficient scaling of continuous kernels, drastically\nreduces computational and memory requirements, and mitigates spectral bias by\nexploiting the Gibbs phenomenon.\n","authors":["Clayton Harper","Luke Wood","Peter Gerstoft","Eric C. Larson"],"pdf_url":"https://arxiv.org/pdf/2409.09875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.12191v6","updated":"2024-09-15T21:37:58Z","published":"2022-01-28T15:45:13Z","title":"Kernelized Concept Erasure","summary":"  The representation space of neural models for textual data emerges in an\nunsupervised manner during training. Understanding how those representations\nencode human-interpretable concepts is a fundamental problem. One prominent\napproach for the identification of concepts in neural representations is\nsearching for a linear subspace whose erasure prevents the prediction of the\nconcept from the representations. However, while many linear erasure algorithms\nare tractable and interpretable, neural networks do not necessarily represent\nconcepts in a linear manner. To identify non-linearly encoded concepts, we\npropose a kernelization of a linear minimax game for concept erasure. We\ndemonstrate that it is possible to prevent specific non-linear adversaries from\npredicting the concept. However, the protection does not transfer to different\nnonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded\nconcept remains an open problem.\n","authors":["Shauli Ravfogel","Francisco Vargas","Yoav Goldberg","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2201.12191v6.pdf","comment":"Accepted as a long paper in EMNLP22"},{"id":"http://arxiv.org/abs/2409.09866v1","updated":"2024-09-15T21:19:24Z","published":"2024-09-15T21:19:24Z","title":"Constructing a Singing Style Caption Dataset","summary":"  Singing voice synthesis and conversion have emerged as significant subdomains\nof voice generation, leading to much demands on prompt-conditioned generation.\nUnlike common voice data, generating a singing voice requires an understanding\nof various associated vocal and musical characteristics, such as the vocal tone\nof the singer or emotional expressions. However, existing open-source\naudio-text datasets for voice generation tend to capture only a very limited\nrange of attributes, often missing musical characteristics of the audio. To\nfill this gap, we introduce S2Cap, an audio-text pair dataset with a diverse\nset of attributes. S2Cap consists of pairs of textual prompts and music audio\nsamples with a wide range of vocal and musical attributes, including pitch,\nvolume, tempo, mood, singer's gender and age, and musical genre and emotional\nexpression. Utilizing S2Cap, we suggest an effective novel baseline algorithm\nfor singing style captioning. Singing style captioning is a relative task to\nvoice generation that generates text descriptions of vocal characteristics,\nwhich we first suggested. First, to mitigate the misalignment between the audio\nencoder and the text decoder, we present a novel mechanism called CRESCENDO,\nwhich utilizes positive-pair similarity learning to synchronize the embedding\nspaces of a pretrained audio encoder to get similar embeddings with a text\nencoder. We additionally supervise the model using the singer's voice, which is\ndemixed by the accompaniment. This supervision allows the model to more\naccurately capture vocal characteristics, leading to improved singing style\ncaptions that better reflect the style of the singer. The dataset and the codes\nare available at \\bulurl{https://github.com/HJ-Ok/S2cap}.\n","authors":["Hyunjong Ok","Jaeho Lee"],"pdf_url":"https://arxiv.org/pdf/2409.09866v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2407.18422v2","updated":"2024-09-15T21:08:44Z","published":"2024-07-25T22:44:39Z","title":"A Hypothesis on Black Swan in Unchanging Environments","summary":"  Black swan events are statistically rare occurrences that carry extremely\nhigh risks. A typical view of defining black swan events is heavily assumed to\noriginate from an unpredictable time-varying environments; however, the\ncommunity lacks a comprehensive definition of black swan events. To this end,\nthis paper challenges that the standard view is incomplete and claims that\nhigh-risk, statistically rare events can also occur in unchanging environments\ndue to human misperception of their value and likelihood, which we call as\nspatial black swan event. We first carefully categorize black swan events,\nfocusing on spatial black swan events, and mathematically formalize the\ndefinition of black swan events. We hope these definitions can pave the way for\nthe development of algorithms to prevent such events by rationally correcting\nhuman perception.\n","authors":["Hyunin Lee","Chanwoo Park","David Abel","Ming Jin"],"pdf_url":"https://arxiv.org/pdf/2407.18422v2.pdf","comment":"Authorship was updated"},{"id":"http://arxiv.org/abs/2407.21652v2","updated":"2024-09-15T21:04:23Z","published":"2024-07-31T14:53:41Z","title":"Spatial Transformer Network YOLO Model for Agricultural Object Detection","summary":"  Object detection plays a crucial role in the field of computer vision by\nautonomously locating and identifying objects of interest. The You Only Look\nOnce (YOLO) model is an effective single-shot detector. However, YOLO faces\nchallenges in cluttered or partially occluded scenes and can struggle with\nsmall, low-contrast objects. We propose a new method that integrates spatial\ntransformer networks (STNs) into YOLO to improve performance. The proposed\nSTN-YOLO aims to enhance the model's effectiveness by focusing on important\nareas of the image and improving the spatial invariance of the model before the\ndetection process. Our proposed method improved object detection performance\nboth qualitatively and quantitatively. We explore the impact of different\nlocalization networks within the STN module as well as the robustness of the\nmodel across different spatial transformations. We apply the STN-YOLO on\nbenchmark datasets for Agricultural object detection as well as a new dataset\nfrom a state-of-the-art plant phenotyping greenhouse facility. Our code and\ndataset are publicly available.\n","authors":["Yash Zambre","Ekdev Rajkitkul","Akshatha Mohan","Joshua Peeples"],"pdf_url":"https://arxiv.org/pdf/2407.21652v2.pdf","comment":"7 pages, 5 figures, accepted to 2024 IEEE International Conference on\n  Machine Learning and Applications"},{"id":"http://arxiv.org/abs/2312.05989v2","updated":"2024-09-15T20:53:05Z","published":"2023-12-10T20:29:58Z","title":"A Note on the Convergence of Denoising Diffusion Probabilistic Models","summary":"  Diffusion models are one of the most important families of deep generative\nmodels. In this note, we derive a quantitative upper bound on the Wasserstein\ndistance between the data-generating distribution and the distribution learned\nby a diffusion model. Unlike previous works in this field, our result does not\nmake assumptions on the learned score function. Moreover, our bound holds for\narbitrary data-generating distributions on bounded instance spaces, even those\nwithout a density w.r.t. the Lebesgue measure, and the upper bound does not\nsuffer from exponential dependencies. Our main result builds upon the recent\nwork of Mbacke et al. (2023) and our proofs are elementary.\n","authors":["Sokhna Diarra Mbacke","Omar Rivasplata"],"pdf_url":"https://arxiv.org/pdf/2312.05989v2.pdf","comment":"Published at TMLR in 2024"},{"id":"http://arxiv.org/abs/2409.09858v1","updated":"2024-09-15T20:41:18Z","published":"2024-09-15T20:41:18Z","title":"A Survey of Out-of-distribution Generalization for Graph Machine\n  Learning from a Causal View","summary":"  Graph machine learning (GML) has been successfully applied across a wide\nrange of tasks. Nonetheless, GML faces significant challenges in generalizing\nover out-of-distribution (OOD) data, which raises concerns about its wider\napplicability. Recent advancements have underscored the crucial role of\ncausality-driven approaches in overcoming these generalization challenges.\nDistinct from traditional GML methods that primarily rely on statistical\ndependencies, causality-focused strategies delve into the underlying causal\nmechanisms of data generation and model prediction, thus significantly\nimproving the generalization of GML across different environments. This paper\noffers a thorough review of recent progress in causality-involved GML\ngeneralization. We elucidate the fundamental concepts of employing causality to\nenhance graph model generalization and categorize the various approaches,\nproviding detailed descriptions of their methodologies and the connections\namong them. Furthermore, we explore the incorporation of causality in other\nrelated important areas of trustworthy GML, such as explanation, fairness, and\nrobustness. Concluding with a discussion on potential future research\ndirections, this review seeks to articulate the continuing development and\nfuture potential of causality in enhancing the trustworthiness of graph machine\nlearning.\n","authors":["Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2409.09858v1.pdf","comment":"15 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2409.09844v1","updated":"2024-09-15T19:50:00Z","published":"2024-09-15T19:50:00Z","title":"A Benchmark Dataset with Larger Context for Non-Factoid Question\n  Answering over Islamic Text","summary":"  Accessing and comprehending religious texts, particularly the Quran (the\nsacred scripture of Islam) and Ahadith (the corpus of the sayings or traditions\nof the Prophet Muhammad), in today's digital era necessitates efficient and\naccurate Question-Answering (QA) systems. Yet, the scarcity of QA systems\ntailored specifically to the detailed nature of inquiries about the Quranic\nTafsir (explanation, interpretation, context of Quran for clarity) and Ahadith\nposes significant challenges. To address this gap, we introduce a comprehensive\ndataset meticulously crafted for QA purposes within the domain of Quranic\nTafsir and Ahadith. This dataset comprises a robust collection of over 73,000\nquestion-answer pairs, standing as the largest reported dataset in this\nspecialized domain. Importantly, both questions and answers within the dataset\nare meticulously enriched with contextual information, serving as invaluable\nresources for training and evaluating tailored QA systems. However, while this\npaper highlights the dataset's contributions and establishes a benchmark for\nevaluating QA performance in the Quran and Ahadith domains, our subsequent\nhuman evaluation uncovered critical insights regarding the limitations of\nexisting automatic evaluation techniques. The discrepancy between automatic\nevaluation metrics, such as ROUGE scores, and human assessments became\napparent. The human evaluation indicated significant disparities: the model's\nverdict consistency with expert scholars ranged between 11% to 20%, while its\ncontextual understanding spanned a broader spectrum of 50% to 90%. These\nfindings underscore the necessity for evaluation techniques that capture the\nnuances and complexities inherent in understanding religious texts, surpassing\nthe limitations of traditional automatic metrics.\n","authors":["Faiza Qamar","Seemab Latif","Rabia Latif"],"pdf_url":"https://arxiv.org/pdf/2409.09844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06637v2","updated":"2024-09-15T19:07:30Z","published":"2024-07-09T08:05:14Z","title":"Early Detection of Network Service Degradation: An Intra-Flow Approach","summary":"  This research presents a novel method for predicting service degradation (SD)\nin computer networks by leveraging early flow features. Our approach focuses on\nthe observable (O) segments of network flows, particularly analyzing Packet\nInter-Arrival Time (PIAT) values and other derived metrics, to infer the\nbehavior of non-observable (NO) segments. Through a comprehensive evaluation,\nwe identify an optimal O/NO split threshold of 10 observed delay samples,\nbalancing prediction accuracy and resource utilization. Evaluating models\nincluding Logistic Regression, XGBoost, and Multi-Layer Perceptron, we find\nXGBoost outperforms others, achieving an F1-score of 0.74, balanced accuracy of\n0.84, and AUROC of 0.97. Our findings highlight the effectiveness of\nincorporating comprehensive early flow features and the potential of our method\nto offer a practical solution for monitoring network traffic in\nresource-constrained environments. This approach ensures enhanced user\nexperience and network performance by preemptively addressing potential SD,\nproviding the basis for a robust framework for maintaining high-quality network\nservices.\n","authors":["Balint Bicski","Adrian Pekar"],"pdf_url":"https://arxiv.org/pdf/2407.06637v2.pdf","comment":"reduced to 5 pages; accepted as a short paper for presentation at the\n  CNSM 2024 conference"},{"id":"http://arxiv.org/abs/2409.09828v1","updated":"2024-09-15T19:04:50Z","published":"2024-09-15T19:04:50Z","title":"Latent Diffusion Models for Controllable RNA Sequence Generation","summary":"  This paper presents RNAdiffusion, a latent diffusion model for generating and\noptimizing discrete RNA sequences. RNA is a particularly dynamic and versatile\nmolecule in biological processes. RNA sequences exhibit high variability and\ndiversity, characterized by their variable lengths, flexible three-dimensional\nstructures, and diverse functions. We utilize pretrained BERT-type models to\nencode raw RNAs into token-level biologically meaningful representations. A\nQ-Former is employed to compress these representations into a fixed-length set\nof latent vectors, with an autoregressive decoder trained to reconstruct RNA\nsequences from these latent variables. We then develop a continuous diffusion\nmodel within this latent space. To enable optimization, we train reward\nnetworks to estimate functional properties of RNA from the latent variables. We\nemploy gradient-based guidance during the backward diffusion process, aiming to\ngenerate RNA sequences that are optimized for higher rewards. Empirical\nexperiments confirm that RNAdiffusion generates non-coding RNAs that align with\nnatural distributions across various biological indicators. We fine-tuned the\ndiffusion model on untranslated regions (UTRs) of mRNA and optimize sample\nsequences for protein translation efficiencies. Our guided diffusion model\neffectively generates diverse UTR sequences with high Mean Ribosome Loading\n(MRL) and Translation Efficiency (TE), surpassing baselines. These results hold\npromise for studies on RNA sequence-function relationships, protein synthesis,\nand enhancing therapeutic RNA design.\n","authors":["Kaixuan Huang","Yukang Yang","Kaidi Fu","Yanyi Chu","Le Cong","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09819v1","updated":"2024-09-15T18:39:22Z","published":"2024-09-15T18:39:22Z","title":"A Simpler Alternative to Variational Regularized Counterfactual Risk\n  Minimization","summary":"  Variance regularized counterfactual risk minimization (VRCRM) has been\nproposed as an alternative off-policy learning (OPL) method. VRCRM method uses\na lower-bound on the $f$-divergence between the logging policy and the target\npolicy as regularization during learning and was shown to improve performance\nover existing OPL alternatives on multi-label classification tasks. In this\nwork, we revisit the original experimental setting of VRCRM and propose to\nminimize the $f$-divergence directly, instead of optimizing for the lower bound\nusing a $f$-GAN approach. Surprisingly, we were unable to reproduce the results\nreported in the original setting. In response, we propose a novel simpler\nalternative to f-divergence optimization by minimizing a direct approximation\nof f-divergence directly, instead of a $f$-GAN based lower bound. Experiments\nshowed that minimizing the divergence using $f$-GANs did not work as expected,\nwhereas our proposed novel simpler alternative works better empirically.\n","authors":["Hua Chang Bakker","Shashank Gupta","Harrie Oosterhuis"],"pdf_url":"https://arxiv.org/pdf/2409.09819v1.pdf","comment":"Accepted at the CONSEQUENCES '24 workshop, co-located with ACM RecSys\n  '24"},{"id":"http://arxiv.org/abs/2409.09811v1","updated":"2024-09-15T18:20:15Z","published":"2024-09-15T18:20:15Z","title":"PROSE-FD: A Multimodal PDE Foundation Model for Learning Multiple\n  Operators for Forecasting Fluid Dynamics","summary":"  We propose PROSE-FD, a zero-shot multimodal PDE foundational model for\nsimultaneous prediction of heterogeneous two-dimensional physical systems\nrelated to distinct fluid dynamics settings. These systems include shallow\nwater equations and the Navier-Stokes equations with incompressible and\ncompressible flow, regular and complex geometries, and different buoyancy\nsettings. This work presents a new transformer-based multi-operator learning\napproach that fuses symbolic information to perform operator-based data\nprediction, i.e. non-autoregressive. By incorporating multiple modalities in\nthe inputs, the PDE foundation model builds in a pathway for including\nmathematical descriptions of the physical behavior. We pre-train our foundation\nmodel on 6 parametric families of equations collected from 13 datasets,\nincluding over 60K trajectories. Our model outperforms popular operator\nlearning, computer vision, and multi-physics models, in benchmark forward\nprediction tasks. We test our architecture choices with ablation studies.\n","authors":["Yuxuan Liu","Jingmin Sun","Xinjie He","Griffin Pinney","Zecheng Zhang","Hayden Schaeffer"],"pdf_url":"https://arxiv.org/pdf/2409.09811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19544v2","updated":"2024-09-15T17:42:20Z","published":"2024-05-29T22:12:52Z","title":"One-Shot Safety Alignment for Large Language Models via Optimal\n  Dualization","summary":"  The growing safety concerns surrounding Large Language Models (LLMs) raise an\nurgent need to align them with diverse human preferences to simultaneously\nenhance their helpfulness and safety. A promising approach is to enforce safety\nconstraints through Reinforcement Learning from Human Feedback (RLHF). For such\nconstrained RLHF, common Lagrangian-based primal-dual policy optimization\nmethods are computationally expensive and often unstable. This paper presents a\ndualization perspective that reduces constrained alignment to an equivalent\nunconstrained alignment problem. We do so by pre-optimizing a smooth and convex\ndual function that has a closed form. This shortcut eliminates the need for\ncumbersome primal-dual policy iterations, thus greatly reducing the\ncomputational burden and improving training stability. Our strategy leads to\ntwo practical algorithms in model-based and preference-based scenarios (MoCAN\nand PeCAN, respectively). A broad range of experiments demonstrate the\neffectiveness of our methods.\n","authors":["Xinmeng Huang","Shuo Li","Edgar Dobriban","Osbert Bastani","Hamed Hassani","Dongsheng Ding"],"pdf_url":"https://arxiv.org/pdf/2405.19544v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09794v1","updated":"2024-09-15T17:04:25Z","published":"2024-09-15T17:04:25Z","title":"Federated Learning in Adversarial Environments: Testbed Design and\n  Poisoning Resilience in Cybersecurity","summary":"  This paper presents the design and implementation of a Federated Learning\n(FL) testbed, focusing on its application in cybersecurity and evaluating its\nresilience against poisoning attacks. Federated Learning allows multiple\nclients to collaboratively train a global model while keeping their data\ndecentralized, addressing critical needs for data privacy and security,\nparticularly in sensitive fields like cybersecurity. Our testbed, built using\nthe Flower framework, facilitates experimentation with various FL frameworks,\nassessing their performance, scalability, and ease of integration. Through a\ncase study on federated intrusion detection systems, we demonstrate the\ntestbed's capabilities in detecting anomalies and securing critical\ninfrastructure without exposing sensitive network data. Comprehensive poisoning\ntests, targeting both model and data integrity, evaluate the system's\nrobustness under adversarial conditions. Our results show that while federated\nlearning enhances data privacy and distributed learning, it remains vulnerable\nto poisoning attacks, which must be mitigated to ensure its reliability in\nreal-world applications.\n","authors":["Hao Jian Huang","Bekzod Iskandarov","Mizanur Rahman","Hakan T. Otal","M. Abdullah Canbaz"],"pdf_url":"https://arxiv.org/pdf/2409.09794v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.16871v2","updated":"2024-09-15T17:03:26Z","published":"2024-03-25T15:37:43Z","title":"Conformal Off-Policy Prediction for Multi-Agent Systems","summary":"  Off-Policy Prediction (OPP), i.e., predicting the outcomes of a target policy\nusing only data collected under a nominal (behavioural) policy, is a paramount\nproblem in data-driven analysis of safety-critical systems where the deployment\nof a new policy may be unsafe. To achieve dependable off-policy predictions,\nrecent work on Conformal Off-Policy Prediction (COPP) leverage the conformal\nprediction framework to derive prediction regions with probabilistic guarantees\nunder the target process. Existing COPP methods can account for the\ndistribution shifts induced by policy switching, but are limited to\nsingle-agent systems and scalar outcomes (e.g., rewards). In this work, we\nintroduce MA-COPP, the first conformal prediction method to solve OPP problems\ninvolving multi-agent systems, deriving joint prediction regions for all\nagents' trajectories when one or more ego agents change their policies. Unlike\nthe single-agent scenario, this setting introduces higher complexity as the\ndistribution shifts affect predictions for all agents, not just the ego agents,\nand the prediction task involves full multi-dimensional trajectories, not just\nreward values. A key contribution of MA-COPP is to avoid enumeration or\nexhaustive search of the output space of agent trajectories, which is instead\nrequired by existing COPP methods to construct the prediction region. We\nachieve this by showing that an over-approximation of the true joint prediction\nregion (JPR) can be constructed, without enumeration, from the maximum density\nratio of the JPR trajectories. We evaluate the effectiveness of MA-COPP in\nmulti-agent systems from the PettingZoo library and the F1TENTH autonomous\nracing environment, achieving nominal coverage in higher dimensions and various\nshift settings.\n","authors":["Tom Kuipers","Renukanandan Tumu","Shuo Yang","Milad Kazemi","Rahul Mangharam","Nicola Paoletti"],"pdf_url":"https://arxiv.org/pdf/2403.16871v2.pdf","comment":"Accepted for publication in the 63rd IEEE Conference on Decision and\n  Control (CDC) 2024"},{"id":"http://arxiv.org/abs/2409.09792v1","updated":"2024-09-15T16:59:15Z","published":"2024-09-15T16:59:15Z","title":"Enhancing Data Quality through Self-learning on Imbalanced Financial\n  Risk Data","summary":"  In the financial risk domain, particularly in credit default prediction and\nfraud detection, accurate identification of high-risk class instances is\nparamount, as their occurrence can have significant economic implications.\nAlthough machine learning models have gained widespread adoption for risk\nprediction, their performance is often hindered by the scarcity and diversity\nof high-quality data. This limitation stems from factors in datasets such as\nsmall risk sample sizes, high labeling costs, and severe class imbalance, which\nimpede the models' ability to learn effectively and accurately forecast\ncritical events. This study investigates data pre-processing techniques to\nenhance existing financial risk datasets by introducing TriEnhance, a\nstraightforward technique that entails: (1) generating synthetic samples\nspecifically tailored to the minority class, (2) filtering using binary\nfeedback to refine samples, and (3) self-learning with pseudo-labels. Our\nexperiments across six benchmark datasets reveal the efficacy of TriEnhance,\nwith a notable focus on improving minority class calibration, a key factor for\ndeveloping more robust financial risk prediction systems.\n","authors":["Xu Sun","Zixuan Qin","Shun Zhang","Yuexian Wang","Li Huang"],"pdf_url":"https://arxiv.org/pdf/2409.09792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09787v1","updated":"2024-09-15T16:41:30Z","published":"2024-09-15T16:41:30Z","title":"BEnDEM:A Boltzmann Sampler Based on Bootstrapped Denoising Energy\n  Matching","summary":"  Developing an efficient sampler capable of generating independent and\nidentically distributed (IID) samples from a Boltzmann distribution is a\ncrucial challenge in scientific research, e.g. molecular dynamics. In this\nwork, we intend to learn neural samplers given energy functions instead of data\nsampled from the Boltzmann distribution. By learning the energies of the noised\ndata, we propose a diffusion-based sampler, ENERGY-BASED DENOISING ENERGY\nMATCHING, which theoretically has lower variance and more complexity compared\nto related works. Furthermore, a novel bootstrapping technique is applied to\nEnDEM to balance between bias and variance. We evaluate EnDEM and BEnDEM on a\n2-dimensional 40 Gaussian Mixture Model (GMM) and a 4-particle double-welling\npotential (DW-4). The experimental results demonstrate that BEnDEM can achieve\nstate-of-the-art performance while being more robust.\n","authors":["RuiKang OuYang","Bo Qiang","Jos√© Miguel Hern√°ndez-Lobato"],"pdf_url":"https://arxiv.org/pdf/2409.09787v1.pdf","comment":"20 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2409.09783v1","updated":"2024-09-15T16:21:55Z","published":"2024-09-15T16:21:55Z","title":"Learning Rate Optimization for Deep Neural Networks Using Lipschitz\n  Bandits","summary":"  Learning rate is a crucial parameter in training of neural networks. A\nproperly tuned learning rate leads to faster training and higher test accuracy.\nIn this paper, we propose a Lipschitz bandit-driven approach for tuning the\nlearning rate of neural networks. The proposed approach is compared with the\npopular HyperOpt technique used extensively for hyperparameter optimization and\nthe recently developed bandit-based algorithm BLiE. The results for multiple\nneural network architectures indicate that our method finds a better learning\nrate using a) fewer evaluations and b) lesser number of epochs per evaluation,\nwhen compared to both HyperOpt and BLiE. Thus, the proposed approach enables\nmore efficient training of neural networks, leading to lower training time and\nlesser computational cost.\n","authors":["Padma Priyanka","Sheetal Kalyani","Avhishek Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2409.09783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09781v1","updated":"2024-09-15T16:10:03Z","published":"2024-09-15T16:10:03Z","title":"RandALO: Out-of-sample risk estimation in no time flat","summary":"  Estimating out-of-sample risk for models trained on large high-dimensional\ndatasets is an expensive but essential part of the machine learning process,\nenabling practitioners to optimally tune hyperparameters. Cross-validation (CV)\nserves as the de facto standard for risk estimation but poorly trades off high\nbias ($K$-fold CV) for computational cost (leave-one-out CV). We propose a\nrandomized approximate leave-one-out (RandALO) risk estimator that is not only\na consistent estimator of risk in high dimensions but also less computationally\nexpensive than $K$-fold CV. We support our claims with extensive simulations on\nsynthetic and real data and provide a user-friendly Python package implementing\nRandALO available on PyPI as randalo and at https://github.com/cvxgrp/randalo.\n","authors":["Parth T. Nobel","Daniel LeJeune","Emmanuel J. Cand√®s"],"pdf_url":"https://arxiv.org/pdf/2409.09781v1.pdf","comment":"25 pages, 9 figures"},{"id":"http://arxiv.org/abs/2409.09778v1","updated":"2024-09-15T15:58:08Z","published":"2024-09-15T15:58:08Z","title":"Rewind-to-Delete: Certified Machine Unlearning for Nonconvex Functions","summary":"  Machine unlearning algorithms aim to efficiently remove data from a model\nwithout retraining it from scratch, in order to enforce data privacy, remove\ncorrupted or outdated data, or respect a user's ``right to be forgotten.\"\nCertified machine unlearning is a strong theoretical guarantee that quantifies\nthe extent to which data is erased from the model weights. Most prior works in\ncertified unlearning focus on models trained on convex or strongly convex loss\nfunctions, which benefit from convenient convergence guarantees and the\nexistence of global minima. For nonconvex objectives, existing algorithms rely\non limiting assumptions and expensive computations that hinder practical\nimplementations. In this work, we propose a simple first-order algorithm for\nunlearning on general nonconvex loss functions which unlearns by ``rewinding\"\nto an earlier step during the learning process and then performs gradient\ndescent on the loss function of the retained data points. Our algorithm is\nblack-box, in that it can be directly applied to models pretrained with vanilla\ngradient descent with no prior consideration of unlearning. We prove\n$(\\epsilon, \\delta)$ certified unlearning and performance guarantees that\nestablish the privacy-utility-complexity tradeoff of our algorithm, with\nspecial consideration for nonconvex functions that satisfy the\nPolyak-Lojasiewicz inequality.\n","authors":["Siqiao Mu","Diego Klabjan"],"pdf_url":"https://arxiv.org/pdf/2409.09778v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09770v1","updated":"2024-09-15T15:41:59Z","published":"2024-09-15T15:41:59Z","title":"Towards Multi-view Graph Anomaly Detection with Similarity-Guided\n  Contrastive Clustering","summary":"  Anomaly detection on graphs plays an important role in many real-world\napplications. Usually, these data are composed of multiple types (e.g., user\ninformation and transaction records for financial data), thus exhibiting view\nheterogeneity. Therefore, it can be challenging to leverage such multi-view\ninformation and learn the graph's contextual information to identify rare\nanomalies. To tackle this problem, many deep learning-based methods utilize\ncontrastive learning loss as a regularization term to learn good\nrepresentations. However, many existing contrastive-based methods show that\ntraditional contrastive learning losses fail to consider the semantic\ninformation (e.g., class membership information). In addition, we theoretically\nshow that clustering-based contrastive learning also easily leads to a\nsub-optimal solution. To address these issues, in this paper, we proposed an\nautoencoder-based clustering framework regularized by a similarity-guided\ncontrastive loss to detect anomalous nodes. Specifically, we build a similarity\nmap to help the model learn robust representations without imposing a hard\nmargin constraint between the positive and negative pairs. Theoretically, we\nshow that the proposed similarity-guided loss is a variant of contrastive\nlearning loss, and how it alleviates the issue of unreliable pseudo-labels with\nthe connection to graph spectral clustering. Experimental results on several\ndatasets demonstrate the effectiveness and efficiency of our proposed\nframework.\n","authors":["Lecheng Zheng","John R. Birge","Yifang Zhang","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2409.09770v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.11737v2","updated":"2024-09-15T15:17:47Z","published":"2023-04-23T20:05:09Z","title":"Sarah Frank-Wolfe: Methods for Constrained Optimization with Best Rates\n  and Practical Features","summary":"  The Frank-Wolfe (FW) method is a popular approach for solving optimization\nproblems with structured constraints that arise in machine learning\napplications. In recent years, stochastic versions of FW have gained\npopularity, motivated by large datasets for which the computation of the full\ngradient is prohibitively expensive. In this paper, we present two new variants\nof the FW algorithms for stochastic finite-sum minimization. Our algorithms\nhave the best convergence guarantees of existing stochastic FW approaches for\nboth convex and non-convex objective functions. Our methods do not have the\nissue of permanently collecting large batches, which is common to many\nstochastic projection-free approaches. Moreover, our second approach does not\nrequire either large batches or full deterministic gradients, which is a\ntypical weakness of many techniques for finite-sum problems. The faster\ntheoretical rates of our approaches are confirmed experimentally.\n","authors":["Aleksandr Beznosikov","David Dobre","Gauthier Gidel"],"pdf_url":"https://arxiv.org/pdf/2304.11737v2.pdf","comment":"Appears in: the 41st International Conference on Machine Learning\n  (ICML 2024). 26 pages, 2 algorithms, 5 figures, 2 tables. Reference:\n  https://proceedings.mlr.press/v235/beznosikov24a.html"},{"id":"http://arxiv.org/abs/2405.13347v2","updated":"2024-09-15T15:10:57Z","published":"2024-05-22T05:07:56Z","title":"Time-Series Forecasting and Sequence Learning Using Memristor-based\n  Reservoir System","summary":"  Pushing the frontiers of time-series information processing in the\never-growing domain of edge devices with stringent resources has been impeded\nby the systems' ability to process information and learn locally on the device.\nLocal processing and learning of time-series information typically demand\nintensive computations and massive storage as the process involves retrieving\ninformation and tuning hundreds of parameters back in time. In this work, we\ndeveloped a memristor-based echo state network accelerator that features\nefficient temporal data processing and in-situ online learning. The proposed\ndesign is benchmarked using various datasets involving real-world tasks, such\nas forecasting the load energy consumption and weather conditions. The\nexperimental results illustrate that the hardware model experiences a marginal\ndegradation in performance as compared to the software counterpart. This is\nmainly attributed to the limited precision and dynamic range of network\nparameters when emulated using memristor devices. The proposed system is\nevaluated for lifespan, robustness, and energy-delay product. It is observed\nthat the system demonstrates reasonable robustness for device failure below\n10%, which may occur due to stuck-at faults. Furthermore, 247X reduction in\nenergy consumption is achieved when compared to a custom CMOS digital design\nimplemented at the same technology node.\n","authors":["Abdullah M. Zyarah","Dhireesha Kudithipudi"],"pdf_url":"https://arxiv.org/pdf/2405.13347v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09755v1","updated":"2024-09-15T14:57:38Z","published":"2024-09-15T14:57:38Z","title":"Analysis of Centrifugal Clutches in Two-Speed Automatic Transmissions\n  with Deep Learning-Based Engagement Prediction","summary":"  This paper presents a comprehensive numerical analysis of centrifugal clutch\nsystems integrated with a two-speed automatic transmission, a key component in\nautomotive torque transfer. Centrifugal clutches enable torque transmission\nbased on rotational speed without external controls. The study systematically\nexamines various clutch configurations effects on transmission dynamics,\nfocusing on torque transfer, upshifting, and downshifting behaviors under\ndifferent conditions. A Deep Neural Network (DNN) model predicts clutch\nengagement using parameters such as spring preload and shoe mass, offering an\nefficient alternative to complex simulations. The integration of deep learning\nand numerical modeling provides critical insights for optimizing clutch\ndesigns, enhancing transmission performance and efficiency.\n","authors":["Bo-Yi Lin","Kai Chun Lin"],"pdf_url":"https://arxiv.org/pdf/2409.09755v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08234v2","updated":"2024-09-15T14:41:42Z","published":"2024-09-12T17:33:06Z","title":"LLM Honeypot: Leveraging Large Language Models as Advanced Interactive\n  Honeypot Systems","summary":"  The rapid evolution of cyber threats necessitates innovative solutions for\ndetecting and analyzing malicious activity. Honeypots, which are decoy systems\ndesigned to lure and interact with attackers, have emerged as a critical\ncomponent in cybersecurity. In this paper, we present a novel approach to\ncreating realistic and interactive honeypot systems using Large Language Models\n(LLMs). By fine-tuning a pre-trained open-source language model on a diverse\ndataset of attacker-generated commands and responses, we developed a honeypot\ncapable of sophisticated engagement with attackers. Our methodology involved\nseveral key steps: data collection and processing, prompt engineering, model\nselection, and supervised fine-tuning to optimize the model's performance.\nEvaluation through similarity metrics and live deployment demonstrated that our\napproach effectively generates accurate and informative responses. The results\nhighlight the potential of LLMs to revolutionize honeypot technology, providing\ncybersecurity professionals with a powerful tool to detect and analyze\nmalicious activity, thereby enhancing overall security infrastructure.\n","authors":["Hakan T. Otal","M. Abdullah Canbaz"],"pdf_url":"https://arxiv.org/pdf/2409.08234v2.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2210.01672v5","updated":"2024-09-15T14:40:59Z","published":"2022-10-04T15:19:24Z","title":"Bringing motion taxonomies to continuous domains via GPLVM on hyperbolic\n  manifolds","summary":"  Human motion taxonomies serve as high-level hierarchical abstractions that\nclassify how humans move and interact with their environment. They have proven\nuseful to analyse grasps, manipulation skills, and whole-body support poses.\nDespite substantial efforts devoted to design their hierarchy and underlying\ncategories, their use remains limited. This may be attributed to the lack of\ncomputational models that fill the gap between the discrete hierarchical\nstructure of the taxonomy and the high-dimensional heterogeneous data\nassociated to its categories. To overcome this problem, we propose to model\ntaxonomy data via hyperbolic embeddings that capture the associated\nhierarchical structure. We achieve this by formulating a novel Gaussian process\nhyperbolic latent variable model that incorporates the taxonomy structure\nthrough graph-based priors on the latent space and distance-preserving back\nconstraints. We validate our model on three different human motion taxonomies\nto learn hyperbolic embeddings that faithfully preserve the original graph\nstructure. We show that our model properly encodes unseen data from existing or\nnew taxonomy categories, and outperforms its Euclidean and VAE-based\ncounterparts. Finally, through proof-of-concept experiments, we show that our\nmodel may be used to generate realistic trajectories between the learned\nembeddings.\n","authors":["No√©mie Jaquier","Leonel Rozo","Miguel Gonz√°lez-Duque","Viacheslav Borovitskiy","Tamim Asfour"],"pdf_url":"https://arxiv.org/pdf/2210.01672v5.pdf","comment":"Intl. Conference on Machine Learning (ICML), 2024"},{"id":"http://arxiv.org/abs/2409.09745v1","updated":"2024-09-15T14:20:03Z","published":"2024-09-15T14:20:03Z","title":"The Optimality of (Accelerated) SGD for High-Dimensional Quadratic\n  Optimization","summary":"  Stochastic gradient descent (SGD) is a widely used algorithm in machine\nlearning, particularly for neural network training. Recent studies on SGD for\ncanonical quadratic optimization or linear regression show it attains well\ngeneralization under suitable high-dimensional settings. However, a fundamental\nquestion -- for what kinds of high-dimensional learning problems SGD and its\naccelerated variants can achieve optimality has yet to be well studied. This\npaper investigates SGD with two essential components in practice: exponentially\ndecaying step size schedule and momentum. We establish the convergence upper\nbound for momentum accelerated SGD (ASGD) and propose concrete classes of\nlearning problems under which SGD or ASGD achieves min-max optimal convergence\nrates. The characterization of the target function is based on standard\npower-law decays in (functional) linear regression. Our results unveil new\ninsights for understanding the learning bias of SGD: (i) SGD is efficient in\nlearning ``dense'' features where the corresponding weights are subject to an\ninfinity norm constraint; (ii) SGD is efficient for easy problem without\nsuffering from the saturation effect; (iii) momentum can accelerate the\nconvergence rate by order when the learning problem is relatively hard. To our\nknowledge, this is the first work to clearly identify the optimal boundary of\nSGD versus ASGD for the problem under mild settings.\n","authors":["Haihan Zhang","Yuanshi Liu","Qianwen Chen","Cong Fang"],"pdf_url":"https://arxiv.org/pdf/2409.09745v1.pdf","comment":"46 pages"},{"id":"http://arxiv.org/abs/2409.09742v1","updated":"2024-09-15T14:19:19Z","published":"2024-09-15T14:19:19Z","title":"OML-AD: Online Machine Learning for Anomaly Detection in Time Series\n  Data","summary":"  Time series are ubiquitous and occur naturally in a variety of applications\n-- from data recorded by sensors in manufacturing processes, over financial\ndata streams to climate data. Different tasks arise, such as regression,\nclassification or segmentation of the time series. However, to reliably solve\nthese challenges, it is important to filter out abnormal observations that\ndeviate from the usual behavior of the time series. While many anomaly\ndetection methods exist for independent data and stationary time series, these\nmethods are not applicable to non-stationary time series. To allow for\nnon-stationarity in the data, while simultaneously detecting anomalies, we\npropose OML-AD, a novel approach for anomaly detection (AD) based on online\nmachine learning (OML). We provide an implementation of OML-AD within the\nPython library River and show that it outperforms state-of-the-art baseline\nmethods in terms of accuracy and computational efficiency.\n","authors":["Sebastian Wette","Florian Heinrichs"],"pdf_url":"https://arxiv.org/pdf/2409.09742v1.pdf","comment":"14 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2409.09727v1","updated":"2024-09-15T13:11:07Z","published":"2024-09-15T13:11:07Z","title":"From Challenges and Pitfalls to Recommendations and Opportunities:\n  Implementing Federated Learning in Healthcare","summary":"  Federated learning holds great potential for enabling large-scale healthcare\nresearch and collaboration across multiple centres while ensuring data privacy\nand security are not compromised. Although numerous recent studies suggest or\nutilize federated learning based methods in healthcare, it remains unclear\nwhich ones have potential clinical utility. This review paper considers and\nanalyzes the most recent studies up to May 2024 that describe federated\nlearning based methods in healthcare. After a thorough review, we find that the\nvast majority are not appropriate for clinical use due to their methodological\nflaws and/or underlying biases which include but are not limited to privacy\nconcerns, generalization issues, and communication costs. As a result, the\neffectiveness of federated learning in healthcare is significantly compromised.\nTo overcome these challenges, we provide recommendations and promising\nopportunities that might be implemented to resolve these problems and improve\nthe quality of model development in federated learning with healthcare.\n","authors":["Ming Li","Pengcheng Xu","Junjie Hu","Zeyu Tang","Guang Yang"],"pdf_url":"https://arxiv.org/pdf/2409.09727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09722v1","updated":"2024-09-15T13:02:50Z","published":"2024-09-15T13:02:50Z","title":"Measuring Recency Bias In Sequential Recommendation Systems","summary":"  Recency bias in a sequential recommendation system refers to the overly high\nemphasis placed on recent items within a user session. This bias can diminish\nthe serendipity of recommendations and hinder the system's ability to capture\nusers' long-term interests, leading to user disengagement. We propose a simple\nyet effective novel metric specifically designed to quantify recency bias. Our\nfindings also demonstrate that high recency bias measured in our proposed\nmetric adversely impacts recommendation performance too, and mitigating it\nresults in improved recommendation performances across all models evaluated in\nour experiments, thus highlighting the importance of measuring recency bias.\n","authors":["Jeonglyul Oh","Sungzoon Cho"],"pdf_url":"https://arxiv.org/pdf/2409.09722v1.pdf","comment":"Accepted at the CONSEQUENCES '24 workshop, co-located with ACM RecSys\n  '24"},{"id":"http://arxiv.org/abs/2409.09721v1","updated":"2024-09-15T13:02:14Z","published":"2024-09-15T13:02:14Z","title":"Finetuning CLIP to Reason about Pairwise Differences","summary":"  Vision-language models (VLMs) such as CLIP are trained via contrastive\nlearning between text and image pairs, resulting in aligned image and text\nembeddings that are useful for many downstream tasks. A notable drawback of\nCLIP, however, is that the resulting embedding space seems to lack some of the\nstructure of their purely text-based alternatives. For instance, while text\nembeddings have been long noted to satisfy \\emph{analogies} in embedding space\nusing vector arithmetic, CLIP has no such property. In this paper, we propose\nan approach to natively train CLIP in a contrastive manner to reason about\ndifferences in embedding space. We finetune CLIP so that the differences in\nimage embedding space correspond to \\emph{text descriptions of the image\ndifferences}, which we synthetically generate with large language models on\nimage-caption paired datasets. We first demonstrate that our approach yields\nsignificantly improved capabilities in ranking images by a certain attribute\n(e.g., elephants are larger than cats), which is useful in retrieval or\nconstructing attribute-based classifiers, and improved zeroshot classification\nperformance on many downstream image classification tasks. In addition, our\napproach enables a new mechanism for inference that we refer to as comparative\nprompting, where we leverage prior knowledge of text descriptions of\ndifferences between classes of interest, achieving even larger performance\ngains in classification. Finally, we illustrate that the resulting embeddings\nobey a larger degree of geometric properties in embedding space, such as in\ntext-to-image generation.\n","authors":["Dylan Sam","Devin Willmott","Joao D. Semedo","J. Zico Kolter"],"pdf_url":"https://arxiv.org/pdf/2409.09721v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2311.11532v2","updated":"2024-09-15T12:55:03Z","published":"2023-11-20T04:34:19Z","title":"Fine-Tuning Adaptive Stochastic Optimizers: Determining the Optimal\n  Hyperparameter $Œµ$ via Gradient Magnitude Histogram Analysis","summary":"  Stochastic optimizers play a crucial role in the successful training of deep\nneural network models. To achieve optimal model performance, designers must\ncarefully select both model and optimizer hyperparameters. However, this\nprocess is frequently demanding in terms of computational resources and\nprocessing time. While it is a well-established practice to tune the entire set\nof optimizer hyperparameters for peak performance, there is still a lack of\nclarity regarding the individual influence of hyperparameters mislabeled as\n\"low priority\", including the safeguard factor $\\epsilon$ and decay rate\n$\\beta$, in leading adaptive stochastic optimizers like the Adam optimizer. In\nthis manuscript, we introduce a new framework based on the empirical\nprobability density function of the loss' gradient magnitude, termed as the\n\"gradient magnitude histogram\", for a thorough analysis of adaptive stochastic\noptimizers and the safeguard hyperparameter $\\epsilon$. This framework reveals\nand justifies valuable relationships and dependencies among hyperparameters in\nconnection to optimal performance across diverse tasks, such as classification,\nlanguage modeling and machine translation. Furthermore, we propose a novel\nalgorithm using gradient magnitude histograms to automatically estimate a\nrefined and accurate search space for the optimal safeguard hyperparameter\n$\\epsilon$, surpassing the conventional trial-and-error methodology by\nestablishing a worst-case search space that is two times narrower.\n","authors":["Gustavo Silva","Paul Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2311.11532v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04910v2","updated":"2024-09-15T12:32:18Z","published":"2024-06-07T13:00:57Z","title":"PolyLUT-Add: FPGA-based LUT Inference with Wide Inputs","summary":"  FPGAs have distinct advantages as a technology for deploying deep neural\nnetworks (DNNs) at the edge. Lookup Table (LUT) based networks, where neurons\nare directly modeled using LUTs, help maximize this promise of offering\nultra-low latency and high area efficiency on FPGAs. Unfortunately, LUT\nresource usage scales exponentially with the number of inputs to the LUT,\nrestricting PolyLUT to small LUT sizes. This work introduces PolyLUT-Add, a\ntechnique that enhances neuron connectivity by combining $A$ PolyLUT\nsub-neurons via addition to improve accuracy. Moreover, we describe a novel\narchitecture to improve its scalability. We evaluated our implementation over\nthe MNIST, Jet Substructure classification, and Network Intrusion Detection\nbenchmark and found that for similar accuracy, PolyLUT-Add achieves a LUT\nreduction of $2.0-13.9\\times$ with a $1.2-1.6\\times$ decrease in latency.\n","authors":["Binglei Lou","Richard Rademacher","David Boland","Philip H. W. Leong"],"pdf_url":"https://arxiv.org/pdf/2406.04910v2.pdf","comment":"The source code for this paper is available at:\n  https://github.com/bingleilou/PolyLUT-Add"},{"id":"http://arxiv.org/abs/2409.01633v3","updated":"2024-09-15T12:17:03Z","published":"2024-09-03T06:04:39Z","title":"Dreaming is All You Need","summary":"  In classification tasks, achieving a harmonious balance between exploration\nand precision is of paramount importance. To this end, this research introduces\ntwo novel deep learning models, SleepNet and DreamNet, to strike this balance.\nSleepNet seamlessly integrates supervised learning with unsupervised ``sleep\"\nstages using pre-trained encoder models. Dedicated neurons within SleepNet are\nembedded in these unsupervised features, forming intermittent ``sleep\" blocks\nthat facilitate exploratory learning. Building upon the foundation of SleepNet,\nDreamNet employs full encoder-decoder frameworks to reconstruct the hidden\nstates, mimicking the human \"dreaming\" process. This reconstruction process\nenables further exploration and refinement of the learned representations.\nMoreover, the principle ideas of our SleepNet and DreamNet are generic and can\nbe applied to both computer vision and natural language processing downstream\ntasks. Through extensive empirical evaluations on diverse image and text\ndatasets, SleepNet and DreanNet have demonstrated superior performance compared\nto state-of-the-art models, showcasing the strengths of unsupervised\nexploration and supervised precision afforded by our innovative approaches.\n","authors":["Mingze Ni","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2409.01633v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09708v1","updated":"2024-09-15T12:14:24Z","published":"2024-09-15T12:14:24Z","title":"ELSA: Exploiting Layer-wise N:M Sparsity for Vision Transformer\n  Acceleration","summary":"  $N{:}M$ sparsity is an emerging model compression method supported by more\nand more accelerators to speed up sparse matrix multiplication in deep neural\nnetworks. Most existing $N{:}M$ sparsity methods compress neural networks with\na uniform setting for all layers in a network or heuristically determine the\nlayer-wise configuration by considering the number of parameters in each layer.\nHowever, very few methods have been designed for obtaining a layer-wise\ncustomized $N{:}M$ sparse configuration for vision transformers (ViTs), which\nusually consist of transformer blocks involving the same number of parameters.\nIn this work, to address the challenge of selecting suitable sparse\nconfiguration for ViTs on $N{:}M$ sparsity-supporting accelerators, we propose\nELSA, Exploiting Layer-wise $N{:}M$ Sparsity for ViTs. Considering not only all\n$N{:}M$ sparsity levels supported by a given accelerator but also the expected\nthroughput improvement, our methodology can reap the benefits of accelerators\nsupporting mixed sparsity by trading off negligible accuracy loss with both\nmemory usage and inference time reduction for ViT models. For instance, our\napproach achieves a noteworthy 2.9$\\times$ reduction in FLOPs for both Swin-B\nand DeiT-B with only a marginal degradation of accuracy on ImageNet. Our code\nwill be released upon paper acceptance.\n","authors":["Ning-Chi Huang","Chi-Chih Chang","Wei-Cheng Lin","Endri Taka","Diana Marculescu","Kai-Chiang Wu"],"pdf_url":"https://arxiv.org/pdf/2409.09708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09704v1","updated":"2024-09-15T11:53:24Z","published":"2024-09-15T11:53:24Z","title":"AlpaPICO: Extraction of PICO Frames from Clinical Trial Documents Using\n  LLMs","summary":"  In recent years, there has been a surge in the publication of clinical trial\nreports, making it challenging to conduct systematic reviews. Automatically\nextracting Population, Intervention, Comparator, and Outcome (PICO) from\nclinical trial studies can alleviate the traditionally time-consuming process\nof manually scrutinizing systematic reviews. Existing approaches of PICO frame\nextraction involves supervised approach that relies on the existence of\nmanually annotated data points in the form of BIO label tagging. Recent\napproaches, such as In-Context Learning (ICL), which has been shown to be\neffective for a number of downstream NLP tasks, require the use of labeled\nexamples. In this work, we adopt ICL strategy by employing the pretrained\nknowledge of Large Language Models (LLMs), gathered during the pretraining\nphase of an LLM, to automatically extract the PICO-related terminologies from\nclinical trial documents in unsupervised set up to bypass the availability of\nlarge number of annotated data instances. Additionally, to showcase the highest\neffectiveness of LLM in oracle scenario where large number of annotated samples\nare available, we adopt the instruction tuning strategy by employing Low Rank\nAdaptation (LORA) to conduct the training of gigantic model in low resource\nenvironment for the PICO frame extraction task. Our empirical results show that\nour proposed ICL-based framework produces comparable results on all the version\nof EBM-NLP datasets and the proposed instruction tuned version of our framework\nproduces state-of-the-art results on all the different EBM-NLP datasets. Our\nproject is available at \\url{https://github.com/shrimonmuke0202/AlpaPICO.git}.\n","authors":["Madhusudan Ghosh","Shrimon Mukherjee","Asmit Ganguly","Partha Basuchowdhuri","Sudip Kumar Naskar","Debasis Ganguly"],"pdf_url":"https://arxiv.org/pdf/2409.09704v1.pdf","comment":"Accepted at Methods"},{"id":"http://arxiv.org/abs/2409.09702v1","updated":"2024-09-15T11:42:17Z","published":"2024-09-15T11:42:17Z","title":"GFlowNet Pretraining with Inexpensive Rewards","summary":"  Generative Flow Networks (GFlowNets), a class of generative models have\nrecently emerged as a suitable framework for generating diverse and\nhigh-quality molecular structures by learning from unnormalized reward\ndistributions. Previous works in this direction often restrict exploration by\nusing predefined molecular fragments as building blocks, limiting the chemical\nspace that can be accessed. In this work, we introduce Atomic GFlowNets\n(A-GFNs), a foundational generative model leveraging individual atoms as\nbuilding blocks to explore drug-like chemical space more comprehensively. We\npropose an unsupervised pre-training approach using offline drug-like molecule\ndatasets, which conditions A-GFNs on inexpensive yet informative molecular\ndescriptors such as drug-likeliness, topological polar surface area, and\nsynthetic accessibility scores. These properties serve as proxy rewards,\nguiding A-GFNs towards regions of chemical space that exhibit desirable\npharmacological properties. We further our method by implementing a\ngoal-conditioned fine-tuning process, which adapts A-GFNs to optimize for\nspecific target properties. In this work, we pretrain A-GFN on the ZINC15\noffline dataset and employ robust evaluation metrics to show the effectiveness\nof our approach when compared to other relevant baseline methods in drug\ndesign.\n","authors":["Mohit Pandey","Gopeshh Subbaraj","Emmanuel Bengio"],"pdf_url":"https://arxiv.org/pdf/2409.09702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09692v1","updated":"2024-09-15T11:02:45Z","published":"2024-09-15T11:02:45Z","title":"Predicting building types and functions at transnational scale","summary":"  Building-specific knowledge such as building type and function information is\nimportant for numerous energy applications. However, comprehensive datasets\ncontaining this information for individual households are missing in many\nregions of Europe. For the first time, we investigate whether it is feasible to\npredict building types and functional classes at a European scale based on only\nopen GIS datasets available across countries. We train a graph neural network\n(GNN) classifier on a large-scale graph dataset consisting of OpenStreetMap\n(OSM) buildings across the EU, Norway, Switzerland, and the UK. To efficiently\nperform training using the large-scale graph, we utilize localized subgraphs. A\ngraph transformer model achieves a high Cohen's kappa coefficient of 0.754 when\nclassifying buildings into 9 classes, and a very high Cohen's kappa coefficient\nof 0.844 when classifying buildings into the residential and non-residential\nclasses. The experimental results imply three core novel contributions to\nliterature. Firstly, we show that building classification across multiple\ncountries is possible using a multi-source dataset consisting of information\nabout 2D building shape, land use, degree of urbanization, and countries as\ninput, and OSM tags as ground truth. Secondly, our results indicate that GNN\nmodels that consider contextual information about building neighborhoods\nimprove predictive performance compared to models that only consider individual\nbuildings and ignore the neighborhood. Thirdly, we show that training with GNNs\non localized subgraphs instead of standard GNNs improves performance for the\ntask of building classification.\n","authors":["Jonas Fill","Michael Eichelbeck","Michael Ebner"],"pdf_url":"https://arxiv.org/pdf/2409.09692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09691v1","updated":"2024-09-15T11:02:01Z","published":"2024-09-15T11:02:01Z","title":"Extrapolative ML Models for Copolymers","summary":"  Machine learning models have been progressively used for predicting materials\nproperties. These models can be built using pre-existing data and are useful\nfor rapidly screening the physicochemical space of a material, which is\nastronomically large. However, ML models are inherently interpolative, and\ntheir efficacy for searching candidates outside a material's known range of\nproperty is unresolved. Moreover, the performance of an ML model is intricately\nconnected to its learning strategy and the volume of training data. Here, we\ndetermine the relationship between the extrapolation ability of an ML model,\nthe size and range of its training dataset, and its learning approach. We focus\non a canonical problem of predicting the properties of a copolymer as a\nfunction of the sequence of its monomers. Tree search algorithms, which learn\nthe similarity between polymer structures, are found to be inefficient for\nextrapolation. Conversely, the extrapolation capability of neural networks and\nXGBoost models, which attempt to learn the underlying functional correlation\nbetween the structure and property of polymers, show strong correlations with\nthe volume and range of training data. These findings have important\nimplications on ML-based new material development.\n","authors":["Israrul H. Hashmi"," Himanshu","Rahul Karmakar","Tarak K Patra"],"pdf_url":"https://arxiv.org/pdf/2409.09691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09687v1","updated":"2024-09-15T10:50:22Z","published":"2024-09-15T10:50:22Z","title":"Training Safe Neural Networks with Global SDP Bounds","summary":"  This paper presents a novel approach to training neural networks with formal\nsafety guarantees using semidefinite programming (SDP) for verification. Our\nmethod focuses on verifying safety over large, high-dimensional input regions,\naddressing limitations of existing techniques that focus on adversarial\nrobustness bounds. We introduce an ADMM-based training scheme for an accurate\nneural network classifier on the Adversarial Spheres dataset, achieving\nprovably perfect recall with input dimensions up to $d=40$. This work advances\nthe development of reliable neural network verification methods for\nhigh-dimensional systems, with potential applications in safe RL policies.\n","authors":["Roman Soletskyi","David \"davidad\" Dalrymple"],"pdf_url":"https://arxiv.org/pdf/2409.09687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09677v1","updated":"2024-09-15T09:58:48Z","published":"2024-09-15T09:58:48Z","title":"Mitigating Dimensionality in 2D Rectangle Packing Problem under\n  Reinforcement Learning Schema","summary":"  This paper explores the application of Reinforcement Learning (RL) to the\ntwo-dimensional rectangular packing problem. We propose a reduced\nrepresentation of the state and action spaces that allow us for high\ngranularity. Leveraging UNet architecture and Proximal Policy Optimization\n(PPO), we achieved a model that is comparable to the MaxRect heuristic.\nHowever, our approach has great potential to be generalized to nonrectangular\npacking problems and complex constraints.\n","authors":["Waldemar Ko≈Çodziejczyk","Mariusz Kaleta"],"pdf_url":"https://arxiv.org/pdf/2409.09677v1.pdf","comment":"5th Polish Conference on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2201.13250v6","updated":"2024-09-15T09:58:20Z","published":"2022-01-31T13:59:28Z","title":"Differentiating and Integrating ZX Diagrams with Applications to Quantum\n  Machine Learning","summary":"  ZX-calculus has proved to be a useful tool for quantum technology with a wide\nrange of successful applications. Most of these applications are of an\nalgebraic nature. However, other tasks that involve differentiation and\nintegration remain unreachable with current ZX techniques. Here we elevate ZX\nto an analytical perspective by realising differentiation and integration\nentirely within the framework of ZX-calculus. We explicitly illustrate the new\nanalytic framework of ZX-calculus by applying it in context of quantum machine\nlearning for the analysis of barren plateaus.\n","authors":["Quanlong Wang","Richie Yeung","Mark Koch"],"pdf_url":"https://arxiv.org/pdf/2201.13250v6.pdf","comment":"Accepted for publication in Quantum"},{"id":"http://arxiv.org/abs/2408.07165v2","updated":"2024-09-15T09:47:29Z","published":"2024-08-13T19:08:56Z","title":"A POD-TANN approach for the multiscale modeling of materials and\n  macroelement derivation in geomechanics","summary":"  This paper introduces a novel approach that combines Proper Orthogonal\nDecomposition (POD) with Thermodynamics-based Artificial Neural Networks (TANN)\nto capture the macroscopic behavior of complex inelastic systems and derive\nmacroelements in geomechanics. The methodology leverages POD to extract\nmacroscopic Internal State Variables (ISVs) from microscopic state information,\nthereby enriching the macroscopic state description used to train an energy\npotential network within the TANN framework. The thermodynamic consistency\nprovided by TANN, combined with the hierarchical nature of POD, allows for\naccurate modeling of complex, non-linear material behavior and reliable\nmacroscopic geomechanical systems responses. The effectiveness of this approach\nis validated through applications of increasing complexity, demonstrating its\ncapability to handle various material behaviors and microstructural topologies.\nThese applications include the homogenization of continuous inelastic\nrepresentative unit cells (RUCs) and the derivation of a macroelement for a\ngeotechnical system involving a monopile in a clay layer subjected to\nhorizontal loading. The results indicate that the proposed POD-TANN methodology\nnot only achieves high accuracy in reproducing stress-strain responses, but\nalso significantly reduces computational costs, making it a practical tool for\nthe multiscale modeling of heterogeneous inelastic systems, and the efficient\nderivation of macroelements for complex geomechanical problems.\n","authors":["Giovanni Piunno","Ioannis Stefanou","Cristina Jommi"],"pdf_url":"https://arxiv.org/pdf/2408.07165v2.pdf","comment":"36 pages, 14 figures, Submitted to International Journal for\n  Numerical and Analytical Methods in Geomechanics"},{"id":"http://arxiv.org/abs/2409.09674v1","updated":"2024-09-15T09:43:59Z","published":"2024-09-15T09:43:59Z","title":"Model Selection Through Model Sorting","summary":"  We propose a novel approach to select the best model of the data. Based on\nthe exclusive properties of the nested models, we find the most parsimonious\nmodel containing the risk minimizer predictor. We prove the existence of\nprobable approximately correct (PAC) bounds on the difference of the minimum\nempirical risk of two successive nested models, called successive empirical\nexcess risk (SEER). Based on these bounds, we propose a model order selection\nmethod called nested empirical risk (NER). By the sorted NER (S-NER) method to\nsort the models intelligently, the minimum risk decreases. We construct a test\nthat predicts whether expanding the model decreases the minimum risk or not.\nWith a high probability, the NER and S-NER choose the true model order and the\nmost parsimonious model containing the risk minimizer predictor, respectively.\nWe use S-NER model selection in the linear regression and show that, the S-NER\nmethod without any prior information can outperform the accuracy of feature\nsorting algorithms like orthogonal matching pursuit (OMP) that aided with prior\nknowledge of the true model order. Also, in the UCR data set, the NER method\nreduces the complexity of the classification of UCR datasets dramatically, with\na negligible loss of accuracy.\n","authors":["Mohammad Ali Hajiani","Babak Seyfe"],"pdf_url":"https://arxiv.org/pdf/2409.09674v1.pdf","comment":"55 pages, 4 figures, submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence, October 26, 2023"},{"id":"http://arxiv.org/abs/2407.13301v2","updated":"2024-09-15T08:43:17Z","published":"2024-07-18T09:06:27Z","title":"CoD, Towards an Interpretable Medical Agent using Chain of Diagnosis","summary":"  The field of medical diagnosis has undergone a significant transformation\nwith the advent of large language models (LLMs), yet the challenges of\ninterpretability within these models remain largely unaddressed. This study\nintroduces Chain-of-Diagnosis (CoD) to enhance the interpretability of\nLLM-based medical diagnostics. CoD transforms the diagnostic process into a\ndiagnostic chain that mirrors a physician's thought process, providing a\ntransparent reasoning pathway. Additionally, CoD outputs the disease confidence\ndistribution to ensure transparency in decision-making. This interpretability\nmakes model diagnostics controllable and aids in identifying critical symptoms\nfor inquiry through the entropy reduction of confidences. With CoD, we\ndeveloped DiagnosisGPT, capable of diagnosing 9604 diseases. Experimental\nresults demonstrate that DiagnosisGPT outperforms other LLMs on diagnostic\nbenchmarks. Moreover, DiagnosisGPT provides interpretability while ensuring\ncontrollability in diagnostic rigor.\n","authors":["Junying Chen","Chi Gui","Anningzhe Gao","Ke Ji","Xidong Wang","Xiang Wan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13301v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09774v2","updated":"2024-09-15T08:41:01Z","published":"2023-11-16T10:56:24Z","title":"HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs","summary":"  Adapting a language model into a specific domain, a.k.a `domain adaption', is\na common practice when specialized knowledge, e.g. medicine, is not\nencapsulated in a general language model like Llama2. The challenge lies in the\nheterogeneity of data across the two training stages, as it varies in\nlanguages, genres, or formats. To tackle this and simplify the learning\nprotocol, we propose to transform heterogeneous data, from the both\npre-training and supervised stages, into a unified, simple input-output pair\nformat. We validate the new protocol in the domains where proprietary LLMs like\nChatGPT perform relatively poorly, such as Traditional Chinese Medicine. The\ndeveloped model, HuatuoGPT-II, has shown state-of-the-art performance in\nChinese medicine domain on a number of benchmarks, e.g. medical licensing\nexams. It even outperforms proprietary models like ChatGPT and GPT-4 in some\naspects, especially in Traditional Chinese Medicine. Expert manual evaluations\nfurther validate HuatuoGPT-II's advantages over existing LLMs. Notably,\nHuatuoGPT-II was benchmarked in a fresh Chinese National Medical Licensing\nExamination where it achieved the best performance, showcasing not only its\neffectiveness but also its generalization capabilities.\n","authors":["Junying Chen","Xidong Wang","Ke Ji","Anningzhe Gao","Feng Jiang","Shunian Chen","Hongbo Zhang","Dingjie Song","Wenya Xie","Chuyi Kong","Jianquan Li","Xiang Wan","Haizhou Li","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2311.09774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09653v1","updated":"2024-09-15T07:52:44Z","published":"2024-09-15T07:52:44Z","title":"KAN v.s. MLP for Offline Reinforcement Learning","summary":"  Kolmogorov-Arnold Networks (KAN) is an emerging neural network architecture\nin machine learning. It has greatly interested the research community about\nwhether KAN can be a promising alternative of the commonly used Multi-Layer\nPerceptions (MLP). Experiments in various fields demonstrated that KAN-based\nmachine learning can achieve comparable if not better performance than\nMLP-based methods, but with much smaller parameter scales and are more\nexplainable. In this paper, we explore the incorporation of KAN into the actor\nand critic networks for offline reinforcement learning (RL). We evaluated the\nperformance, parameter scales, and training efficiency of various KAN and MLP\nbased conservative Q-learning (CQL) on the the classical D4RL benchmark for\noffline RL. Our study demonstrates that KAN can achieve performance close to\nthe commonly used MLP with significantly fewer parameters. This provides us an\noption to choose the base networks according to the requirements of the offline\nRL tasks.\n","authors":["Haihong Guo","Fengxin Li","Jiao Li","Hongyan Liu"],"pdf_url":"https://arxiv.org/pdf/2409.09653v1.pdf","comment":"5 pages,2 figures"},{"id":"http://arxiv.org/abs/2409.09650v1","updated":"2024-09-15T07:48:40Z","published":"2024-09-15T07:48:40Z","title":"Conditional sampling within generative diffusion models","summary":"  Generative diffusions are a powerful class of Monte Carlo samplers that\nleverage bridging Markov processes to approximate complex, high-dimensional\ndistributions, such as those found in image processing and language models.\nDespite their success in these domains, an important open challenge remains:\nextending these techniques to sample from conditional distributions, as\nrequired in, for example, Bayesian inverse problems. In this paper, we present\na comprehensive review of existing computational approaches to conditional\nsampling within generative diffusion models. Specifically, we highlight key\nmethodologies that either utilise the joint distribution, or rely on\n(pre-trained) marginal distributions with explicit likelihoods, to construct\nconditional generative samplers.\n","authors":["Zheng Zhao","Ziwei Luo","Jens Sj√∂lund","Thomas B. Sch√∂n"],"pdf_url":"https://arxiv.org/pdf/2409.09650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09645v1","updated":"2024-09-15T07:41:55Z","published":"2024-09-15T07:41:55Z","title":"COSCO: A Sharpness-Aware Training Framework for Few-shot Multivariate\n  Time Series Classification","summary":"  Multivariate time series classification is an important task with widespread\ndomains of applications. Recently, deep neural networks (DNN) have achieved\nstate-of-the-art performance in time series classification. However, they often\nrequire large expert-labeled training datasets which can be infeasible in\npractice. In few-shot settings, i.e. only a limited number of samples per class\nare available in training data, DNNs show a significant drop in testing\naccuracy and poor generalization ability. In this paper, we propose to address\nthese problems from an optimization and a loss function perspective.\nSpecifically, we propose a new learning framework named COSCO consisting of a\nsharpness-aware minimization (SAM) optimization and a Prototypical loss\nfunction to improve the generalization ability of DNN for multivariate time\nseries classification problems under few-shot setting. Our experiments\ndemonstrate our proposed method outperforms the existing baseline methods. Our\nsource code is available at: https://github.com/JRB9/COSCO.\n","authors":["Jesus Barreda","Ashley Gomez","Ruben Puga","Kaixiong Zhou","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.09645v1.pdf","comment":"5 pages, 5 figures, CIKM '24 Short Paper Track"},{"id":"http://arxiv.org/abs/2406.19280v2","updated":"2024-09-15T07:25:49Z","published":"2024-06-27T15:50:41Z","title":"HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale","summary":"  The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.\n","authors":["Junying Chen","Chi Gui","Ruyi Ouyang","Anningzhe Gao","Shunian Chen","Guiming Hardy Chen","Xidong Wang","Ruifei Zhang","Zhenyang Cai","Ke Ji","Guangjun Yu","Xiang Wan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2406.19280v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09642v1","updated":"2024-09-15T07:25:08Z","published":"2024-09-15T07:25:08Z","title":"Extract and Diffuse: Latent Integration for Improved Diffusion-based\n  Speech and Vocal Enhancement","summary":"  Diffusion-based generative models have recently achieved remarkable results\nin speech and vocal enhancement due to their ability to model complex speech\ndata distributions. While these models generalize well to unseen acoustic\nenvironments, they may not achieve the same level of fidelity as the\ndiscriminative models specifically trained to enhance particular acoustic\nconditions. In this paper, we propose Ex-Diff, a novel score-based diffusion\nmodel that integrates the latent representations produced by a discriminative\nmodel to improve speech and vocal enhancement, which combines the strengths of\nboth generative and discriminative models. Experimental results on the widely\nused MUSDB dataset show relative improvements of 3.7% in SI-SDR and 10.0% in\nSI-SIR compared to the baseline diffusion model for speech and vocal\nenhancement tasks, respectively. Additionally, case studies are provided to\nfurther illustrate and analyze the complementary nature of generative and\ndiscriminative models in this context.\n","authors":["Yudong Yang","Zhan Liu","Wenyi Yu","Guangzhi Sun","Qiuqiang Kong","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.09642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12112v2","updated":"2024-09-15T07:16:38Z","published":"2024-08-22T03:54:08Z","title":"Balancing Act: Prioritization Strategies for LLM-Designed Restless\n  Bandit Rewards","summary":"  LLMs are increasingly used to design reward functions based on human\npreferences in Reinforcement Learning (RL). We focus on LLM-designed rewards\nfor Restless Multi-Armed Bandits, a framework for allocating limited resources\namong agents. In applications such as public health, this approach empowers\ngrassroots health workers to tailor automated allocation decisions to community\nneeds. In the presence of multiple agents, altering the reward function based\non human preferences can impact subpopulations very differently, leading to\ncomplex tradeoffs and a multi-objective resource allocation problem. We are the\nfirst to present a principled method termed Social Choice Language Model for\ndealing with these tradeoffs for LLM-designed rewards for multiagent planners\nin general and restless bandits in particular. The novel part of our model is a\ntransparent and configurable selection component, called an adjudicator,\nexternal to the LLM that controls complex tradeoffs via a user-selected social\nwelfare function. Our experiments demonstrate that our model reliably selects\nmore effective, aligned, and balanced reward functions compared to purely\nLLM-based approaches.\n","authors":["Shresth Verma","Niclas Boehmer","Lingkai Kong","Milind Tambe"],"pdf_url":"https://arxiv.org/pdf/2408.12112v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09635v1","updated":"2024-09-15T07:12:33Z","published":"2024-09-15T07:12:33Z","title":"A Novel Framework For Text Detection From Natural Scene Images With\n  Complex Background","summary":"  Recognizing texts from camera images is a known hard problem because of the\ndifficulties in text detection from the varied and complicated background. In\nthis paper we propose a novel and efficient method to detect text region from\nimages with complex background using Wavelet Transforms. The framework uses\nWavelet Transformation of the original image in its grayscale form followed by\nSub-band filtering. Then Region clustering technique is applied using centroids\nof the regions, further Bounding box is fitted to each region thus identifying\nthe text regions. This method is much sophisticated and efficient than the\nprevious methods as it doesn't stick to a particular font size of the text\nthus, making it generalized. The sample set used for experimental purpose\nconsists of 50 images with varying backgrounds. Images with edge prominence are\nconsidered. Furthermore, our method can be easily customized for applications\nwith different scopes.\n","authors":["Basavaraj Kaladagi","Jagadeesh Pujari"],"pdf_url":"https://arxiv.org/pdf/2409.09635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14953v2","updated":"2024-09-15T06:57:04Z","published":"2024-05-23T18:01:11Z","title":"Mallows-DPO: Fine-Tune Your LLM with Preference Dispersions","summary":"  Direct Preference Optimization (DPO) has recently emerged as a popular\napproach to improve reinforcement learning with human feedback (RLHF), leading\nto better techniques to fine-tune large language models (LLM). A weakness of\nDPO, however, lies in its lack of capability to characterize the diversity of\nhuman preferences. Inspired by Mallows' theory of preference ranking, we\ndevelop in this paper a new approach, the Mallows-DPO. A distinct feature of\nthis approach is a dispersion index, which reflects the dispersion of human\npreference to prompts. We show that existing DPO models can be reduced to\nspecial cases of this dispersion index, thus unified with Mallows-DPO. More\nimportantly, we demonstrate (empirically) how to use this dispersion index to\nenhance the performance of DPO in a broad array of benchmark tasks, from\nsynthetic bandit selection to controllable generations and dialogues, while\nmaintaining great generalization capabilities.\n","authors":["Haoxian Chen","Hanyang Zhao","Henry Lam","David Yao","Wenpin Tang"],"pdf_url":"https://arxiv.org/pdf/2405.14953v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09626v1","updated":"2024-09-15T06:37:12Z","published":"2024-09-15T06:37:12Z","title":"Understanding Simplicity Bias towards Compositional Mappings via\n  Learning Dynamics","summary":"  Obtaining compositional mappings is important for the model to generalize\nwell compositionally. To better understand when and how to encourage the model\nto learn such mappings, we study their uniqueness through different\nperspectives. Specifically, we first show that the compositional mappings are\nthe simplest bijections through the lens of coding length (i.e., an upper bound\nof their Kolmogorov complexity). This property explains why models having such\nmappings can generalize well. We further show that the simplicity bias is\nusually an intrinsic property of neural network training via gradient descent.\nThat partially explains why some models spontaneously generalize well when they\nare trained appropriately.\n","authors":["Yi Ren","Danica J. Sutherland"],"pdf_url":"https://arxiv.org/pdf/2409.09626v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2409.09614v1","updated":"2024-09-15T05:30:54Z","published":"2024-09-15T05:30:54Z","title":"HJ-sampler: A Bayesian sampler for inverse problems of a stochastic\n  process by leveraging Hamilton-Jacobi PDEs and score-based generative models","summary":"  The interplay between stochastic processes and optimal control has been\nextensively explored in the literature. With the recent surge in the use of\ndiffusion models, stochastic processes have increasingly been applied to sample\ngeneration. This paper builds on the log transform, known as the Cole-Hopf\ntransform in Brownian motion contexts, and extends it within a more abstract\nframework that includes a linear operator. Within this framework, we found that\nthe well-known relationship between the Cole-Hopf transform and optimal\ntransport is a particular instance where the linear operator acts as the\ninfinitesimal generator of a stochastic process. We also introduce a novel\nscenario where the linear operator is the adjoint of the generator, linking to\nBayesian inference under specific initial and terminal conditions. Leveraging\nthis theoretical foundation, we develop a new algorithm, named the HJ-sampler,\nfor Bayesian inference for the inverse problem of a stochastic differential\nequation with given terminal observations. The HJ-sampler involves two stages:\n(1) solving the viscous Hamilton-Jacobi partial differential equations, and (2)\nsampling from the associated stochastic optimal control problem. Our proposed\nalgorithm naturally allows for flexibility in selecting the numerical solver\nfor viscous HJ PDEs. We introduce two variants of the solver: the\nRiccati-HJ-sampler, based on the Riccati method, and the SGM-HJ-sampler, which\nutilizes diffusion models. We demonstrate the effectiveness and flexibility of\nthe proposed methods by applying them to solve Bayesian inverse problems\ninvolving various stochastic processes and prior distributions, including\napplications that address model misspecifications and quantifying model\nuncertainty.\n","authors":["Tingwei Meng","Zongren Zou","J√©r√¥me Darbon","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2409.09614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05956v5","updated":"2024-09-15T04:57:36Z","published":"2024-02-04T15:33:58Z","title":"Pathformer: Multi-scale Transformers with Adaptive Pathways for Time\n  Series Forecasting","summary":"  Transformers for time series forecasting mainly model time series from\nlimited or fixed scales, making it challenging to capture different\ncharacteristics spanning various scales. We propose Pathformer, a multi-scale\nTransformer with adaptive pathways. It integrates both temporal resolution and\ntemporal distance for multi-scale modeling. Multi-scale division divides the\ntime series into different temporal resolutions using patches of various sizes.\nBased on the division of each scale, dual attention is performed over these\npatches to capture global correlations and local details as temporal\ndependencies. We further enrich the multi-scale Transformer with adaptive\npathways, which adaptively adjust the multi-scale modeling process based on the\nvarying temporal dynamics of the input, improving the accuracy and\ngeneralization of Pathformer. Extensive experiments on eleven real-world\ndatasets demonstrate that Pathformer not only achieves state-of-the-art\nperformance by surpassing all current models but also exhibits stronger\ngeneralization abilities under various transfer scenarios. The code is made\navailable at https://github.com/decisionintelligence/pathformer.\n","authors":["Peng Chen","Yingying Zhang","Yunyao Cheng","Yang Shu","Yihang Wang","Qingsong Wen","Bin Yang","Chenjuan Guo"],"pdf_url":"https://arxiv.org/pdf/2402.05956v5.pdf","comment":"Accepted by the 12th International Conference on Learning\n  Representations (ICLR 2024)"},{"id":"http://arxiv.org/abs/2409.09611v1","updated":"2024-09-15T04:43:00Z","published":"2024-09-15T04:43:00Z","title":"Integrating Audio Narrations to Strengthen Domain Generalization in\n  Multimodal First-Person Action Recognition","summary":"  First-person activity recognition is rapidly growing due to the widespread\nuse of wearable cameras but faces challenges from domain shifts across\ndifferent environments, such as varying objects or background scenes. We\npropose a multimodal framework that improves domain generalization by\nintegrating motion, audio, and appearance features. Key contributions include\nanalyzing the resilience of audio and motion features to domain shifts, using\naudio narrations for enhanced audio-text alignment, and applying consistency\nratings between audio and visual narrations to optimize the impact of audio in\nrecognition during training. Our approach achieves state-of-the-art performance\non the ARGO1M dataset, effectively generalizing across unseen scenarios and\nlocations.\n","authors":["Cagri Gungor","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2409.09611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17326v7","updated":"2024-09-15T04:15:20Z","published":"2023-05-27T02:04:25Z","title":"Matrix Information Theory for Self-Supervised Learning","summary":"  The maximum entropy encoding framework provides a unified perspective for\nmany non-contrastive learning methods like SimSiam, Barlow Twins, and MEC.\nInspired by this framework, we introduce Matrix-SSL, a novel approach that\nleverages matrix information theory to interpret the maximum entropy encoding\nloss as matrix uniformity loss. Furthermore, Matrix-SSL enhances the maximum\nentropy encoding method by seamlessly incorporating matrix alignment loss,\ndirectly aligning covariance matrices in different branches. Experimental\nresults reveal that Matrix-SSL outperforms state-of-the-art methods on the\nImageNet dataset under linear evaluation settings and on MS-COCO for transfer\nlearning tasks. Specifically, when performing transfer learning tasks on\nMS-COCO, our method outperforms previous SOTA methods such as MoCo v2 and BYOL\nup to 3.3% with only 400 epochs compared to 800 epochs pre-training. We also\ntry to introduce representation learning into the language modeling regime by\nfine-tuning a 7B model using matrix cross-entropy loss, with a margin of 3.1%\non the GSM8K dataset over the standard cross-entropy loss. Code available at\nhttps://github.com/yifanzhang-pro/Matrix-SSL.\n","authors":["Yifan Zhang","Zhiquan Tan","Jingqin Yang","Weiran Huang","Yang Yuan"],"pdf_url":"https://arxiv.org/pdf/2305.17326v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09603v1","updated":"2024-09-15T03:55:03Z","published":"2024-09-15T03:55:03Z","title":"Towards Data-Centric RLHF: Simple Metrics for Preference Dataset\n  Comparison","summary":"  The goal of aligning language models to human preferences requires data that\nreveal these preferences. Ideally, time and money can be spent carefully\ncollecting and tailoring bespoke preference data to each downstream\napplication. However, in practice, a select few publicly available preference\ndatasets are often used to train reward models for reinforcement learning from\nhuman feedback (RLHF). While new preference datasets are being introduced with\nincreasing frequency, there are currently no existing efforts to measure and\ncompare these datasets. In this paper, we systematically study preference\ndatasets through three perspectives: scale, label noise, and information\ncontent. We propose specific metrics for each of these perspectives and uncover\ndifferent axes of comparison for a better understanding of preference datasets.\nOur work is a first step towards a data-centric approach to alignment by\nproviding perspectives that aid in training efficiency and iterative data\ncollection for RLHF.\n","authors":["Judy Hanwen Shen","Archit Sharma","Jun Qin"],"pdf_url":"https://arxiv.org/pdf/2409.09603v1.pdf","comment":"Working Paper"},{"id":"http://arxiv.org/abs/2405.20606v2","updated":"2024-09-15T03:32:03Z","published":"2024-05-31T03:40:15Z","title":"Vision-Language Meets the Skeleton: Progressively Distillation with\n  Cross-Modal Knowledge for 3D Action Representation Learning","summary":"  Skeleton-based action representation learning aims to interpret and\nunderstand human behaviors by encoding the skeleton sequences, which can be\ncategorized into two primary training paradigms: supervised learning and\nself-supervised learning. However, the former one-hot classification requires\nlabor-intensive predefined action categories annotations, while the latter\ninvolves skeleton transformations (e.g., cropping) in the pretext tasks that\nmay impair the skeleton structure. To address these challenges, we introduce a\nnovel skeleton-based training framework (C$^2$VL) based on Cross-modal\nContrastive learning that uses the progressive distillation to learn\ntask-agnostic human skeleton action representation from the Vision-Language\nknowledge prompts. Specifically, we establish the vision-language action\nconcept space through vision-language knowledge prompts generated by\npre-trained large multimodal models (LMMs), which enrich the fine-grained\ndetails that the skeleton action space lacks. Moreover, we propose the\nintra-modal self-similarity and inter-modal cross-consistency softened targets\nin the cross-modal representation learning process to progressively control and\nguide the degree of pulling vision-language knowledge prompts and corresponding\nskeletons closer. These soft instance discrimination and self-knowledge\ndistillation strategies contribute to the learning of better skeleton-based\naction representations from the noisy skeleton-vision-language pairs. During\nthe inference phase, our method requires only the skeleton data as the input\nfor action recognition and no longer for vision-language prompts. Extensive\nexperiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets demonstrate\nthat our method outperforms the previous methods and achieves state-of-the-art\nresults. Code is available at: https://github.com/cseeyangchen/C2VL.\n","authors":["Yang Chen","Tian He","Junfeng Fu","Ling Wang","Jingcai Guo","Ting Hu","Hong Cheng"],"pdf_url":"https://arxiv.org/pdf/2405.20606v2.pdf","comment":"Accepted by IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2409.09591v1","updated":"2024-09-15T02:36:26Z","published":"2024-09-15T02:36:26Z","title":"Open-World Test-Time Training: Self-Training with Contrast Learning","summary":"  Traditional test-time training (TTT) methods, while addressing domain shifts,\noften assume a consistent class set, limiting their applicability in real-world\nscenarios characterized by infinite variety. Open-World Test-Time Training\n(OWTTT) addresses the challenge of generalizing deep learning models to unknown\ntarget domain distributions, especially in the presence of strong\nOut-of-Distribution (OOD) data. Existing TTT methods often struggle to maintain\nperformance when confronted with strong OOD data. In OWTTT, the focus has\npredominantly been on distinguishing between overall strong and weak OOD data.\nHowever, during the early stages of TTT, initial feature extraction is hampered\nby interference from strong OOD and corruptions, resulting in diminished\ncontrast and premature classification of certain classes as strong OOD. To\naddress this, we introduce Open World Dynamic Contrastive Learning (OWDCL), an\ninnovative approach that utilizes contrastive learning to augment positive\nsample pairs. This strategy not only bolsters contrast in the early stages but\nalso significantly enhances model robustness in subsequent stages. In\ncomparison datasets, our OWDCL model has produced the most advanced\nperformance.\n","authors":["Houcheng Su","Mengzhu Wang","Jiao Li","Bingli Wang","Daixian Liu","Zeheng Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09591v1.pdf","comment":"10page"},{"id":"http://arxiv.org/abs/2409.09583v1","updated":"2024-09-15T01:56:09Z","published":"2024-09-15T01:56:09Z","title":"Machine learning assisted screening of metal binary alloys for anode\n  materials","summary":"  In the dynamic and rapidly advancing battery field, alloy anode materials are\na focal point due to their superior electrochemical performance. Traditional\nscreening methods are inefficient and time-consuming. Our research introduces a\nmachine learning-assisted strategy to expedite the discovery and optimization\nof these materials. We compiled a vast dataset from the MP and AFLOW databases,\nencompassing tens of thousands of alloy compositions and properties. Utilizing\na CGCNN, we accurately predicted the potential and specific capacity of alloy\nanodes, validated against experimental data. This approach identified\napproximately 120 low potential and high specific capacity alloy anodes\nsuitable for various battery systems including Li, Na, K, Zn, Mg, Ca, and\nAl-based. Our method not only streamlines the screening of battery anode\nmaterials but also propels the advancement of battery material research and\ninnovation in energy storage technology.\n","authors":["Xingyue Shi","Linming Zhou","Yuhui Huang","Yongjun Wu","Zijian Hong"],"pdf_url":"https://arxiv.org/pdf/2409.09583v1.pdf","comment":"41 pages include SI, 5 figures in main"},{"id":"http://arxiv.org/abs/2409.09569v1","updated":"2024-09-15T01:09:55Z","published":"2024-09-15T01:09:55Z","title":"Bias Begets Bias: The Impact of Biased Embeddings on Diffusion Models","summary":"  With the growing adoption of Text-to-Image (TTI) systems, the social biases\nof these models have come under increased scrutiny. Herein we conduct a\nsystematic investigation of one such source of bias for diffusion models:\nembedding spaces. First, because traditional classifier-based fairness\ndefinitions require true labels not present in generative modeling, we propose\nstatistical group fairness criteria based on a model's internal representation\nof the world. Using these definitions, we demonstrate theoretically and\nempirically that an unbiased text embedding space for input prompts is a\nnecessary condition for representationally balanced diffusion models, meaning\nthe distribution of generated images satisfy diversity requirements with\nrespect to protected attributes. Next, we investigate the impact of biased\nembeddings on evaluating the alignment between generated images and prompts, a\nprocess which is commonly used to assess diffusion models. We find that biased\nmultimodal embeddings like CLIP can result in lower alignment scores for\nrepresentationally balanced TTI models, thus rewarding unfair behavior.\nFinally, we develop a theoretical framework through which biases in alignment\nevaluation can be studied and propose bias mitigation methods. By specifically\nadapting the perspective of embedding spaces, we establish new fairness\nconditions for diffusion model development and evaluation.\n","authors":["Sahil Kuchlous","Marvin Li","Jeffrey G. Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09569v1.pdf","comment":"19 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.09563v1","updated":"2024-09-15T00:34:30Z","published":"2024-09-15T00:34:30Z","title":"Astrometric Binary Classification Via Artificial Neural Networks","summary":"  With nearly two billion stars observed and their corresponding astrometric\nparameters evaluated in the recent Gaia mission, the number of astrometric\nbinary candidates have risen significantly. Due to the surplus of astrometric\ndata, the current computational methods employed to inspect these astrometric\nbinary candidates are both computationally expensive and cannot be executed in\na reasonable time frame. In light of this, a machine learning (ML) technique to\nautomatically classify whether a set of stars belong to an astrometric binary\npair via an artificial neural network (ANN) is proposed. Using data from Gaia\nDR3, the ANN was trained and tested on 1.5 million highly probable true and\nvisual binaries, considering the proper motions, parallaxes, and angular and\nphysical separations as features. The ANN achieves high classification scores,\nwith an accuracy of 99.3%, a precision rate of 0.988, a recall rate of 0.991,\nand an AUC of 0.999, indicating that the utilized ML technique is a highly\neffective method for classifying astrometric binaries. Thus, the proposed ANN\nis a promising alternative to the existing methods for the classification of\nastrometric binaries.\n","authors":["Joe Smith"],"pdf_url":"https://arxiv.org/pdf/2409.09563v1.pdf","comment":"Accepted for publication in Astrophysical Journal (ApJ)"}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.09846v1","updated":"2024-09-15T19:53:30Z","published":"2024-09-15T19:53:30Z","title":"A Global Perspective on the Past, Present, and Future of Video Streaming\n  over Starlink","summary":"  This study presents the first global analysis of on-demand video streaming\nover Low Earth Orbit (LEO) satellite networks, using data from over one million\nhouseholds across 85 countries. We highlight Starlink's role as a major LEO\nprovider, enhancing connectivity in underserved regions. Our findings reveal\nthat while overall video quality on Starlink matches that of traditional\nnetworks, the inherent variability in LEO conditions -- such as throughput\nfluctuations and packet loss -- leads to an increase in bitrate switches and\nrebuffers. To further improve the quality of experience for the LEO community,\nwe manipulate existing congestion control and adaptive bitrate streaming\nalgorithms using simulation and real A/B tests deployed on over one million\nhouseholds. Our results underscore the need for video streaming and congestion\ncontrol algorithms to adapt to rapidly evolving network landscapes, ensuring\nhigh-quality service across diverse and dynamic network types.\n","authors":["Liz Izhikevich","Reese Enghardt","Te-Yuan Huang","Renata Teixeira"],"pdf_url":"https://arxiv.org/pdf/2409.09846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09823v1","updated":"2024-09-15T18:51:18Z","published":"2024-09-15T18:51:18Z","title":"Efficient Video to Audio Mapper with Visual Scene Detection","summary":"  Video-to-audio (V2A) generation aims to produce corresponding audio given\nsilent video inputs. This task is particularly challenging due to the\ncross-modality and sequential nature of the audio-visual features involved.\nRecent works have made significant progress in bridging the domain gap between\nvideo and audio, generating audio that is semantically aligned with the video\ncontent. However, a critical limitation of these approaches is their inability\nto effectively recognize and handle multiple scenes within a video, often\nleading to suboptimal audio generation in such cases. In this paper, we first\nreimplement a state-of-the-art V2A model with a slightly modified light-weight\narchitecture, achieving results that outperform the baseline. We then propose\nan improved V2A model that incorporates a scene detector to address the\nchallenge of switching between multiple visual scenes. Results on VGGSound show\nthat our model can recognize and handle multiple scenes within a video and\nachieve superior performance against the baseline for both fidelity and\nrelevance.\n","authors":["Mingjing Yi","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2409.09823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09638v1","updated":"2024-09-15T07:15:55Z","published":"2024-09-15T07:15:55Z","title":"Multi-view Hypergraph-based Contrastive Learning Model for Cold-Start\n  Micro-video Recommendation","summary":"  With the widespread use of mobile devices and the rapid growth of micro-video\nplatforms such as TikTok and Kwai, the demand for personalized micro-video\nrecommendation systems has significantly increased. Micro-videos typically\ncontain diverse information, such as textual metadata, visual cues (e.g., cover\nimages), and dynamic video content, significantly affecting user interaction\nand engagement patterns. However, most existing approaches often suffer from\nthe problem of over-smoothing, which limits their ability to capture\ncomprehensive interaction information effectively. Additionally, cold-start\nscenarios present ongoing challenges due to sparse interaction data and the\nunderutilization of available interaction signals.\n  To address these issues, we propose a Multi-view Hypergraph-based Contrastive\nlearning model for cold-start micro-video Recommendation (MHCR). MHCR\nintroduces a multi-view multimodal feature extraction layer to capture\ninteraction signals from various perspectives and incorporates multi-view\nself-supervised learning tasks to provide additional supervisory signals.\nThrough extensive experiments on two real-world datasets, we show that MHCR\nsignificantly outperforms existing video recommendation models and effectively\nmitigates cold-start challenges. Our code is available at\nhttps://anonymous.4open.science/r/MHCR-02EF.\n","authors":["Sisuo Lyu","Xiuze Zhou","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2409.09638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09601v1","updated":"2024-09-15T03:34:14Z","published":"2024-09-15T03:34:14Z","title":"A Survey of Foundation Models for Music Understanding","summary":"  Music is essential in daily life, fulfilling emotional and entertainment\nneeds, and connecting us personally, socially, and culturally. A better\nunderstanding of music can enhance our emotions, cognitive skills, and cultural\nconnections. The rapid advancement of artificial intelligence (AI) has\nintroduced new ways to analyze music, aiming to replicate human understanding\nof music and provide related services. While the traditional models focused on\naudio features and simple tasks, the recent development of large language\nmodels (LLMs) and foundation models (FMs), which excel in various fields by\nintegrating semantic information and demonstrating strong reasoning abilities,\ncould capture complex musical features and patterns, integrate music with\nlanguage and incorporate rich musical, emotional and psychological knowledge.\nTherefore, they have the potential in handling complex music understanding\ntasks from a semantic perspective, producing outputs closer to human\nperception. This work, to our best knowledge, is one of the early reviews of\nthe intersection of AI techniques and music understanding. We investigated,\nanalyzed, and tested recent large-scale music foundation models in respect of\ntheir music comprehension abilities. We also discussed their limitations and\nproposed possible future directions, offering insights for researchers in this\nfield.\n","authors":["Wenjun Li","Ying Cai","Ziyang Wu","Wenyi Zhang","Yifan Chen","Rundong Qi","Mengqi Dong","Peigen Chen","Xiao Dong","Fenghao Shi","Lei Guo","Junwei Han","Bao Ge","Tianming Liu","Lin Gan","Tuo Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.09601v1.pdf","comment":"20 pages, 2 figures"},{"id":"http://arxiv.org/abs/2405.20606v2","updated":"2024-09-15T03:32:03Z","published":"2024-05-31T03:40:15Z","title":"Vision-Language Meets the Skeleton: Progressively Distillation with\n  Cross-Modal Knowledge for 3D Action Representation Learning","summary":"  Skeleton-based action representation learning aims to interpret and\nunderstand human behaviors by encoding the skeleton sequences, which can be\ncategorized into two primary training paradigms: supervised learning and\nself-supervised learning. However, the former one-hot classification requires\nlabor-intensive predefined action categories annotations, while the latter\ninvolves skeleton transformations (e.g., cropping) in the pretext tasks that\nmay impair the skeleton structure. To address these challenges, we introduce a\nnovel skeleton-based training framework (C$^2$VL) based on Cross-modal\nContrastive learning that uses the progressive distillation to learn\ntask-agnostic human skeleton action representation from the Vision-Language\nknowledge prompts. Specifically, we establish the vision-language action\nconcept space through vision-language knowledge prompts generated by\npre-trained large multimodal models (LMMs), which enrich the fine-grained\ndetails that the skeleton action space lacks. Moreover, we propose the\nintra-modal self-similarity and inter-modal cross-consistency softened targets\nin the cross-modal representation learning process to progressively control and\nguide the degree of pulling vision-language knowledge prompts and corresponding\nskeletons closer. These soft instance discrimination and self-knowledge\ndistillation strategies contribute to the learning of better skeleton-based\naction representations from the noisy skeleton-vision-language pairs. During\nthe inference phase, our method requires only the skeleton data as the input\nfor action recognition and no longer for vision-language prompts. Extensive\nexperiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets demonstrate\nthat our method outperforms the previous methods and achieves state-of-the-art\nresults. Code is available at: https://github.com/cseeyangchen/C2VL.\n","authors":["Yang Chen","Tian He","Junfeng Fu","Ling Wang","Jingcai Guo","Ting Hu","Hong Cheng"],"pdf_url":"https://arxiv.org/pdf/2405.20606v2.pdf","comment":"Accepted by IEEE Transactions on Multimedia"}]},"2024-09-14T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2409.09554v1","updated":"2024-09-14T23:33:38Z","published":"2024-09-14T23:33:38Z","title":"ASR Error Correction using Large Language Models","summary":"  Error correction (EC) models play a crucial role in refining Automatic Speech\nRecognition (ASR) transcriptions, enhancing the readability and quality of\ntranscriptions. Without requiring access to the underlying code or model\nweights, EC can improve performance and provide domain adaptation for black-box\nASR systems. This work investigates the use of large language models (LLMs) for\nerror correction across diverse scenarios. 1-best ASR hypotheses are commonly\nused as the input to EC models. We propose building high-performance EC models\nusing ASR N-best lists which should provide more contextual information for the\ncorrection process. Additionally, the generation process of a standard EC model\nis unrestricted in the sense that any output sequence can be generated. For\nsome scenarios, such as unseen domains, this flexibility may impact\nperformance. To address this, we introduce a constrained decoding approach\nbased on the N-best list or an ASR lattice. Finally, most EC models are trained\nfor a specific ASR system requiring retraining whenever the underlying ASR\nsystem is changed. This paper explores the ability of EC models to operate on\nthe output of different ASR systems. This concept is further extended to\nzero-shot error correction using LLMs, such as ChatGPT. Experiments on three\nstandard datasets demonstrate the efficacy of our proposed methods for both\nTransducer and attention-based encoder-decoder ASR systems. In addition, the\nproposed method can serve as an effective method for model ensembling.\n","authors":["Rao Ma","Mengjie Qian","Mark Gales","Kate Knill"],"pdf_url":"https://arxiv.org/pdf/2409.09554v1.pdf","comment":"Submitted to IEEE Transactions on Audio, Speech and Language\n  Processing"},{"id":"http://arxiv.org/abs/2408.14496v2","updated":"2024-09-14T23:27:14Z","published":"2024-08-23T16:33:57Z","title":"A New Era in Computational Pathology: A Survey on Foundation and\n  Vision-Language Models","summary":"  Recent advances in deep learning have completely transformed the domain of\ncomputational pathology (CPath), which in turn altered the diagnostic workflow\nof pathologists by integrating foundation models (FMs) and vision-language\nmodels (VLMs) in their assessment and decision-making process. FMs overcome the\nlimitations of existing deep learning approaches in CPath by learning a\nrepresentation space that can be adapted to a wide variety of downstream tasks\nwithout explicit supervision. VLMs allow pathology reports written in natural\nlanguage to be used as a rich semantic information source to improve existing\nmodels as well as generate predictions in natural language form. In this\nsurvey, a holistic and systematic overview of recent innovations in FMs and\nVLMs in CPath is presented. Furthermore, the tools, datasets and training\nschemes for these models are summarized in addition to categorizing them into\ndistinct groups. This extensive survey highlights the current trends in CPath\nand the way it is going to be transformed through FMs and VLMs in the future.\n","authors":["Dibaloke Chanda","Milan Aryal","Nasim Yahya Soltani","Masoud Ganji"],"pdf_url":"https://arxiv.org/pdf/2408.14496v2.pdf","comment":"20 pages, 19 figures and 9 tables"},{"id":"http://arxiv.org/abs/2408.06266v5","updated":"2024-09-14T23:09:07Z","published":"2024-08-12T16:24:51Z","title":"Anchored Preference Optimization and Contrastive Revisions: Addressing\n  Underspecification in Alignment","summary":"  Large Language Models (LLMs) are often aligned using contrastive alignment\nobjectives and preference pair datasets. The interaction between model, paired\ndata, and objective makes alignment a complicated procedure, sometimes\nproducing subpar results. We study this and find that (i) preference data gives\na better learning signal when the underlying responses are contrastive, and\n(ii) alignment objectives lead to better performance when they specify more\ncontrol over the model during training. Based on these insights, we introduce\nContrastive Learning from AI Revisions (CLAIR), a data-creation method which\nleads to more contrastive preference pairs, and Anchored Preference\nOptimization (APO), a controllable and more stable alignment objective. We\nalign Llama-3-8B-Instruct using various comparable datasets and alignment\nobjectives and measure MixEval-Hard scores, which correlate highly with human\njudgments. The CLAIR preferences lead to the strongest performance out of all\ndatasets, and APO consistently outperforms less controllable objectives. Our\nbest model, trained on 32K CLAIR preferences with APO, improves\nLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code\nis available at https://github.com/ContextualAI/CLAIR_and_APO.\n","authors":["Karel D'Oosterlinck","Winnie Xu","Chris Develder","Thomas Demeester","Amanpreet Singh","Christopher Potts","Douwe Kiela","Shikib Mehri"],"pdf_url":"https://arxiv.org/pdf/2408.06266v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07369v2","updated":"2024-09-14T22:31:37Z","published":"2023-09-14T01:07:36Z","title":"Hybrid Attention-based Encoder-decoder Model for Efficient Language\n  Model Adaptation","summary":"  The attention-based encoder-decoder (AED) speech recognition model has been\nwidely successful in recent years. However, the joint optimization of acoustic\nmodel and language model in end-to-end manner has created challenges for text\nadaptation. In particular, effective, quick and inexpensive adaptation with\ntext input has become a primary concern for deploying AED systems in the\nindustry. To address this issue, we propose a novel model, the hybrid\nattention-based encoder-decoder (HAED) speech recognition model that preserves\nthe modularity of conventional hybrid automatic speech recognition systems. Our\nHAED model separates the acoustic and language models, allowing for the use of\nconventional text-based language model adaptation techniques. We demonstrate\nthat the proposed HAED model yields 23% relative Word Error Rate (WER)\nimprovements when out-of-domain text data is used for language model\nadaptation, with only a minor degradation in WER on a general test set compared\nwith the conventional AED model.\n","authors":["Shaoshi Ling","Guoli Ye","Rui Zhao","Yifan Gong"],"pdf_url":"https://arxiv.org/pdf/2309.07369v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11322v5","updated":"2024-09-14T21:55:08Z","published":"2024-03-17T19:54:16Z","title":"StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows","summary":"  It is a notable trend to use Large Language Models (LLMs) to tackle complex\ntasks, e.g., tasks that require a sequence of actions and dynamic interaction\nwith tools and external environments. In this paper, we propose StateFlow, a\nnovel LLM-based task-solving paradigm that conceptualizes complex task-solving\nprocesses as state machines. In StateFlow, we distinguish between \"process\ngrounding\" (via state and state transitions) and \"sub-task solving\" (through\nactions within a state), enhancing control and interpretability of the\ntask-solving procedure. A state represents the status of a running process. The\ntransitions between states are controlled by heuristic rules or decisions made\nby the LLM, allowing for a dynamic and adaptive progression. Upon entering a\nstate, a series of actions is executed, involving not only calling LLMs guided\nby different prompts, but also the utilization of external tools as needed. Our\nresults show that StateFlow significantly enhances LLMs' efficiency. For\ninstance, StateFlow achieves 13% and 28% higher success rates compared to ReAct\nin InterCode SQL and ALFWorld benchmark, with 5x and 3x less cost respectively.\nWe also show that StateFlow can be combined with iterative refining methods\nlike Reflexion to further improve performance.\n","authors":["Yiran Wu","Tianwei Yue","Shaokun Zhang","Chi Wang","Qingyun Wu"],"pdf_url":"https://arxiv.org/pdf/2403.11322v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04183v3","updated":"2024-09-14T21:28:20Z","published":"2024-07-04T23:05:58Z","title":"Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality\n  Norms","summary":"  Large language models (LLMs) are trained on broad corpora and then used in\ncommunities with specialized norms. Is providing LLMs with community rules\nenough for models to follow these norms? We evaluate LLMs' capacity to detect\n(Task 1) and correct (Task 2) biased Wikipedia edits according to Wikipedia's\nNeutral Point of View (NPOV) policy. LLMs struggled with bias detection,\nachieving only 64% accuracy on a balanced dataset. Models exhibited contrasting\nbiases (some under- and others over-predicted bias), suggesting distinct priors\nabout neutrality. LLMs performed better at generation, removing 79% of words\nremoved by Wikipedia editors. However, LLMs made additional changes beyond\nWikipedia editors' simpler neutralizations, resulting in high-recall but\nlow-precision editing. Interestingly, crowdworkers rated AI rewrites as more\nneutral (70%) and fluent (61%) than Wikipedia-editor rewrites. Qualitative\nanalysis found LLMs sometimes applied NPOV more comprehensively than Wikipedia\neditors but often made extraneous non-NPOV-related changes (such as grammar).\nLLMs may apply rules in ways that resonate with the public but diverge from\ncommunity experts. While potentially effective for generation, LLMs may reduce\neditor agency and increase moderation workload (e.g., verifying additions).\nEven when rules are easy to articulate, having LLMs apply them like community\nmembers may still be difficult.\n","authors":["Joshua Ashkinaze","Ruijia Guan","Laura Kurek","Eytan Adar","Ceren Budak","Eric Gilbert"],"pdf_url":"https://arxiv.org/pdf/2407.04183v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13163v2","updated":"2024-09-14T21:24:09Z","published":"2024-04-19T20:14:15Z","title":"Course-Skill Atlas: A national longitudinal dataset of skills taught in\n  U.S. higher education curricula","summary":"  Higher education plays a critical role in driving an innovative economy by\nequipping students with knowledge and skills demanded by the workforce. While\nresearchers and practitioners have developed data systems to track detailed\noccupational skills, such as those established by the U.S. Department of Labor\n(DOL), much less effort has been made to document which of these skills are\nbeing developed in higher education at a similar granularity. Here, we fill\nthis gap by presenting Course-Skill Atlas -- a longitudinal dataset of skills\ninferred from over three million course syllabi taught at nearly three thousand\nU.S. higher education institutions. To construct Course-Skill Atlas, we apply\nnatural language processing to quantify the alignment between course syllabi\nand detailed workplace activities (DWAs) used by the DOL to describe\noccupations. We then aggregate these alignment scores to create skill profiles\nfor institutions and academic majors. Our dataset offers a large-scale\nrepresentation of college education's role in preparing students for the labor\nmarket. Overall, Course-Skill Atlas can enable new research on the source of\nskills in the context of workforce development and provide actionable insights\nfor shaping the future of higher education to meet evolving labor demands,\nespecially in the face of new technologies.\n","authors":["Alireza Javadian Sabet","Sarah H. Bana","Renzhe Yu","Morgan R. Frank"],"pdf_url":"https://arxiv.org/pdf/2404.13163v2.pdf","comment":"24 pages, 14 figures, 7 tables"},{"id":"http://arxiv.org/abs/2408.16221v2","updated":"2024-09-14T20:57:08Z","published":"2024-08-29T02:35:53Z","title":"SSDM: Scalable Speech Dysfluency Modeling","summary":"  Speech dysfluency modeling is the core module for spoken language learning,\nand speech therapy. However, there are three challenges. First, current\nstate-of-the-art solutions suffer from poor scalability. Second, there is a\nlack of a large-scale dysfluency corpus. Third, there is not an effective\nlearning framework. In this paper, we propose \\textit{SSDM: Scalable Speech\nDysfluency Modeling}, which (1) adopts articulatory gestures as scalable forced\nalignment; (2) introduces connectionist subsequence aligner (CSA) to achieve\ndysfluency alignment; (3) introduces a large-scale simulated dysfluency corpus\ncalled Libri-Dys; and (4) develops an end-to-end system by leveraging the power\nof large language models (LLMs). We expect SSDM to serve as a standard in the\narea of dysfluency modeling. Demo is available at\n\\url{https://eureka235.github.io}.\n","authors":["Jiachen Lian","Xuanru Zhou","Zoe Ezzes","Jet Vonk","Brittany Morin","David Baquirin","Zachary Mille","Maria Luisa Gorno Tempini","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2408.16221v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06097v2","updated":"2024-09-14T20:55:13Z","published":"2024-09-09T22:29:35Z","title":"ClarQ-LLM: A Benchmark for Models Clarifying and Requesting Information\n  in Task-Oriented Dialog","summary":"  We introduce ClarQ-LLM, an evaluation framework consisting of bilingual\nEnglish-Chinese conversation tasks, conversational agents and evaluation\nmetrics, designed to serve as a strong benchmark for assessing agents' ability\nto ask clarification questions in task-oriented dialogues. The benchmark\nincludes 31 different task types, each with 10 unique dialogue scenarios\nbetween information seeker and provider agents. The scenarios require the\nseeker to ask questions to resolve uncertainty and gather necessary information\nto complete tasks. Unlike traditional benchmarks that evaluate agents based on\nfixed dialogue content, ClarQ-LLM includes a provider conversational agent to\nreplicate the original human provider in the benchmark. This allows both\ncurrent and future seeker agents to test their ability to complete information\ngathering tasks through dialogue by directly interacting with our provider\nagent. In tests, LLAMA3.1 405B seeker agent managed a maximum success rate of\nonly 60.05\\%, showing that ClarQ-LLM presents a strong challenge for future\nresearch.\n","authors":["Yujian Gan","Changling Li","Jinxia Xie","Luou Wen","Matthew Purver","Massimo Poesio"],"pdf_url":"https://arxiv.org/pdf/2409.06097v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13951v3","updated":"2024-09-14T20:24:21Z","published":"2023-11-23T12:04:25Z","title":"MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria","summary":"  Multimodal large language models (MLLMs) have broadened the scope of AI\napplications. Existing automatic evaluation methodologies for MLLMs are mainly\nlimited in evaluating queries without considering user experiences,\ninadequately addressing the nuances of creative and associative multimodal\ntasks. However, the open-ended and subjective nature of such tasks poses a\nsignificant challenge to the evaluation methodology, where it is difficult to\ndefine the ground-truth answers for them. To this end, in our paper, we propose\na new evaluation paradigm for MLLMs, which is evaluating MLLMs with per-sample\ncriteria using potent MLLM as the judge. To validate the feasibility and\neffectiveness of this paradigm, we design a benchmark, dubbed MLLM-Bench, by\ncurating the evaluation samples across six comprehensive cognitive levels. We\nbenchmark 21 popular MLLMs in a pairwise-comparison fashion, showing diverse\nperformance across models. Moreover, the validity of our benchmark manifests\nitself in reaching 88.02% agreement with human evaluation. We contend that the\nproposed paradigm explores the potential of MLLMs as effective evaluation tools\nwith the help of per-sample criteria. See online leaderboard at\n\\url{https://mllm-bench.llmzoo.com}.\n","authors":["Wentao Ge","Shunian Chen","Guiming Hardy Chen","Junying Chen","Zhihong Chen","Nuo Chen","Wenya Xie","Shuo Yan","Chenghao Zhu","Ziyue Lin","Song Dingjie","Xidong Wang","Anningzhe Gao","Zhang Zhiyi","Jianquan Li","Xiang Wan","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2311.13951v3.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2409.09513v1","updated":"2024-09-14T19:30:53Z","published":"2024-09-14T19:30:53Z","title":"Planning Transformer: Long-Horizon Offline Reinforcement Learning with\n  Planning Tokens","summary":"  Supervised learning approaches to offline reinforcement learning,\nparticularly those utilizing the Decision Transformer, have shown effectiveness\nin continuous environments and for sparse rewards. However, they often struggle\nwith long-horizon tasks due to the high compounding error of auto-regressive\nmodels. To overcome this limitation, we go beyond next-token prediction and\nintroduce Planning Tokens, which contain high-level, long time-scale\ninformation about the agent's future. Predicting dual time-scale tokens at\nregular intervals enables our model to use these long-horizon Planning Tokens\nas a form of implicit planning to guide its low-level policy and reduce\ncompounding error. This architectural modification significantly enhances\nperformance on long-horizon tasks, establishing a new state-of-the-art in\ncomplex D4RL environments. Additionally, we demonstrate that Planning Tokens\nimprove the interpretability of the model's policy through the interpretable\nplan visualisations and attention map.\n","authors":["Joseph Clinton","Robert Lieck"],"pdf_url":"https://arxiv.org/pdf/2409.09513v1.pdf","comment":"11 pages, 5 figures, Submitted to AAAI"},{"id":"http://arxiv.org/abs/2409.09510v1","updated":"2024-09-14T19:18:26Z","published":"2024-09-14T19:18:26Z","title":"Comparing Retrieval-Augmentation and Parameter-Efficient Fine-Tuning for\n  Privacy-Preserving Personalization of Large Language Models","summary":"  Privacy-preserving methods for personalizing large language models (LLMs) are\nrelatively under-explored. There are two schools of thought on this topic: (1)\ngenerating personalized outputs by personalizing the input prompt through\nretrieval augmentation from the user's personal information (RAG-based\nmethods), and (2) parameter-efficient fine-tuning of LLMs per user that\nconsiders efficiency and space limitations (PEFT-based methods). This paper\npresents the first systematic comparison between two approaches on a wide range\nof personalization tasks using seven diverse datasets. Our results indicate\nthat RAG-based and PEFT-based personalization methods on average yield 14.92%\nand 1.07% improvements over the non-personalized LLM, respectively. We find\nthat combining RAG with PEFT elevates these improvements to 15.98%.\nAdditionally, we identify a positive correlation between the amount of user\ndata and PEFT's effectiveness, indicating that RAG is a better choice for\ncold-start users (i.e., user's with limited personal data).\n","authors":["Alireza Salemi","Hamed Zamani"],"pdf_url":"https://arxiv.org/pdf/2409.09510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09504v1","updated":"2024-09-14T18:37:27Z","published":"2024-09-14T18:37:27Z","title":"Uddessho: An Extensive Benchmark Dataset for Multimodal Author Intent\n  Classification in Low-Resource Bangla Language","summary":"  With the increasing popularity of daily information sharing and acquisition\non the Internet, this paper introduces an innovative approach for intent\nclassification in Bangla language, focusing on social media posts where\nindividuals share their thoughts and opinions. The proposed method leverages\nmultimodal data with particular emphasis on authorship identification, aiming\nto understand the underlying purpose behind textual content, especially in the\ncontext of varied user-generated posts on social media. Current methods often\nface challenges in low-resource languages like Bangla, particularly when author\ntraits intricately link with intent, as observed in social media posts. To\naddress this, we present the Multimodal-based Author Bangla Intent\nClassification (MABIC) framework, utilizing text and images to gain deeper\ninsights into the conveyed intentions. We have created a dataset named\n\"Uddessho,\" comprising 3,048 instances sourced from social media. Our\nmethodology comprises two approaches for classifying textual intent and\nmultimodal author intent, incorporating early fusion and late fusion\ntechniques. In our experiments, the unimodal approach achieved an accuracy of\n64.53% in interpreting Bangla textual intent. In contrast, our multimodal\napproach significantly outperformed traditional unimodal methods, achieving an\naccuracy of 76.19%. This represents an improvement of 11.66%. To our best\nknowledge, this is the first research work on multimodal-based author intent\nclassification for low-resource Bangla language social media posts.\n","authors":["Fatema Tuj Johora Faria","Mukaffi Bin Moin","Md. Mahfuzur Rahman","Md Morshed Alam Shanto","Asif Iftekher Fahim","Md. Moinul Hoque"],"pdf_url":"https://arxiv.org/pdf/2409.09504v1.pdf","comment":"Accepted for publication in \"18th International Conference on\n  Information Technology and Applications (ICITA 2024)\""},{"id":"http://arxiv.org/abs/2409.09501v1","updated":"2024-09-14T18:15:07Z","published":"2024-09-14T18:15:07Z","title":"Synthetic4Health: Generating Annotated Synthetic Clinical Letters","summary":"  Since clinical letters contain sensitive information, clinical-related\ndatasets can not be widely applied in model training, medical research, and\nteaching. This work aims to generate reliable, various, and de-identified\nsynthetic clinical letters. To achieve this goal, we explored different\npre-trained language models (PLMs) for masking and generating text. After that,\nwe worked on Bio\\_ClinicalBERT, a high-performing model, and experimented with\ndifferent masking strategies. Both qualitative and quantitative methods were\nused for evaluation. Additionally, a downstream task, Named Entity Recognition\n(NER), was also implemented to assess the usability of these synthetic letters.\n  The results indicate that 1) encoder-only models outperform encoder-decoder\nmodels. 2) Among encoder-only models, those trained on general corpora perform\ncomparably to those trained on clinical data when clinical information is\npreserved. 3) Additionally, preserving clinical entities and document structure\nbetter aligns with our objectives than simply fine-tuning the model. 4)\nFurthermore, different masking strategies can impact the quality of synthetic\nclinical letters. Masking stopwords has a positive impact, while masking nouns\nor verbs has a negative effect. 5) For evaluation, BERTScore should be the\nprimary quantitative evaluation metric, with other metrics serving as\nsupplementary references. 6) Contextual information does not significantly\nimpact the models' understanding, so the synthetic clinical letters have the\npotential to replace the original ones in downstream tasks.\n","authors":["Libo Ren","Samuel Belkadi","Lifeng Han","Warren Del-Pinto","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2409.09501v1.pdf","comment":"ongoing work, 48 pages"},{"id":"http://arxiv.org/abs/2408.03326v2","updated":"2024-09-14T16:39:26Z","published":"2024-08-06T17:59:44Z","title":"LLaVA-OneVision: Easy Visual Task Transfer","summary":"  We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.\n","authors":["Bo Li","Yuanhan Zhang","Dong Guo","Renrui Zhang","Feng Li","Hao Zhang","Kaichen Zhang","Yanwei Li","Ziwei Liu","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2408.03326v2.pdf","comment":"Project Homepage:\n  https://llava-vl.github.io/blog/2024-08-05-llava-onevision/"},{"id":"http://arxiv.org/abs/2409.09467v1","updated":"2024-09-14T15:27:43Z","published":"2024-09-14T15:27:43Z","title":"Keeping Humans in the Loop: Human-Centered Automated Annotation with\n  Generative AI","summary":"  Automated text annotation is a compelling use case for generative large\nlanguage models (LLMs) in social media research. Recent work suggests that LLMs\ncan achieve strong performance on annotation tasks; however, these studies\nevaluate LLMs on a small number of tasks and likely suffer from contamination\ndue to a reliance on public benchmark datasets. Here, we test a human-centered\nframework for responsibly evaluating artificial intelligence tools used in\nautomated annotation. We use GPT-4 to replicate 27 annotation tasks across 11\npassword-protected datasets from recently published computational social\nscience articles in high-impact journals. For each task, we compare GPT-4\nannotations against human-annotated ground-truth labels and against annotations\nfrom separate supervised classification models fine-tuned on human-generated\nlabels. Although the quality of LLM labels is generally high, we find\nsignificant variation in LLM performance across tasks, even within datasets.\nOur findings underscore the importance of a human-centered workflow and careful\nevaluation standards: Automated annotations significantly diverge from human\njudgment in numerous scenarios, despite various optimization strategies such as\nprompt tuning. Grounding automated annotation in validation labels generated by\nhumans is essential for responsible evaluation.\n","authors":["Nicholas Pangakis","Samuel Wolken"],"pdf_url":"https://arxiv.org/pdf/2409.09467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09464v1","updated":"2024-09-14T15:17:34Z","published":"2024-09-14T15:17:34Z","title":"Rethinking the Influence of Source Code on Test Case Generation","summary":"  Large language models (LLMs) have been widely applied to assist test\ngeneration with the source code under test provided as the context. This paper\naims to answer the question: If the source code under test is incorrect, will\nLLMs be misguided when generating tests? The effectiveness of test cases is\nmeasured by their accuracy, coverage, and bug detection effectiveness. Our\nevaluation results with five open- and six closed-source LLMs on four datasets\ndemonstrate that incorrect code can significantly mislead LLMs in generating\ncorrect, high-coverage, and bug-revealing tests. For instance, in the HumanEval\ndataset, LLMs achieve 80.45% test accuracy when provided with task descriptions\nand correct code, but only 57.12% when given task descriptions and incorrect\ncode. For the APPS dataset, prompts with correct code yield tests that detect\n39.85% of the bugs, while prompts with incorrect code detect only 19.61%. These\nfindings have important implications for the deployment of LLM-based testing:\nusing it on mature code may help protect against future regression, but on\nearly-stage immature code, it may simply bake in errors. Our findings also\nunderscore the need for further research to improve LLMs resilience against\nincorrect code in generating reliable and bug-revealing tests.\n","authors":["Dong Huang","Jie M. Zhang","Mingzhe Du","Mark Harman","Heming Cui"],"pdf_url":"https://arxiv.org/pdf/2409.09464v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2405.19846v3","updated":"2024-09-14T11:57:54Z","published":"2024-05-30T08:50:55Z","title":"Quest: Query-centric Data Synthesis Approach for Long-context Scaling of\n  Large Language Model","summary":"  Large language models, initially pre-trained with a limited context length,\ncan better handle longer texts by continuing training on a corpus with extended\ncontexts. However, obtaining effective long-context data is challenging due to\nthe scarcity and uneven distribution of long documents across different\ndomains. To address this issue, we propose a Query-centric data synthesis\nmethod, abbreviated as Quest. Quest is an interpretable method based on the\nobservation that documents retrieved by similar queries are relevant but\nlow-redundant, thus well-suited for synthesizing long-context data. The method\nis also scalable and capable of constructing large amounts of long-context\ndata. Using Quest, we synthesize a long-context dataset up to 128k context\nlength, significantly outperforming other data synthesis methods on multiple\nlong-context benchmark datasets. In addition, we further verify that the Quest\nmethod is predictable through scaling law experiments, making it a reliable\nsolution for advancing long-context models.\n","authors":["Chaochen Gao","Xing Wu","Qi Fu","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2405.19846v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07865v5","updated":"2024-09-14T11:41:49Z","published":"2024-03-12T17:55:38Z","title":"CodeAttack: Revealing Safety Generalization Challenges of Large Language\n  Models via Code Completion","summary":"  The rapid advancement of Large Language Models (LLMs) has brought about\nremarkable generative capabilities but also raised concerns about their\npotential misuse. While strategies like supervised fine-tuning and\nreinforcement learning from human feedback have enhanced their safety, these\nmethods primarily focus on natural languages, which may not generalize to other\ndomains. This paper introduces CodeAttack, a framework that transforms natural\nlanguage inputs into code inputs, presenting a novel environment for testing\nthe safety generalization of LLMs. Our comprehensive studies on\nstate-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a\nnew and universal safety vulnerability of these models against code input:\nCodeAttack bypasses the safety guardrails of all models more than 80\\% of the\ntime. We find that a larger distribution gap between CodeAttack and natural\nlanguage leads to weaker safety generalization, such as encoding natural\nlanguage input with data structures. Furthermore, we give our hypotheses about\nthe success of CodeAttack: the misaligned bias acquired by LLMs during code\ntraining, prioritizing code completion over avoiding the potential safety risk.\nFinally, we analyze potential mitigation measures. These findings highlight new\nsafety risks in the code domain and the need for more robust safety alignment\nalgorithms to match the code capabilities of LLMs.\n","authors":["Qibing Ren","Chang Gao","Jing Shao","Junchi Yan","Xin Tan","Wai Lam","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.07865v5.pdf","comment":"ACL Findings 2024, Code is available at\n  https://github.com/renqibing/CodeAttack"},{"id":"http://arxiv.org/abs/2409.09415v1","updated":"2024-09-14T11:12:07Z","published":"2024-09-14T11:12:07Z","title":"Enhancing LLM Problem Solving with REAP: Reflection, Explicit Problem\n  Deconstruction, and Advanced Prompting","summary":"  Large Language Models (LLMs) have transformed natural language processing,\nyet improving their problem-solving capabilities, particularly for complex,\nreasoning-intensive tasks, remains a persistent challenge. This paper\nintroduces the REAP (Reflection, Explicit Problem Deconstruction, and Advanced\nPrompting) method, an innovative approach within the dynamic context generation\nframework. REAP guides LLMs through reflection on the query, deconstructing it\ninto manageable components, and generating relevant context to enhance the\nsolution process. We evaluated REAP using a dataset designed to expose LLM\nlimitations, comparing zero-shot prompting with REAP-enhanced prompts across\nsix state-of-the-art models: OpenAI's o1-preview, o1-mini, GPT-4o, GPT-4o-mini,\nGoogle's Gemini 1.5 Pro, and Claude 3.5 Sonnet. The results demonstrate notable\nperformance gains, with o1-mini improving by 40.97%, GPT-4o by 66.26%, and\nGPT-4o-mini by 112.93%. Despite the already strong baseline performance of\nOpenAI's o1-preview, modest gains were observed. Beyond performance\nimprovements, REAP offers a cost-effective solution; for example, GPT-4o-mini,\nwhich is approximately 100 times cheaper than o1-preview, delivered competitive\nresults. REAP also improves the clarity of model outputs, making it easier for\nhumans to understand the reasoning behind the results and simplifying the\nprocess of identifying and addressing any issues. These findings demonstrate\nREAP's potential to greatly improve the capabilities of LLMs, providing both\nbetter performance and increased cost-efficiency across a wide range of\napplications.\n","authors":["Ryan Lingo","Martin Arroyo","Rajeev Chhajer"],"pdf_url":"https://arxiv.org/pdf/2409.09415v1.pdf","comment":"524 pages, 3 figures"},{"id":"http://arxiv.org/abs/2409.09413v1","updated":"2024-09-14T11:03:12Z","published":"2024-09-14T11:03:12Z","title":"Constructive Approach to Bidirectional Causation between Qualia\n  Structure and Language Emergence","summary":"  This paper presents a novel perspective on the bidirectional causation\nbetween language emergence and relational structure of subjective experiences,\ntermed qualia structure, and lays out the constructive approach to the\nintricate dependency between the two. We hypothesize that languages with\ndistributional semantics, e.g., syntactic-semantic structures, may have emerged\nthrough the process of aligning internal representations among individuals, and\nsuch alignment of internal representations facilitates more structured\nlanguage. This mutual dependency is suggested by the recent advancements in AI\nand symbol emergence robotics, and collective predictive coding (CPC)\nhypothesis, in particular. Computational studies show that neural network-based\nlanguage models form systematically structured internal representations, and\nmultimodal language models can share representations between language and\nperceptual information. This perspective suggests that language emergence\nserves not only as a mechanism creating a communication tool but also as a\nmechanism for allowing people to realize shared understanding of qualitative\nexperiences. The paper discusses the implications of this bidirectional\ncausation in the context of consciousness studies, linguistics, and cognitive\nscience, and outlines future constructive research directions to further\nexplore this dynamic relationship between language emergence and qualia\nstructure.\n","authors":["Tadahiro Taniguchi","Masafumi Oizumi","Noburo Saji","Takato Horii","Naotsugu Tsuchiya"],"pdf_url":"https://arxiv.org/pdf/2409.09413v1.pdf","comment":"20 pages, 4 Figures"},{"id":"http://arxiv.org/abs/2409.09401v1","updated":"2024-09-14T10:23:35Z","published":"2024-09-14T10:23:35Z","title":"Towards Diverse and Efficient Audio Captioning via Diffusion Models","summary":"  We introduce Diffusion-based Audio Captioning (DAC), a non-autoregressive\ndiffusion model tailored for diverse and efficient audio captioning. Although\nexisting captioning models relying on language backbones have achieved\nremarkable success in various captioning tasks, their insufficient performance\nin terms of generation speed and diversity impede progress in audio\nunderstanding and multimedia applications. Our diffusion-based framework offers\nunique advantages stemming from its inherent stochasticity and holistic context\nmodeling in captioning. Through rigorous evaluation, we demonstrate that DAC\nnot only achieves SOTA performance levels compared to existing benchmarks in\nthe caption quality, but also significantly outperforms them in terms of\ngeneration speed and diversity. The success of DAC illustrates that text\ngeneration can also be seamlessly integrated with audio and visual generation\ntasks using a diffusion backbone, paving the way for a unified, audio-related\ngenerative model across different modalities.\n","authors":["Manjie Xu","Chenxing Li","Xinyi Tu","Yong Ren","Ruibo Fu","Wei Liang","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2409.09401v1.pdf","comment":"https://sites.google.com/view/diffusion-audio-captioning"},{"id":"http://arxiv.org/abs/2403.03640v5","updated":"2024-09-14T08:48:58Z","published":"2024-03-06T11:56:02Z","title":"Apollo: A Lightweight Multilingual Medical LLM towards Democratizing\n  Medical AI to 6B People","summary":"  Despite the vast repository of global medical knowledge predominantly being\nin English, local languages are crucial for delivering tailored healthcare\nservices, particularly in areas with limited medical resources. To extend the\nreach of medical AI advancements to a broader population, we aim to develop\nmedical LLMs across the six most widely spoken languages, encompassing a global\npopulation of 6.1 billion. This effort culminates in the creation of the\nApolloCorpora multilingual medical dataset and the XMedBench benchmark. In the\nmultilingual medical benchmark, the released Apollo models, at various\nrelatively-small sizes (i.e., 0.5B, 1.8B, 2B, 6B, and 7B), achieve the best\nperformance among models of equivalent size. Especially, Apollo-7B is the\nstate-of-the-art multilingual medical LLMs up to 70B. Additionally, these lite\nmodels could be used to improve the multi-lingual medical capabilities of\nlarger models without fine-tuning in a proxy-tuning fashion. We will\nopen-source training corpora, code, model weights and evaluation benchmark.\n","authors":["Xidong Wang","Nuo Chen","Junyin Chen","Yidong Wang","Guorui Zhen","Chunxian Zhang","Xiangbo Wu","Yan Hu","Anningzhe Gao","Xiang Wan","Haizhou Li","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2403.03640v5.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2409.09362v1","updated":"2024-09-14T08:30:59Z","published":"2024-09-14T08:30:59Z","title":"Generating Event-oriented Attribution for Movies via Two-Stage\n  Prefix-Enhanced Multimodal LLM","summary":"  The prosperity of social media platforms has raised the urgent demand for\nsemantic-rich services, e.g., event and storyline attribution. However, most\nexisting research focuses on clip-level event understanding, primarily through\nbasic captioning tasks, without analyzing the causes of events across an entire\nmovie. This is a significant challenge, as even advanced multimodal large\nlanguage models (MLLMs) struggle with extensive multimodal information due to\nlimited context length. To address this issue, we propose a Two-Stage\nPrefix-Enhanced MLLM (TSPE) approach for event attribution, i.e., connecting\nassociated events with their causal semantics, in movie videos. In the local\nstage, we introduce an interaction-aware prefix that guides the model to focus\non the relevant multimodal information within a single clip, briefly\nsummarizing the single event. Correspondingly, in the global stage, we\nstrengthen the connections between associated events using an inferential\nknowledge graph, and design an event-aware prefix that directs the model to\nfocus on associated events rather than all preceding clips, resulting in\naccurate event attribution. Comprehensive evaluations of two real-world\ndatasets demonstrate that our framework outperforms state-of-the-art methods.\n","authors":["Yuanjie Lyu","Tong Xu","Zihan Niu","Bo Peng","Jing Ke","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2409.09362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03563v2","updated":"2024-09-14T08:05:22Z","published":"2024-07-04T01:25:20Z","title":"Learning Video Temporal Dynamics with Cross-Modal Attention for Robust\n  Audio-Visual Speech Recognition","summary":"  Audio-visual speech recognition (AVSR) aims to transcribe human speech using\nboth audio and video modalities. In practical environments with noise-corrupted\naudio, the role of video information becomes crucial. However, prior works have\nprimarily focused on enhancing audio features in AVSR, overlooking the\nimportance of video features. In this study, we strengthen the video features\nby learning three temporal dynamics in video data: context order, playback\ndirection, and the speed of video frames. Cross-modal attention modules are\nintroduced to enrich video features with audio information so that speech\nvariability can be taken into account when training on the video temporal\ndynamics. Based on our approach, we achieve the state-of-the-art performance on\nthe LRS2 and LRS3 AVSR benchmarks for the noise-dominant settings. Our approach\nexcels in scenarios especially for babble and speech noise, indicating the\nability to distinguish the speech signal that should be recognized from lip\nmovements in the video modality. We support the validity of our methodology by\noffering the ablation experiments for the temporal dynamics losses and the\ncross-modal attention architecture design.\n","authors":["Sungnyun Kim","Kangwook Jang","Sangmin Bae","Hoirin Kim","Se-Young Yun"],"pdf_url":"https://arxiv.org/pdf/2407.03563v2.pdf","comment":"Accepted at SLT 2024 Main Conference"},{"id":"http://arxiv.org/abs/2409.09353v1","updated":"2024-09-14T07:49:29Z","published":"2024-09-14T07:49:29Z","title":"Overcoming linguistic barriers in code assistants: creating a QLoRA\n  adapter to improve support for Russian-language code writing instructions","summary":"  In this paper, an approach to training and evaluating an adapter model for\nthe popular language model \"zephyr-7b-beta\" is described. The adapter was\ndeveloped to improve the performance of the base model in tasks related to\nprogramming and understanding the Russian language. Considering the high\nquality of the original model in tasks in the English language, the goal of the\nresearch was to expand its linguistic and technical spectrum. The proposed\nadapter was trained using a large and diverse dataset, including\nquestion-answer pairs related to programming, as well code-related texts in\nRussian language. The applied training methodology ensures an improvement in\nthe model's quality of answers in understanding and generating Python code\nbased on Russian instructions. We evaluated the performance of the base model\nwith the installed adapter using various metrics, comparing it to the base\nmodel as well as other state-of-the-art models in this field. The obtained\nresults showed significant improvement, both in tasks related to writing Python\ncode and in processing the Russian language, confirming the effectiveness of\nthe proposed adapter.\n","authors":["C. B. Pronin","A. V. Volosova","A. V. Ostroukh","Yu. N. Strogov"],"pdf_url":"https://arxiv.org/pdf/2409.09353v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2305.05964v2","updated":"2024-09-14T07:45:27Z","published":"2023-05-10T08:16:36Z","title":"Interpretable Multimodal Misinformation Detection with Logic Reasoning","summary":"  Multimodal misinformation on online social platforms is becoming a critical\nconcern due to increasing credibility and easier dissemination brought by\nmultimedia content, compared to traditional text-only information. While\nexisting multimodal detection approaches have achieved high performance, the\nlack of interpretability hinders these systems' reliability and practical\ndeployment. Inspired by NeuralSymbolic AI which combines the learning ability\nof neural networks with the explainability of symbolic learning, we propose a\nnovel logic-based neural model for multimodal misinformation detection which\nintegrates interpretable logic clauses to express the reasoning process of the\ntarget task. To make learning effective, we parameterize symbolic logical\nelements using neural representations, which facilitate the automatic\ngeneration and evaluation of meaningful logic clauses. Additionally, to make\nour framework generalizable across diverse misinformation sources, we introduce\nfive meta-predicates that can be instantiated with different correlations.\nResults on three public datasets (Twitter, Weibo, and Sarcasm) demonstrate the\nfeasibility and versatility of our model.\n","authors":["Hui Liu","Wenya Wang","Haoliang Li"],"pdf_url":"https://arxiv.org/pdf/2305.05964v2.pdf","comment":"Accepted by Findings of ACL 23. 9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.16672v4","updated":"2024-09-14T07:41:06Z","published":"2024-08-29T16:21:00Z","title":"Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction\n  Retriever","summary":"  Multi-vector dense models, such as ColBERT, have proven highly effective in\ninformation retrieval. ColBERT's late interaction scoring approximates the\njoint query-document attention seen in cross-encoders while maintaining\ninference efficiency closer to traditional dense retrieval models, thanks to\nits bi-encoder architecture and recent optimizations in indexing and search. In\nthis work we propose a number of incremental improvements to the ColBERT model\narchitecture and training pipeline, using methods shown to work in the more\nmature single-vector embedding model training paradigm, particularly those that\napply to heterogeneous multilingual data or boost efficiency with little\ntradeoff. Our new model, Jina-ColBERT-v2, demonstrates strong performance\nacross a range of English and multilingual retrieval tasks.\n","authors":["Rohan Jha","Bo Wang","Michael G√ºnther","Georgios Mastrapas","Saba Sturua","Isabelle Mohr","Andreas Koukounas","Mohammad Kalim Akram","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2408.16672v4.pdf","comment":"8 pages, references at pp7,8; EMNLP workshop submission"},{"id":"http://arxiv.org/abs/2402.11291v3","updated":"2024-09-14T06:12:36Z","published":"2024-02-17T14:19:38Z","title":"Puzzle Solving using Reasoning of Large Language Models: A Survey","summary":"  Exploring the capabilities of Large Language Models (LLMs) in puzzle solving\nunveils critical insights into their potential and challenges in AI, marking a\nsignificant step towards understanding their applicability in complex reasoning\ntasks. This survey leverages a unique taxonomy -- dividing puzzles into\nrule-based and rule-less categories -- to critically assess LLMs through\nvarious methodologies, including prompting techniques, neuro-symbolic\napproaches, and fine-tuning. Through a critical review of relevant datasets and\nbenchmarks, we assess LLMs' performance, identifying significant challenges in\ncomplex puzzle scenarios. Our findings highlight the disparity between LLM\ncapabilities and human-like reasoning, particularly in those requiring advanced\nlogical inference. The survey underscores the necessity for novel strategies\nand richer datasets to advance LLMs' puzzle-solving proficiency and contribute\nto AI's logical reasoning and creative problem-solving advancements.\n","authors":["Panagiotis Giadikiaroglou","Maria Lymperaiou","Giorgos Filandrianos","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2402.11291v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09324v1","updated":"2024-09-14T06:02:17Z","published":"2024-09-14T06:02:17Z","title":"Efficient Fine-Tuning of Large Language Models for Automated Medical\n  Documentation","summary":"  Scientific research indicates that for every hour spent in direct patient\ncare, physicians spend nearly two additional hours on administrative tasks,\nparticularly on electronic health records (EHRs) and desk work. This excessive\nadministrative burden not only reduces the time available for patient care but\nalso contributes to physician burnout and inefficiencies in healthcare\ndelivery. To address these challenges, this study introduces MediGen, a\nfine-tuned large language model (LLM) designed to automate the generation of\nmedical reports from medical dialogues. By leveraging state-of-the-art\nmethodologies for fine-tuning open-source pretrained models, including\nLLaMA3-8B, MediGen achieves high accuracy in transcribing and summarizing\nclinical interactions. The fine-tuned LLaMA3-8B model demonstrated promising\nresults, achieving a ROUGE score of 58% and a BERTScore-F1 of 72%, indicating\nits effectiveness in generating accurate and clinically relevant medical\nreports. These findings suggest that MediGen has the potential to significantly\nreduce the administrative workload on physicians, improving both healthcare\nefficiency and physician well-being.\n","authors":["Hui Yi Leong","Yi Fan Gao","Ji Shuai","Uktu Pamuksuz"],"pdf_url":"https://arxiv.org/pdf/2409.09324v1.pdf","comment":"4 pages, 3 Figures, 3 Tables, This is a preprint version of the\n  article. The final version will be published in the proceedings of the IEEE\n  conference"},{"id":"http://arxiv.org/abs/2409.09322v1","updated":"2024-09-14T05:51:50Z","published":"2024-09-14T05:51:50Z","title":"A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction","summary":"  Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods.\n","authors":["Wanlong Liu","Enqi Zhang","Li Zhou","Dingyi Zeng","Shaohuan Cheng","Chen Zhang","Malu Zhang","Wenyu Chen"],"pdf_url":"https://arxiv.org/pdf/2409.09322v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2409.09318v1","updated":"2024-09-14T05:31:29Z","published":"2024-09-14T05:31:29Z","title":"ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language\n  Models","summary":"  Hallucination poses a significant challenge for multimodal large language\nmodels (MLLMs). However, existing benchmarks for evaluating hallucinations are\nstatic, which can lead to potential data contamination. This paper introduces\nODE, an open-set, dynamic protocol for evaluating object existence\nhallucinations in MLLMs. Our framework employs graph structures to model\nassociations between real-word concepts and generates novel samples for both\ngeneral and domain-specific scenarios. The dynamic combination of concepts,\nalong with various combination principles, ensures a broad sample distribution.\nExperimental results show that MLLMs exhibit higher hallucination rates with\nODE-generated samples, effectively avoiding data contamination. Moreover, these\nsamples can also be used for fine-tuning to improve MLLM performance on\nexisting benchmarks.\n","authors":["Yahan Tu","Rui Hu","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2409.09318v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04915v3","updated":"2024-09-14T04:49:08Z","published":"2023-11-02T02:21:39Z","title":"Chain of Empathy: Enhancing Empathetic Response of Large Language Models\n  Based on Psychotherapy Models","summary":"  We present a novel method, the Chain of Empathy (CoE) prompting, that\nutilizes insights from psychotherapy to induce Large Language Models (LLMs) to\nreason about human emotional states. This method is inspired by various\npsychotherapy approaches including Cognitive Behavioral Therapy (CBT),\nDialectical Behavior Therapy (DBT), Person Centered Therapy (PCT), and Reality\nTherapy (RT), each leading to different patterns of interpreting clients'\nmental states. LLMs without reasoning generated predominantly exploratory\nresponses. However, when LLMs used CoE reasoning, we found a more comprehensive\nrange of empathetic responses aligned with the different reasoning patterns of\neach psychotherapy model. The CBT based CoE resulted in the most balanced\ngeneration of empathetic responses. The findings underscore the importance of\nunderstanding the emotional context and how it affects human and AI\ncommunication. Our research contributes to understanding how psychotherapeutic\nmodels can be incorporated into LLMs, facilitating the development of\ncontext-specific, safer, and empathetic AI.\n","authors":["Yoon Kyung Lee","Inju Lee","Minjung Shin","Seoyeon Bae","Sowon Hahn"],"pdf_url":"https://arxiv.org/pdf/2311.04915v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00965v2","updated":"2024-09-14T04:16:43Z","published":"2024-09-02T06:04:07Z","title":"What does it take to get state of the art in simultaneous\n  speech-to-speech translation?","summary":"  This paper presents an in-depth analysis of the latency characteristics\nobserved in simultaneous speech-to-speech model's performance, particularly\nfocusing on hallucination-induced latency spikes. By systematically\nexperimenting with various input parameters and conditions, we propose methods\nto minimize latency spikes and improve overall performance. The findings\nsuggest that a combination of careful input management and strategic parameter\nadjustments can significantly enhance speech-to-speech model's latency\nbehavior.\n","authors":["Vincent Wilmet","Johnson Du"],"pdf_url":"https://arxiv.org/pdf/2409.00965v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00088v2","updated":"2024-09-14T04:01:09Z","published":"2024-08-26T03:33:36Z","title":"On-Device Language Models: A Comprehensive Review","summary":"  The advent of large language models (LLMs) revolutionized natural language\nprocessing applications, and running LLMs on edge devices has become\nincreasingly attractive for reasons including reduced latency, data\nlocalization, and personalized user experiences. This comprehensive review\nexamines the challenges of deploying computationally expensive LLMs on\nresource-constrained devices and explores innovative solutions across multiple\ndomains. The paper investigates the development of on-device language models,\ntheir efficient architectures, including parameter sharing and modular designs,\nas well as state-of-the-art compression techniques like quantization, pruning,\nand knowledge distillation. Hardware acceleration strategies and collaborative\nedge-cloud deployment approaches are analyzed, highlighting the intricate\nbalance between performance and resource utilization. Case studies of on-device\nlanguage models from major mobile manufacturers demonstrate real-world\napplications and potential benefits. The review also addresses critical aspects\nsuch as adaptive learning, multi-modal capabilities, and personalization. By\nidentifying key research directions and open challenges, this paper provides a\nroadmap for future advancements in on-device language models, emphasizing the\nneed for interdisciplinary efforts to realize the full potential of ubiquitous,\nintelligent computing while ensuring responsible and ethical deployment. For a\ncomprehensive review of research work and educational resources on on-device\nlarge language models (LLMs), please visit\nhttps://github.com/NexaAI/Awesome-LLMs-on-device. To download and run on-device\nLLMs, visit https://www.nexaai.com/models.\n","authors":["Jiajun Xu","Zhiyuan Li","Wei Chen","Qun Wang","Xin Gao","Qi Cai","Ziyuan Ling"],"pdf_url":"https://arxiv.org/pdf/2409.00088v2.pdf","comment":"38 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.04585v3","updated":"2024-09-14T03:19:10Z","published":"2024-08-08T16:54:40Z","title":"Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency,\n  Performance, and Adversarial Robustness","summary":"  With the increasing demand for practical applications of Large Language\nModels (LLMs), many attention-efficient models have been developed to balance\nperformance and computational cost. However, the adversarial robustness of\nthese models remains under-explored. In this work, we design a framework to\ninvestigate the trade-off between efficiency, performance, and adversarial\nrobustness of LLMs and conduct extensive experiments on three prominent models\nwith varying levels of complexity and efficiency -- Transformer++, Gated Linear\nAttention (GLA) Transformer, and MatMul-Free LM -- utilizing the GLUE and\nAdvGLUE datasets. The AdvGLUE dataset extends the GLUE dataset with adversarial\nsamples designed to challenge model robustness. Our results show that while the\nGLA Transformer and MatMul-Free LM achieve slightly lower accuracy on GLUE\ntasks, they demonstrate higher efficiency and either superior or comparative\nrobustness on AdvGLUE tasks compared to Transformer++ across different attack\nlevels. These findings highlight the potential of simplified architectures to\nachieve a compelling balance between efficiency, performance, and adversarial\nrobustness, offering valuable insights for applications where resource\nconstraints and resilience to adversarial attacks are critical.\n","authors":["Xiaojing Fan","Chunliang Tao"],"pdf_url":"https://arxiv.org/pdf/2408.04585v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07461v2","updated":"2024-09-14T03:14:09Z","published":"2024-04-11T03:51:29Z","title":"An Audit on the Perspectives and Challenges of Hallucinations in NLP","summary":"  We audit how hallucination in large language models (LLMs) is characterized\nin peer-reviewed literature, using a critical examination of 103 publications\nacross NLP research. Through the examination of the literature, we identify a\nlack of agreement with the term `hallucination' in the field of NLP.\nAdditionally, to compliment our audit, we conduct a survey with 171\npractitioners from the field of NLP and AI to capture varying perspectives on\nhallucination. Our analysis calls for the necessity of explicit definitions and\nframeworks outlining hallucination within NLP, highlighting potential\nchallenges, and our survey inputs provide a thematic understanding of the\ninfluence and ramifications of hallucination in society.\n","authors":["Pranav Narayanan Venkit","Tatiana Chakravorti","Vipul Gupta","Heidi Biggs","Mukund Srinath","Koustava Goswami","Sarah Rajtmajer","Shomir Wilson"],"pdf_url":"https://arxiv.org/pdf/2404.07461v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09281v1","updated":"2024-09-14T03:11:00Z","published":"2024-09-14T03:11:00Z","title":"Language Models \"Grok\" to Copy","summary":"  We examine the pre-training dynamics of language models, focusing on their\nability to copy text from preceding context--a fundamental skill for various\nLLM applications, including in-context learning (ICL) and retrieval-augmented\ngeneration (RAG). We propose a novel perspective that Transformer-based\nlanguage models develop copying abilities similarly to grokking, which refers\nto sudden generalization on test set long after the model fit to the training\nset. Our experiments yield three arguments: (1) The pre-training loss decreases\nrapidly, while the context copying ability of models initially lags and then\nabruptly saturates. (2) The speed of developing copying ability is independent\nof the number of tokens trained, similarly to how grokking speed is unaffected\nby dataset size as long as the data distribution is preserved. (3) Induction\nheads, the attention heads responsible for copying, form from shallow to deep\nlayers during training, mirroring the development of circuits in deeper layers\nduring grokking. We contend that the connection between grokking and context\ncopying can provide valuable insights for more effective language model\ntraining, ultimately improving in-context performance. For example, we\ndemonstrated that techniques that enhance grokking, such as regularization,\neither accelerate or enhance the development of context copying.\n","authors":["Ang Lv","Ruobing Xie","Xingwu Sun","Zhanhui Kang","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2409.09281v1.pdf","comment":"5 pages, 7 figures"},{"id":"http://arxiv.org/abs/2409.09280v1","updated":"2024-09-14T03:08:10Z","published":"2024-09-14T03:08:10Z","title":"An empirical evaluation of using ChatGPT to summarize disputes for\n  recommending similar labor and employment cases in Chinese","summary":"  We present a hybrid mechanism for recommending similar cases of labor and\nemployment litigations. The classifier determines the similarity based on the\nitemized disputes of the two cases, that the courts prepared. We cluster the\ndisputes, compute the cosine similarity between the disputes, and use the\nresults as the features for the classification tasks. Experimental results\nindicate that this hybrid approach outperformed our previous system, which\nconsidered only the information about the clusters of the disputes. We replaced\nthe disputes that were prepared by the courts with the itemized disputes that\nwere generated by GPT-3.5 and GPT-4, and repeated the same experiments. Using\nthe disputes generated by GPT-4 led to better results. Although our classifier\ndid not perform as well when using the disputes that the ChatGPT generated, the\nresults were satisfactory. Hence, we hope that the future large-language models\nwill become practically useful.\n","authors":["Po-Hsien Wu","Chao-Lin Liu","Wei-Jie Li"],"pdf_url":"https://arxiv.org/pdf/2409.09280v1.pdf","comment":"14 pages, 5 figures, 2 tables, the 18th Int'l Workshop on\n  Juris-Informatics (JURISIN 2024), associated with the 16th JSAI International\n  Symposium on AI (JSAI-isAI 2024)"},{"id":"http://arxiv.org/abs/2404.06666v2","updated":"2024-09-14T02:46:06Z","published":"2024-04-10T00:26:08Z","title":"SafeGen: Mitigating Sexually Explicit Content Generation in\n  Text-to-Image Models","summary":"  Text-to-image (T2I) models, such as Stable Diffusion, have exhibited\nremarkable performance in generating high-quality images from text descriptions\nin recent years. However, text-to-image models may be tricked into generating\nnot-safe-for-work (NSFW) content, particularly in sexually explicit scenarios.\nExisting countermeasures mostly focus on filtering inappropriate inputs and\noutputs, or suppressing improper text embeddings, which can block sexually\nexplicit content (e.g., naked) but may still be vulnerable to adversarial\nprompts -- inputs that appear innocent but are ill-intended. In this paper, we\npresent SafeGen, a framework to mitigate sexual content generation by\ntext-to-image models in a text-agnostic manner. The key idea is to eliminate\nexplicit visual representations from the model regardless of the text input. In\nthis way, the text-to-image model is resistant to adversarial prompts since\nsuch unsafe visual representations are obstructed from within. Extensive\nexperiments conducted on four datasets and large-scale user studies demonstrate\nSafeGen's effectiveness in mitigating sexually explicit content generation\nwhile preserving the high-fidelity of benign images. SafeGen outperforms eight\nstate-of-the-art baseline methods and achieves 99.4% sexual content removal\nperformance. Furthermore, our constructed benchmark of adversarial prompts\nprovides a basis for future development and evaluation of anti-NSFW-generation\nmethods.\n","authors":["Xinfeng Li","Yuchen Yang","Jiangyi Deng","Chen Yan","Yanjiao Chen","Xiaoyu Ji","Wenyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2404.06666v2.pdf","comment":"Accepted by ACM CCS 2024. Please cite this paper as \"Xinfeng Li,\n  Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu Ji, Wenyuan Xu.\n  SafeGen: Mitigating Sexually Explicit Content Generation in Text-to-Image\n  Models. In Proceedings of ACM Conference on Computer and Communications\n  Security (CCS), 2024.\""},{"id":"http://arxiv.org/abs/2408.00761v3","updated":"2024-09-14T02:43:00Z","published":"2024-08-01T17:59:12Z","title":"Tamper-Resistant Safeguards for Open-Weight LLMs","summary":"  Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after thousands of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that tamper-resistance is a tractable\nproblem, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.\n","authors":["Rishub Tamirisa","Bhrugu Bharathi","Long Phan","Andy Zhou","Alice Gatti","Tarun Suresh","Maxwell Lin","Justin Wang","Rowan Wang","Ron Arel","Andy Zou","Dawn Song","Bo Li","Dan Hendrycks","Mantas Mazeika"],"pdf_url":"https://arxiv.org/pdf/2408.00761v3.pdf","comment":"Website: https://www.tamper-resistant-safeguards.com"},{"id":"http://arxiv.org/abs/2404.03163v2","updated":"2024-09-14T02:42:04Z","published":"2024-04-04T02:31:05Z","title":"Uncertainty in Language Models: Assessment through Rank-Calibration","summary":"  Language Models (LMs) have shown promising performance in natural language\ngeneration. However, as LMs often generate incorrect or hallucinated responses,\nit is crucial to correctly quantify their uncertainty in responding to given\ninputs. In addition to verbalized confidence elicited via prompting, many\nuncertainty measures ($e.g.$, semantic entropy and affinity-graph-based\nmeasures) have been proposed. However, these measures can differ greatly, and\nit is unclear how to compare them, partly because they take values over\ndifferent ranges ($e.g.$, $[0,\\infty)$ or $[0,1]$). In this work, we address\nthis issue by developing a novel and practical framework, termed\n$Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs.\nOur key tenet is that higher uncertainty (or lower confidence) should imply\nlower generation quality, on average. Rank-calibration quantifies deviations\nfrom this ideal relationship in a principled manner, without requiring ad hoc\nbinary thresholding of the correctness score ($e.g.$, ROUGE or METEOR). The\nbroad applicability and the granular interpretability of our methods are\ndemonstrated empirically.\n","authors":["Xinmeng Huang","Shuo Li","Mengxin Yu","Matteo Sesia","Hamed Hassani","Insup Lee","Osbert Bastani","Edgar Dobriban"],"pdf_url":"https://arxiv.org/pdf/2404.03163v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09269v1","updated":"2024-09-14T02:29:36Z","published":"2024-09-14T02:29:36Z","title":"Guiding Vision-Language Model Selection for Visual Question-Answering\n  Across Tasks, Domains, and Knowledge Types","summary":"  Visual Question-Answering (VQA) has become a key use-case in several\napplications to aid user experience, particularly after Vision-Language Models\n(VLMs) achieving good results in zero-shot inference. But evaluating different\nVLMs for an application requirement using a standardized framework in practical\nsettings is still challenging. This paper introduces a comprehensive framework\nfor evaluating VLMs tailored to VQA tasks in practical settings. We present a\nnovel dataset derived from established VQA benchmarks, annotated with task\ntypes, application domains, and knowledge types, three key practical aspects on\nwhich tasks can vary. We also introduce GoEval, a multimodal evaluation metric\ndeveloped using GPT-4o, achieving a correlation factor of 56.71% with human\njudgments. Our experiments with ten state-of-the-art VLMs reveals that no\nsingle model excelling universally, making appropriate selection a key design\ndecision. Proprietary models such as Gemini-1.5-Pro and GPT-4o-mini generally\noutperform others, though open-source models like InternVL-2-8B and\nCogVLM-2-Llama-3-19B demonstrate competitive strengths in specific contexts,\nwhile providing additional advantages. This study guides the selection of VLMs\nbased on specific task requirements and resource constraints, and can also be\nextended to other vision-language tasks.\n","authors":["Neelabh Sinha","Vinija Jain","Aman Chadha"],"pdf_url":"https://arxiv.org/pdf/2409.09269v1.pdf","comment":"8 pages + references + 6 pages of Appendix"},{"id":"http://arxiv.org/abs/2409.09261v1","updated":"2024-09-14T02:15:50Z","published":"2024-09-14T02:15:50Z","title":"What Is Wrong with My Model? Identifying Systematic Problems with\n  Semantic Data Slicing","summary":"  Machine learning models make mistakes, yet sometimes it is difficult to\nidentify the systematic problems behind the mistakes. Practitioners engage in\nvarious activities, including error analysis, testing, auditing, and\nred-teaming, to form hypotheses of what can go (or has gone) wrong with their\nmodels. To validate these hypotheses, practitioners employ data slicing to\nidentify relevant examples. However, traditional data slicing is limited by\navailable features and programmatic slicing functions. In this work, we propose\nSemSlicer, a framework that supports semantic data slicing, which identifies a\nsemantically coherent slice, without the need for existing features. SemSlicer\nuses Large Language Models to annotate datasets and generate slices from any\nuser-defined slicing criteria. We show that SemSlicer generates accurate slices\nwith low cost, allows flexible trade-offs between different design dimensions,\nreliably identifies under-performing data slices, and helps practitioners\nidentify useful data slices that reflect systematic problems.\n","authors":["Chenyang Yang","Yining Hong","Grace A. Lewis","Tongshuang Wu","Christian K√§stner"],"pdf_url":"https://arxiv.org/pdf/2409.09261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09260v1","updated":"2024-09-14T02:13:56Z","published":"2024-09-14T02:13:56Z","title":"Analyzing Correlations Between Intrinsic and Extrinsic Bias Metrics of\n  Static Word Embeddings With Their Measuring Biases Aligned","summary":"  We examine the abilities of intrinsic bias metrics of static word embeddings\nto predict whether Natural Language Processing (NLP) systems exhibit biased\nbehavior. A word embedding is one of the fundamental NLP technologies that\nrepresents the meanings of words through real vectors, and problematically, it\nalso learns social biases such as stereotypes. An intrinsic bias metric\nmeasures bias by examining a characteristic of vectors, while an extrinsic bias\nmetric checks whether an NLP system trained with a word embedding is biased. A\nprevious study found that a common intrinsic bias metric usually does not\ncorrelate with extrinsic bias metrics. However, the intrinsic and extrinsic\nbias metrics did not measure the same bias in most cases, which makes us\nquestion whether the lack of correlation is genuine. In this paper, we extract\ncharacteristic words from datasets of extrinsic bias metrics and analyze\ncorrelations with intrinsic bias metrics with those words to ensure both\nmetrics measure the same bias. We observed moderate to high correlations with\nsome extrinsic bias metrics but little to no correlations with the others. This\nresult suggests that intrinsic bias metrics can predict biased behavior in\nparticular settings but not in others. Experiment codes are available at\nGitHub.\n","authors":["Taisei Kat√¥","Yusuke Miyao"],"pdf_url":"https://arxiv.org/pdf/2409.09260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11727v2","updated":"2024-09-14T02:04:15Z","published":"2024-08-21T15:54:04Z","title":"Efficient Detection of Toxic Prompts in Large Language Models","summary":"  Large language models (LLMs) like ChatGPT and Gemini have significantly\nadvanced natural language processing, enabling various applications such as\nchatbots and automated content generation. However, these models can be\nexploited by malicious individuals who craft toxic prompts to elicit harmful or\nunethical responses. These individuals often employ jailbreaking techniques to\nbypass safety mechanisms, highlighting the need for robust toxic prompt\ndetection methods. Existing detection techniques, both blackbox and whitebox,\nface challenges related to the diversity of toxic prompts, scalability, and\ncomputational efficiency. In response, we propose ToxicDetector, a lightweight\ngreybox method designed to efficiently detect toxic prompts in LLMs.\nToxicDetector leverages LLMs to create toxic concept prompts, uses embedding\nvectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP)\nclassifier for prompt classification. Our evaluation on various versions of the\nLLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector\nachieves a high accuracy of 96.39\\% and a low false positive rate of 2.00\\%,\noutperforming state-of-the-art methods. Additionally, ToxicDetector's\nprocessing time of 0.0780 seconds per prompt makes it highly suitable for\nreal-time applications. ToxicDetector achieves high accuracy, efficiency, and\nscalability, making it a practical method for toxic prompt detection in LLMs.\n","authors":["Yi Liu","Junzhe Yu","Huijia Sun","Ling Shi","Gelei Deng","Yuqi Chen","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.11727v2.pdf","comment":"Accepted by the 39th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE 2024)"},{"id":"http://arxiv.org/abs/2409.09253v1","updated":"2024-09-14T01:45:04Z","published":"2024-09-14T01:45:04Z","title":"Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower\n  Dynamic Semantic Token Generator","summary":"  Owing to the unprecedented capability in semantic understanding and logical\nreasoning, the pre-trained large language models (LLMs) have shown fantastic\npotential in developing the next-generation recommender systems (RSs). However,\nthe static index paradigm adopted by current methods greatly restricts the\nutilization of LLMs capacity for recommendation, leading to not only the\ninsufficient alignment between semantic and collaborative knowledge, but also\nthe neglect of high-order user-item interaction patterns. In this paper, we\npropose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS\nwhich adopts dynamic semantic index paradigm, targeting at resolving the above\nproblems simultaneously. To be more specific, we for the first time contrive a\ndynamic knowledge fusion framework which integrates a twin-tower semantic token\ngenerator into the LLM-based recommender, hierarchically allocating meaningful\nsemantic index for items and users, and accordingly predicting the semantic\nindex of target item. Furthermore, a dual-modality variational auto-encoder is\nproposed to facilitate multi-grained alignment between semantic and\ncollaborative knowledge. Eventually, a series of novel tuning tasks specially\ncustomized for capturing high-order user-item interaction patterns are proposed\nto take advantages of user historical behavior. Extensive experiments across\nthree public datasets demonstrate the superiority of the proposed methodology\nin developing LLM-based generative RSs. The proposed TTDS recommender achieves\nan average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric,\ncompared with the leading baseline methods.\n","authors":["Jun Yin","Zhengxin Zeng","Mingzheng Li","Hao Yan","Chaozhuo Li","Weihao Han","Jianjin Zhang","Ruochen Liu","Allen Sun","Denvy Deng","Feng Sun","Qi Zhang","Shirui Pan","Senzhang Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09249v1","updated":"2024-09-14T01:21:56Z","published":"2024-09-14T01:21:56Z","title":"NovAScore: A New Automated Metric for Evaluating Document Level Novelty","summary":"  The rapid expansion of online content has intensified the issue of\ninformation redundancy, underscoring the need for solutions that can identify\ngenuinely new information. Despite this challenge, the research community has\nseen a decline in focus on novelty detection, particularly with the rise of\nlarge language models (LLMs). Additionally, previous approaches have relied\nheavily on human annotation, which is time-consuming, costly, and particularly\nchallenging when annotators must compare a target document against a vast\nnumber of historical documents. In this work, we introduce NovAScore (Novelty\nEvaluation in Atomicity Score), an automated metric for evaluating\ndocument-level novelty. NovAScore aggregates the novelty and salience scores of\natomic information, providing high interpretability and a detailed analysis of\na document's novelty. With its dynamic weight adjustment scheme, NovAScore\noffers enhanced flexibility and an additional dimension to assess both the\nnovelty level and the importance of information within a document. Our\nexperiments show that NovAScore strongly correlates with human judgments of\nnovelty, achieving a 0.626 Point-Biserial correlation on the TAP-DLND 1.0\ndataset and a 0.920 Pearson correlation on an internal human-annotated dataset.\n","authors":["Lin Ai","Ziwei Gong","Harshsaiprasad Deshpande","Alexander Johnson","Emmy Phung","Ahmad Emami","Julia Hirschberg"],"pdf_url":"https://arxiv.org/pdf/2409.09249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09245v1","updated":"2024-09-14T00:57:32Z","published":"2024-09-14T00:57:32Z","title":"Robust Training of Neural Networks at Arbitrary Precision and Sparsity","summary":"  The discontinuous operations inherent in quantization and sparsification\nintroduce obstacles to backpropagation. This is particularly challenging when\ntraining deep neural networks in ultra-low precision and sparse regimes. We\npropose a novel, robust, and universal solution: a denoising affine transform\nthat stabilizes training under these challenging conditions. By formulating\nquantization and sparsification as perturbations during training, we derive a\nperturbation-resilient approach based on ridge regression. Our solution employs\na piecewise constant backbone model to ensure a performance lower bound and\nfeatures an inherent noise reduction mechanism to mitigate perturbation-induced\ncorruption. This formulation allows existing models to be trained at\narbitrarily low precision and sparsity levels with off-the-shelf recipes.\nFurthermore, our method provides a novel perspective on training temporal\nbinary neural networks, contributing to ongoing efforts to narrow the gap\nbetween artificial and biological neural networks.\n","authors":["Chengxi Ye","Grace Chu","Yanfeng Liu","Yichi Zhang","Lukasz Lew","Andrew Howard"],"pdf_url":"https://arxiv.org/pdf/2409.09245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09239v1","updated":"2024-09-14T00:30:57Z","published":"2024-09-14T00:30:57Z","title":"Autoregressive + Chain of Thought (CoT) $\\simeq$ Recurrent: Recurrence's\n  Role in Language Models and a Revist of Recurrent Transformer","summary":"  The Transformer architecture excels in a variety of language modeling tasks,\noutperforming traditional neural architectures such as RNN and LSTM. This is\npartially due to its elimination of recurrent connections, which allows for\nparallel training and a smoother flow of gradients. However, this move away\nfrom recurrent structures places the Transformer model at the lower end of\nChomsky's computational hierarchy, imposing limitations on its computational\nabilities. Consequently, even advanced Transformer-based models face\nconsiderable difficulties in tasks like counting, string reversal, bracket\npairing, and multiplication. These tasks, though seemingly elementary, require\na level of computational complexity that exceeds the capabilities of the\nTransformer architecture. Concurrently, the emergence of ``Chain of Thought\"\n(CoT) prompting has enabled Transformer-based language models to tackle tasks\nthat were previously impossible or poorly executed. Despite some previous\nresearch primarily interpreting CoT from a psychological perspective, a\ncomprehensive understanding of \\textit{why} CoT proves so effective in the\nreasoning process remains elusive. In this work, we thoroughly investigate the\ninfluence of recurrent structures in language models on their reasoning\nabilities, shedding light on how the CoT approach can mimic recurrent\ncomputation and act as a bridge between autoregression and recurrence. It is\nthis approximated recurrence that notably improves the model's performance and\ncomputational capacity. Moreover, we revisit recent recurrent-based Transformer\nmodel designs, focusing on their computational abilities through our proposed\nconcept of ``recurrence-completeness\" and identify key theoretical limitations\nin models like Linear Transformer and RWKV. Through this, we aim to provide\ninsight into the neural model architectures and prompt better model design.\n","authors":["Xiang Zhang","Muhammad Abdul-Mageed","Laks V. S. Lakshmanan"],"pdf_url":"https://arxiv.org/pdf/2409.09239v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2409.09560v1","updated":"2024-09-14T23:50:23Z","published":"2024-09-14T23:50:23Z","title":"Evaluating authenticity and quality of image captions via sentiment and\n  semantic analyses","summary":"  The growth of deep learning (DL) relies heavily on huge amounts of labelled\ndata for tasks such as natural language processing and computer vision.\nSpecifically, in image-to-text or image-to-image pipelines, opinion (sentiment)\nmay be inadvertently learned by a model from human-generated image captions.\nAdditionally, learning may be affected by the variety and diversity of the\nprovided captions. While labelling large datasets has largely relied on\ncrowd-sourcing or data-worker pools, evaluating the quality of such training\ndata is crucial.\n  This study proposes an evaluation method focused on sentiment and semantic\nrichness. That method was applied to the COCO-MS dataset, comprising\napproximately 150K images with segmented objects and corresponding\ncrowd-sourced captions. We employed pre-trained models (Twitter-RoBERTa-base\nand BERT-base) to extract sentiment scores and variability of semantic\nembeddings from captions. The relation of the sentiment score and semantic\nvariability with object categories was examined using multiple linear\nregression. Results indicate that while most captions were neutral, about 6% of\nthe captions exhibited strong sentiment influenced by specific object\ncategories. Semantic variability of within-image captions remained low and\nuncorrelated with object categories. Model-generated captions showed less than\n1.5% of strong sentiment which was not influenced by object categories and did\nnot correlate with the sentiment of the respective human-generated captions.\nThis research demonstrates an approach to assess the quality of crowd- or\nworker-sourced captions informed by image content.\n","authors":["Aleksei Krotov","Alison Tebo","Dylan K. Picart","Aaron Dean Algave"],"pdf_url":"https://arxiv.org/pdf/2409.09560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09555v1","updated":"2024-09-14T23:34:12Z","published":"2024-09-14T23:34:12Z","title":"Enhancing Printed Circuit Board Defect Detection through Ensemble\n  Learning","summary":"  The quality control of printed circuit boards (PCBs) is paramount in\nadvancing electronic device technology. While numerous machine learning\nmethodologies have been utilized to augment defect detection efficiency and\naccuracy, previous studies have predominantly focused on optimizing individual\nmodels for specific defect types, often overlooking the potential synergies\nbetween different approaches. This paper introduces a comprehensive inspection\nframework leveraging an ensemble learning strategy to address this gap.\nInitially, we utilize four distinct PCB defect detection models utilizing\nstate-of-the-art methods: EfficientDet, MobileNet SSDv2, Faster RCNN, and\nYOLOv5. Each method is capable of identifying PCB defects independently.\nSubsequently, we integrate these models into an ensemble learning framework to\nenhance detection performance. A comparative analysis reveals that our ensemble\nlearning framework significantly outperforms individual methods, achieving a\n95% accuracy in detecting diverse PCB defects. These findings underscore the\nefficacy of our proposed ensemble learning framework in enhancing PCB quality\ncontrol processes.\n","authors":["Ka Nam Canaan Law","Mingshuo Yu","Lianglei Zhang","Yiyi Zhang","Peng Xu","Jerry Gao","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2409.09555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07760v3","updated":"2024-09-14T22:25:51Z","published":"2023-09-14T14:48:01Z","title":"PRE: Vision-Language Prompt Learning with Reparameterization Encoder","summary":"  Large pre-trained vision-language models such as CLIP have demonstrated great\npotential in zero-shot transferability to downstream tasks. However, to attain\noptimal performance, the manual selection of prompts is necessary to improve\nalignment between the downstream image distribution and the textual class\ndescriptions. This manual prompt engineering is the major challenge for\ndeploying such models in practice since it requires domain expertise and is\nextremely time-consuming. To avoid non-trivial prompt engineering, recent work\nContext Optimization (CoOp) introduced the concept of prompt learning to the\nvision domain using learnable textual tokens. While CoOp can achieve\nsubstantial improvements over manual prompts, its learned context is worse\ngeneralizable to wider unseen classes within the same dataset. In this work, we\npresent Prompt Learning with Reparameterization Encoder (PRE) - a simple and\nefficient method that enhances the generalization ability of the learnable\nprompt to unseen classes while maintaining the capacity to learn Base classes.\nInstead of directly optimizing the prompts, PRE employs a prompt encoder to\nreparameterize the input prompt embeddings, enhancing the exploration of\ntask-specific knowledge from few-shot samples. Experiments and extensive\nablation studies on 8 benchmarks demonstrate that our approach is an efficient\nmethod for prompt learning. Specifically, PRE achieves a notable enhancement of\n5.60% in average accuracy on New classes and 3% in Harmonic mean compared to\nCoOp in the 16-shot setting, all achieved within a good training time.\n","authors":["Thi Minh Anh Pham","An Duc Nguyen","Cephas Svosve","Vasileios Argyriou","Georgios Tzimiropoulos"],"pdf_url":"https://arxiv.org/pdf/2309.07760v3.pdf","comment":"10 pages excluding References and Appendix"},{"id":"http://arxiv.org/abs/2305.06024v4","updated":"2024-09-14T21:47:21Z","published":"2023-05-10T10:19:31Z","title":"A Survey on the Robustness of Computer Vision Models against Common\n  Corruptions","summary":"  The performance of computer vision models are susceptible to unexpected\nchanges in input images caused by sensor errors or extreme imaging\nenvironments, known as common corruptions (e.g. noise, blur, illumination\nchanges). These corruptions can significantly hinder the reliability of these\nmodels when deployed in real-world scenarios, yet they are often overlooked\nwhen testing model generalization and robustness. In this survey, we present a\ncomprehensive overview of methods that improve the robustness of computer\nvision models against common corruptions. We categorize methods into three\ngroups based on the model components and training methods they target: data\naugmentation, learning strategies, and network components. We release a unified\nbenchmark framework (available at\n\\url{https://github.com/nis-research/CorruptionBenchCV}) to compare robustness\nperformance across several datasets, and we address the inconsistencies of\nevaluation practices in the literature. Our experimental analysis highlights\nthe base corruption robustness of popular vision backbones, revealing that\ncorruption robustness does not necessarily scale with model size and data size.\nLarge models gain negligible robustness improvements, considering the increased\ncomputational requirements. To achieve generalizable and robust computer vision\nmodels, we foresee the need of developing new learning strategies that\nefficiently exploit limited data and mitigate unreliable learning behaviors.\n","authors":["Shunxin Wang","Raymond Veldhuis","Christoph Brune","Nicola Strisciuglio"],"pdf_url":"https://arxiv.org/pdf/2305.06024v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09542v1","updated":"2024-09-14T21:42:38Z","published":"2024-09-14T21:42:38Z","title":"MANGO: Disentangled Image Transformation Manifolds with Grouped\n  Operators","summary":"  Learning semantically meaningful image transformations (i.e. rotation,\nthickness, blur) directly from examples can be a challenging task. Recently,\nthe Manifold Autoencoder (MAE) proposed using a set of Lie group operators to\nlearn image transformations directly from examples. However, this approach has\nlimitations, as the learned operators are not guaranteed to be disentangled and\nthe training routine is prohibitively expensive when scaling up the model. To\naddress these limitations, we propose MANGO (transformation Manifolds with\nGrouped Operators) for learning disentangled operators that describe image\ntransformations in distinct latent subspaces. Moreover, our approach allows\npractitioners the ability to define which transformations they aim to model,\nthus improving the semantic meaning of the learned operators. Through our\nexperiments, we demonstrate that MANGO enables composition of image\ntransformations and introduces a one-phase training routine that leads to a\n100x speedup over prior works.\n","authors":["Brighton Ancelin","Yenho Chen","Peimeng Guan","Chiraag Kaushik","Belen Martin-Urcelay","Alex Saad-Falcon","Nakul Singh"],"pdf_url":"https://arxiv.org/pdf/2409.09542v1.pdf","comment":"Submitted to IEEE ICASSP 2025. This work has been submitted to the\n  IEEE for possible publication. Copyright may be transferred without notice,\n  after which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2409.09530v1","updated":"2024-09-14T21:01:49Z","published":"2024-09-14T21:01:49Z","title":"An Augmentation-based Model Re-adaptation Framework for Robust Image\n  Segmentation","summary":"  Image segmentation is a crucial task in computer vision, with wide-ranging\napplications in industry. The Segment Anything Model (SAM) has recently\nattracted intensive attention; however, its application in industrial\ninspection, particularly for segmenting commercial anti-counterfeit codes,\nremains challenging. Unlike open-source datasets, industrial settings often\nface issues such as small sample sizes and complex textures. Additionally,\ncomputational cost is a key concern due to the varying number of trainable\nparameters. To address these challenges, we propose an Augmentation-based Model\nRe-adaptation Framework (AMRF). This framework leverages data augmentation\ntechniques during training to enhance the generalisation of segmentation\nmodels, allowing them to adapt to newly released datasets with temporal\ndisparity. By observing segmentation masks from conventional models (FCN and\nU-Net) and a pre-trained SAM model, we determine a minimal augmentation set\nthat optimally balances training efficiency and model performance. Our results\ndemonstrate that the fine-tuned FCN surpasses its baseline by 3.29% and 3.02%\nin cropping accuracy, and 5.27% and 4.04% in classification accuracy on two\ntemporally continuous datasets. Similarly, the fine-tuned U-Net improves upon\nits baseline by 7.34% and 4.94% in cropping, and 8.02% and 5.52% in\nclassification. Both models outperform the top-performing SAM models (ViT-Large\nand ViT-Base) by an average of 11.75% and 9.01% in cropping accuracy, and 2.93%\nand 4.83% in classification accuracy, respectively.\n","authors":["Zheming Zuo","Joseph Smith","Jonathan Stonehouse","Boguslaw Obara"],"pdf_url":"https://arxiv.org/pdf/2409.09530v1.pdf","comment":"Accepted in the European Conference on Computer Vision (ECCV) 2024\n  workshop"},{"id":"http://arxiv.org/abs/2409.09520v1","updated":"2024-09-14T20:11:25Z","published":"2024-09-14T20:11:25Z","title":"Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery\n  with SAM Empowerment","summary":"  Current AI-assisted skin image diagnosis has achieved dermatologist-level\nperformance in classifying skin cancer, driven by rapid advancements in deep\nlearning architectures. However, unlike traditional vision tasks, skin images\nin general present unique challenges due to the limited availability of\nwell-annotated datasets, complex variations in conditions, and the necessity\nfor detailed interpretations to ensure patient safety. Previous segmentation\nmethods have sought to reduce image noise and enhance diagnostic performance,\nbut these techniques require fine-grained, pixel-level ground truth masks for\ntraining. In contrast, with the rise of foundation models, the Segment Anything\nModel (SAM) has been introduced to facilitate promptable segmentation, enabling\nthe automation of the segmentation process with simple yet effective prompts.\nEfforts applying SAM predominantly focus on dermatoscopy images, which present\nmore easily identifiable lesion boundaries than clinical photos taken with\nsmartphones. This limitation constrains the practicality of these approaches to\nreal-world applications. To overcome the challenges posed by noisy clinical\nphotos acquired via non-standardized protocols and to improve diagnostic\naccessibility, we propose a novel Cross-Attentive Fusion framework for\ninterpretable skin lesion diagnosis. Our method leverages SAM to generate\nvisual concepts for skin diseases using prompts, integrating local visual\nconcepts with global image features to enhance model performance. Extensive\nevaluation on two skin disease datasets demonstrates our proposed method's\neffectiveness on lesion diagnosis and interpretability.\n","authors":["Xin Hu","Janet Wang","Jihun Hamm","Rie R Yotsu","Zhengming Ding"],"pdf_url":"https://arxiv.org/pdf/2409.09520v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14869v2","updated":"2024-09-14T19:08:50Z","published":"2024-05-23T17:59:56Z","title":"PuzzleAvatar: Assembling 3D Avatars from Personal Albums","summary":"  Generating personalized 3D avatars is crucial for AR/VR. However, recent\ntext-to-3D methods that generate avatars for celebrities or fictional\ncharacters, struggle with everyday people. Methods for faithful reconstruction\ntypically require full-body images in controlled settings. What if a user could\njust upload their personal \"OOTD\" (Outfit Of The Day) photo collection and get\na faithful avatar in return? The challenge is that such casual photo\ncollections contain diverse poses, challenging viewpoints, cropped views, and\nocclusion (albeit with a consistent outfit, accessories and hairstyle). We\naddress this novel \"Album2Human\" task by developing PuzzleAvatar, a novel model\nthat generates a faithful 3D avatar (in a canonical pose) from a personal OOTD\nalbum, while bypassing the challenging estimation of body and camera pose. To\nthis end, we fine-tune a foundational vision-language model (VLM) on such\nphotos, encoding the appearance, identity, garments, hairstyles, and\naccessories of a person into (separate) learned tokens and instilling these\ncues into the VLM. In effect, we exploit the learned tokens as \"puzzle pieces\"\nfrom which we assemble a faithful, personalized 3D avatar. Importantly, we can\ncustomize avatars by simply inter-changing tokens. As a benchmark for this new\ntask, we collect a new dataset, called PuzzleIOI, with 41 subjects in a total\nof nearly 1K OOTD configurations, in challenging partial photos with paired\nground-truth 3D bodies. Evaluation shows that PuzzleAvatar not only has high\nreconstruction accuracy, outperforming TeCH and MVDreamBooth, but also a unique\nscalability to album photos, and strong robustness. Our code and data are\npublicly available for research purpose at https://puzzleavatar.is.tue.mpg.de/\n","authors":["Yuliang Xiu","Yufei Ye","Zhen Liu","Dimitrios Tzionas","Michael J. Black"],"pdf_url":"https://arxiv.org/pdf/2405.14869v2.pdf","comment":"Page: https://puzzleavatar.is.tue.mpg.de/, Code:\n  https://github.com/YuliangXiu/PuzzleAvatar, Video:\n  https://youtu.be/0hpXH2tVPk4"},{"id":"http://arxiv.org/abs/2302.11552v6","updated":"2024-09-14T18:55:50Z","published":"2023-02-22T18:48:46Z","title":"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based\n  Diffusion Models and MCMC","summary":"  Since their introduction, diffusion models have quickly become the prevailing\napproach to generative modeling in many domains. They can be interpreted as\nlearning the gradients of a time-varying sequence of log-probability density\nfunctions. This interpretation has motivated classifier-based and\nclassifier-free guidance as methods for post-hoc control of diffusion models.\nIn this work, we build upon these ideas using the score-based interpretation of\ndiffusion models, and explore alternative ways to condition, modify, and reuse\ndiffusion models for tasks involving compositional generation and guidance. In\nparticular, we investigate why certain types of composition fail using current\ntechniques and present a number of solutions. We conclude that the sampler (not\nthe model) is responsible for this failure and propose new samplers, inspired\nby MCMC, which enable successful compositional generation. Further, we propose\nan energy-based parameterization of diffusion models which enables the use of\nnew compositional operators and more sophisticated, Metropolis-corrected\nsamplers. Intriguingly we find these samplers lead to notable improvements in\ncompositional generation across a wide set of problems such as\nclassifier-guided ImageNet modeling and compositional text-to-image generation.\n","authors":["Yilun Du","Conor Durkan","Robin Strudel","Joshua B. Tenenbaum","Sander Dieleman","Rob Fergus","Jascha Sohl-Dickstein","Arnaud Doucet","Will Grathwohl"],"pdf_url":"https://arxiv.org/pdf/2302.11552v6.pdf","comment":"ICML 2023, Project Webpage:\n  https://energy-based-model.github.io/reduce-reuse-recycle/"},{"id":"http://arxiv.org/abs/2409.09502v1","updated":"2024-09-14T18:26:26Z","published":"2024-09-14T18:26:26Z","title":"One missing piece in Vision and Language: A Survey on Comics\n  Understanding","summary":"  Vision-language models have recently evolved into versatile systems capable\nof high performance across a range of tasks, such as document understanding,\nvisual question answering, and grounding, often in zero-shot settings. Comics\nUnderstanding, a complex and multifaceted field, stands to greatly benefit from\nthese advances. Comics, as a medium, combine rich visual and textual\nnarratives, challenging AI models with tasks that span image classification,\nobject detection, instance segmentation, and deeper narrative comprehension\nthrough sequential panels. However, the unique structure of comics --\ncharacterized by creative variations in style, reading order, and non-linear\nstorytelling -- presents a set of challenges distinct from those in other\nvisual-language domains. In this survey, we present a comprehensive review of\nComics Understanding from both dataset and task perspectives. Our contributions\nare fivefold: (1) We analyze the structure of the comics medium, detailing its\ndistinctive compositional elements; (2) We survey the widely used datasets and\ntasks in comics research, emphasizing their role in advancing the field; (3) We\nintroduce the Layer of Comics Understanding (LoCU) framework, a novel taxonomy\nthat redefines vision-language tasks within comics and lays the foundation for\nfuture work; (4) We provide a detailed review and categorization of existing\nmethods following the LoCU framework; (5) Finally, we highlight current\nresearch challenges and propose directions for future exploration, particularly\nin the context of vision-language models applied to comics. This survey is the\nfirst to propose a task-oriented framework for comics intelligence and aims to\nguide future research by addressing critical gaps in data availability and task\ndefinition. A project associated with this survey is available at\nhttps://github.com/emanuelevivoli/awesome-comics-understanding.\n","authors":["Emanuele Vivoli","Andrey Barsky","Mohamed Ali Souibgui","Artemis LLabres","Marco Bertini","Dimosthenis Karatzas"],"pdf_url":"https://arxiv.org/pdf/2409.09502v1.pdf","comment":"under review. project website:\n  https://github.com/emanuelevivoli/awesome-comics-understanding"},{"id":"http://arxiv.org/abs/2409.09497v1","updated":"2024-09-14T17:52:59Z","published":"2024-09-14T17:52:59Z","title":"Multi-Scale Grouped Prototypes for Interpretable Semantic Segmentation","summary":"  Prototypical part learning is emerging as a promising approach for making\nsemantic segmentation interpretable. The model selects real patches seen during\ntraining as prototypes and constructs the dense prediction map based on the\nsimilarity between parts of the test image and the prototypes. This improves\ninterpretability since the user can inspect the link between the predicted\noutput and the patterns learned by the model in terms of prototypical\ninformation. In this paper, we propose a method for interpretable semantic\nsegmentation that leverages multi-scale image representation for prototypical\npart learning. First, we introduce a prototype layer that explicitly learns\ndiverse prototypical parts at several scales, leading to multi-scale\nrepresentations in the prototype activation output. Then, we propose a sparse\ngrouping mechanism that produces multi-scale sparse groups of these\nscale-specific prototypical parts. This provides a deeper understanding of the\ninteractions between multi-scale object representations while enhancing the\ninterpretability of the segmentation model. The experiments conducted on Pascal\nVOC, Cityscapes, and ADE20K demonstrate that the proposed method increases\nmodel sparsity, improves interpretability over existing prototype-based\nmethods, and narrows the performance gap with the non-interpretable counterpart\nmodels. Code is available at github.com/eceo-epfl/ScaleProtoSeg.\n","authors":["Hugo Porta","Emanuele Dalsasso","Diego Marcos","Devis Tuia"],"pdf_url":"https://arxiv.org/pdf/2409.09497v1.pdf","comment":"8 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2409.09484v1","updated":"2024-09-14T17:11:37Z","published":"2024-09-14T17:11:37Z","title":"Self-Prompting Polyp Segmentation in Colonoscopy using Hybrid Yolo-SAM 2\n  Model","summary":"  Early diagnosis and treatment of polyps during colonoscopy are essential for\nreducing the incidence and mortality of Colorectal Cancer (CRC). However, the\nvariability in polyp characteristics and the presence of artifacts in\ncolonoscopy images and videos pose significant challenges for accurate and\nefficient polyp detection and segmentation. This paper presents a novel\napproach to polyp segmentation by integrating the Segment Anything Model (SAM\n2) with the YOLOv8 model. Our method leverages YOLOv8's bounding box\npredictions to autonomously generate input prompts for SAM 2, thereby reducing\nthe need for manual annotations. We conducted exhaustive tests on five\nbenchmark colonoscopy image datasets and two colonoscopy video datasets,\ndemonstrating that our method exceeds state-of-the-art models in both image and\nvideo segmentation tasks. Notably, our approach achieves high segmentation\naccuracy using only bounding box annotations, significantly reducing annotation\ntime and effort. This advancement holds promise for enhancing the efficiency\nand scalability of polyp detection in clinical settings\nhttps://github.com/sajjad-sh33/YOLO_SAM2.\n","authors":["Mobina Mansoori","Sajjad Shahabodini","Jamshid Abouei","Konstantinos N. Plataniotis","Arash Mohammadi"],"pdf_url":"https://arxiv.org/pdf/2409.09484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09479v1","updated":"2024-09-14T16:49:42Z","published":"2024-09-14T16:49:42Z","title":"MAC-VO: Metrics-aware Covariance for Learning-based Stereo Visual\n  Odometry","summary":"  We propose the MAC-VO, a novel learning-based stereo VO that leverages the\nlearned metrics-aware matching uncertainty for dual purposes: selecting\nkeypoint and weighing the residual in pose graph optimization. Compared to\ntraditional geometric methods prioritizing texture-affluent features like\nedges, our keypoint selector employs the learned uncertainty to filter out the\nlow-quality features based on global inconsistency. In contrast to the\nlearning-based algorithms that model the scale-agnostic diagonal weight matrix\nfor covariance, we design a metrics-aware covariance model to capture the\nspatial error during keypoint registration and the correlations between\ndifferent axes. Integrating this covariance model into pose graph optimization\nenhances the robustness and reliability of pose estimation, particularly in\nchallenging environments with varying illumination, feature density, and motion\npatterns. On public benchmark datasets, MAC-VO outperforms existing VO\nalgorithms and even some SLAM algorithms in challenging environments. The\ncovariance map also provides valuable information about the reliability of the\nestimated poses, which can benefit decision-making for autonomous systems.\n","authors":["Yuheng Qiu","Yutian Chen","Zihao Zhang","Wenshan Wang","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2409.09479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03326v2","updated":"2024-09-14T16:39:26Z","published":"2024-08-06T17:59:44Z","title":"LLaVA-OneVision: Easy Visual Task Transfer","summary":"  We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.\n","authors":["Bo Li","Yuanhan Zhang","Dong Guo","Renrui Zhang","Feng Li","Hao Zhang","Kaichen Zhang","Yanwei Li","Ziwei Liu","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2408.03326v2.pdf","comment":"Project Homepage:\n  https://llava-vl.github.io/blog/2024-08-05-llava-onevision/"},{"id":"http://arxiv.org/abs/2409.09478v1","updated":"2024-09-14T16:39:17Z","published":"2024-09-14T16:39:17Z","title":"From FDG to PSMA: A Hitchhiker's Guide to Multitracer, Multicenter\n  Lesion Segmentation in PET/CT Imaging","summary":"  Automated lesion segmentation in PET/CT scans is crucial for improving\nclinical workflows and advancing cancer diagnostics. However, the task is\nchallenging due to physiological variability, different tracers used in PET\nimaging, and diverse imaging protocols across medical centers. To address this,\nthe autoPET series was created to challenge researchers to develop algorithms\nthat generalize across diverse PET/CT environments. This paper presents our\nsolution for the autoPET III challenge, targeting multitracer, multicenter\ngeneralization using the nnU-Net framework with the ResEncL architecture. Key\ntechniques include misalignment data augmentation and multi-modal pretraining\nacross CT, MR, and PET datasets to provide an initial anatomical understanding.\nWe incorporate organ supervision as a multitask approach, enabling the model to\ndistinguish between physiological uptake and tracer-specific patterns, which is\nparticularly beneficial in cases where no lesions are present. Compared to the\ndefault nnU-Net, which achieved a Dice score of 57.61, or the larger ResEncL\n(65.31) our model significantly improved performance with a Dice score of\n68.40, alongside a reduction in false positive (FPvol: 7.82) and false negative\n(FNvol: 10.35) volumes. These results underscore the effectiveness of combining\nadvanced network design, augmentation, pretraining, and multitask learning for\nPET/CT lesion segmentation. Code is publicly available at\nhttps://github.com/MIC-DKFZ/autopet-3-submission.\n","authors":["Maximilian Rokuss","Balint Kovacs","Yannick Kirchhoff","Shuhan Xiao","Constantin Ulrich","Klaus H. Maier-Hein","Fabian Isensee"],"pdf_url":"https://arxiv.org/pdf/2409.09478v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.07946v2","updated":"2024-09-14T15:49:09Z","published":"2024-09-12T11:14:25Z","title":"Collaborative Automatic Modulation Classification via Deep Edge\n  Inference for Hierarchical Cognitive Radio Networks","summary":"  In hierarchical cognitive radio networks, edge or cloud servers utilize the\ndata collected by edge devices for modulation classification, which, however,\nis faced with problems of the transmission overhead, data privacy, and\ncomputation load. In this article, an edge learning (EL) based framework\njointly mobilizing the edge device and the edge server for intelligent\nco-inference is proposed to realize the collaborative automatic modulation\nclassification (C-AMC) between them. A spectrum semantic compression neural\nnetwork (SSCNet) with the lightweight structure is designed for the edge device\nto compress the collected raw data into a compact semantic message that is then\nsent to the edge server via the wireless channel. On the edge server side, a\nmodulation classification neural network (MCNet) combining bidirectional long\nshort-term memory (Bi-LSTM) and multi-head attention layers is elaborated to\ndetermine the modulation type from the noisy semantic message. By leveraging\nthe computation resources of both the edge device and the edge server, high\ntransmission overhead and risks of data privacy leakage are avoided. The\nsimulation results verify the effectiveness of the proposed C-AMC framework,\nsignificantly reducing the model size and computational complexity.\n","authors":["Chaowei He","Peihao Dong","Fuhui Zhou","Qihui Wu"],"pdf_url":"https://arxiv.org/pdf/2409.07946v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2407.20772"},{"id":"http://arxiv.org/abs/2408.16672v4","updated":"2024-09-14T07:41:06Z","published":"2024-08-29T16:21:00Z","title":"Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction\n  Retriever","summary":"  Multi-vector dense models, such as ColBERT, have proven highly effective in\ninformation retrieval. ColBERT's late interaction scoring approximates the\njoint query-document attention seen in cross-encoders while maintaining\ninference efficiency closer to traditional dense retrieval models, thanks to\nits bi-encoder architecture and recent optimizations in indexing and search. In\nthis work we propose a number of incremental improvements to the ColBERT model\narchitecture and training pipeline, using methods shown to work in the more\nmature single-vector embedding model training paradigm, particularly those that\napply to heterogeneous multilingual data or boost efficiency with little\ntradeoff. Our new model, Jina-ColBERT-v2, demonstrates strong performance\nacross a range of English and multilingual retrieval tasks.\n","authors":["Rohan Jha","Bo Wang","Michael G√ºnther","Georgios Mastrapas","Saba Sturua","Isabelle Mohr","Andreas Koukounas","Mohammad Kalim Akram","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2408.16672v4.pdf","comment":"8 pages, references at pp7,8; EMNLP workshop submission"},{"id":"http://arxiv.org/abs/2308.05902v2","updated":"2024-09-14T03:58:04Z","published":"2023-08-11T01:52:23Z","title":"LTP-MMF: Towards Long-term Provider Max-min Fairness Under\n  Recommendation Feedback Loops","summary":"  Multi-stakeholder recommender systems involve various roles, such as users,\nand providers. Previous work pointed out that max-min fairness (MMF) is a\nbetter metric to support weak providers. However, when considering MMF, the\nfeatures or parameters of these roles vary over time, how to ensure long-term\nprovider MMF has become a significant challenge. We observed that\nrecommendation feedback loops (named RFL) will greatly influence the provider\nMMF in the long term. RFL means that recommender systems can only receive\nfeedback on exposed items from users and update recommender models\nincrementally based on this feedback. When utilizing the feedback, the\nrecommender model will regard the unexposed items as negative. In this way, the\ntail provider will not get the opportunity to be exposed, and its items will\nalways be considered negative samples. Such phenomena will become more and more\nserious in RFL. To alleviate the problem, this paper proposes an online ranking\nmodel named Long-Term Provider Max-min Fairness (named LTP-MMF). Theoretical\nanalysis shows that the long-term regret of LTP-MMF enjoys a sub-linear bound.\nExperimental results on three public recommendation benchmarks demonstrated\nthat LTP-MMF can outperform the baselines in the long term.\n","authors":["Chen Xu","Xiaopeng Ye","Jun Xu","Xiao Zhang","Weiran Shen","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2308.05902v2.pdf","comment":"accepted in TOIS 2024"},{"id":"http://arxiv.org/abs/2409.09253v1","updated":"2024-09-14T01:45:04Z","published":"2024-09-14T01:45:04Z","title":"Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower\n  Dynamic Semantic Token Generator","summary":"  Owing to the unprecedented capability in semantic understanding and logical\nreasoning, the pre-trained large language models (LLMs) have shown fantastic\npotential in developing the next-generation recommender systems (RSs). However,\nthe static index paradigm adopted by current methods greatly restricts the\nutilization of LLMs capacity for recommendation, leading to not only the\ninsufficient alignment between semantic and collaborative knowledge, but also\nthe neglect of high-order user-item interaction patterns. In this paper, we\npropose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS\nwhich adopts dynamic semantic index paradigm, targeting at resolving the above\nproblems simultaneously. To be more specific, we for the first time contrive a\ndynamic knowledge fusion framework which integrates a twin-tower semantic token\ngenerator into the LLM-based recommender, hierarchically allocating meaningful\nsemantic index for items and users, and accordingly predicting the semantic\nindex of target item. Furthermore, a dual-modality variational auto-encoder is\nproposed to facilitate multi-grained alignment between semantic and\ncollaborative knowledge. Eventually, a series of novel tuning tasks specially\ncustomized for capturing high-order user-item interaction patterns are proposed\nto take advantages of user historical behavior. Extensive experiments across\nthree public datasets demonstrate the superiority of the proposed methodology\nin developing LLM-based generative RSs. The proposed TTDS recommender achieves\nan average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric,\ncompared with the leading baseline methods.\n","authors":["Jun Yin","Zhengxin Zeng","Mingzheng Li","Hao Yan","Chaozhuo Li","Weihao Han","Jianjin Zhang","Ruochen Liu","Allen Sun","Denvy Deng","Feng Sun","Qi Zhang","Shirui Pan","Senzhang Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09253v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.19340v3","updated":"2024-09-14T16:19:07Z","published":"2024-07-27T21:00:36Z","title":"Integrating Large Language Models into a Tri-Modal Architecture for\n  Automated Depression Classification","summary":"  Major Depressive Disorder (MDD) is a pervasive mental health condition that\naffects 300 million people worldwide. This work presents a novel, BiLSTM-based\ntri-modal model-level fusion architecture for the binary classification of\ndepression from clinical interview recordings. The proposed architecture\nincorporates Mel Frequency Cepstral Coefficients, Facial Action Units, and uses\na two-shot learning based GPT-4 model to process text data. This is the first\nwork to incorporate large language models into a multi-modal architecture for\nthis task. It achieves impressive results on the DAIC-WOZ AVEC 2016 Challenge\ncross-validation split and Leave-One-Subject-Out cross-validation split,\nsurpassing all baseline models and multiple state-of-the-art models. In\nLeave-One-Subject-Out testing, it achieves an accuracy of 91.01%, an F1-Score\nof 85.95%, a precision of 80%, and a recall of 92.86%.\n","authors":["Santosh V. Patapati"],"pdf_url":"https://arxiv.org/pdf/2407.19340v3.pdf","comment":"Keywords: Multi-Modal Neural Networks, Deep Learning, Large Language\n  Models, Depression Diagnosis, Biomedical Informatics, DAIC-WOZ"},{"id":"http://arxiv.org/abs/2409.04398v3","updated":"2024-09-14T15:48:40Z","published":"2024-09-06T16:43:04Z","title":"HiSC4D: Human-centered interaction and 4D Scene Capture in Large-scale\n  Space Using Wearable IMUs and LiDAR","summary":"  We introduce HiSC4D, a novel Human-centered interaction and 4D Scene Capture\nmethod, aimed at accurately and efficiently creating a dynamic digital world,\ncontaining large-scale indoor-outdoor scenes, diverse human motions, rich\nhuman-human interactions, and human-environment interactions. By utilizing\nbody-mounted IMUs and a head-mounted LiDAR, HiSC4D can capture egocentric human\nmotions in unconstrained space without the need for external devices and\npre-built maps. This affords great flexibility and accessibility for\nhuman-centered interaction and 4D scene capturing in various environments.\nTaking into account that IMUs can capture human spatially unrestricted poses\nbut are prone to drifting for long-period using, and while LiDAR is stable for\nglobal localization but rough for local positions and orientations, HiSC4D\nemploys a joint optimization method, harmonizing all sensors and utilizing\nenvironment cues, yielding promising results for long-term capture in large\nscenes. To promote research of egocentric human interaction in large scenes and\nfacilitate downstream tasks, we also present a dataset, containing 8 sequences\nin 4 large scenes (200 to 5,000 $m^2$), providing 36k frames of accurate 4D\nhuman motions with SMPL annotations and dynamic scenes, 31k frames of cropped\nhuman point clouds, and scene mesh of the environment. A variety of scenarios,\nsuch as the basketball gym and commercial street, alongside challenging human\nmotions, such as daily greeting, one-on-one basketball playing, and tour\nguiding, demonstrate the effectiveness and the generalization ability of\nHiSC4D. The dataset and code will be publicated on\nwww.lidarhumanmotion.net/hisc4d available for research purposes.\n","authors":["Yudi Dai","Zhiyong Wang","Xiping Lin","Chenglu Wen","Lan Xu","Siqi Shen","Yuexin Ma","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2409.04398v3.pdf","comment":"17 pages, 10 figures, Jornal"},{"id":"http://arxiv.org/abs/2409.09427v1","updated":"2024-09-14T12:51:29Z","published":"2024-09-14T12:51:29Z","title":"Prototypical Prompting for Text-to-image Person Re-identification","summary":"  In this paper, we study the problem of Text-to-Image Person Re-identification\n(TIReID), which aims to find images of the same identity described by a text\nsentence from a pool of candidate images. Benefiting from Vision-Language\nPre-training, such as CLIP (Contrastive Language-Image Pretraining), the TIReID\ntechniques have achieved remarkable progress recently. However, most existing\nmethods only focus on instance-level matching and ignore identity-level\nmatching, which involves associating multiple images and texts belonging to the\nsame person. In this paper, we propose a novel prototypical prompting framework\n(Propot) designed to simultaneously model instance-level and identity-level\nmatching for TIReID. Our Propot transforms the identity-level matching problem\ninto a prototype learning problem, aiming to learn identity-enriched\nprototypes. Specifically, Propot works by 'initialize, adapt, enrich, then\naggregate'. We first use CLIP to generate high-quality initial prototypes.\nThen, we propose a domain-conditional prototypical prompting (DPP) module to\nadapt the prototypes to the TIReID task using task-related information.\nFurther, we propose an instance-conditional prototypical prompting (IPP) module\nto update prototypes conditioned on intra-modal and inter-modal instances to\nensure prototype diversity. Finally, we design an adaptive prototype\naggregation module to aggregate these prototypes, generating final\nidentity-enriched prototypes. With identity-enriched prototypes, we diffuse its\nrich identity information to instances through prototype-to-instance\ncontrastive loss to facilitate identity-level matching. Extensive experiments\nconducted on three benchmarks demonstrate the superiority of Propot compared to\nexisting TIReID methods.\n","authors":["Shuanglin Yan","Jun Liu","Neng Dong","Liyan Zhang","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2409.09427v1.pdf","comment":"Accepted by ACM MM2024"},{"id":"http://arxiv.org/abs/2409.04828v2","updated":"2024-09-14T12:41:35Z","published":"2024-09-07T13:41:37Z","title":"POINTS: Improving Your Vision-language Model with Affordable Strategies","summary":"  In recent years, vision-language models have made significant strides,\nexcelling in tasks like optical character recognition and geometric\nproblem-solving. However, several critical issues remain: 1) Proprietary models\noften lack transparency about their architectures, while open-source models\nneed more detailed ablations of their training strategies. 2) Pre-training data\nin open-source works is under-explored, with datasets added empirically, making\nthe process cumbersome. 3) Fine-tuning often focuses on adding datasets,\nleading to diminishing returns. To address these issues, we propose the\nfollowing contributions: 1) We trained a robust baseline model using the latest\nadvancements in vision-language models, introducing effective improvements and\nconducting comprehensive ablation and validation for each technique. 2)\nInspired by recent work on large language models, we filtered pre-training data\nusing perplexity, selecting the lowest perplexity data for training. This\napproach allowed us to train on a curated 1M dataset, achieving competitive\nperformance. 3) During visual instruction tuning, we used model soup on\ndifferent datasets when adding more datasets yielded marginal improvements.\nThese innovations resulted in a 9B parameter model that performs competitively\nwith state-of-the-art models. Our strategies are efficient and lightweight,\nmaking them easily adoptable by the community.\n","authors":["Yuan Liu","Zhongyin Zhao","Ziyuan Zhuang","Le Tian","Xiao Zhou","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.04828v2.pdf","comment":"v1"},{"id":"http://arxiv.org/abs/2409.09403v1","updated":"2024-09-14T10:27:36Z","published":"2024-09-14T10:27:36Z","title":"AI-Driven Virtual Teacher for Enhanced Educational Efficiency:\n  Leveraging Large Pretrain Models for Autonomous Error Analysis and Correction","summary":"  Students frequently make mistakes while solving mathematical problems, and\ntraditional error correction methods are both time-consuming and\nlabor-intensive. This paper introduces an innovative \\textbf{V}irtual\n\\textbf{A}I \\textbf{T}eacher system designed to autonomously analyze and\ncorrect student \\textbf{E}rrors (VATE). Leveraging advanced large language\nmodels (LLMs), the system uses student drafts as a primary source for error\nanalysis, which enhances understanding of the student's learning process. It\nincorporates sophisticated prompt engineering and maintains an error pool to\nreduce computational overhead. The AI-driven system also features a real-time\ndialogue component for efficient student interaction. Our approach demonstrates\nsignificant advantages over traditional and machine learning-based error\ncorrection methods, including reduced educational costs, high scalability, and\nsuperior generalizability. The system has been deployed on the Squirrel AI\nlearning platform for elementary mathematics education, where it achieves\n78.3\\% accuracy in error analysis and shows a marked improvement in student\nlearning efficiency. Satisfaction surveys indicate a strong positive reception,\nhighlighting the system's potential to transform educational practices.\n","authors":["Tianlong Xu","Yi-Fan Zhang","Zhendong Chu","Shen Wang","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2409.09403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09378v1","updated":"2024-09-14T09:06:43Z","published":"2024-09-14T09:06:43Z","title":"Prevailing Research Areas for Music AI in the Era of Foundation Models","summary":"  In tandem with the recent advancements in foundation model research, there\nhas been a surge of generative music AI applications within the past few years.\nAs the idea of AI-generated or AI-augmented music becomes more mainstream, many\nresearchers in the music AI community may be wondering what avenues of research\nare left. With regards to music generative models, we outline the current areas\nof research with significant room for exploration. Firstly, we pose the\nquestion of foundational representation of these generative models and\ninvestigate approaches towards explainability. Next, we discuss the current\nstate of music datasets and their limitations. We then overview different\ngenerative models, forms of evaluating these models, and their computational\nconstraints/limitations. Subsequently, we highlight applications of these\ngenerative models towards extensions to multiple modalities and integration\nwith artists' workflow as well as music education systems. Finally, we survey\nthe potential copyright implications of generative music and discuss strategies\nfor protecting the rights of musicians. While it is not meant to be exhaustive,\nour survey calls to attention a variety of research directions enabled by music\nfoundation models.\n","authors":["Megan Wei","Mateusz Modrzejewski","Aswin Sivaraman","Dorien Herremans"],"pdf_url":"https://arxiv.org/pdf/2409.09378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09366v1","updated":"2024-09-14T08:42:39Z","published":"2024-09-14T08:42:39Z","title":"MHAD: Multimodal Home Activity Dataset with Multi-Angle Videos and\n  Synchronized Physiological Signals","summary":"  Video-based physiology, exemplified by remote photoplethysmography (rPPG),\nextracts physiological signals such as pulse and respiration by analyzing\nsubtle changes in video recordings. This non-contact, real-time monitoring\nmethod holds great potential for home settings. Despite the valuable\ncontributions of public benchmark datasets to this technology, there is\ncurrently no dataset specifically designed for passive home monitoring.\nExisting datasets are often limited to close-up, static, frontal recordings and\ntypically include only 1-2 physiological signals. To advance video-based\nphysiology in real home settings, we introduce the MHAD dataset. It comprises\n1,440 videos from 40 subjects, capturing 6 typical activities from 3 angles in\na real home environment. Additionally, 5 physiological signals were recorded,\nmaking it a comprehensive video-based physiology dataset. MHAD is compatible\nwith the rPPG-toolbox and has been validated using several unsupervised and\nsupervised methods. Our dataset is publicly available at\nhttps://github.com/jdh-algo/MHAD-Dataset.\n","authors":["Lei Yu","Jintao Fei","Xinyi Liu","Yang Yao","Jun Zhao","Guoxin Wang","Xin Li"],"pdf_url":"https://arxiv.org/pdf/2409.09366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05964v2","updated":"2024-09-14T07:45:27Z","published":"2023-05-10T08:16:36Z","title":"Interpretable Multimodal Misinformation Detection with Logic Reasoning","summary":"  Multimodal misinformation on online social platforms is becoming a critical\nconcern due to increasing credibility and easier dissemination brought by\nmultimedia content, compared to traditional text-only information. While\nexisting multimodal detection approaches have achieved high performance, the\nlack of interpretability hinders these systems' reliability and practical\ndeployment. Inspired by NeuralSymbolic AI which combines the learning ability\nof neural networks with the explainability of symbolic learning, we propose a\nnovel logic-based neural model for multimodal misinformation detection which\nintegrates interpretable logic clauses to express the reasoning process of the\ntarget task. To make learning effective, we parameterize symbolic logical\nelements using neural representations, which facilitate the automatic\ngeneration and evaluation of meaningful logic clauses. Additionally, to make\nour framework generalizable across diverse misinformation sources, we introduce\nfive meta-predicates that can be instantiated with different correlations.\nResults on three public datasets (Twitter, Weibo, and Sarcasm) demonstrate the\nfeasibility and versatility of our model.\n","authors":["Hui Liu","Wenya Wang","Haoliang Li"],"pdf_url":"https://arxiv.org/pdf/2305.05964v2.pdf","comment":"Accepted by Findings of ACL 23. 9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.09289v1","updated":"2024-09-14T03:40:48Z","published":"2024-09-14T03:40:48Z","title":"DSCLAP: Domain-Specific Contrastive Language-Audio Pre-Training","summary":"  Analyzing real-world multimodal signals is an essential and challenging task\nfor intelligent voice assistants (IVAs). Mainstream approaches have achieved\nremarkable performance on various downstream tasks of IVAs with pre-trained\naudio models and text models. However, these models are pre-trained\nindependently and usually on tasks different from target domains, resulting in\nsub-optimal modality representations for downstream tasks. Moreover, in many\ndomains, collecting enough language-audio pairs is extremely hard, and\ntranscribing raw audio also requires high professional skills, making it\ndifficult or even infeasible to joint pre-training. To address these\npainpoints, we propose DSCLAP, a simple and effective framework that enables\nlanguage-audio pre-training with only raw audio signal input. Specifically,\nDSCLAP converts raw audio signals into text via an ASR system and combines a\ncontrastive learning objective and a language-audio matching objective to align\nthe audio and ASR transcriptions. We pre-train DSCLAP on 12,107 hours of\nin-vehicle domain audio. Empirical results on two downstream tasks show that\nwhile conceptually simple, DSCLAP significantly outperforms the baseline models\nin all metrics, showing great promise for domain-specific IVAs applications.\n","authors":["Shengqiang Liu","Da Liu","Anna Wang","Zhiyu Zhang","Jie Gao","Yali Li"],"pdf_url":"https://arxiv.org/pdf/2409.09289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09284v1","updated":"2024-09-14T03:24:23Z","published":"2024-09-14T03:24:23Z","title":"M$^{3}$V: A multi-modal multi-view approach for Device-Directed Speech\n  Detection","summary":"  With the goal of more natural and human-like interaction with virtual voice\nassistants, recent research in the field has focused on full duplex interaction\nmode without relying on repeated wake-up words. This requires that in scenes\nwith complex sound sources, the voice assistant must classify utterances as\ndevice-oriented or non-device-oriented. The dual-encoder structure, which is\njointly modeled by text and speech, has become the paradigm of device-directed\nspeech detection. However, in practice, these models often produce incorrect\npredictions for unaligned input pairs due to the unavoidable errors of\nautomatic speech recognition (ASR).To address this challenge, we propose\nM$^{3}$V, a multi-modal multi-view approach for device-directed speech\ndetection, which frames we frame the problem as a multi-view learning task that\nintroduces unimodal views and a text-audio alignment view in the network\nbesides the multi-modal. Experimental results show that M$^{3}$V significantly\noutperforms models trained using only single or multi-modality and surpasses\nhuman judgment performance on ASR error data for the first time.\n","authors":["Anna Wang","Da Liu","Zhiyu Zhang","Shengqiang Liu","Jie Gao","Yali Li"],"pdf_url":"https://arxiv.org/pdf/2409.09284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09282v1","updated":"2024-09-14T03:15:34Z","published":"2024-09-14T03:15:34Z","title":"Turbo your multi-modal classification with contrastive learning","summary":"  Contrastive learning has become one of the most impressive approaches for\nmulti-modal representation learning. However, previous multi-modal works mainly\nfocused on cross-modal understanding, ignoring in-modal contrastive learning,\nwhich limits the representation of each modality. In this paper, we propose a\nnovel contrastive learning strategy, called $Turbo$, to promote multi-modal\nunderstanding by joint in-modal and cross-modal contrastive learning.\nSpecifically, multi-modal data pairs are sent through the forward pass twice\nwith different hidden dropout masks to get two different representations for\neach modality. With these representations, we obtain multiple in-modal and\ncross-modal contrastive objectives for training. Finally, we combine the\nself-supervised Turbo with the supervised multi-modal classification and\ndemonstrate its effectiveness on two audio-text classification tasks, where the\nstate-of-the-art performance is achieved on a speech emotion recognition\nbenchmark dataset.\n","authors":["Zhiyu Zhang","Da Liu","Shengqiang Liu","Anna Wang","Jie Gao","Yali Li"],"pdf_url":"https://arxiv.org/pdf/2409.09282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09272v1","updated":"2024-09-14T02:45:09Z","published":"2024-09-14T02:45:09Z","title":"SafeEar: Content Privacy-Preserving Audio Deepfake Detection","summary":"  Text-to-Speech (TTS) and Voice Conversion (VC) models have exhibited\nremarkable performance in generating realistic and natural audio. However,\ntheir dark side, audio deepfake poses a significant threat to both society and\nindividuals. Existing countermeasures largely focus on determining the\ngenuineness of speech based on complete original audio recordings, which\nhowever often contain private content. This oversight may refrain deepfake\ndetection from many applications, particularly in scenarios involving sensitive\ninformation like business secrets. In this paper, we propose SafeEar, a novel\nframework that aims to detect deepfake audios without relying on accessing the\nspeech content within. Our key idea is to devise a neural audio codec into a\nnovel decoupling model that well separates the semantic and acoustic\ninformation from audio samples, and only use the acoustic information (e.g.,\nprosody and timbre) for deepfake detection. In this way, no semantic content\nwill be exposed to the detector. To overcome the challenge of identifying\ndiverse deepfake audio without semantic clues, we enhance our deepfake detector\nwith real-world codec augmentation. Extensive experiments conducted on four\nbenchmark datasets demonstrate SafeEar's effectiveness in detecting various\ndeepfake techniques with an equal error rate (EER) down to 2.02%.\nSimultaneously, it shields five-language speech content from being deciphered\nby both machine and human auditory analysis, demonstrated by word error rates\n(WERs) all above 93.93% and our user study. Furthermore, our benchmark\nconstructed for anti-deepfake and anti-content recovery evaluation helps\nprovide a basis for future research in the realms of audio privacy preservation\nand deepfake detection.\n","authors":["Xinfeng Li","Kai Li","Yifan Zheng","Chen Yan","Xiaoyu Ji","Wenyuan Xu"],"pdf_url":"https://arxiv.org/pdf/2409.09272v1.pdf","comment":"Accepted by ACM CCS 2024. Please cite this paper as \"Xinfeng Li, Kai\n  Li, Yifan Zheng, Chen Yan, Xiaoyu Ji, Wenyuan Xu. SafeEar: Content\n  Privacy-Preserving Audio Deepfake Detection. In Proceedings of ACM Conference\n  on Computer and Communications Security (CCS), 2024.\""}]},"2024-09-13T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2409.09221v1","updated":"2024-09-13T22:18:45Z","published":"2024-09-13T22:18:45Z","title":"Multi-modal Speech Transformer Decoders: When Do Multiple Modalities\n  Improve Accuracy?","summary":"  Decoder-only discrete-token language models have recently achieved\nsignificant success in automatic speech recognition. However, systematic\nanalyses of how different modalities impact performance in specific scenarios\nremain limited. In this paper, we investigate the effects of multiple\nmodalities on recognition accuracy on both synthetic and real-world datasets.\nOur experiments suggest that: (1) Integrating more modalities can increase\naccuracy; in particular, our paper is, to our best knowledge, the first to show\nthe benefit of combining audio, image context, and lip information; (2) Images\nas a supplementary modality for speech recognition provide the greatest benefit\nat moderate noise levels, moreover, they exhibit a different trend compared to\ninherently synchronized modalities like lip movements; (3) Performance improves\non both synthetic and real-world datasets when the most relevant visual\ninformation is filtered as a preprocessing step.\n","authors":["Yiwen Guan","Viet Anh Trinh","Vivek Voleti","Jacob Whitehill"],"pdf_url":"https://arxiv.org/pdf/2409.09221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06859v2","updated":"2024-09-13T22:13:01Z","published":"2024-09-10T20:49:05Z","title":"NSP: A Neuro-Symbolic Natural Language Navigational Planner","summary":"  Path planners that can interpret free-form natural language instructions hold\npromise to automate a wide range of robotics applications. These planners\nsimplify user interactions and enable intuitive control over complex\nsemi-autonomous systems. While existing symbolic approaches offer guarantees on\nthe correctness and efficiency, they struggle to parse free-form natural\nlanguage inputs. Conversely, neural approaches based on pre-trained Large\nLanguage Models (LLMs) can manage natural language inputs but lack performance\nguarantees. In this paper, we propose a neuro-symbolic framework for path\nplanning from natural language inputs called NSP. The framework leverages the\nneural reasoning abilities of LLMs to i) craft symbolic representations of the\nenvironment and ii) a symbolic path planning algorithm. Next, a solution to the\npath planning problem is obtained by executing the algorithm on the environment\nrepresentation. The framework uses a feedback loop from the symbolic execution\nenvironment to the neural generation process to self-correct syntax errors and\nsatisfy execution time constraints. We evaluate our neuro-symbolic approach\nusing a benchmark suite with 1500 path-planning problems. The experimental\nevaluation shows that our neuro-symbolic approach produces 90.1% valid paths\nthat are on average 19-77% shorter than state-of-the-art neural approaches.\n","authors":["William English","Dominic Simon","Sumit Jha","Rickard Ewetz"],"pdf_url":"https://arxiv.org/pdf/2409.06859v2.pdf","comment":"10 pages, Preprint of paper accepted at 23rd International Conference\n  on Machine Learning and Applications (ICMLA) 2024"},{"id":"http://arxiv.org/abs/2409.09213v1","updated":"2024-09-13T21:58:20Z","published":"2024-09-13T21:58:20Z","title":"ReCLAP: Improving Zero Shot Audio Classification by Describing Sounds","summary":"  Open-vocabulary audio-language models, like CLAP, offer a promising approach\nfor zero-shot audio classification (ZSAC) by enabling classification with any\narbitrary set of categories specified with natural language prompts. In this\npaper, we propose a simple but effective method to improve ZSAC with CLAP.\nSpecifically, we shift from the conventional method of using prompts with\nabstract category labels (e.g., Sound of an organ) to prompts that describe\nsounds using their inherent descriptive features in a diverse context (e.g.,The\norgan's deep and resonant tones filled the cathedral.). To achieve this, we\nfirst propose ReCLAP, a CLAP model trained with rewritten audio captions for\nimproved understanding of sounds in the wild. These rewritten captions describe\neach sound event in the original caption using their unique discriminative\ncharacteristics. ReCLAP outperforms all baselines on both multi-modal\naudio-text retrieval and ZSAC. Next, to improve zero-shot audio classification\nwith ReCLAP, we propose prompt augmentation. In contrast to the traditional\nmethod of employing hand-written template prompts, we generate custom prompts\nfor each unique label in the dataset. These custom prompts first describe the\nsound event in the label and then employ them in diverse scenes. Our proposed\nmethod improves ReCLAP's performance on ZSAC by 1%-18% and outperforms all\nbaselines by 1% - 55%.\n","authors":["Sreyan Ghosh","Sonal Kumar","Chandra Kiran Reddy Evuru","Oriol Nieto","Ramani Duraiswami","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2409.09213v1.pdf","comment":"Code and Checkpoints: https://github.com/Sreyan88/ReCLAP"},{"id":"http://arxiv.org/abs/2409.09201v1","updated":"2024-09-13T21:28:54Z","published":"2024-09-13T21:28:54Z","title":"Contextual Evaluation of Large Language Models for Classifying Tropical\n  and Infectious Diseases","summary":"  While large language models (LLMs) have shown promise for medical question\nanswering, there is limited work focused on tropical and infectious\ndisease-specific exploration. We build on an opensource tropical and infectious\ndiseases (TRINDs) dataset, expanding it to include demographic and semantic\nclinical and consumer augmentations yielding 11000+ prompts. We evaluate LLM\nperformance on these, comparing generalist and medical LLMs, as well as LLM\noutcomes to human experts. We demonstrate through systematic experimentation,\nthe benefit of contextual information such as demographics, location, gender,\nrisk factors for optimal LLM response. Finally we develop a prototype of\nTRINDs-LM, a research tool that provides a playground to navigate how context\nimpacts LLM outputs for health.\n","authors":["Mercy Asiedu","Nenad Tomasev","Chintan Ghate","Tiya Tiyasirichokchai","Awa Dieng","Oluwatosin Akande","Geoffrey Siwo","Steve Adudans","Sylvanus Aitkins","Odianosen Ehiakhamen","Katherine Heller"],"pdf_url":"https://arxiv.org/pdf/2409.09201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04081v2","updated":"2024-09-13T21:08:40Z","published":"2024-09-06T07:44:44Z","title":"UI-JEPA: Towards Active Perception of User Intent through Onscreen User\n  Activity","summary":"  Generating user intent from a sequence of user interface (UI) actions is a\ncore challenge in comprehensive UI understanding. Recent advancements in\nmultimodal large language models (MLLMs) have led to substantial progress in\nthis area, but their demands for extensive model parameters, computing power,\nand high latency makes them impractical for scenarios requiring lightweight,\non-device solutions with low latency or heightened privacy. Additionally, the\nlack of high-quality datasets has hindered the development of such lightweight\nmodels. To address these challenges, we propose UI-JEPA, a novel framework that\nemploys masking strategies to learn abstract UI embeddings from unlabeled data\nthrough self-supervised learning, combined with an LLM decoder fine-tuned for\nuser intent prediction. We also introduce two new UI-grounded multimodal\ndatasets, \"Intent in the Wild\" (IIW) and \"Intent in the Tame\" (IIT), designed\nfor few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos\nacross 219 intent categories, while IIT contains 914 videos across 10\ncategories. We establish the first baselines for these datasets, showing that\nrepresentations learned using a JEPA-style objective, combined with an LLM\ndecoder, can achieve user intent predictions that match the performance of\nstate-of-the-art large MLLMs, but with significantly reduced annotation and\ndeployment resources. Measured by intent similarity scores, UI-JEPA outperforms\nGPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged\nacross two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x\nreduction in computational cost and a 6.6x improvement in latency in the IIW\ndataset. These results underscore the effectiveness of UI-JEPA, highlighting\nits potential for lightweight, high-performance UI understanding.\n","authors":["Yicheng Fu","Raviteja Anantha","Prabal Vashisht","Jianpeng Cheng","Etai Littwin"],"pdf_url":"https://arxiv.org/pdf/2409.04081v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04927v2","updated":"2024-09-13T20:42:20Z","published":"2024-06-07T13:33:22Z","title":"LLM-based speaker diarization correction: A generalizable approach","summary":"  Speaker diarization is necessary for interpreting conversations transcribed\nusing automated speech recognition (ASR) tools. Despite significant\ndevelopments in diarization methods, diarization accuracy remains an issue.\nHere, we investigate the use of large language models (LLMs) for diarization\ncorrection as a post-processing step. LLMs were fine-tuned using the Fisher\ncorpus, a large dataset of transcribed conversations. The ability of the models\nto improve diarization accuracy in a holdout dataset from the Fisher corpus as\nwell as an independent dataset was measured. We report that fine-tuned LLMs can\nmarkedly improve diarization accuracy. However, model performance is\nconstrained to transcripts produced using the same ASR tool as the transcripts\nused for fine-tuning, limiting generalizability. To address this constraint, an\nensemble model was developed by combining weights from three separate models,\neach fine-tuned using transcripts from a different ASR tool. The ensemble model\ndemonstrated better overall performance than each of the ASR-specific models,\nsuggesting that a generalizable and ASR-agnostic approach may be achievable. We\nhave made the weights of these models publicly available on HuggingFace at\nhttps://huggingface.co/bklynhlth.\n","authors":["Georgios Efstathiadis","Vijay Yadav","Anzar Abbas"],"pdf_url":"https://arxiv.org/pdf/2406.04927v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09177v1","updated":"2024-09-13T20:30:29Z","published":"2024-09-13T20:30:29Z","title":"Transformer with Controlled Attention for Synchronous Motion Captioning","summary":"  In this paper, we address a challenging task, synchronous motion captioning,\nthat aim to generate a language description synchronized with human motion\nsequences. This task pertains to numerous applications, such as aligned sign\nlanguage transcription, unsupervised action segmentation and temporal\ngrounding. Our method introduces mechanisms to control self- and\ncross-attention distributions of the Transformer, allowing interpretability and\ntime-aligned text generation. We achieve this through masking strategies and\nstructuring losses that push the model to maximize attention only on the most\nimportant frames contributing to the generation of a motion word. These\nconstraints aim to prevent undesired mixing of information in attention maps\nand to provide a monotonic attention distribution across tokens. Thus, the\ncross attentions of tokens are used for progressive text generation in\nsynchronization with human motion sequences. We demonstrate the superior\nperformance of our approach through evaluation on the two available benchmark\ndatasets, KIT-ML and HumanML3D. As visual evaluation is essential for this\ntask, we provide a comprehensive set of animated visual illustrations in the\ncode repository: https://github.com/rd20karim/Synch-Transformer.\n","authors":["Karim Radouane","Sylvie Ranwez","Julien Lagarde","Andon Tchechmedjiev"],"pdf_url":"https://arxiv.org/pdf/2409.09177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00729v2","updated":"2024-09-13T20:26:40Z","published":"2024-09-01T14:36:36Z","title":"ContextCite: Attributing Model Generation to Context","summary":"  How do language models use information provided as context when generating a\nresponse? Can we infer whether a particular generated statement is actually\ngrounded in the context, a misinterpretation, or fabricated? To help answer\nthese questions, we introduce the problem of context attribution: pinpointing\nthe parts of the context (if any) that led a model to generate a particular\nstatement. We then present ContextCite, a simple and scalable method for\ncontext attribution that can be applied on top of any existing language model.\nFinally, we showcase the utility of ContextCite through three applications: (1)\nhelping verify generated statements (2) improving response quality by pruning\nthe context and (3) detecting poisoning attacks. We provide code for\nContextCite at https://github.com/MadryLab/context-cite.\n","authors":["Benjamin Cohen-Wang","Harshay Shah","Kristian Georgiev","Aleksander Madry"],"pdf_url":"https://arxiv.org/pdf/2409.00729v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09170v1","updated":"2024-09-13T20:01:13Z","published":"2024-09-13T20:01:13Z","title":"Towards Precision Characterization of Communication Disorders using\n  Models of Perceived Pragmatic Similarity","summary":"  The diagnosis and treatment of individuals with communication disorders\noffers many opportunities for the application of speech technology, but\nresearch so far has not adequately considered: the diversity of conditions, the\nrole of pragmatic deficits, and the challenges of limited data. This paper\nexplores how a general-purpose model of perceived pragmatic similarity may\novercome these limitations. It explains how it might support several use cases\nfor clinicians and clients, and presents evidence that a simple model can\nprovide value, and in particular can capture utterance aspects that are\nrelevant to diagnoses of autism and specific language impairment.\n","authors":["Nigel G. Ward","Andres Segura","Georgina Bugarini","Heike Lehnert-LeHouillier","Dancheng Liu","Jinjun Xiong","Olac Fuentes"],"pdf_url":"https://arxiv.org/pdf/2409.09170v1.pdf","comment":"submitted to IEEE ICASSP 2025"},{"id":"http://arxiv.org/abs/2409.09143v1","updated":"2024-09-13T18:59:13Z","published":"2024-09-13T18:59:13Z","title":"DomURLs_BERT: Pre-trained BERT-based Model for Malicious Domains and\n  URLs Detection and Classification","summary":"  Detecting and classifying suspicious or malicious domain names and URLs is\nfundamental task in cybersecurity. To leverage such indicators of compromise,\ncybersecurity vendors and practitioners often maintain and update blacklists of\nknown malicious domains and URLs. However, blacklists frequently fail to\nidentify emerging and obfuscated threats. Over the past few decades, there has\nbeen significant interest in developing machine learning models that\nautomatically detect malicious domains and URLs, addressing the limitations of\nblacklists maintenance and updates. In this paper, we introduce DomURLs_BERT, a\npre-trained BERT-based encoder adapted for detecting and classifying\nsuspicious/malicious domains and URLs. DomURLs_BERT is pre-trained using the\nMasked Language Modeling (MLM) objective on a large multilingual corpus of\nURLs, domain names, and Domain Generation Algorithms (DGA) dataset. In order to\nassess the performance of DomURLs_BERT, we have conducted experiments on\nseveral binary and multi-class classification tasks involving domain names and\nURLs, covering phishing, malware, DGA, and DNS tunneling. The evaluations\nresults show that the proposed encoder outperforms state-of-the-art\ncharacter-based deep learning models and cybersecurity-focused BERT models\nacross multiple tasks and datasets. The pre-training dataset, the pre-trained\nDomURLs_BERT encoder, and the experiments source code are publicly available.\n","authors":["Abdelkader El Mahdaouy","Salima Lamsiyah","Meryem Janati Idrissi","Hamza Alami","Zakaria Yartaoui","Ismail Berrada"],"pdf_url":"https://arxiv.org/pdf/2409.09143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09135v1","updated":"2024-09-13T18:28:12Z","published":"2024-09-13T18:28:12Z","title":"Multimodal Fusion with LLMs for Engagement Prediction in Natural\n  Conversation","summary":"  Over the past decade, wearable computing devices (``smart glasses'') have\nundergone remarkable advancements in sensor technology, design, and processing\npower, ushering in a new era of opportunity for high-density human behavior\ndata. Equipped with wearable cameras, these glasses offer a unique opportunity\nto analyze non-verbal behavior in natural settings as individuals interact. Our\nfocus lies in predicting engagement in dyadic interactions by scrutinizing\nverbal and non-verbal cues, aiming to detect signs of disinterest or confusion.\nLeveraging such analyses may revolutionize our understanding of human\ncommunication, foster more effective collaboration in professional\nenvironments, provide better mental health support through empathetic virtual\ninteractions, and enhance accessibility for those with communication barriers.\n  In this work, we collect a dataset featuring 34 participants engaged in\ncasual dyadic conversations, each providing self-reported engagement ratings at\nthe end of each conversation. We introduce a novel fusion strategy using Large\nLanguage Models (LLMs) to integrate multiple behavior modalities into a\n``multimodal transcript'' that can be processed by an LLM for behavioral\nreasoning tasks. Remarkably, this method achieves performance comparable to\nestablished fusion techniques even in its preliminary implementation,\nindicating strong potential for further research and optimization. This fusion\nmethod is one of the first to approach ``reasoning'' about real-world human\nbehavior through a language model. Smart glasses provide us the ability to\nunobtrusively gather high-density multimodal data on human behavior, paving the\nway for new approaches to understanding and improving human communication with\nthe potential for important societal benefits. The features and data collected\nduring the studies will be made publicly available to promote further research.\n","authors":["Cheng Charles Ma","Kevin Hyekang Joo","Alexandria K. Vail","Sunreeta Bhattacharya","√Ålvaro Fern√°ndez Garc√≠a","Kailana Baker-Matsuoka","Sheryl Mathew","Lori L. Holt","Fernando De la Torre"],"pdf_url":"https://arxiv.org/pdf/2409.09135v1.pdf","comment":"22 pages, first three authors equal contribution"},{"id":"http://arxiv.org/abs/2409.09030v1","updated":"2024-09-13T17:55:58Z","published":"2024-09-13T17:55:58Z","title":"Agents in Software Engineering: Survey, Landscape, and Vision","summary":"  In recent years, Large Language Models (LLMs) have achieved remarkable\nsuccess and have been widely used in various downstream tasks, especially in\nthe tasks of the software engineering (SE) field. We find that many studies\ncombining LLMs with SE have employed the concept of agents either explicitly or\nimplicitly. However, there is a lack of an in-depth survey to sort out the\ndevelopment context of existing works, analyze how existing works combine the\nLLM-based agent technologies to optimize various tasks, and clarify the\nframework of LLM-based agents in SE. In this paper, we conduct the first survey\nof the studies on combining LLM-based agents with SE and present a framework of\nLLM-based agents in SE which includes three key modules: perception, memory,\nand action. We also summarize the current challenges in combining the two\nfields and propose future opportunities in response to existing challenges. We\nmaintain a GitHub repository of the related papers at:\nhttps://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE.\n","authors":["Yanxian Huang","Wanjun Zhong","Ensheng Shi","Min Yang","Jiachi Chen","Hui Li","Yuchi Ma","Qianxiang Wang","Zibin Zheng","Yanlin Wang"],"pdf_url":"https://arxiv.org/pdf/2409.09030v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.09013v1","updated":"2024-09-13T17:41:12Z","published":"2024-09-13T17:41:12Z","title":"AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM\n  Agents","summary":"  To be safely and successfully deployed, LLMs must simultaneously satisfy\ntruthfulness and utility goals. Yet, often these two goals compete (e.g., an AI\nagent assisting a used car salesman selling a car with flaws), partly due to\nambiguous or misleading user instructions. We propose AI-LieDar, a framework to\nstudy how LLM-based agents navigate scenarios with utility-truthfulness\nconflicts in a multi-turn interactive setting. We design a set of realistic\nscenarios where language agents are instructed to achieve goals that are in\nconflict with being truthful during a multi-turn conversation with simulated\nhuman agents. To evaluate the truthfulness at large scale, we develop a\ntruthfulness detector inspired by psychological literature to assess the\nagents' responses. Our experiment demonstrates that all models are truthful\nless than 50% of the time, although truthfulness and goal achievement (utility)\nrates vary across models. We further test the steerability of LLMs towards\ntruthfulness, finding that models follow malicious instructions to deceive, and\neven truth-steered models can still lie. These findings reveal the complex\nnature of truthfulness in LLMs and underscore the importance of further\nresearch to ensure the safe and reliable deployment of LLMs and AI agents.\n","authors":["Zhe Su","Xuhui Zhou","Sanketh Rangreji","Anubha Kabra","Julia Mendelsohn","Faeze Brahman","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2409.09013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09009v1","updated":"2024-09-13T17:38:03Z","published":"2024-09-13T17:38:03Z","title":"Optimizing Rare Word Accuracy in Direct Speech Translation with a\n  Retrieval-and-Demonstration Approach","summary":"  Direct speech translation (ST) models often struggle with rare words.\nIncorrect translation of these words can have severe consequences, impacting\ntranslation quality and user trust. While rare word translation is inherently\nchallenging for neural models due to sparse learning signals, real-world\nscenarios often allow access to translations of past recordings on similar\ntopics. To leverage these valuable resources, we propose a\nretrieval-and-demonstration approach to enhance rare word translation accuracy\nin direct ST models. First, we adapt existing ST models to incorporate\nretrieved examples for rare word translation, which allows the model to benefit\nfrom prepended examples, similar to in-context learning. We then develop a\ncross-modal (speech-to-speech, speech-to-text, text-to-text) retriever to\nlocate suitable examples. We demonstrate that standard ST models can be\neffectively adapted to leverage examples for rare word translation, improving\nrare word translation accuracy over the baseline by 17.6% with gold examples\nand 8.5% with retrieved examples. Moreover, our speech-to-speech retrieval\napproach outperforms other modalities and exhibits higher robustness to unseen\nspeakers. Our code is publicly available\n(https://github.com/SiqiLii/Retrieve-and-Demonstration-ST).\n","authors":["Siqi Li","Danni Liu","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2409.09009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09001v1","updated":"2024-09-13T17:31:09Z","published":"2024-09-13T17:31:09Z","title":"E2MoCase: A Dataset for Emotional, Event and Moral Observations in News\n  Articles on High-impact Legal Cases","summary":"  The way media reports on legal cases can significantly shape public opinion,\noften embedding subtle biases that influence societal views on justice and\nmorality. Analyzing these biases requires a holistic approach that captures the\nemotional tone, moral framing, and specific events within the narratives. In\nthis work we introduce E2MoCase, a novel dataset designed to facilitate the\nintegrated analysis of emotions, moral values, and events within legal\nnarratives and media coverage. By leveraging advanced models for emotion\ndetection, moral value identification, and event extraction, E2MoCase offers a\nmulti-dimensional perspective on how legal cases are portrayed in news\narticles.\n","authors":["Candida M. Greco","Lorenzo Zangari","Davide Picca","Andrea Tagarelli"],"pdf_url":"https://arxiv.org/pdf/2409.09001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.05161v2","updated":"2024-09-13T17:24:52Z","published":"2024-05-08T15:54:12Z","title":"Motion Capture Analysis of Verb and Adjective Types in Austrian Sign\n  Language","summary":"  Across a number of sign languages, temporal and spatial characteristics of\ndominant hand articulation are used to express semantic and grammatical\nfeatures. In this study of Austrian Sign Language (\\\"Osterreichische\nGeb\\\"ardensprache, or \\\"OGS), motion capture data of four Deaf signers is used\nto quantitatively characterize the kinematic parameters of sign production in\nverbs and adjectives. We investigate (1) the difference in production between\nverbs involving a natural endpoint (telic verbs; e.g. arrive) and verbs lacking\nan endpoint (atelic verbs; e.g. analyze), and (2) adjective signs in\nintensified vs. non-intensified (plain) forms. Motion capture data analysis\nusing linear-mixed effects models (LME) indicates that both the endpoint\nmarking in verbs, as well as marking of intensification in adjectives, are\nexpressed by movement modulation in \\\"OGS. While the semantic distinction\nbetween verb types (telic/atelic) is marked by higher peak velocity and shorter\nduration for telic signs compared to atelic ones, the grammatical distinction\n(intensification) in adjectives is expressed by longer duration for intensified\ncompared to non-intensified adjectives. The observed individual differences of\nsigners might be interpreted as personal signing style.\n","authors":["Julia Krebs","Evie Malaia","Ronnie B. Wilbur","Isabella Fessl","Hans-Peter Wiesinger","Hermann Schwameder","Dietmar Roehm"],"pdf_url":"https://arxiv.org/pdf/2405.05161v2.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2405.06468v3","updated":"2024-09-13T16:44:17Z","published":"2024-05-10T13:27:32Z","title":"Pseudo-Prompt Generating in Pre-trained Vision-Language Models for\n  Multi-Label Medical Image Classification","summary":"  The task of medical image recognition is notably complicated by the presence\nof varied and multiple pathological indications, presenting a unique challenge\nin multi-label classification with unseen labels. This complexity underlines\nthe need for computer-aided diagnosis methods employing multi-label zero-shot\nlearning. Recent advancements in pre-trained vision-language models (VLMs) have\nshowcased notable zero-shot classification abilities on medical images.\nHowever, these methods have limitations on leveraging extensive pre-trained\nknowledge from broader image datasets, and often depend on manual prompt\nconstruction by expert radiologists. By automating the process of prompt\ntuning, prompt learning techniques have emerged as an efficient way to adapt\nVLMs to downstream tasks. Yet, existing CoOp-based strategies fall short in\nperforming class-specific prompts on unseen categories, limiting\ngeneralizability in fine-grained scenarios. To overcome these constraints, we\nintroduce a novel prompt generation approach inspirited by text generation in\nnatural language processing (NLP). Our method, named Pseudo-Prompt Generating\n(PsPG), capitalizes on the priori knowledge of multi-modal features. Featuring\na RNN-based decoder, PsPG autoregressively generates class-tailored embedding\nvectors, i.e., pseudo-prompts. Comparative evaluations on various multi-label\nchest radiograph datasets affirm the superiority of our approach against\nleading medical vision-language and multi-label prompt learning methods. The\nsource code is available at https://github.com/fallingnight/PsPG\n","authors":["Yaoqin Ye","Junjie Zhang","Hongwei Shi"],"pdf_url":"https://arxiv.org/pdf/2405.06468v3.pdf","comment":"Accepted by PRCV 2024"},{"id":"http://arxiv.org/abs/2409.08963v1","updated":"2024-09-13T16:29:25Z","published":"2024-09-13T16:29:25Z","title":"Safeguarding Decentralized Social Media: LLM Agents for Automating\n  Community Rule Compliance","summary":"  Ensuring content compliance with community guidelines is crucial for\nmaintaining healthy online social environments. However, traditional\nhuman-based compliance checking struggles with scaling due to the increasing\nvolume of user-generated content and a limited number of moderators. Recent\nadvancements in Natural Language Understanding demonstrated by Large Language\nModels unlock new opportunities for automated content compliance verification.\nThis work evaluates six AI-agents built on Open-LLMs for automated rule\ncompliance checking in Decentralized Social Networks, a challenging environment\ndue to heterogeneous community scopes and rules. Analyzing over 50,000 posts\nfrom hundreds of Mastodon servers, we find that AI-agents effectively detect\nnon-compliant content, grasp linguistic subtleties, and adapt to diverse\ncommunity contexts. Most agents also show high inter-rater reliability and\nconsistency in score justification and suggestions for compliance. Human-based\nevaluation with domain experts confirmed the agents' reliability and\nusefulness, rendering them promising tools for semi-automated or\nhuman-in-the-loop content moderation systems.\n","authors":["Lucio La Cava","Andrea Tagarelli"],"pdf_url":"https://arxiv.org/pdf/2409.08963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08936v1","updated":"2024-09-13T15:55:15Z","published":"2024-09-13T15:55:15Z","title":"SynSUM -- Synthetic Benchmark with Structured and Unstructured Medical\n  Records","summary":"  We present the SynSUM benchmark, a synthetic dataset linking unstructured\nclinical notes to structured background variables. The dataset consists of\n10,000 artificial patient records containing tabular variables (like symptoms,\ndiagnoses and underlying conditions) and related notes describing the fictional\npatient encounter in the domain of respiratory diseases. The tabular portion of\nthe data is generated through a Bayesian network, where both the causal\nstructure between the variables and the conditional probabilities are proposed\nby an expert based on domain knowledge. We then prompt a large language model\n(GPT-4o) to generate a clinical note related to this patient encounter,\ndescribing the patient symptoms and additional context. The SynSUM dataset is\nprimarily designed to facilitate research on clinical information extraction in\nthe presence of tabular background variables, which can be linked through\ndomain knowledge to concepts of interest to be extracted from the text - the\nsymptoms, in the case of SynSUM. Secondary uses include research on the\nautomation of clinical reasoning over both tabular data and text, causal effect\nestimation in the presence of tabular and/or textual confounders, and\nmulti-modal synthetic data generation. The dataset can be downloaded from\nhttps://github.com/prabaey/SynSUM.\n","authors":["Paloma Rabaey","Henri Arno","Stefan Heytens","Thomas Demeester"],"pdf_url":"https://arxiv.org/pdf/2409.08936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08907v1","updated":"2024-09-13T15:20:18Z","published":"2024-09-13T15:20:18Z","title":"Affective Computing Has Changed: The Foundation Model Disruption","summary":"  The dawn of Foundation Models has on the one hand revolutionised a wide range\nof research problems, and, on the other hand, democratised the access and use\nof AI-based tools by the general public. We even observe an incursion of these\nmodels into disciplines related to human psychology, such as the Affective\nComputing domain, suggesting their affective, emerging capabilities. In this\nwork, we aim to raise awareness of the power of Foundation Models in the field\nof Affective Computing by synthetically generating and analysing multimodal\naffective data, focusing on vision, linguistics, and speech (acoustics). We\nalso discuss some fundamental problems, such as ethical issues and regulatory\naspects, related to the use of Foundation Models in this research area.\n","authors":["Bj√∂rn Schuller","Adria Mallol-Ragolta","Alejandro Pe√±a Almansa","Iosif Tsangko","Mostafa M. Amin","Anastasia Semertzidou","Lukas Christ","Shahin Amiriparian"],"pdf_url":"https://arxiv.org/pdf/2409.08907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.04674v2","updated":"2024-09-13T15:00:45Z","published":"2021-08-10T13:25:29Z","title":"Natural Language Processing with Commonsense Knowledge: A Survey","summary":"  Commonsense knowledge is essential for advancing natural language processing\n(NLP) by enabling models to engage in human-like reasoning, which requires a\ndeeper understanding of context and often involves making inferences based on\nimplicit external knowledge. This paper explores the integration of commonsense\nknowledge into various NLP tasks. We begin by reviewing prominent commonsense\nknowledge bases and then discuss the benchmarks used to evaluate the\ncommonsense reasoning capabilities of NLP models, particularly language models.\nFurthermore, we highlight key methodologies for incorporating commonsense\nknowledge and their applications across different NLP tasks. The paper also\nexamines the challenges and emerging trends in enhancing NLP systems with\ncommonsense reasoning. All literature referenced in this survey can be accessed\nvia our GitHub repository: https://github.com/yuboxie/awesome-commonsense.\n","authors":["Yubo Xie","Zonghui Liu","Zongyang Ma","Fanyuan Meng","Yan Xiao","Fahui Miao","Pearl Pu"],"pdf_url":"https://arxiv.org/pdf/2108.04674v2.pdf","comment":"20 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2404.01903v2","updated":"2024-09-13T14:56:46Z","published":"2024-04-02T12:44:44Z","title":"Understanding How CodeLLMs (Mis)Predict Types with Activation Steering","summary":"  CodeLLMs are transforming software development as we know it. This is\nespecially true for tasks where rule-based approaches fall short, like type\nprediction. The type prediction task consists in adding a new type annotation\nto a partially typed program, such that the resulting program is closer to\nbeing fully typed. The intractability of rule-based approaches and high cost of\nmanual annotation make CodeLLMs an attractive solution to the problem. However,\nCodeLLMs are still far from being deployed on the large-scale due to doubts\nsurrounding their reliability.\n  To shed some light on how CodeLLMs approach type prediction, we investigate\nwhat happens when a model mispredicts a type. We show that by applying\nsemantics-preserving edits to code, CodeLLMs are eventually misled into\nmispredicting type annotations. However, by leveraging activation steering we\nare able to \"steer\" the model back to the correct prediction, making models\nmore robust against semantically irrelevant prompt features. We show that\nsteering achieves comparable performance to fine-tuning directly on the type\nprediction task. Furthermore, we find that steering vectors computed from\nPython code are effective at correcting TypeScript mispredictions, and vice\nversa. To our knowledge, this is the first evidence of its kind to suggest that\nCodeLLMs learn task representations that transfer across languages.\n","authors":["Francesca Lucchetti","Arjun Guha"],"pdf_url":"https://arxiv.org/pdf/2404.01903v2.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2409.08887v1","updated":"2024-09-13T14:54:37Z","published":"2024-09-13T14:54:37Z","title":"Visual Language Tracking with Multi-modal Interaction: A Robust\n  Benchmark","summary":"  Visual Language Tracking (VLT) enhances tracking by mitigating the\nlimitations of relying solely on the visual modality, utilizing high-level\nsemantic information through language. This integration of the language enables\nmore advanced human-machine interaction. The essence of interaction is\ncognitive alignment, which typically requires multiple information exchanges,\nespecially in the sequential decision-making process of VLT. However, current\nVLT benchmarks do not account for multi-round interactions during tracking.\nThey provide only an initial text and bounding box (bbox) in the first frame,\nwith no further interaction as tracking progresses, deviating from the original\nmotivation of the VLT task. To address these limitations, we propose a novel\nand robust benchmark, VLT-MI (Visual Language Tracking with Multi-modal\nInteraction), which introduces multi-round interaction into the VLT task for\nthe first time. (1) We generate diverse, multi-granularity texts for\nmulti-round, multi-modal interaction based on existing mainstream VLT\nbenchmarks using DTLLM-VLT, leveraging the world knowledge of LLMs. (2) We\npropose a new VLT interaction paradigm that achieves multi-round interaction\nthrough text updates and object recovery. When multiple tracking failures\noccur, we provide the tracker with more aligned texts and corrected bboxes\nthrough interaction, thereby expanding the scope of VLT downstream tasks. (3)\nWe conduct comparative experiments on both traditional VLT benchmarks and\nVLT-MI, evaluating and analyzing the accuracy and robustness of trackers under\nthe interactive paradigm. This work offers new insights and paradigms for the\nVLT task, enabling a fine-grained evaluation of multi-modal trackers. We\nbelieve this approach can be extended to additional datasets in the future,\nsupporting broader evaluations and comparisons of video-language model\ncapabilities.\n","authors":["Xuchen Li","Shiyu Hu","Xiaokun Feng","Dailing Zhang","Meiqi Wu","Jing Zhang","Kaiqi Huang"],"pdf_url":"https://arxiv.org/pdf/2409.08887v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2407.06650v2","updated":"2024-09-13T14:39:33Z","published":"2024-07-09T08:21:40Z","title":"An Automatic Quality Metric for Evaluating Simultaneous Interpretation","summary":"  Simultaneous interpretation (SI), the translation of one language to another\nin real time, starts translation before the original speech has finished. Its\nevaluation needs to consider both latency and quality. This trade-off is\nchallenging especially for distant word order language pairs such as English\nand Japanese. To handle this word order gap, interpreters maintain the word\norder of the source language as much as possible to keep up with original\nlanguage to minimize its latency while maintaining its quality, whereas in\ntranslation reordering happens to keep fluency in the target language. This\nmeans outputs synchronized with the source language are desirable based on the\nreal SI situation, and it's a key for further progress in computational SI and\nsimultaneous machine translation (SiMT). In this work, we propose an automatic\nevaluation metric for SI and SiMT focusing on word order synchronization. Our\nevaluation metric is based on rank correlation coefficients, leveraging\ncross-lingual pre-trained language models. Our experimental results on\nNAIST-SIC-Aligned and JNPC showed our metrics' effectiveness to measure word\norder synchronization between source and target language.\n","authors":["Mana Makinae","Katsuhito Sudoh","Mararu Yamada","Satoshi Nakamura"],"pdf_url":"https://arxiv.org/pdf/2407.06650v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08872v1","updated":"2024-09-13T14:35:47Z","published":"2024-09-13T14:35:47Z","title":"Exploring the Impact of Data Quantity on ASR in Extremely Low-resource\n  Languages","summary":"  This study investigates the efficacy of data augmentation techniques for\nlow-resource automatic speech recognition (ASR), focusing on two endangered\nAustronesian languages, Amis and Seediq. Recognizing the potential of\nself-supervised learning (SSL) in low-resource settings, we explore the impact\nof data volume on the continued pre-training of SSL models. We propose a novel\ndata-selection scheme leveraging a multilingual corpus to augment the limited\ntarget language data. This scheme utilizes a language classifier to extract\nutterance embeddings and employs one-class classifiers to identify utterances\nphonetically and phonologically proximate to the target languages. Utterances\nare ranked and selected based on their decision scores, ensuring the inclusion\nof highly relevant data in the SSL-ASR pipeline. Our experimental results\ndemonstrate the effectiveness of this approach, yielding substantial\nimprovements in ASR performance for both Amis and Seediq. These findings\nunderscore the feasibility and promise of data augmentation through\ncross-lingual transfer learning for low-resource language ASR.\n","authors":["Yao-Fei Cheng","Li-Wei Chen","Hung-Shin Lee","Hsin-Min Wang"],"pdf_url":"https://arxiv.org/pdf/2409.08872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.14880v2","updated":"2024-09-13T14:06:19Z","published":"2022-11-27T16:31:33Z","title":"Combining Data Generation and Active Learning for Low-Resource Question\n  Answering","summary":"  Neural approaches have become very popular in Question Answering (QA),\nhowever, they require a large amount of annotated data. In this work, we\npropose a novel approach that combines data augmentation via question-answer\ngeneration with Active Learning to improve performance in low-resource\nsettings, where the target domains are diverse in terms of difficulty and\nsimilarity to the source domain. We also investigate Active Learning for\nquestion answering in different stages, overall reducing the annotation effort\nof humans. For this purpose, we consider target domains in realistic settings,\nwith an extremely low amount of annotated samples but with many unlabeled\ndocuments, which we assume can be obtained with little effort. Additionally, we\nassume a sufficient amount of labeled data from the source domain being\navailable. We perform extensive experiments to find the best setup for\nincorporating domain experts. Our findings show that our novel approach, where\nhumans are incorporated in a data generation approach, boosts performance in\nthe low-resource, domain-specific setting, allowing for low-labeling-effort\nquestion answering systems in new, specialized domains. They further\ndemonstrate how human annotation affects the performance of QA depending on the\nstage it is performed.\n","authors":["Maximilian Kimmich","Andrea Bartezzaghi","Jasmina Bogojeska","Cristiano Malossi","Ngoc Thang Vu"],"pdf_url":"https://arxiv.org/pdf/2211.14880v2.pdf","comment":"ICANN 2024"},{"id":"http://arxiv.org/abs/2409.08846v1","updated":"2024-09-13T14:04:39Z","published":"2024-09-13T14:04:39Z","title":"FP-VEC: Fingerprinting Large Language Models via Efficient Vector\n  Addition","summary":"  Training Large Language Models (LLMs) requires immense computational power\nand vast amounts of data. As a result, protecting the intellectual property of\nthese models through fingerprinting is essential for ownership authentication.\nWhile adding fingerprints to LLMs through fine-tuning has been attempted, it\nremains costly and unscalable. In this paper, we introduce FP-VEC, a pilot\nstudy on using fingerprint vectors as an efficient fingerprinting method for\nLLMs. Our approach generates a fingerprint vector that represents a\nconfidential signature embedded in the model, allowing the same fingerprint to\nbe seamlessly incorporated into an unlimited number of LLMs via vector\naddition. Results on several LLMs show that FP-VEC is lightweight by running on\nCPU-only devices for fingerprinting, scalable with a single training and\nunlimited fingerprinting process, and preserves the model's normal behavior.\nThe project page is available at https://fingerprintvector.github.io .\n","authors":["Zhenhua Xu","Wenpeng Xing","Zhebo Wang","Chang Hu","Chen Jie","Meng Han"],"pdf_url":"https://arxiv.org/pdf/2409.08846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08845v1","updated":"2024-09-13T14:03:49Z","published":"2024-09-13T14:03:49Z","title":"AIPO: Improving Training Objective for Iterative Preference Optimization","summary":"  Preference Optimization (PO), is gaining popularity as an alternative choice\nof Proximal Policy Optimization (PPO) for aligning Large Language Models\n(LLMs). Recent research on aligning LLMs iteratively with synthetic or\npartially synthetic data shows promising results in scaling up PO training for\nboth academic settings and proprietary trained models such as Llama3. Despite\nits success, our study shows that the length exploitation issue present in PO\nis even more severe in Iterative Preference Optimization (IPO) due to the\niterative nature of the process. In this work, we study iterative preference\noptimization with synthetic data. We share the findings and analysis along the\nway of building the iterative preference optimization pipeline. More\nspecifically, we discuss the length exploitation issue during iterative\npreference optimization and propose our training objective for iterative\npreference optimization, namely Agreement-aware Iterative Preference\nOptimization (AIPO). To demonstrate the effectiveness of our method, we conduct\ncomprehensive experiments and achieve state-of-the-art performance on MT-Bench,\nAlpacaEval 2.0, and Arena-Hard. Our implementation and model checkpoints will\nbe made available at https://github.com/bytedance/AIPO.\n","authors":["Yaojie Shen","Xinyao Wang","Yulei Niu","Ying Zhou","Lexin Tang","Libo Zhang","Fan Chen","Longyin Wen"],"pdf_url":"https://arxiv.org/pdf/2409.08845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11850v4","updated":"2024-09-13T13:53:58Z","published":"2023-01-27T16:56:24Z","title":"Predicting Sentence-Level Factuality of News and Bias of Media Outlets","summary":"  Automated news credibility and fact-checking at scale require accurately\npredicting news factuality and media bias. This paper introduces a large\nsentence-level dataset, titled \"FactNews\", composed of 6,191 sentences expertly\nannotated according to factuality and media bias definitions proposed by\nAllSides. We use FactNews to assess the overall reliability of news sources, by\nformulating two text classification problems for predicting sentence-level\nfactuality of news reporting and bias of media outlets. Our experiments\ndemonstrate that biased sentences present a higher number of words compared to\nfactual sentences, besides having a predominance of emotions. Hence, the\nfine-grained analysis of subjectivity and impartiality of news articles\nprovided promising results for predicting the reliability of media outlets.\nFinally, due to the severity of fake news and political polarization in Brazil,\nand the lack of research for Portuguese, both dataset and baseline were\nproposed for Brazilian Portuguese.\n","authors":["Francielle Vargas","Kokil Jaidka","Thiago A. S. Pardo","Fabr√≠cio Benevenuto"],"pdf_url":"https://arxiv.org/pdf/2301.11850v4.pdf","comment":"Proceedings of the 14th International Conference on Recent Advances\n  in Natural Language Processing (RANLP 2023).\n  https://aclanthology.org/2023.ranlp-1.127"},{"id":"http://arxiv.org/abs/2409.08813v1","updated":"2024-09-13T13:24:52Z","published":"2024-09-13T13:24:52Z","title":"Your Weak LLM is Secretly a Strong Teacher for Alignment","summary":"  The burgeoning capabilities of large language models (LLMs) have underscored\nthe need for alignment to ensure these models act in accordance with human\nvalues and intentions. Existing alignment frameworks present constraints either\nin the form of expensive human effort or high computational costs. This paper\nexplores a promising middle ground, where we employ a weak LLM that is\nsignificantly less resource-intensive than top-tier models, yet offers more\nautomation than purely human feedback. We present a systematic study to\nevaluate and understand weak LLM's ability to generate feedback for alignment.\nOur empirical findings demonstrate that weak LLMs can provide feedback that\nrivals or even exceeds that of fully human-annotated data. Our study indicates\na minimized impact of model size on feedback efficacy, shedding light on a\nscalable and sustainable alignment strategy. To deepen our understanding of\nalignment under weak LLM feedback, we conduct a series of qualitative and\nquantitative analyses, offering novel insights into the quality discrepancies\nbetween human feedback vs. weak LLM feedback.\n","authors":["Leitian Tao","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2409.08813v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2409.08805v1","updated":"2024-09-13T13:13:39Z","published":"2024-09-13T13:13:39Z","title":"Exploring SSL Discrete Tokens for Multilingual ASR","summary":"  With the advancement of Self-supervised Learning (SSL) in speech-related\ntasks, there has been growing interest in utilizing discrete tokens generated\nby SSL for automatic speech recognition (ASR), as they offer faster processing\ntechniques. However, previous studies primarily focused on multilingual ASR\nwith Fbank features or English ASR with discrete tokens, leaving a gap in\nadapting discrete tokens for multilingual ASR scenarios. This study presents a\ncomprehensive comparison of discrete tokens generated by various leading SSL\nmodels across multiple language domains. We aim to explore the performance and\nefficiency of speech discrete tokens across multiple language domains for both\nmonolingual and multilingual ASR scenarios. Experimental results demonstrate\nthat discrete tokens achieve comparable results against systems trained on\nFbank features in ASR tasks across seven language domains with an average word\nerror rate (WER) reduction of 0.31% and 1.76% absolute (2.80% and 15.70%\nrelative) on dev and test sets respectively, with particularly WER reduction of\n6.82% absolute (41.48% relative) on the Polish test set.\n","authors":["Mingyu Cui","Daxin Tan","Yifan Yang","Dingdong Wang","Huimeng Wang","Xiao Chen","Xie Chen","Xunying Liu"],"pdf_url":"https://arxiv.org/pdf/2409.08805v1.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2409.08797v1","updated":"2024-09-13T13:01:09Z","published":"2024-09-13T13:01:09Z","title":"Exploring SSL Discrete Speech Features for Zipformer-based Contextual\n  ASR","summary":"  Self-supervised learning (SSL) based discrete speech representations are\nhighly compact and domain adaptable. In this paper, SSL discrete speech\nfeatures extracted from WavLM models are used as additional cross-utterance\nacoustic context features in Zipformer-Transducer ASR systems. The efficacy of\nreplacing Fbank features with discrete token features for modelling either\ncross-utterance contexts (from preceding and future segments), or current\nutterance's internal contexts alone, or both at the same time, are demonstrated\nthoroughly on the Gigaspeech 1000-hr corpus. The best Zipformer-Transducer\nsystem using discrete tokens based cross-utterance context features outperforms\nthe baseline using utterance internal context only with statistically\nsignificant word error rate (WER) reductions of 0.32% to 0.41% absolute (2.78%\nto 3.54% relative) on the dev and test data. The lowest published WER of 11.15%\nand 11.14% were obtained on the dev and test sets. Our work is open-source and\npublicly available at\nhttps://github.com/open-creator/icefall/tree/master/egs/gigaspeech/Context\\_ASR.\n","authors":["Mingyu Cui","Yifan Yang","Jiajun Deng","Jiawen Kang","Shujie Hu","Tianzi Wang","Zhaoqing Li","Shiliang Zhang","Xie Chen","Xunying Liu"],"pdf_url":"https://arxiv.org/pdf/2409.08797v1.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2409.08792v1","updated":"2024-09-13T12:55:45Z","published":"2024-09-13T12:55:45Z","title":"Optimizing Ingredient Substitution Using Large Language Models to\n  Enhance Phytochemical Content in Recipes","summary":"  In the emerging field of computational gastronomy, aligning culinary\npractices with scientifically supported nutritional goals is increasingly\nimportant. This study explores how large language models (LLMs) can be applied\nto optimize ingredient substitutions in recipes, specifically to enhance the\nphytochemical content of meals. Phytochemicals are bioactive compounds found in\nplants, which, based on preclinical studies, may offer potential health\nbenefits. We fine-tuned models, including OpenAI's GPT-3.5, DaVinci, and Meta's\nTinyLlama, using an ingredient substitution dataset. These models were used to\npredict substitutions that enhance phytochemical content and create a\ncorresponding enriched recipe dataset. Our approach improved Hit@1 accuracy on\ningredient substitution tasks, from the baseline 34.53 plus-minus 0.10% to\n38.03 plus-minus 0.28% on the original GISMo dataset, and from 40.24 plus-minus\n0.36% to 54.46 plus-minus 0.29% on a refined version of the same dataset. These\nsubstitutions led to the creation of 1,951 phytochemically enriched ingredient\npairings and 1,639 unique recipes. While this approach demonstrates potential\nin optimizing ingredient substitutions, caution must be taken when drawing\nconclusions about health benefits, as the claims are based on preclinical\nevidence. Future work should include clinical validation and broader datasets\nto further evaluate the nutritional impact of these substitutions. This\nresearch represents a step forward in using AI to promote healthier eating\npractices, providing potential pathways for integrating computational methods\nwith nutritional science.\n","authors":["Luis Rita","Josh Southern","Ivan Laponogov","Kyle Higgins","Kirill Veselkov"],"pdf_url":"https://arxiv.org/pdf/2409.08792v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2409.08780v1","updated":"2024-09-13T12:36:52Z","published":"2024-09-13T12:36:52Z","title":"Sign Language Sense Disambiguation","summary":"  This project explores methods to enhance sign language translation of German\nsign language, specifically focusing on disambiguation of homonyms. Sign\nlanguage is ambiguous and understudied which is the basis for our experiments.\nWe approach the improvement by training transformer-based models on various\nbodypart representations to shift the focus on said bodypart. To determine the\nimpact of, e.g., the hand or mouth representations, we experiment with\ndifferent combinations. The results show that focusing on the mouth increases\nthe performance in small dataset settings while shifting the focus on the hands\nretrieves better results in larger dataset settings. Our results contribute to\nbetter accessibility for non-hearing persons by improving the systems powering\ndigital assistants, enabling a more accurate interaction. The code for this\nproject can be found on GitHub.\n","authors":["Jana Grimm","Miriam Winkler","Oliver Kraus","Tanalp Agustoslu"],"pdf_url":"https://arxiv.org/pdf/2409.08780v1.pdf","comment":"LIMO2024 @ KONVENS 2024, 8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.09895v4","updated":"2024-09-13T12:28:45Z","published":"2024-08-19T11:09:12Z","title":"Performance Law of Large Language Models","summary":"  Guided by the belief of the scaling law, large language models (LLMs) have\nachieved impressive performance in recent years. However, scaling law only\ngives a qualitative estimation of loss, which is influenced by various factors\nsuch as model architectures, data distributions, tokenizers, and computation\nprecision. Thus, estimating the real performance of LLMs with different\ntraining settings rather than loss may be quite useful in practical\ndevelopment. In this article, we present an empirical equation named\n\"Performance Law\" to directly predict the MMLU score of an LLM, which is a\nwidely used metric to indicate the general capability of LLMs in real-world\nconversations and applications. Based on only a few key hyperparameters of the\nLLM architecture and the size of training data, we obtain a quite accurate MMLU\nprediction of various LLMs with diverse sizes and architectures developed by\ndifferent organizations in different years. Performance law can be used to\nguide the choice of LLM architecture and the effective allocation of\ncomputational resources without extensive experiments.\n","authors":["Chuhan Wu","Ruiming Tang"],"pdf_url":"https://arxiv.org/pdf/2408.09895v4.pdf","comment":"Personal opinions of the authors"},{"id":"http://arxiv.org/abs/2409.08761v1","updated":"2024-09-13T12:09:20Z","published":"2024-09-13T12:09:20Z","title":"Journalists, Emotions, and the Introduction of Generative AI Chatbots: A\n  Large-Scale Analysis of Tweets Before and After the Launch of ChatGPT","summary":"  As part of a broader look at the impact of generative AI, this study\ninvestigated the emotional responses of journalists to the release of ChatGPT\nat the time of its launch. By analyzing nearly 1 million Tweets from\njournalists at major U.S. news outlets, we tracked changes in emotional tone\nand sentiment before and after the introduction of ChatGPT in November 2022.\nUsing various computational and natural language processing techniques to\nmeasure emotional shifts in response to ChatGPT's release, we found an increase\nin positive emotion and a more favorable tone post-launch, suggesting initial\noptimism toward AI's potential. This research underscores the pivotal role of\njournalists as interpreters of technological innovation and disruption,\nhighlighting how their emotional reactions may shape public narratives around\nemerging technologies. The study contributes to understanding the intersection\nof journalism, emotion, and AI, offering insights into the broader societal\nimpact of generative AI tools.\n","authors":["Seth C. Lewis","David M. Markowitz","Jon Benedik Bunquin"],"pdf_url":"https://arxiv.org/pdf/2409.08761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08253v2","updated":"2024-09-13T11:22:28Z","published":"2024-09-12T17:50:05Z","title":"The Design of Informative Take-Over Requests for Semi-Autonomous\n  Cyber-Physical Systems: Combining Spoken Language and Visual Icons in a\n  Drone-Controller Setting","summary":"  The question of how cyber-physical systems should interact with human\npartners that can take over control or exert oversight is becoming more\npressing, as these systems are deployed for an ever larger range of tasks.\nDrawing on the literatures on handing over control during semi-autonomous\ndriving and human-robot interaction, we propose a design of a take-over request\nthat combines an abstract pre-alert with an informative TOR: Relevant sensor\ninformation is highlighted on the controller's display, while a spoken message\nverbalizes the reason for the TOR. We conduct our study in the context of a\nsemi-autonomous drone control scenario as our testbed. The goal of our online\nstudy is to assess in more detail what form a language-based TOR should take.\nSpecifically, we compare a full sentence condition to shorter fragments, and\ntest whether the visual highlighting should be done synchronously or\nasynchronously with the speech. Participants showed a higher accuracy in\nchoosing the correct solution with our bi-modal TOR and felt that they were\nbetter able to recognize the critical situation. Using only fragments in the\nspoken message rather than full sentences did not lead to improved accuracy or\nfaster reactions. Also, synchronizing the visual highlighting with the spoken\nmessage did not result in better accuracy and response times were even\nincreased in this condition.\n","authors":["Ashwini Gundappa","Emilia Ellsiepen","Lukas Schmitz","Frederik Wiehr","Vera Demberg"],"pdf_url":"https://arxiv.org/pdf/2409.08253v2.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.08719v1","updated":"2024-09-13T11:10:16Z","published":"2024-09-13T11:10:16Z","title":"Distilling Monolingual and Crosslingual Word-in-Context Representations","summary":"  In this study, we propose a method that distils representations of word\nmeaning in context from a pre-trained masked language model in both monolingual\nand crosslingual settings. Word representations are the basis for context-aware\nlexical semantics and unsupervised semantic textual similarity (STS)\nestimation. Different from existing approaches, our method does not require\nhuman-annotated corpora nor updates of the parameters of the pre-trained model.\nThe latter feature is appealing for practical scenarios where the off-the-shelf\npre-trained model is a common asset among different applications. Specifically,\nour method learns to combine the outputs of different hidden layers of the\npre-trained model using self-attention. Our auto-encoder based training only\nrequires an automatically generated corpus. To evaluate the performance of the\nproposed approach, we performed extensive experiments using various benchmark\ntasks. The results on the monolingual tasks confirmed that our representations\nexhibited a competitive performance compared to that of the previous study for\nthe context-aware lexical semantic tasks and outperformed it for STS\nestimation. The results of the crosslingual tasks revealed that the proposed\nmethod largely improved crosslingual word representations of multilingual\npre-trained models.\n","authors":["Yuki Arase","Tomoyuki Kajiwara"],"pdf_url":"https://arxiv.org/pdf/2409.08719v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08712v1","updated":"2024-09-13T10:59:24Z","published":"2024-09-13T10:59:24Z","title":"Layerwise Change of Knowledge in Neural Networks","summary":"  This paper aims to explain how a deep neural network (DNN) gradually extracts\nnew knowledge and forgets noisy features through layers in forward propagation.\nUp to now, although the definition of knowledge encoded by the DNN has not\nreached a consensus, Previous studies have derived a series of mathematical\nevidence to take interactions as symbolic primitive inference patterns encoded\nby a DNN. We extend the definition of interactions and, for the first time,\nextract interactions encoded by intermediate layers. We quantify and track the\nnewly emerged interactions and the forgotten interactions in each layer during\nthe forward propagation, which shed new light on the learning behavior of DNNs.\nThe layer-wise change of interactions also reveals the change of the\ngeneralization capacity and instability of feature representations of a DNN.\n","authors":["Xu Cheng","Lei Cheng","Zhaoran Peng","Yang Xu","Tian Han","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.08712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08706v1","updated":"2024-09-13T10:48:35Z","published":"2024-09-13T10:48:35Z","title":"L3Cube-IndicQuest: A Benchmark Questing Answering Dataset for Evaluating\n  Knowledge of LLMs in Indic Context","summary":"  Large Language Models (LLMs) have made significant progress in incorporating\nIndic languages within multilingual models. However, it is crucial to\nquantitatively assess whether these languages perform comparably to globally\ndominant ones, such as English. Currently, there is a lack of benchmark\ndatasets specifically designed to evaluate the regional knowledge of LLMs in\nvarious Indic languages. In this paper, we present the L3Cube-IndicQuest, a\ngold-standard question-answering benchmark dataset designed to evaluate how\nwell multilingual LLMs capture regional knowledge across various Indic\nlanguages. The dataset contains 200 question-answer pairs, each for English and\n19 Indic languages, covering five domains specific to the Indic region. We aim\nfor this dataset to serve as a benchmark, providing ground truth for evaluating\nthe performance of LLMs in understanding and representing knowledge relevant to\nthe Indian context. The IndicQuest can be used for both reference-based\nevaluation and LLM-as-a-judge evaluation. The dataset is shared publicly at\nhttps://github.com/l3cube-pune/indic-nlp .\n","authors":["Pritika Rohera","Chaitrali Ginimav","Akanksha Salunke","Gayatri Sawant","Raviraj Joshi"],"pdf_url":"https://arxiv.org/pdf/2409.08706v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.09010v1","updated":"2024-09-13T17:38:47Z","published":"2024-09-13T17:38:47Z","title":"Contri(e)ve: Context + Retrieve for Scholarly Question Answering","summary":"  Scholarly communication is a rapid growing field containing a wealth of\nknowledge. However, due to its unstructured and document format, it is\nchallenging to extract useful information from them through conventional\ndocument retrieval methods. Scholarly knowledge graphs solve this problem, by\nrepresenting the documents in a semantic network, providing, hidden insights,\nsummaries and ease of accessibility through queries. Naturally, question\nanswering for scholarly graphs expands the accessibility to a wider audience.\nBut some of the knowledge in this domain is still presented as unstructured\ntext, thus requiring a hybrid solution for question answering systems. In this\npaper, we present a two step solution using open source Large Language\nModel(LLM): Llama3.1 for Scholarly-QALD dataset. Firstly, we extract the\ncontext pertaining to the question from different structured and unstructured\ndata sources: DBLP, SemOpenAlex knowledge graphs and Wikipedia text. Secondly,\nwe implement prompt engineering to improve the information retrieval\nperformance of the LLM. Our approach achieved an F1 score of 40% and also\nobserved some anomalous responses from the LLM, that are discussed in the final\npart of the paper.\n","authors":["Kanchan Shivashankar","Nadine Steinmetz"],"pdf_url":"https://arxiv.org/pdf/2409.09010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08987v1","updated":"2024-09-13T17:03:56Z","published":"2024-09-13T17:03:56Z","title":"Comparative Analysis of Pretrained Audio Representations in Music\n  Recommender Systems","summary":"  Over the years, Music Information Retrieval (MIR) has proposed various models\npretrained on large amounts of music data. Transfer learning showcases the\nproven effectiveness of pretrained backend models with a broad spectrum of\ndownstream tasks, including auto-tagging and genre classification. However, MIR\npapers generally do not explore the efficiency of pretrained models for Music\nRecommender Systems (MRS). In addition, the Recommender Systems community tends\nto favour traditional end-to-end neural network learning over these models. Our\nresearch addresses this gap and evaluates the applicability of six pretrained\nbackend models (MusicFM, Music2Vec, MERT, EncodecMAE, Jukebox, and MusiCNN) in\nthe context of MRS. We assess their performance using three recommendation\nmodels: K-nearest neighbours (KNN), shallow neural network, and BERT4Rec. Our\nfindings suggest that pretrained audio representations exhibit significant\nperformance variability between traditional MIR tasks and MRS, indicating that\nvaluable aspects of musical information captured by backend models may differ\ndepending on the task. This study establishes a foundation for further\nexploration of pretrained audio representations to enhance music recommendation\nsystems.\n","authors":["Yan-Martin Tamm","Anna Aljanaki"],"pdf_url":"https://arxiv.org/pdf/2409.08987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08975v1","updated":"2024-09-13T16:48:39Z","published":"2024-09-13T16:48:39Z","title":"Accurate and Fast Estimation of Temporal Motifs using Path Sampling","summary":"  Counting the number of small subgraphs, called motifs, is a fundamental\nproblem in social network analysis and graph mining. Many real-world networks\nare directed and temporal, where edges have timestamps. Motif counting in\ndirected, temporal graphs is especially challenging because there are a\nplethora of different kinds of patterns. Temporal motif counts reveal much\nricher information and there is a need for scalable algorithms for motif\ncounting.\n  A major challenge in counting is that there can be trillions of temporal\nmotif matches even with a graph with only millions of vertices. Both the motifs\nand the input graphs can have multiple edges between two vertices, leading to a\ncombinatorial explosion problem. Counting temporal motifs involving just four\nvertices is not feasible with current state-of-the-art algorithms.\n  We design an algorithm, TEACUPS, that addresses this problem using a novel\ntechnique of temporal path sampling. We combine a path sampling method with\ncarefully designed temporal data structures, to propose an efficient\napproximate algorithm for temporal motif counting. TEACUPS is an unbiased\nestimator with provable concentration behavior, which can be used to bound the\nestimation error. For a Bitcoin graph with hundreds of millions of edges,\nTEACUPS runs in less than 1 minute, while the exact counting algorithm takes\nmore than a day. We empirically demonstrate the accuracy of TEACUPS on large\ndatasets, showing an average of 30$\\times$ speedup (up to 2000$\\times$ speedup)\ncompared to existing GPU-based exact counting methods while preserving high\ncount estimation accuracy.\n","authors":["Yunjie Pan","Omkar Bhalerao","C. Seshadhri","Nishil Talati"],"pdf_url":"https://arxiv.org/pdf/2409.08975v1.pdf","comment":"Accepted for ICDM'24"},{"id":"http://arxiv.org/abs/2409.08934v1","updated":"2024-09-13T15:53:40Z","published":"2024-09-13T15:53:40Z","title":"Proactive Recommendation in Social Networks: Steering User Interest via\n  Neighbor Influence","summary":"  Recommending items solely catering to users' historical interests narrows\nusers' horizons. Recent works have considered steering target users beyond\ntheir historical interests by directly adjusting items exposed to them.\nHowever, the recommended items for direct steering might not align perfectly\nwith users' interests evolution, detrimentally affecting target users'\nexperience. To avoid this issue, we propose a new task named Proactive\nRecommendation in Social Networks (PRSN) that indirectly steers users' interest\nby utilizing the influence of social neighbors, i.e., indirect steering by\nadjusting the exposure of a target item to target users' neighbors. The key to\nPRSN lies in answering an interventional question: what would a target user's\nfeedback be on a target item if the item is exposed to the user's different\nneighbors? To answer this question, we resort to causal inference and formalize\nPRSN as: (1) estimating the potential feedback of a user on an item, under the\nnetwork interference by the item's exposure to the user's neighbors; and (2)\nadjusting the exposure of a target item to target users' neighbors to trade off\nsteering performance and the damage to the neighbors' experience. To this end,\nwe propose a Neighbor Interference Recommendation (NIRec) framework with two\nkey modules: (1)an interference representation-based estimation module for\nmodeling potential feedback; and (2) a post-learning-based optimization module\nfor optimizing a target item's exposure to trade off steering performance and\nthe neighbors' experience by greedy search. We conduct extensive\nsemi-simulation experiments based on three real-world datasets, validating the\nsteering effectiveness of NIRec.\n","authors":["Hang Pan","Shuxian Bi","Wenjie Wang","Haoxuan Li","Peng Wu","Fuli Feng","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2409.08934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08931v1","updated":"2024-09-13T15:47:50Z","published":"2024-09-13T15:47:50Z","title":"LLM-based Weak Supervision Framework for Query Intent Classification in\n  Video Search","summary":"  Streaming services have reshaped how we discover and engage with digital\nentertainment. Despite these advancements, effectively understanding the wide\nspectrum of user search queries continues to pose a significant challenge. An\naccurate query understanding system that can handle a variety of entities that\nrepresent different user intents is essential for delivering an enhanced user\nexperience. We can build such a system by training a natural language\nunderstanding (NLU) model; however, obtaining high-quality labeled training\ndata in this specialized domain is a substantial obstacle. Manual annotation is\ncostly and impractical for capturing users' vast vocabulary variations. To\naddress this, we introduce a novel approach that leverages large language\nmodels (LLMs) through weak supervision to automatically annotate a vast\ncollection of user search queries. Using prompt engineering and a diverse set\nof LLM personas, we generate training data that matches human annotator\nexpectations. By incorporating domain knowledge via Chain of Thought and\nIn-Context Learning, our approach leverages the labeled data to train\nlow-latency models optimized for real-time inference. Extensive evaluations\ndemonstrated that our approach outperformed the baseline with an average\nrelative gain of 113% in recall. Furthermore, our novel prompt engineering\nframework yields higher quality LLM-generated data to be used for weak\nsupervision; we observed 47.60% improvement over baseline in agreement rate\nbetween LLM predictions and human annotations with respect to F1 score,\nweighted according to the distribution of occurrences of the search queries.\nOur persona selection routing mechanism further adds an additional 3.67%\nincrease in weighted F1 score on top of our novel prompt engineering framework.\n","authors":["Farnoosh Javadi","Phanideep Gampa","Alyssa Woo","Xingxing Geng","Hang Zhang","Jose Sepulveda","Belhassen Bayar","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2409.08931v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.05462v2","updated":"2024-09-13T13:36:40Z","published":"2024-09-09T09:42:46Z","title":"Federated Transfer Learning Based Cooperative Wideband Spectrum Sensing\n  with Model Pruning","summary":"  For ultra-wideband and high-rate wireless communication systems, wideband\nspectrum sensing (WSS) is critical, since it empowers secondary users (SUs) to\ncapture the spectrum holes for opportunistic transmission. However, WSS\nencounters challenges such as excessive costs of hardware and computation due\nto the high sampling rate, as well as robustness issues arising from scenario\nmismatch. In this paper, a WSS neural network (WSSNet) is proposed by\nexploiting multicoset preprocessing to enable the sub-Nyquist sampling, with\nthe two dimensional convolution design specifically tailored to work with the\npreprocessed samples. A federated transfer learning (FTL) based framework\nmobilizing multiple SUs is further developed to achieve a robust model\nadaptable to various scenarios, which is paved by the selective weight pruning\nfor the fast model adaptation and inference. Simulation results demonstrate\nthat the proposed FTL-WSSNet achieves the fairly good performance in different\ntarget scenarios even without local adaptation samples.\n","authors":["Jibin Jia","Peihao Dong","Fuhui Zhou","Qihui Wu"],"pdf_url":"https://arxiv.org/pdf/2409.05462v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07272v2","updated":"2024-09-13T12:03:06Z","published":"2024-09-11T13:46:52Z","title":"RePlay: a Recommendation Framework for Experimentation and Production\n  Use","summary":"  Using a single tool to build and compare recommender systems significantly\nreduces the time to market for new models. In addition, the comparison results\nwhen using such tools look more consistent. This is why many different tools\nand libraries for researchers in the field of recommendations have recently\nappeared. Unfortunately, most of these frameworks are aimed primarily at\nresearchers and require modification for use in production due to the inability\nto work on large datasets or an inappropriate architecture. In this demo, we\npresent our open-source toolkit RePlay - a framework containing an end-to-end\npipeline for building recommender systems, which is ready for production use.\nRePlay also allows you to use a suitable stack for the pipeline on each stage:\nPandas, Polars, or Spark. This allows the library to scale computations and\ndeploy to a cluster. Thus, RePlay allows data scientists to easily move from\nresearch mode to production mode using the same interfaces.\n","authors":["Alexey Vasilev","Anna Volodkevich","Denis Kulandin","Tatiana Bysheva","Anton Klenitskiy"],"pdf_url":"https://arxiv.org/pdf/2409.07272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08703v1","updated":"2024-09-13T10:43:18Z","published":"2024-09-13T10:43:18Z","title":"NeSHFS: Neighborhood Search with Heuristic-based Feature Selection for\n  Click-Through Rate Prediction","summary":"  Click-through-rate (CTR) prediction plays an important role in online\nadvertising and ad recommender systems. In the past decade, maximizing CTR has\nbeen the main focus of model development and solution creation. Therefore,\nresearchers and practitioners have proposed various models and solutions to\nenhance the effectiveness of CTR prediction. Most of the existing literature\nfocuses on capturing either implicit or explicit feature interactions. Although\nimplicit interactions are successfully captured in some studies, explicit\ninteractions present a challenge for achieving high CTR by extracting both\nlow-order and high-order feature interactions. Unnecessary and irrelevant\nfeatures may cause high computational time and low prediction performance.\nFurthermore, certain features may perform well with specific predictive models\nwhile underperforming with others. Also, feature distribution may fluctuate due\nto traffic variations. Most importantly, in live production environments,\nresources are limited, and the time for inference is just as crucial as\ntraining time. Because of all these reasons, feature selection is one of the\nmost important factors in enhancing CTR prediction model performance. Simple\nfilter-based feature selection algorithms do not perform well and they are not\nsufficient. An effective and efficient feature selection algorithm is needed to\nconsistently filter the most useful features during live CTR prediction\nprocess. In this paper, we propose a heuristic algorithm named Neighborhood\nSearch with Heuristic-based Feature Selection (NeSHFS) to enhance CTR\nprediction performance while reducing dimensionality and training time costs.\nWe conduct comprehensive experiments on three public datasets to validate the\nefficiency and effectiveness of our proposed solution.\n","authors":["Dogukan Aksu","Ismail Hakki Toroslu","Hasan Davulcu"],"pdf_url":"https://arxiv.org/pdf/2409.08703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08543v1","updated":"2024-09-13T05:33:09Z","published":"2024-09-13T05:33:09Z","title":"ATFLRec: A Multimodal Recommender System with Audio-Text Fusion and\n  Low-Rank Adaptation via Instruction-Tuned Large Language Model","summary":"  Recommender Systems (RS) play a pivotal role in boosting user satisfaction by\nproviding personalized product suggestions in domains such as e-commerce and\nentertainment. This study examines the integration of multimodal data text and\naudio into large language models (LLMs) with the aim of enhancing\nrecommendation performance. Traditional text and audio recommenders encounter\nlimitations such as the cold-start problem, and recent advancements in LLMs,\nwhile promising, are computationally expensive. To address these issues,\nLow-Rank Adaptation (LoRA) is introduced, which enhances efficiency without\ncompromising performance. The ATFLRec framework is proposed to integrate audio\nand text modalities into a multimodal recommendation system, utilizing various\nLoRA configurations and modality fusion techniques. Results indicate that\nATFLRec outperforms baseline models, including traditional and graph neural\nnetwork-based approaches, achieving higher AUC scores. Furthermore, separate\nfine-tuning of audio and text data with distinct LoRA modules yields optimal\nperformance, with different pooling methods and Mel filter bank numbers\nsignificantly impacting performance. This research offers valuable insights\ninto optimizing multimodal recommender systems and advancing the integration of\ndiverse data modalities in LLMs.\n","authors":["Zezheng Qin"],"pdf_url":"https://arxiv.org/pdf/2409.08543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07276v2","updated":"2024-09-13T04:16:55Z","published":"2024-09-11T13:49:48Z","title":"STORE: Streamlining Semantic Tokenization and Generative Recommendation\n  with A Single LLM","summary":"  Traditional recommendation models often rely on unique item identifiers (IDs)\nto distinguish between items, which can hinder their ability to effectively\nleverage item content information and generalize to long-tail or cold-start\nitems. Recently, semantic tokenization has been proposed as a promising\nsolution that aims to tokenize each item's semantic representation into a\nsequence of discrete tokens. In this way, it preserves the item's semantics\nwithin these tokens and ensures that semantically similar items are represented\nby similar tokens. These semantic tokens have become fundamental in training\ngenerative recommendation models. However, existing generative recommendation\nmethods typically involve multiple sub-models for embedding, quantization, and\nrecommendation, leading to an overly complex system. In this paper, we propose\nto streamline the semantic tokenization and generative recommendation process\nwith a unified framework, dubbed STORE, which leverages a single large language\nmodel (LLM) for both tasks. Specifically, we formulate semantic tokenization as\na text-to-token task and generative recommendation as a token-to-token task,\nsupplemented by a token-to-text reconstruction task and a text-to-token\nauxiliary task. All these tasks are framed in a generative manner and trained\nusing a single LLM backbone. Extensive experiments have been conducted to\nvalidate the effectiveness of our STORE framework across various recommendation\ntasks and datasets. We will release the source code and configurations for\nreproducible research.\n","authors":["Qijiong Liu","Jieming Zhu","Lu Fan","Zhou Zhao","Xiao-Ming Wu"],"pdf_url":"https://arxiv.org/pdf/2409.07276v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08479v1","updated":"2024-09-13T02:08:47Z","published":"2024-09-13T02:08:47Z","title":"Exploring Information Retrieval Landscapes: An Investigation of a Novel\n  Evaluation Techniques and Comparative Document Splitting Methods","summary":"  The performance of Retrieval-Augmented Generation (RAG) systems in\ninformation retrieval is significantly influenced by the characteristics of the\ndocuments being processed. In this study, the structured nature of textbooks,\nthe conciseness of articles, and the narrative complexity of novels are shown\nto require distinct retrieval strategies. A comparative evaluation of multiple\ndocument-splitting methods reveals that the Recursive Character Splitter\noutperforms the Token-based Splitter in preserving contextual integrity. A\nnovel evaluation technique is introduced, utilizing an open-source model to\ngenerate a comprehensive dataset of question-and-answer pairs, simulating\nrealistic retrieval scenarios to enhance testing efficiency and metric\nreliability. The evaluation employs weighted scoring metrics, including\nSequenceMatcher, BLEU, METEOR, and BERT Score, to assess the system's accuracy\nand relevance. This approach establishes a refined standard for evaluating the\nprecision of RAG systems, with future research focusing on optimizing chunk and\noverlap sizes to improve retrieval accuracy and efficiency.\n","authors":["Esmaeil Narimissa","David Raithel"],"pdf_url":"https://arxiv.org/pdf/2409.08479v1.pdf","comment":"This article is 16 pages long and includes detailed comparisons of\n  RAG systems and document splitting techniques"}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.09221v1","updated":"2024-09-13T22:18:45Z","published":"2024-09-13T22:18:45Z","title":"Multi-modal Speech Transformer Decoders: When Do Multiple Modalities\n  Improve Accuracy?","summary":"  Decoder-only discrete-token language models have recently achieved\nsignificant success in automatic speech recognition. However, systematic\nanalyses of how different modalities impact performance in specific scenarios\nremain limited. In this paper, we investigate the effects of multiple\nmodalities on recognition accuracy on both synthetic and real-world datasets.\nOur experiments suggest that: (1) Integrating more modalities can increase\naccuracy; in particular, our paper is, to our best knowledge, the first to show\nthe benefit of combining audio, image context, and lip information; (2) Images\nas a supplementary modality for speech recognition provide the greatest benefit\nat moderate noise levels, moreover, they exhibit a different trend compared to\ninherently synchronized modalities like lip movements; (3) Performance improves\non both synthetic and real-world datasets when the most relevant visual\ninformation is filtered as a preprocessing step.\n","authors":["Yiwen Guan","Viet Anh Trinh","Vivek Voleti","Jacob Whitehill"],"pdf_url":"https://arxiv.org/pdf/2409.09221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08772v1","updated":"2024-09-13T12:30:15Z","published":"2024-09-13T12:30:15Z","title":"On the Computation of BD-Rate over a Set of Videos for Fair Assessment\n  of Performance of Learned Video Codecs","summary":"  The Bj{\\o}ntegaard Delta (BD) measure is widely employed to evaluate and\nquantify the variations in the rate-distortion(RD) performance across different\ncodecs. Many researchers report the average BD value over multiple videos\nwithin a dataset for different codecs. We claim that the current practice in\nthe learned video compression community of computing the average BD value over\na dataset based on the average RD curve of multiple videos can lead to\nmisleading conclusions. We show both by analysis of a simplistic case of linear\nRD curves and experimental results with two recent learned video codecs that\naveraging RD curves can lead to a single video to disproportionately influence\nthe average BD value especially when the operating bitrate range of different\ncodecs do not exactly match. Instead, we advocate for calculating the BD\nmeasure per-video basis, as commonly done by the traditional video compression\ncommunity, followed by averaging the individual BD values over videos, to\nprovide a fair comparison of learned video codecs. Our experimental results\ndemonstrate that the comparison of two recent learned video codecs is affected\nby how we evaluate the average BD measure.\n","authors":["M. Akin Yilmaz","Onur Kele≈ü","A. Murat Tekalp"],"pdf_url":"https://arxiv.org/pdf/2409.08772v1.pdf","comment":"Submitted to IEEE ICASSP 2025"},{"id":"http://arxiv.org/abs/2309.08751v2","updated":"2024-09-13T12:06:13Z","published":"2023-09-15T20:27:47Z","title":"Diverse Neural Audio Embeddings -- Bringing Features back !","summary":"  With the advent of modern AI architectures, a shift has happened towards\nend-to-end architectures. This pivot has led to neural architectures being\ntrained without domain-specific biases/knowledge, optimized according to the\ntask. We in this paper, learn audio embeddings via diverse feature\nrepresentations, in this case, domain-specific. For the case of audio\nclassification over hundreds of categories of sound, we learn robust separate\nembeddings for diverse audio properties such as pitch, timbre, and neural\nrepresentation, along with also learning it via an end-to-end architecture. We\nobserve handcrafted embeddings, e.g., pitch and timbre-based, although on their\nown, are not able to beat a fully end-to-end representation, yet adding these\ntogether with end-to-end embedding helps us, significantly improve performance.\nThis work would pave the way to bring some domain expertise with end-to-end\nmodels to learn robust, diverse representations, surpassing the performance of\njust training end-to-end models.\n","authors":["Prateek Verma"],"pdf_url":"https://arxiv.org/pdf/2309.08751v2.pdf","comment":"6 pages, 1 figure, 2 table, Under Review for 50th IEEE ICASSP 2025,\n  Hyderabad, India"},{"id":"http://arxiv.org/abs/2409.08628v1","updated":"2024-09-13T08:33:03Z","published":"2024-09-13T08:33:03Z","title":"Rhythmic Foley: A Framework For Seamless Audio-Visual Alignment In\n  Video-to-Audio Synthesis","summary":"  Our research introduces an innovative framework for video-to-audio synthesis,\nwhich solves the problems of audio-video desynchronization and semantic loss in\nthe audio. By incorporating a semantic alignment adapter and a temporal\nsynchronization adapter, our method significantly improves semantic integrity\nand the precision of beat point synchronization, particularly in fast-paced\naction sequences. Utilizing a contrastive audio-visual pre-trained encoder, our\nmodel is trained with video and high-quality audio data, improving the quality\nof the generated audio. This dual-adapter approach empowers users with enhanced\ncontrol over audio semantics and beat effects, allowing the adjustment of the\ncontroller to achieve better results. Extensive experiments substantiate the\neffectiveness of our framework in achieving seamless audio-visual alignment.\n","authors":["Zhiqi Huang","Dan Luo","Jun Wang","Huan Liao","Zhiheng Li","Zhiyong Wu"],"pdf_url":"https://arxiv.org/pdf/2409.08628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07728v3","updated":"2024-09-13T07:41:02Z","published":"2024-07-10T15:00:08Z","title":"SaMoye: Zero-shot Singing Voice Conversion Model Based on Feature\n  Disentanglement and Enhancement","summary":"  Singing voice conversion (SVC) aims to convert a singer's voice to another\nsinger's from a reference audio while keeping the original semantics. However,\nexisting SVC methods can hardly perform zero-shot due to incomplete feature\ndisentanglement or dependence on the speaker look-up table. We propose the\nfirst open-source high-quality zero-shot SVC model SaMoye that can convert\nsinging to human and non-human timbre. SaMoye disentangles the singing voice's\nfeatures into content, timbre, and pitch features, where we combine multiple\nASR models and compress the content features to reduce timbre leaks. Besides,\nwe enhance the timbre features by unfreezing the speaker encoder and mixing the\nspeaker embedding with top-3 similar speakers. We also establish an\nunparalleled large-scale dataset to guarantee zero-shot performance, which\ncomprises more than 1,815 hours of pure singing voice and 6,367 speakers. We\nconduct objective and subjective experiments to find that SaMoye outperforms\nother models in zero-shot SVC tasks even under extreme conditions like\nconverting singing to animals' timbre. The code and weight of SaMoye are\navailable on https://github.com/CarlWangChina/SaMoye-SVC.\n","authors":["Zihao Wang","Le Ma","Yongsheng Feng","Xin Pan","Yuhang Jin","Kejun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.07728v3.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.08601v1","updated":"2024-09-13T07:31:44Z","published":"2024-09-13T07:31:44Z","title":"STA-V2A: Video-to-Audio Generation with Semantic and Temporal Alignment","summary":"  Visual and auditory perception are two crucial ways humans experience the\nworld. Text-to-video generation has made remarkable progress over the past\nyear, but the absence of harmonious audio in generated video limits its broader\napplications. In this paper, we propose Semantic and Temporal Aligned\nVideo-to-Audio (STA-V2A), an approach that enhances audio generation from\nvideos by extracting both local temporal and global semantic video features and\ncombining these refined video features with text as cross-modal guidance. To\naddress the issue of information redundancy in videos, we propose an onset\nprediction pretext task for local temporal feature extraction and an attentive\npooling module for global semantic feature extraction. To supplement the\ninsufficient semantic information in videos, we propose a Latent Diffusion\nModel with Text-to-Audio priors initialization and cross-modal guidance. We\nalso introduce Audio-Audio Align, a new metric to assess audio-temporal\nalignment. Subjective and objective metrics demonstrate that our method\nsurpasses existing Video-to-Audio models in generating audio with better\nquality, semantic consistency, and temporal alignment. The ablation experiment\nvalidated the effectiveness of each module. Audio samples are available at\nhttps://y-ren16.github.io/STAV2A.\n","authors":["Yong Ren","Chenxing Li","Manjie Xu","Wei Liang","Yu Gu","Rilin Chen","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2409.08601v1.pdf","comment":"Submitted to ICASSP2025"},{"id":"http://arxiv.org/abs/2211.10881v3","updated":"2024-09-13T07:06:19Z","published":"2022-11-20T06:31:23Z","title":"Deepfake Detection: A Comprehensive Survey from the Reliability\n  Perspective","summary":"  The mushroomed Deepfake synthetic materials circulated on the internet have\nraised a profound social impact on politicians, celebrities, and individuals\nworldwide. In this survey, we provide a thorough review of the existing\nDeepfake detection studies from the reliability perspective. We identify three\nreliability-oriented research challenges in the current Deepfake detection\ndomain: transferability, interpretability, and robustness. Moreover, while\nsolutions have been frequently addressed regarding the three challenges, the\ngeneral reliability of a detection model has been barely considered, leading to\nthe lack of reliable evidence in real-life usages and even for prosecutions on\nDeepfake-related cases in court. We, therefore, introduce a model reliability\nstudy metric using statistical random sampling knowledge and the publicly\navailable benchmark datasets to review the reliability of the existing\ndetection models on arbitrary Deepfake candidate suspects. Case studies are\nfurther executed to justify the real-life Deepfake cases including different\ngroups of victims with the help of the reliably qualified detection models as\nreviewed in this survey. Reviews and experiments on the existing approaches\nprovide informative discussions and future research directions for Deepfake\ndetection.\n","authors":["Tianyi Wang","Xin Liao","Kam Pui Chow","Xiaodong Lin","Yinglong Wang"],"pdf_url":"https://arxiv.org/pdf/2211.10881v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08489v1","updated":"2024-09-13T02:32:10Z","published":"2024-09-13T02:32:10Z","title":"Confidence Calibration for Audio Captioning Models","summary":"  Systems that automatically generate text captions for audio, images and video\nlack a confidence indicator of the relevance and correctness of the generated\nsequences. To address this, we build on existing methods of confidence\nmeasurement for text by introduce selective pooling of token probabilities,\nwhich aligns better with traditional correctness measures than conventional\npooling does. Further, we propose directly measuring the similarity between\ninput audio and text in a shared embedding space. To measure self-consistency,\nwe adapt semantic entropy for audio captioning, and find that these two methods\nalign even better than pooling-based metrics with the correctness measure that\ncalculates acoustic similarity between captions. Finally, we explain why\ntemperature scaling of confidences improves calibration.\n","authors":["Rehana Mahfuz","Yinyi Guo","Erik Visser"],"pdf_url":"https://arxiv.org/pdf/2409.08489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08353v2","updated":"2024-09-13T01:48:15Z","published":"2024-06-12T15:59:25Z","title":"Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study\n  on Word Error Rate and Fusion Techniques","summary":"  Text data is commonly utilized as a primary input to enhance Speech Emotion\nRecognition (SER) performance and reliability. However, the reliance on\nhuman-transcribed text in most studies impedes the development of practical SER\nsystems, creating a gap between in-lab research and real-world scenarios where\nAutomatic Speech Recognition (ASR) serves as the text source. Hence, this study\nbenchmarks SER performance using ASR transcripts with varying Word Error Rates\n(WERs) from eleven models on three well-known corpora: IEMOCAP, CMU-MOSI, and\nMSP-Podcast. Our evaluation includes both text-only and bimodal SER with six\nfusion techniques, aiming for a comprehensive analysis that uncovers novel\nfindings and challenges faced by current SER research. Additionally, we propose\na unified ASR error-robust framework integrating ASR error correction and\nmodality-gated fusion, achieving lower WER and higher SER results compared to\nthe best-performing ASR transcript. These findings provide insights into SER\nwith ASR assistance, especially for real-world applications.\n","authors":["Yuanchao Li","Peter Bell","Catherine Lai"],"pdf_url":"https://arxiv.org/pdf/2406.08353v2.pdf","comment":"Accepted to IEEE SLT 2024"},{"id":"http://arxiv.org/abs/2401.17800v2","updated":"2024-09-13T01:43:44Z","published":"2024-01-31T12:51:26Z","title":"Dance-to-Music Generation with Encoder-based Textual Inversion","summary":"  The seamless integration of music with dance movements is essential for\ncommunicating the artistic intent of a dance piece. This alignment also\nsignificantly improves the immersive quality of gaming experiences and\nanimation productions. Although there has been remarkable advancement in\ncreating high-fidelity music from textual descriptions, current methodologies\nmainly focus on modulating overall characteristics such as genre and emotional\ntone. They often overlook the nuanced management of temporal rhythm, which is\nindispensable in crafting music for dance, since it intricately aligns the\nmusical beats with the dancers' movements. Recognizing this gap, we propose an\nencoder-based textual inversion technique to augment text-to-music models with\nvisual control, facilitating personalized music generation. Specifically, we\ndevelop dual-path rhythm-genre inversion to effectively integrate the rhythm\nand genre of a dance motion sequence into the textual space of a text-to-music\nmodel. Contrary to traditional textual inversion methods, which directly update\ntext embeddings to reconstruct a single target object, our approach utilizes\nseparate rhythm and genre encoders to obtain text embeddings for two\npseudo-words, adapting to the varying rhythms and genres. We collect a new\ndataset called In-the-wild Dance Videos (InDV) and demonstrate that our\napproach outperforms state-of-the-art methods across multiple evaluation\nmetrics. Furthermore, our method is able to adapt to changes in tempo and\neffectively integrates with the inherent text-guided generation capability of\nthe pre-trained model. Our source code and demo videos are available at\n\\url{https://github.com/lsfhuihuiff/Dance-to-music_Siggraph_Asia_2024}\n","authors":["Sifei Li","Weiming Dong","Yuxin Zhang","Fan Tang","Chongyang Ma","Oliver Deussen","Tong-Yee Lee","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2401.17800v2.pdf","comment":"11 pages, 5 figures, SIGGRAPH ASIA 2024"}]},"2024-09-12T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.06096v2","updated":"2024-09-12T19:13:17Z","published":"2024-09-09T22:16:48Z","title":"Latent Diffusion Bridges for Unsupervised Musical Audio Timbre Transfer","summary":"  Music timbre transfer is a challenging task that involves modifying the\ntimbral characteristics of an audio signal while preserving its melodic\nstructure. In this paper, we propose a novel method based on dual diffusion\nbridges, trained using the CocoChorales Dataset, which consists of unpaired\nmonophonic single-instrument audio data. Each diffusion model is trained on a\nspecific instrument with a Gaussian prior. During inference, a model is\ndesignated as the source model to map the input audio to its corresponding\nGaussian prior, and another model is designated as the target model to\nreconstruct the target audio from this Gaussian prior, thereby facilitating\ntimbre transfer. We compare our approach against existing unsupervised timbre\ntransfer models such as VAEGAN and Gaussian Flow Bridges (GFB). Experimental\nresults demonstrate that our method achieves both better Fr\\'echet Audio\nDistance (FAD) and melody preservation, as reflected by lower pitch distances\n(DPD) compared to VAEGAN and GFB. Additionally, we discover that the noise\nlevel from the Gaussian prior, $\\sigma$, can be adjusted to control the degree\nof melody preservation and amount of timbre transferred.\n","authors":["Michele Mancusi","Yurii Halychanskyi","Kin Wai Cheuk","Chieh-Hsin Lai","Stefan Uhlich","Junghyun Koo","Marco A. Mart√≠nez-Ram√≠rez","Wei-Hsiang Liao","Giorgio Fabbro","Yuhki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2409.06096v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08046v1","updated":"2024-09-12T13:51:06Z","published":"2024-09-12T13:51:06Z","title":"On the challenges of studying bias in Recommender Systems: A UserKNN\n  case study","summary":"  Statements on the propagation of bias by recommender systems are often hard\nto verify or falsify. Research on bias tends to draw from a small pool of\npublicly available datasets and is therefore bound by their specific\nproperties. Additionally, implementation choices are often not explicitly\ndescribed or motivated in research, while they may have an effect on bias\npropagation. In this paper, we explore the challenges of measuring and\nreporting popularity bias. We showcase the impact of data properties and\nalgorithm configurations on popularity bias by combining synthetic data with\nwell known recommender systems frameworks that implement UserKNN. First, we\nidentify data characteristics that might impact popularity bias, based on the\nfunctionality of UserKNN. Accordingly, we generate various datasets that\ncombine these characteristics. Second, we locate UserKNN configurations that\nvary across implementations in literature. We evaluate popularity bias for five\nsynthetic datasets and five UserKNN configurations, and offer insights on their\njoint effect. We find that, depending on the data characteristics, various\nUserKNN configurations can lead to different conclusions regarding the\npropagation of popularity bias. These results motivate the need for explicitly\naddressing algorithmic configuration and data properties when reporting and\ninterpreting bias in recommender systems.\n","authors":["Savvina Daniil","Manel Slokom","Mirjam Cuper","Cynthia C. S. Liem","Jacco van Ossenbruggen","Laura Hollink"],"pdf_url":"https://arxiv.org/pdf/2409.08046v1.pdf","comment":"Accepted at FAccTRec@RecSys 2024, 11 pages"},{"id":"http://arxiv.org/abs/2409.08014v1","updated":"2024-09-12T12:57:08Z","published":"2024-09-12T12:57:08Z","title":"An Evaluation Framework for Attributed Information Retrieval using Large\n  Language Models","summary":"  With the growing success of Large Language models (LLMs) in\ninformation-seeking scenarios, search engines are now adopting generative\napproaches to provide answers along with in-line citations as attribution.\nWhile existing work focuses mainly on attributed question answering, in this\npaper, we target information-seeking scenarios which are often more challenging\ndue to the open-ended nature of the queries and the size of the label space in\nterms of the diversity of candidate-attributed answers per query. We propose a\nreproducible framework to evaluate and benchmark attributed information\nseeking, using any backbone LLM, and different architectural designs: (1)\nGenerate (2) Retrieve then Generate, and (3) Generate then Retrieve.\nExperiments using HAGRID, an attributed information-seeking dataset, show the\nimpact of different scenarios on both the correctness and attributability of\nanswers.\n","authors":["Hanane Djeddal","Pierre Erbacher","Raouf Toukal","Laure Soulier","Karen Pinel-Sauvagnat","Sophia Katrenko","Lynda Tamine"],"pdf_url":"https://arxiv.org/pdf/2409.08014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07850v1","updated":"2024-09-12T08:53:11Z","published":"2024-09-12T08:53:11Z","title":"Enhancing Cross-Market Recommendation System with Graph Isomorphism\n  Networks: A Novel Approach to Personalized User Experience","summary":"  In today's world of globalized commerce, cross-market recommendation systems\n(CMRs) are crucial for providing personalized user experiences across diverse\nmarket segments. However, traditional recommendation algorithms have\ndifficulties dealing with market specificity and data sparsity, especially in\nnew or emerging markets. In this paper, we propose the CrossGR model, which\nutilizes Graph Isomorphism Networks (GINs) to improve CMR systems. It\noutperforms existing benchmarks in NDCG@10 and HR@10 metrics, demonstrating its\nadaptability and accuracy in handling diverse market segments. The CrossGR\nmodel is adaptable and accurate, making it well-suited for handling the\ncomplexities of cross-market recommendation tasks. Its robustness is\ndemonstrated by consistent performance across different evaluation timeframes,\nindicating its potential to cater to evolving market trends and user\npreferences. Our findings suggest that GINs represent a promising direction for\nCMRs, paving the way for more sophisticated, personalized, and context-aware\nrecommendation systems in the dynamic landscape of global e-commerce.\n","authors":["S√ºmeyye √ñzt√ºrk","Ahmed Burak Ercan","Resul Tugay","≈ûule G√ºnd√ºz √ñƒü√ºd√ºc√º"],"pdf_url":"https://arxiv.org/pdf/2409.07850v1.pdf","comment":"7 pages, 1 figure, 3 tables, 5 equations"},{"id":"http://arxiv.org/abs/2409.07773v1","updated":"2024-09-12T06:13:07Z","published":"2024-09-12T06:13:07Z","title":"PDC-FRS: Privacy-preserving Data Contribution for Federated Recommender\n  System","summary":"  Federated recommender systems (FedRecs) have emerged as a popular research\ndirection for protecting users' privacy in on-device recommendations. In\nFedRecs, users keep their data locally and only contribute their local\ncollaborative information by uploading model parameters to a central server.\nWhile this rigid framework protects users' raw data during training, it\nseverely compromises the recommendation model's performance due to the\nfollowing reasons: (1) Due to the power law distribution nature of user\nbehavior data, individual users have few data points to train a recommendation\nmodel, resulting in uploaded model updates that may be far from optimal; (2) As\neach user's uploaded parameters are learned from local data, which lacks global\ncollaborative information, relying solely on parameter aggregation methods such\nas FedAvg to fuse global collaborative information may be suboptimal. To bridge\nthis performance gap, we propose a novel federated recommendation framework,\nPDC-FRS. Specifically, we design a privacy-preserving data contribution\nmechanism that allows users to share their data with a differential privacy\nguarantee. Based on the shared but perturbed data, an auxiliary model is\ntrained in parallel with the original federated recommendation process. This\nauxiliary model enhances FedRec by augmenting each user's local dataset and\nintegrating global collaborative information. To demonstrate the effectiveness\nof PDC-FRS, we conduct extensive experiments on two widely used recommendation\ndatasets. The empirical results showcase the superiority of PDC-FRS compared to\nbaseline methods.\n","authors":["Chaoqun Yang","Wei Yuan","Liang Qu","Thanh Tam Nguyen"],"pdf_url":"https://arxiv.org/pdf/2409.07773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10555v2","updated":"2024-09-12T05:52:05Z","published":"2024-08-20T05:38:47Z","title":"GACL: Graph Attention Collaborative Learning for Temporal QoS Prediction","summary":"  Accurate prediction of temporal QoS is crucial for maintaining service\nreliability and enhancing user satisfaction in dynamic service-oriented\nenvironments. However, current methods often neglect high-order latent\ncollaborative relationships and fail to dynamically adjust feature learning for\nspecific user-service invocations, which are critical for precise feature\nextraction within each time slice. Moreover, the prevalent use of RNNs for\nmodeling temporal feature evolution patterns is constrained by their inherent\ndifficulty in managing long-range dependencies, thereby limiting the detection\nof long-term QoS trends across multiple time slices. These shortcomings\ndramatically degrade the performance of temporal QoS prediction. To address the\ntwo issues, we propose a novel Graph Attention Collaborative Learning (GACL)\nframework for temporal QoS prediction. Building on a dynamic user-service\ninvocation graph to comprehensively model historical interactions, it designs a\ntarget-prompt graph attention network to extract deep latent features of users\nand services at each time slice, considering implicit target-neighboring\ncollaborative relationships and historical QoS values. Additionally, a\nmulti-layer Transformer encoder is introduced to uncover temporal feature\nevolution patterns, enhancing temporal QoS prediction. Extensive experiments on\nthe WS-DREAM dataset demonstrate that GACL significantly outperforms\nstate-of-the-art methods for temporal QoS prediction across multiple evaluation\nmetrics, achieving the improvements of up to 38.80%.\n","authors":["Shengxiang Hu","Guobing Zou","Bofeng Zhang","Shaogang Wu","Shiyi Lin","Yanglan Gan","Yixin Chen"],"pdf_url":"https://arxiv.org/pdf/2408.10555v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2409.07709v1","updated":"2024-09-12T02:25:41Z","published":"2024-09-12T02:25:41Z","title":"Harnessing TI Feeds for Exploitation Detection","summary":"  Many organizations rely on Threat Intelligence (TI) feeds to assess the risk\nassociated with security threats. Due to the volume and heterogeneity of data,\nit is prohibitive to manually analyze the threat information available in\ndifferent loosely structured TI feeds. Thus, there is a need to develop\nautomated methods to vet and extract actionable information from TI feeds. To\nthis end, we present a machine learning pipeline to automatically detect\nvulnerability exploitation from TI feeds. We first model threat vocabulary in\nloosely structured TI feeds using state-of-the-art embedding techniques\n(Doc2Vec and BERT) and then use it to train a supervised machine learning\nclassifier to detect exploitation of security vulnerabilities. We use our\napproach to identify exploitation events in 191 different TI feeds. Our\nlongitudinal evaluation shows that it is able to accurately identify\nexploitation events from TI feeds only using past data for training and even on\nTI feeds withheld from training. Our proposed approach is useful for a variety\nof downstream tasks such as data-driven vulnerability risk assessment.\n","authors":["Kajal Patel","Zubair Shafiq","Mateus Nogueira","Daniel Sadoc Menasch√©","Enrico Lovat","Taimur Kashif","Ashton Woiwood","Matheus Martins"],"pdf_url":"https://arxiv.org/pdf/2409.07709v1.pdf","comment":"This paper appears at IEEE International Conference on Cyber Security\n  and Resilience (IEEE CSR 2024)"},{"id":"http://arxiv.org/abs/2409.07691v1","updated":"2024-09-12T01:51:06Z","published":"2024-09-12T01:51:06Z","title":"Enhancing Q&A Text Retrieval with Ranking Models: Benchmarking,\n  fine-tuning and deploying Rerankers for RAG","summary":"  Ranking models play a crucial role in enhancing overall accuracy of text\nretrieval systems. These multi-stage systems typically utilize either dense\nembedding models or sparse lexical indices to retrieve relevant passages based\non a given query, followed by ranking models that refine the ordering of the\ncandidate passages by its relevance to the query.\n  This paper benchmarks various publicly available ranking models and examines\ntheir impact on ranking accuracy. We focus on text retrieval for\nquestion-answering tasks, a common use case for Retrieval-Augmented Generation\nsystems. Our evaluation benchmarks include models some of which are\ncommercially viable for industrial applications.\n  We introduce a state-of-the-art ranking model, NV-RerankQA-Mistral-4B-v3,\nwhich achieves a significant accuracy increase of ~14% compared to pipelines\nwith other rerankers. We also provide an ablation study comparing the\nfine-tuning of ranking models with different sizes, losses and self-attention\nmechanisms.\n  Finally, we discuss challenges of text retrieval pipelines with ranking\nmodels in real-world industry applications, in particular the trade-offs among\nmodel size, ranking accuracy and system requirements like indexing and serving\nlatency / throughput.\n","authors":["Gabriel de Souza P. Moreira","Ronay Ak","Benedikt Schifferer","Mengyao Xu","Radek Osmulski","Even Oldridge"],"pdf_url":"https://arxiv.org/pdf/2409.07691v1.pdf","comment":"Accepted for the 1st Workshop on GenAI and RAG Systems for Enterprise\n  @ CIKM 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.08381v1","updated":"2024-09-12T20:02:51Z","published":"2024-09-12T20:02:51Z","title":"Rethinking Prompting Strategies for Multi-Label Recognition with Partial\n  Annotations","summary":"  Vision-language models (VLMs) like CLIP have been adapted for Multi-Label\nRecognition (MLR) with partial annotations by leveraging prompt-learning, where\npositive and negative prompts are learned for each class to associate their\nembeddings with class presence or absence in the shared vision-text feature\nspace. While this approach improves MLR performance by relying on VLM priors,\nwe hypothesize that learning negative prompts may be suboptimal, as the\ndatasets used to train VLMs lack image-caption pairs explicitly focusing on\nclass absence. To analyze the impact of positive and negative prompt learning\non MLR, we introduce PositiveCoOp and NegativeCoOp, where only one prompt is\nlearned with VLM guidance while the other is replaced by an embedding vector\nlearned directly in the shared feature space without relying on the text\nencoder. Through empirical analysis, we observe that negative prompts degrade\nMLR performance, and learning only positive prompts, combined with learned\nnegative embeddings (PositiveCoOp), outperforms dual prompt learning\napproaches. Moreover, we quantify the performance benefits that prompt-learning\noffers over a simple vision-features-only baseline, observing that the baseline\ndisplays strong performance comparable to dual prompt learning approach\n(DualCoOp), when the proportion of missing labels is low, while requiring half\nthe training compute and 16 times fewer parameters\n","authors":["Samyak Rawlekar","Shubhang Bhatnagar","Narendra Ahuja"],"pdf_url":"https://arxiv.org/pdf/2409.08381v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08270v1","updated":"2024-09-12T17:58:13Z","published":"2024-09-12T17:58:13Z","title":"FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally","summary":"  This study addresses the challenge of accurately segmenting 3D Gaussian\nSplatting from 2D masks. Conventional methods often rely on iterative gradient\ndescent to assign each Gaussian a unique label, leading to lengthy optimization\nand sub-optimal solutions. Instead, we propose a straightforward yet globally\noptimal solver for 3D-GS segmentation. The core insight of our method is that,\nwith a reconstructed 3D-GS scene, the rendering of the 2D masks is essentially\na linear function with respect to the labels of each Gaussian. As such, the\noptimal label assignment can be solved via linear programming in closed form.\nThis solution capitalizes on the alpha blending characteristic of the splatting\nprocess for single step optimization. By incorporating the background bias in\nour objective function, our method shows superior robustness in 3D segmentation\nagainst noises. Remarkably, our optimization completes within 30 seconds, about\n50$\\times$ faster than the best existing methods. Extensive experiments\ndemonstrate the efficiency and robustness of our method in segmenting various\nscenes, and its superior performance in downstream tasks such as object removal\nand inpainting. Demos and code will be available at\nhttps://github.com/florinshen/FlashSplat.\n","authors":["Qiuhong Shen","Xingyi Yang","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2409.08270v1.pdf","comment":"ECCV'2024"},{"id":"http://arxiv.org/abs/2409.08260v1","updated":"2024-09-12T17:55:37Z","published":"2024-09-12T17:55:37Z","title":"Improving Text-guided Object Inpainting with Semantic Pre-inpainting","summary":"  Recent years have witnessed the success of large text-to-image diffusion\nmodels and their remarkable potential to generate high-quality images. The\nfurther pursuit of enhancing the editability of images has sparked significant\ninterest in the downstream task of inpainting a novel object described by a\ntext prompt within a designated region in the image. Nevertheless, the problem\nis not trivial from two aspects: 1) Solely relying on one single U-Net to align\ntext prompt and visual object across all the denoising timesteps is\ninsufficient to generate desired objects; 2) The controllability of object\ngeneration is not guaranteed in the intricate sampling space of diffusion\nmodel. In this paper, we propose to decompose the typical single-stage object\ninpainting into two cascaded processes: 1) semantic pre-inpainting that infers\nthe semantic features of desired objects in a multi-modal feature space; 2)\nhigh-fieldity object generation in diffusion latent space that pivots on such\ninpainted semantic features. To achieve this, we cascade a Transformer-based\nsemantic inpainter and an object inpainting diffusion model, leading to a novel\nCAscaded Transformer-Diffusion (CAT-Diffusion) framework for text-guided object\ninpainting. Technically, the semantic inpainter is trained to predict the\nsemantic features of the target object conditioning on unmasked context and\ntext prompt. The outputs of the semantic inpainter then act as the informative\nvisual prompts to guide high-fieldity object generation through a reference\nadapter layer, leading to controllable object inpainting. Extensive evaluations\non OpenImages-V6 and MSCOCO validate the superiority of CAT-Diffusion against\nthe state-of-the-art methods. Code is available at\n\\url{https://github.com/Nnn-s/CATdiffusion}.\n","authors":["Yifu Chen","Jingwen Chen","Yingwei Pan","Yehao Li","Ting Yao","Zhineng Chen","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2409.08260v1.pdf","comment":"ECCV 2024. Source code is available at\n  https://github.com/Nnn-s/CATdiffusion"},{"id":"http://arxiv.org/abs/2409.08258v1","updated":"2024-09-12T17:55:11Z","published":"2024-09-12T17:55:11Z","title":"Improving Virtual Try-On with Garment-focused Diffusion Models","summary":"  Diffusion models have led to the revolutionizing of generative modeling in\nnumerous image synthesis tasks. Nevertheless, it is not trivial to directly\napply diffusion models for synthesizing an image of a target person wearing a\ngiven in-shop garment, i.e., image-based virtual try-on (VTON) task. The\ndifficulty originates from the aspect that the diffusion process should not\nonly produce holistically high-fidelity photorealistic image of the target\nperson, but also locally preserve every appearance and texture detail of the\ngiven garment. To address this, we shape a new Diffusion model, namely GarDiff,\nwhich triggers the garment-focused diffusion process with amplified guidance of\nboth basic visual appearance and detailed textures (i.e., high-frequency\ndetails) derived from the given garment. GarDiff first remoulds a pre-trained\nlatent diffusion model with additional appearance priors derived from the CLIP\nand VAE encodings of the reference garment. Meanwhile, a novel garment-focused\nadapter is integrated into the UNet of diffusion model, pursuing local\nfine-grained alignment with the visual appearance of reference garment and\nhuman pose. We specifically design an appearance loss over the synthesized\ngarment to enhance the crucial, high-frequency details. Extensive experiments\non VITON-HD and DressCode datasets demonstrate the superiority of our GarDiff\nwhen compared to state-of-the-art VTON approaches. Code is publicly available\nat:\n\\href{https://github.com/siqi0905/GarDiff/tree/master}{https://github.com/siqi0905/GarDiff/tree/master}.\n","authors":["Siqi Wan","Yehao Li","Jingwen Chen","Yingwei Pan","Ting Yao","Yang Cao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2409.08258v1.pdf","comment":"ECCV 2024. Source code is available at\n  https://github.com/siqi0905/GarDiff/tree/master"},{"id":"http://arxiv.org/abs/2409.08206v1","updated":"2024-09-12T16:46:41Z","published":"2024-09-12T16:46:41Z","title":"ComAlign: Compositional Alignment in Vision-Language Models","summary":"  Vision-language models (VLMs) like CLIP have showcased a remarkable ability\nto extract transferable features for downstream tasks. Nonetheless, the\ntraining process of these models is usually based on a coarse-grained\ncontrastive loss between the global embedding of images and texts which may\nlose the compositional structure of these modalities. Many recent studies have\nshown VLMs lack compositional understandings like attribute binding and\nidentifying object relationships. Although some recent methods have tried to\nachieve finer-level alignments, they either are not based on extracting\nmeaningful components of proper granularity or don't properly utilize the\nmodalities' correspondence (especially in image-text pairs with more\ningredients). Addressing these limitations, we introduce Compositional\nAlignment (ComAlign), a fine-grained approach to discover more exact\ncorrespondence of text and image components using only the weak supervision in\nthe form of image-text pairs. Our methodology emphasizes that the compositional\nstructure (including entities and relations) extracted from the text modality\nmust also be retained in the image modality. To enforce correspondence of\nfine-grained concepts in image and text modalities, we train a lightweight\nnetwork lying on top of existing visual and language encoders using a small\ndataset. The network is trained to align nodes and edges of the structure\nacross the modalities. Experimental results on various VLMs and datasets\ndemonstrate significant improvements in retrieval and compositional benchmarks,\naffirming the effectiveness of our plugin model.\n","authors":["Ali Abdollah","Amirmohammad Izadi","Armin Saghafian","Reza Vahidimajd","Mohammad Mozafari","Amirreza Mirzaei","Mohammadmahdi Samiei","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2409.08206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21757v2","updated":"2024-09-12T14:01:56Z","published":"2024-07-31T17:23:57Z","title":"Learning Video Context as Interleaved Multimodal Sequences","summary":"  Narrative videos, such as movies, pose significant challenges in video\nunderstanding due to their rich contexts (characters, dialogues, storylines)\nand diverse demands (identify who, relationship, and reason). In this paper, we\nintroduce MovieSeq, a multimodal language model developed to address the wide\nrange of challenges in understanding video contexts. Our core idea is to\nrepresent videos as interleaved multimodal sequences (including images, plots,\nvideos, and subtitles), either by linking external knowledge databases or using\noffline models (such as whisper for subtitles). Through instruction-tuning,\nthis approach empowers the language model to interact with videos using\ninterleaved multimodal instructions. For example, instead of solely relying on\nvideo as input, we jointly provide character photos alongside their names and\ndialogues, allowing the model to associate these elements and generate more\ncomprehensive responses. To demonstrate its effectiveness, we validate\nMovieSeq's performance on six datasets (LVU, MAD, Movienet, CMD, TVC, MovieQA)\nacross five settings (video classification, audio description, video-text\nretrieval, video captioning, and video question-answering). The code will be\npublic at https://github.com/showlab/MovieSeq.\n","authors":["Kevin Qinghong Lin","Pengchuan Zhang","Difei Gao","Xide Xia","Joya Chen","Ziteng Gao","Jinheng Xie","Xuhong Xiao","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2407.21757v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2403.05192v2","updated":"2024-09-12T11:46:32Z","published":"2024-03-08T10:14:32Z","title":"An End-to-End Pipeline Perspective on Video Streaming in Best-Effort\n  Networks: A Survey and Tutorial","summary":"  Remaining a dominant force in Internet traffic, video streaming captivates\nend users, service providers, and researchers. This paper takes a pragmatic\napproach to reviewing recent advances in the field by focusing on the prevalent\nstreaming paradigm that involves delivering long-form two-dimensional videos\nover the best-effort Internet with client-side adaptive bitrate (ABR)\nalgorithms and assistance from content delivery networks (CDNs). To enhance\naccessibility, we supplement the survey with tutorial material. Unlike existing\nsurveys that offer fragmented views, our work provides a holistic perspective\non the entire end-to-end streaming pipeline, from video capture by a\ncamera-equipped device to playback by the end user. Our novel perspective\ncovers the ingestion, processing, and distribution stages of the pipeline and\naddresses key challenges such as video compression, upload, transcoding, ABR\nalgorithms, CDN support, and quality of experience. We review over 200 papers\nand classify streaming designs by their problem-solving methodology, whether\nbased on intuition (simple heuristics), theory (formal optimization), or\nmachine learning (generalizable data patterns). The survey further refines\nthese methodology-based categories and characterizes each design by additional\ntraits such as compatible codecs and use of super resolution. We connect the\nreviewed research to real-world applications by discussing the practices of\ncommercial streaming platforms. Finally, the survey highlights prominent\ncurrent trends and outlines future directions in video streaming.\n","authors":["Leonardo Peroni","Sergey Gorinsky"],"pdf_url":"https://arxiv.org/pdf/2403.05192v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07901v1","updated":"2024-09-12T10:10:22Z","published":"2024-09-12T10:10:22Z","title":"Bridging Discrete and Continuous: A Multimodal Strategy for Complex\n  Emotion Detection","summary":"  In the domain of human-computer interaction, accurately recognizing and\ninterpreting human emotions is crucial yet challenging due to the complexity\nand subtlety of emotional expressions. This study explores the potential for\ndetecting a rich and flexible range of emotions through a multimodal approach\nwhich integrates facial expressions, voice tones, and transcript from video\nclips. We propose a novel framework that maps variety of emotions in a\nthree-dimensional Valence-Arousal-Dominance (VAD) space, which could reflect\nthe fluctuations and positivity/negativity of emotions to enable a more variety\nand comprehensive representation of emotional states. We employed K-means\nclustering to transit emotions from traditional discrete categorization to a\ncontinuous labeling system and built a classifier for emotion recognition upon\nthis system. The effectiveness of the proposed model is evaluated using the\nMER2024 dataset, which contains culturally consistent video clips from Chinese\nmovies and TV series, annotated with both discrete and open-vocabulary emotion\nlabels. Our experiment successfully achieved the transformation between\ndiscrete and continuous models, and the proposed model generated a more diverse\nand comprehensive set of emotion vocabulary while maintaining strong accuracy.\n","authors":["Jiehui Jia","Huan Zhang","Jinhua Liang"],"pdf_url":"https://arxiv.org/pdf/2409.07901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07855v1","updated":"2024-09-12T09:00:56Z","published":"2024-09-12T09:00:56Z","title":"MSMF: Multi-Scale Multi-Modal Fusion for Enhanced Stock Market\n  Prediction","summary":"  This paper presents MSMF (Multi-Scale Multi-Modal Fusion), a novel approach\nfor enhanced stock market prediction. MSMF addresses key challenges in\nmulti-modal stock analysis by integrating a modality completion encoder,\nmulti-scale feature extraction, and an innovative fusion mechanism. Our model\nleverages blank learning and progressive fusion to balance complementarity and\nredundancy across modalities, while multi-scale alignment facilitates direct\ncorrelations between heterogeneous data types. We introduce Multi-Granularity\nGates and a specialized architecture to optimize the integration of local and\nglobal information for different tasks. Additionally, a Task-targeted\nPrediction layer is employed to preserve both coarse and fine-grained features\nduring fusion. Experimental results demonstrate that MSMF outperforms existing\nmethods, achieving significant improvements in accuracy and reducing prediction\nerrors across various stock market forecasting tasks. This research contributes\nvaluable insights to the field of multi-modal financial analysis and offers a\nrobust framework for enhanced market prediction.\n","authors":["Jiahao Qin"],"pdf_url":"https://arxiv.org/pdf/2409.07855v1.pdf","comment":"15 pages, 1 figures, 7 tables"},{"id":"http://arxiv.org/abs/2409.07827v1","updated":"2024-09-12T08:19:25Z","published":"2024-09-12T08:19:25Z","title":"Bridging Paintings and Music -- Exploring Emotion based Music Generation\n  through Paintings","summary":"  Rapid advancements in artificial intelligence have significantly enhanced\ngenerative tasks involving music and images, employing both unimodal and\nmultimodal approaches. This research develops a model capable of generating\nmusic that resonates with the emotions depicted in visual arts, integrating\nemotion labeling, image captioning, and language models to transform visual\ninputs into musical compositions. Addressing the scarcity of aligned art and\nmusic data, we curated the Emotion Painting Music Dataset, pairing paintings\nwith corresponding music for effective training and evaluation. Our dual-stage\nframework converts images to text descriptions of emotional content and then\ntransforms these descriptions into music, facilitating efficient learning with\nminimal data. Performance is evaluated using metrics such as Fr\\'echet Audio\nDistance (FAD), Total Harmonic Distortion (THD), Inception Score (IS), and KL\ndivergence, with audio-emotion text similarity confirmed by the pre-trained\nCLAP model to demonstrate high alignment between generated music and text. This\nsynthesis tool bridges visual art and music, enhancing accessibility for the\nvisually impaired and opening avenues in educational and therapeutic\napplications by providing enriched multi-sensory experiences.\n","authors":["Tanisha Hisariya","Huan Zhang","Jinhua Liang"],"pdf_url":"https://arxiv.org/pdf/2409.07827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07759v1","updated":"2024-09-12T05:33:15Z","published":"2024-09-12T05:33:15Z","title":"SwinGS: Sliding Window Gaussian Splatting for Volumetric Video Streaming\n  with Arbitrary Length","summary":"  Recent advances in 3D Gaussian Splatting (3DGS) have garnered significant\nattention in computer vision and computer graphics due to its high rendering\nspeed and remarkable quality. While extant research has endeavored to extend\nthe application of 3DGS from static to dynamic scenes, such efforts have been\nconsistently impeded by excessive model sizes, constraints on video duration,\nand content deviation. These limitations significantly compromise the\nstreamability of dynamic 3D Gaussian models, thereby restricting their utility\nin downstream applications, including volumetric video, autonomous vehicle, and\nimmersive technologies such as virtual, augmented, and mixed reality.\n  This paper introduces SwinGS, a novel framework for training, delivering, and\nrendering volumetric video in a real-time streaming fashion. To address the\naforementioned challenges and enhance streamability, SwinGS integrates\nspacetime Gaussian with Markov Chain Monte Carlo (MCMC) to adapt the model to\nfit various 3D scenes across frames, in the meantime employing a sliding window\ncaptures Gaussian snapshots for each frame in an accumulative way. We implement\na prototype of SwinGS and demonstrate its streamability across various datasets\nand scenes. Additionally, we develop an interactive WebGL viewer enabling\nreal-time volumetric video playback on most devices with modern browsers,\nincluding smartphones and tablets. Experimental results show that SwinGS\nreduces transmission costs by 83.6% compared to previous work with ignorable\ncompromise in PSNR. Moreover, SwinGS easily scales to long video sequences\nwithout compromising quality.\n","authors":["Bangya Liu","Suman Banerjee"],"pdf_url":"https://arxiv.org/pdf/2409.07759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07701v1","updated":"2024-09-12T02:04:26Z","published":"2024-09-12T02:04:26Z","title":"TMFNet: Two-Stream Multi-Channels Fusion Networks for Color Image\n  Operation Chain Detection","summary":"  Image operation chain detection techniques have gained increasing attention\nrecently in the field of multimedia forensics. However, existing detection\nmethods suffer from the generalization problem. Moreover, the channel\ncorrelation of color images that provides additional forensic evidence is often\nignored. To solve these issues, in this article, we propose a novel two-stream\nmulti-channels fusion networks for color image operation chain detection in\nwhich the spatial artifact stream and the noise residual stream are explored in\na complementary manner. Specifically, we first propose a novel deep residual\narchitecture without pooling in the spatial artifact stream for learning the\nglobal features representation of multi-channel correlation. Then, a set of\nfilters is designed to aggregate the correlation information of multi-channels\nwhile capturing the low-level features in the noise residual stream.\nSubsequently, the high-level features are extracted by the deep residual model.\nFinally, features from the two streams are fed into a fusion module, to\neffectively learn richer discriminative representations of the operation chain.\nExtensive experiments show that the proposed method achieves state-of-the-art\ngeneralization ability while maintaining robustness to JPEG compression. The\nsource code used in these experiments will be released at\nhttps://github.com/LeiTan-98/TMFNet.\n","authors":["Yakun Niu","Lei Tan","Lei Zhang","Xianyu Zuo"],"pdf_url":"https://arxiv.org/pdf/2409.07701v1.pdf","comment":"15 pages, 12 figures"}]},"2024-09-11T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.01137v3","updated":"2024-09-11T22:59:34Z","published":"2024-09-02T10:19:31Z","title":"Smart E-commerce Recommendations with Semantic AI","summary":"  In e-commerce, web mining for page recommendations is widely used but often\nfails to meet user needs. To address this, we propose a novel solution\ncombining semantic web mining with BP neural networks. We process user search\nlogs to extract five key features: content priority, time spent, user feedback,\nrecommendation semantics, and input deviation. These features are then fed into\na BP neural network to classify and prioritize web pages. The prioritized pages\nare recommended to users. Using book sales pages for testing, our results\ndemonstrate that this solution can quickly and accurately identify the pages\nusers need. Our approach ensures that recommendations are more relevant and\ntailored to individual preferences, enhancing the online shopping experience.\nBy leveraging advanced semantic analysis and neural network techniques, we\nbridge the gap between user expectations and actual recommendations. This\ninnovative method not only improves accuracy but also speeds up the\nrecommendation process, making it a valuable tool for e-commerce platforms\naiming to boost user satisfaction and engagement. Additionally, our system\nability to handle large datasets and provide real-time recommendations makes it\na scalable and efficient solution for modern e-commerce challenges.\n","authors":["M. Badouch","M. Boutaounte"],"pdf_url":"https://arxiv.org/pdf/2409.01137v3.pdf","comment":"My paper contain some errors"},{"id":"http://arxiv.org/abs/2409.07627v1","updated":"2024-09-11T21:18:21Z","published":"2024-09-11T21:18:21Z","title":"Leveraging User-Generated Reviews for Recommender Systems with Dynamic\n  Headers","summary":"  E-commerce platforms have a vast catalog of items to cater to their\ncustomers' shopping interests. Most of these platforms assist their customers\nin the shopping process by offering optimized recommendation carousels,\ndesigned to help customers quickly locate their desired items. Many models have\nbeen proposed in academic literature to generate and enhance the ranking and\nrecall set of items in these carousels. Conventionally, the accompanying\ncarousel title text (header) of these carousels remains static. In most\ninstances, a generic text such as \"Items similar to your current viewing\" is\nutilized. Fixed variations such as the inclusion of specific attributes \"Other\nitems from a similar seller\" or \"Items from a similar brand\" in addition to\n\"frequently bought together\" or \"considered together\" are observed as well.\nThis work proposes a novel approach to customize the header generation process\nof these carousels. Our work leverages user-generated reviews that lay focus on\nspecific attributes (aspects) of an item that were favorably perceived by users\nduring their interaction with the given item. We extract these aspects from\nreviews and train a graph neural network-based model under the framework of a\nconditional ranking task. We refer to our innovative methodology as Dynamic\nText Snippets (DTS) which generates multiple header texts for an anchor item\nand its recall set. Our approach demonstrates the potential of utilizing\nuser-generated reviews and presents a unique paradigm for exploring\nincreasingly context-aware recommendation systems.\n","authors":["Shanu Vashishtha","Abhay Kumar","Lalitesh Morishetti","Kaushiki Nag","Kannan Achan"],"pdf_url":"https://arxiv.org/pdf/2409.07627v1.pdf","comment":"7 pages, 3 figures, PAIS 2024 (ECAI)"},{"id":"http://arxiv.org/abs/2409.07604v1","updated":"2024-09-11T20:31:42Z","published":"2024-09-11T20:31:42Z","title":"Multilingual Prompts in LLM-Based Recommenders: Performance Across\n  Languages","summary":"  Large language models (LLMs) are increasingly used in natural language\nprocessing tasks. Recommender systems traditionally use methods such as\ncollaborative filtering and matrix factorization, as well as advanced\ntechniques like deep learning and reinforcement learning. Although language\nmodels have been applied in recommendation, the recent trend have focused on\nleveraging the generative capabilities of LLMs for more personalized\nsuggestions. While current research focuses on English due to its resource\nrichness, this work explores the impact of non-English prompts on\nrecommendation performance. Using OpenP5, a platform for developing and\nevaluating LLM-based recommendations, we expanded its English prompt templates\nto include Spanish and Turkish. Evaluation on three real-world datasets, namely\nML1M, LastFM, and Amazon-Beauty, showed that usage of non-English prompts\ngenerally reduce performance, especially in less-resourced languages like\nTurkish. We also retrained an LLM-based recommender model with multilingual\nprompts to analyze performance variations. Retraining with multilingual prompts\nresulted in more balanced performance across languages, but slightly reduced\nEnglish performance. This work highlights the need for diverse language support\nin LLM-based recommenders and suggests future research on creating evaluation\ndatasets, using newer models and additional languages.\n","authors":["Makbule Gulcin Ozsoy"],"pdf_url":"https://arxiv.org/pdf/2409.07604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07433v1","updated":"2024-09-11T17:27:04Z","published":"2024-09-11T17:27:04Z","title":"Dot Product is All You Need: Bridging the Gap Between Item\n  Recommendation and Link Prediction","summary":"  Item recommendation (the task of predicting if a user may interact with new\nitems from the catalogue in a recommendation system) and link prediction (the\ntask of identifying missing links in a knowledge graph) have long been regarded\nas distinct problems. In this work, we show that the item recommendation\nproblem can be seen as an instance of the link prediction problem, where\nentities in the graph represent users and items, and the task consists of\npredicting missing instances of the relation type <<interactsWith>>. In a\npreliminary attempt to demonstrate the assumption, we decide to test three\npopular factorisation-based link prediction models on the item recommendation\ntask, showing that their predictive accuracy is competitive with ten\nstate-of-the-art recommendation models. The purpose is to show how the former\nmay be seamlessly and effectively applied to the recommendation task without\nany specific modification to their architectures. Finally, while beginning to\nunveil the key reasons behind the recommendation performance of the selected\nlink prediction models, we explore different settings for their hyper-parameter\nvalues, paving the way for future directions.\n","authors":["Daniele Malitesta","Alberto Carlo Maria Mancino","Pasquale Minervini","Tommaso Di Noia"],"pdf_url":"https://arxiv.org/pdf/2409.07433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07416v1","updated":"2024-09-11T17:01:06Z","published":"2024-09-11T17:01:06Z","title":"Hierarchical Reinforcement Learning for Temporal Abstraction of Listwise\n  Recommendation","summary":"  Modern listwise recommendation systems need to consider both long-term user\nperceptions and short-term interest shifts. Reinforcement learning can be\napplied on recommendation to study such a problem but is also subject to large\nsearch space, sparse user feedback and long interactive latency. Motivated by\nrecent progress in hierarchical reinforcement learning, we propose a novel\nframework called mccHRL to provide different levels of temporal abstraction on\nlistwise recommendation. Within the hierarchical framework, the high-level\nagent studies the evolution of user perception, while the low-level agent\nproduces the item selection policy by modeling the process as a sequential\ndecision-making problem. We argue that such framework has a well-defined\ndecomposition of the outra-session context and the intra-session context, which\nare encoded by the high-level and low-level agents, respectively. To verify\nthis argument, we implement both a simulator-based environment and an\nindustrial dataset-based experiment. Results observe significant performance\nimprovement by our method, compared with several well-known baselines. Data and\ncodes have been made public.\n","authors":["Luo Ji","Gao Liu","Mingyang Yin","Hongxia Yang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.07416v1.pdf","comment":"18 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.07367v1","updated":"2024-09-11T15:56:05Z","published":"2024-09-11T15:56:05Z","title":"Enhancing Sequential Music Recommendation with Negative\n  Feedback-informed Contrastive Learning","summary":"  Modern music streaming services are heavily based on recommendation engines\nto serve content to users. Sequential recommendation -- continuously providing\nnew items within a single session in a contextually coherent manner -- has been\nan emerging topic in current literature. User feedback -- a positive or\nnegative response to the item presented -- is used to drive content\nrecommendations by learning user preferences. We extend this idea to\nsession-based recommendation to provide context-coherent music recommendations\nby modelling negative user feedback, i.e., skips, in the loss function. We\npropose a sequence-aware contrastive sub-task to structure item embeddings in\nsession-based music recommendation, such that true next-positive items\n(ignoring skipped items) are structured closer in the session embedding space,\nwhile skipped tracks are structured farther away from all items in the session.\nThis directly affects item rankings using a K-nearest-neighbors search for\nnext-item recommendations, while also promoting the rank of the true next item.\nExperiments incorporating this task into SoTA methods for sequential item\nrecommendation show consistent performance gains in terms of next-item hit\nrate, item ranking, and skip down-ranking on three music recommendation\ndatasets, strongly benefiting from the increasing presence of user feedback.\n","authors":["Pavan Seshadri","Shahrzad Shashaani","Peter Knees"],"pdf_url":"https://arxiv.org/pdf/2409.07367v1.pdf","comment":"To-appear at 18th ACM Conference on Recommendation Systems"},{"id":"http://arxiv.org/abs/2311.04916v3","updated":"2024-09-11T14:59:21Z","published":"2023-11-02T04:01:04Z","title":"Explainable Identification of Hate Speech towards Islam using Graph\n  Neural Networks","summary":"  Islamophobic language on online platforms fosters intolerance, making\ndetection and elimination crucial for promoting harmony. Traditional hate\nspeech detection models rely on NLP techniques like tokenization,\npart-of-speech tagging, and encoder-decoder models. However, Graph Neural\nNetworks (GNNs), with their ability to utilize relationships between data\npoints, offer more effective detection and greater explainability. In this\nwork, we represent speeches as nodes and connect them with edges based on their\ncontext and similarity to develop the graph. This study introduces a novel\nparadigm using GNNs to identify and explain hate speech towards Islam. Our\nmodel leverages GNNs to understand the context and patterns of hate speech by\nconnecting texts via pretrained NLP-generated word embeddings, achieving\nstate-of-the-art performance and enhancing detection accuracy while providing\nvaluable explanations. This highlights the potential of GNNs in combating\nonline hate speech and fostering a safer, more inclusive online environment.\n","authors":["Azmine Toushik Wasi"],"pdf_url":"https://arxiv.org/pdf/2311.04916v3.pdf","comment":"Accepted in: (i) NeurIPS 2023 : Muslims in ML Workshop (non-archival)\n  (https://www.musiml.org/schedule/#:~:text=Azmine%20Toushik%20Wasi) (ii) EMNLP\n  2024 : NLP for Positive Impact Workshop (archival)"},{"id":"http://arxiv.org/abs/2403.17372v4","updated":"2024-09-11T14:56:27Z","published":"2024-03-26T04:16:57Z","title":"An Empirical Study of Training ID-Agnostic Multi-modal Sequential\n  Recommenders","summary":"  Sequential Recommendation (SR) aims to predict future user-item interactions\nbased on historical interactions. While many SR approaches concentrate on user\nIDs and item IDs, the human perception of the world through multi-modal\nsignals, like text and images, has inspired researchers to delve into\nconstructing SR from multi-modal information without using IDs. However, the\ncomplexity of multi-modal learning manifests in diverse feature extractors,\nfusion methods, and pre-trained models. Consequently, designing a simple and\nuniversal \\textbf{M}ulti-\\textbf{M}odal \\textbf{S}equential\n\\textbf{R}ecommendation (\\textbf{MMSR}) framework remains a formidable\nchallenge. We systematically summarize the existing multi-modal related SR\nmethods and distill the essence into four core components: visual encoder, text\nencoder, multimodal fusion module, and sequential architecture. Along these\ndimensions, we dissect the model designs, and answer the following\nsub-questions: First, we explore how to construct MMSR from scratch, ensuring\nits performance either on par with or exceeds existing SR methods without\ncomplex techniques. Second, we examine if MMSR can benefit from existing\nmulti-modal pre-training paradigms. Third, we assess MMSR's capability in\ntackling common challenges like cold start and domain transferring. Our\nexperiment results across four real-world recommendation scenarios demonstrate\nthe great potential ID-agnostic multi-modal sequential recommendation. Our\nframework can be found at: https://github.com/MMSR23/MMSR.\n","authors":["Youhua Li","Hanwen Du","Yongxin Ni","Yuanqi He","Junchen Fu","Xiangyan Liu","Qi Guo"],"pdf_url":"https://arxiv.org/pdf/2403.17372v4.pdf","comment":"We are requesting to withdraw the paper due to a significant\n  methodological error discovered in the experimental setup, specifically in\n  Section 4.3. This error affects the validity of the results and conclusions\n  drawn from the study. We intend to address these issues and submit a\n  corrected version in the future"},{"id":"http://arxiv.org/abs/2409.07238v1","updated":"2024-09-11T12:51:41Z","published":"2024-09-11T12:51:41Z","title":"Diff-VPS: Video Polyp Segmentation via a Multi-task Diffusion Network\n  with Adversarial Temporal Reasoning","summary":"  Diffusion Probabilistic Models have recently attracted significant attention\nin the community of computer vision due to their outstanding performance.\nHowever, while a substantial amount of diffusion-based research has focused on\ngenerative tasks, no work introduces diffusion models to advance the results of\npolyp segmentation in videos, which is frequently challenged by polyps' high\ncamouflage and redundant temporal cues.In this paper, we present a novel\ndiffusion-based network for video polyp segmentation task, dubbed as Diff-VPS.\nWe incorporate multi-task supervision into diffusion models to promote the\ndiscrimination of diffusion models on pixel-by-pixel segmentation. This\nintegrates the contextual high-level information achieved by the joint\nclassification and detection tasks. To explore the temporal dependency,\nTemporal Reasoning Module (TRM) is devised via reasoning and reconstructing the\ntarget frame from the previous frames. We further equip TRM with a generative\nadversarial self-supervised strategy to produce more realistic frames and thus\ncapture better dynamic cues. Extensive experiments are conducted on SUN-SEG,\nand the results indicate that our proposed Diff-VPS significantly achieves\nstate-of-the-art performance. Code is available at\nhttps://github.com/lydia-yllu/Diff-VPS.\n","authors":["Yingling Lu","Yijun Yang","Zhaohu Xing","Qiong Wang","Lei Zhu"],"pdf_url":"https://arxiv.org/pdf/2409.07238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07237v1","updated":"2024-09-11T12:48:52Z","published":"2024-09-11T12:48:52Z","title":"Negative Sampling in Recommendation: A Survey and Future Directions","summary":"  Recommender systems aim to capture users' personalized preferences from the\ncast amount of user behaviors, making them pivotal in the era of information\nexplosion. However, the presence of the dynamic preference, the \"information\ncocoons\", and the inherent feedback loops in recommendation make users interact\nwith a limited number of items. Conventional recommendation algorithms\ntypically focus on the positive historical behaviors, while neglecting the\nessential role of negative feedback in user interest understanding. As a\npromising but easy-to-ignored area, negative sampling is proficients in\nrevealing the genuine negative aspect inherent in user behaviors, emerging as\nan inescapable procedure in recommendation. In this survey, we first discuss\nthe role of negative sampling in recommendation and thoroughly analyze\nchallenges that consistently impede its progress. Then, we conduct an extensive\nliterature review on the existing negative sampling strategies in\nrecommendation and classify them into five categories with their discrepant\ntechniques. Finally, we detail the insights of the tailored negative sampling\nstrategies in diverse recommendation scenarios and outline an overview of the\nprospective research directions toward which the community may engage and\nbenefit.\n","authors":["Haokai Ma","Ruobing Xie","Lei Meng","Fuli Feng","Xiaoyu Du","Xingwu Sun","Zhanhui Kang","Xiangxu Meng"],"pdf_url":"https://arxiv.org/pdf/2409.07237v1.pdf","comment":"38 pages, 9 figures; Under review"},{"id":"http://arxiv.org/abs/2409.05570v2","updated":"2024-09-11T07:51:10Z","published":"2024-09-09T12:53:06Z","title":"Rs4rs: Semantically Find Recent Publications from Top Recommendation\n  System-Related Venues","summary":"  Rs4rs is a web application designed to perform semantic search on recent\npapers from top conferences and journals related to Recommender Systems.\nCurrent scholarly search engine tools like Google Scholar, Semantic Scholar,\nand ResearchGate often yield broad results that fail to target the most\nrelevant high-quality publications. Moreover, manually visiting individual\nconference and journal websites is a time-consuming process that primarily\nsupports only syntactic searches. Rs4rs addresses these issues by providing a\nuser-friendly platform where researchers can input their topic of interest and\nreceive a list of recent, relevant papers from top Recommender Systems venues.\nUtilizing semantic search techniques, Rs4rs ensures that the search results are\nnot only precise and relevant but also comprehensive, capturing papers\nregardless of variations in wording. This tool significantly enhances research\nefficiency and accuracy, thereby benefitting the research community and public\nby facilitating access to high-quality, pertinent academic resources in the\nfield of Recommender Systems. Rs4rs is available at https://rs4rs.com.\n","authors":["Tri Kurniawan Wijaya","Edoardo D'Amico","Gabor Fodor","Manuel V. Loureiro"],"pdf_url":"https://arxiv.org/pdf/2409.05570v2.pdf","comment":"Accepted in ACM RecSys 2024"},{"id":"http://arxiv.org/abs/2405.02219v2","updated":"2024-09-11T07:27:51Z","published":"2024-05-03T16:25:27Z","title":"A Normative Framework for Benchmarking Consumer Fairness in Large\n  Language Model Recommender System","summary":"  The rapid adoption of large language models (LLMs) in recommender systems\n(RS) presents new challenges in understanding and evaluating their biases,\nwhich can result in unfairness or the amplification of stereotypes. Traditional\nfairness evaluations in RS primarily focus on collaborative filtering (CF)\nsettings, which may not fully capture the complexities of LLMs, as these models\noften inherit biases from large, unregulated data. This paper proposes a\nnormative framework to benchmark consumer fairness in LLM-powered recommender\nsystems (RecLLMs).\n  We critically examine how fairness norms in classical RS fall short in\naddressing the challenges posed by LLMs. We argue that this gap can lead to\narbitrary conclusions about fairness, and we propose a more structured, formal\napproach to evaluate fairness in such systems. Our experiments on the MovieLens\ndataset on consumer fairness, using in-context learning (zero-shot vs.\nfew-shot) reveal fairness deviations in age-based recommendations, particularly\nwhen additional contextual examples are introduced (ICL-2). Statistical\nsignificance tests confirm that these deviations are not random, highlighting\nthe need for robust evaluation methods. While this work offers a preliminary\ndiscussion on a proposed normative framework, our hope is that it could provide\na formal, principled approach for auditing and mitigating bias in RecLLMs. The\ncode and dataset used for this work will be shared at \"gihub-anonymized\".\n","authors":["Yashar Deldjoo","Fatemeh Nazary"],"pdf_url":"https://arxiv.org/pdf/2405.02219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07033v1","updated":"2024-09-11T06:03:02Z","published":"2024-09-11T06:03:02Z","title":"E-commerce Webpage Recommendation Scheme Base on Semantic Mining and\n  Neural Networks","summary":"  In e-commerce websites, web mining web page recommendation technology has\nbeen widely used. However, recommendation solutions often cannot meet the\nactual application needs of online shopping users. To address this problem,\nthis paper proposes an e-commerce web page recommendation solution that\ncombines semantic web mining and BP neural networks. First, the web logs of\nuser searches are processed, and 5 features are extracted: content priority,\ntime consumption priority, online shopping users' explicit/implicit feedback on\nthe website, recommendation semantics and input deviation amount. Then, these\nfeatures are used as input features of the BP neural network to classify and\nidentify the priority of the final output web page. Finally, the web pages are\nsorted according to priority and recommended to users. This project uses book\nsales webpages as samples for experiments. The results show that this solution\ncan quickly and accurately identify the webpages required by users.\n","authors":["Wenchao Zhao","Xiaoyi Liu","Ruilin Xu","Lingxi Xiao","Muqing Li"],"pdf_url":"https://arxiv.org/pdf/2409.07033v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2409.01137"},{"id":"http://arxiv.org/abs/2409.05878v2","updated":"2024-09-11T04:47:52Z","published":"2024-08-25T12:12:08Z","title":"CF-KAN: Kolmogorov-Arnold Network-based Collaborative Filtering to\n  Mitigate Catastrophic Forgetting in Recommender Systems","summary":"  Collaborative filtering (CF) remains essential in recommender systems,\nleveraging user--item interactions to provide personalized recommendations.\nMeanwhile, a number of CF techniques have evolved into sophisticated model\narchitectures based on multi-layer perceptrons (MLPs). However, MLPs often\nsuffer from catastrophic forgetting, and thus lose previously acquired\nknowledge when new information is learned, particularly in dynamic environments\nrequiring continual learning. To tackle this problem, we propose CF-KAN, a new\nCF method utilizing Kolmogorov-Arnold networks (KANs). By learning nonlinear\nfunctions on the edge level, KANs are more robust to the catastrophic\nforgetting problem than MLPs. Built upon a KAN-based autoencoder, CF-KAN is\ndesigned in the sense of effectively capturing the intricacies of sparse\nuser--item interactions and retaining information from previous data instances.\nDespite its simplicity, our extensive experiments demonstrate 1) CF-KAN's\nsuperiority over state-of-the-art methods in recommendation accuracy, 2)\nCF-KAN's resilience to catastrophic forgetting, underscoring its effectiveness\nin both static and dynamic recommendation scenarios, and 3) CF-KAN's edge-level\ninterpretation facilitating the explainability of recommendations.\n","authors":["Jin-Duk Park","Kyung-Min Kim","Won-Yong Shin"],"pdf_url":"https://arxiv.org/pdf/2409.05878v2.pdf","comment":"9 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2409.05405v2","updated":"2024-09-11T02:44:52Z","published":"2024-09-09T08:06:50Z","title":"A Survey of Multimodal Composite Editing and Retrieval","summary":"  In the real world, where information is abundant and diverse across different\nmodalities, understanding and utilizing various data types to improve retrieval\nsystems is a key focus of research. Multimodal composite retrieval integrates\ndiverse modalities such as text, image and audio, etc. to provide more\naccurate, personalized, and contextually relevant results. To facilitate a\ndeeper understanding of this promising direction, this survey explores\nmultimodal composite editing and retrieval in depth, covering image-text\ncomposite editing, image-text composite retrieval, and other multimodal\ncomposite retrieval. In this survey, we systematically organize the application\nscenarios, methods, benchmarks, experiments, and future directions. Multimodal\nlearning is a hot topic in large model era, and have also witnessed some\nsurveys in multimodal learning and vision-language models with transformers\npublished in the PAMI journal. To the best of our knowledge, this survey is the\nfirst comprehensive review of the literature on multimodal composite retrieval,\nwhich is a timely complement of multimodal fusion to existing reviews. To help\nreaders' quickly track this field, we build the project page for this survey,\nwhich can be found at\nhttps://github.com/fuxianghuang1/Multimodal-Composite-Editing-and-Retrieval.\n","authors":["Suyan Li","Fuxiang Huang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.05405v2.pdf","comment":"20 pages, 3 figures, and 11 tables"},{"id":"http://arxiv.org/abs/2405.17795v3","updated":"2024-09-11T02:07:20Z","published":"2024-05-28T03:45:34Z","title":"Dataset Regeneration for Sequential Recommendation","summary":"  The sequential recommender (SR) system is a crucial component of modern\nrecommender systems, as it aims to capture the evolving preferences of users.\nSignificant efforts have been made to enhance the capabilities of SR systems.\nThese methods typically follow the model-centric paradigm, which involves\ndeveloping effective models based on fixed datasets. However, this approach\noften overlooks potential quality issues and flaws inherent in the data. Driven\nby the potential of data-centric AI, we propose a novel data-centric paradigm\nfor developing an ideal training dataset using a model-agnostic dataset\nregeneration framework called DR4SR. This framework enables the regeneration of\na dataset with exceptional cross-architecture generalizability. Additionally,\nwe introduce the DR4SR+ framework, which incorporates a model-aware dataset\npersonalizer to tailor the regenerated dataset specifically for a target model.\nTo demonstrate the effectiveness of the data-centric paradigm, we integrate our\nframework with various model-centric methods and observe significant\nperformance improvements across four widely adopted datasets. Furthermore, we\nconduct in-depth analyses to explore the potential of the data-centric paradigm\nand provide valuable insights. The code can be found at\nhttps://github.com/USTC-StarTeam/DR4SR.\n","authors":["Mingjia Yin","Hao Wang","Wei Guo","Yong Liu","Suojuan Zhang","Sirui Zhao","Defu Lian","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2405.17795v3.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.07454v1","updated":"2024-09-11T17:59:02Z","published":"2024-09-11T17:59:02Z","title":"DreamMesh: Jointly Manipulating and Texturing Triangle Meshes for\n  Text-to-3D Generation","summary":"  Learning radiance fields (NeRF) with powerful 2D diffusion models has\ngarnered popularity for text-to-3D generation. Nevertheless, the implicit 3D\nrepresentations of NeRF lack explicit modeling of meshes and textures over\nsurfaces, and such surface-undefined way may suffer from the issues, e.g.,\nnoisy surfaces with ambiguous texture details or cross-view inconsistency. To\nalleviate this, we present DreamMesh, a novel text-to-3D architecture that\npivots on well-defined surfaces (triangle meshes) to generate high-fidelity\nexplicit 3D model. Technically, DreamMesh capitalizes on a distinctive\ncoarse-to-fine scheme. In the coarse stage, the mesh is first deformed by\ntext-guided Jacobians and then DreamMesh textures the mesh with an interlaced\nuse of 2D diffusion models in a tuning free manner from multiple viewpoints. In\nthe fine stage, DreamMesh jointly manipulates the mesh and refines the texture\nmap, leading to high-quality triangle meshes with high-fidelity textured\nmaterials. Extensive experiments demonstrate that DreamMesh significantly\noutperforms state-of-the-art text-to-3D methods in faithfully generating 3D\ncontent with richer textual details and enhanced geometry. Our project page is\navailable at https://dreammesh.github.io.\n","authors":["Haibo Yang","Yang Chen","Yingwei Pan","Ting Yao","Zhineng Chen","Zuxuan Wu","Yu-Gang Jiang","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2409.07454v1.pdf","comment":"ECCV 2024. Project page is available at\n  \\url{https://dreammesh.github.io}"},{"id":"http://arxiv.org/abs/2409.07452v1","updated":"2024-09-11T17:58:57Z","published":"2024-09-11T17:58:57Z","title":"Hi3D: Pursuing High-Resolution Image-to-3D Generation with Video\n  Diffusion Models","summary":"  Despite having tremendous progress in image-to-3D generation, existing\nmethods still struggle to produce multi-view consistent images with\nhigh-resolution textures in detail, especially in the paradigm of 2D diffusion\nthat lacks 3D awareness. In this work, we present High-resolution Image-to-3D\nmodel (Hi3D), a new video diffusion based paradigm that redefines a single\nimage to multi-view images as 3D-aware sequential image generation (i.e.,\norbital video generation). This methodology delves into the underlying temporal\nconsistency knowledge in video diffusion model that generalizes well to\ngeometry consistency across multiple views in 3D generation. Technically, Hi3D\nfirst empowers the pre-trained video diffusion model with 3D-aware prior\n(camera pose condition), yielding multi-view images with low-resolution texture\ndetails. A 3D-aware video-to-video refiner is learnt to further scale up the\nmulti-view images with high-resolution texture details. Such high-resolution\nmulti-view images are further augmented with novel views through 3D Gaussian\nSplatting, which are finally leveraged to obtain high-fidelity meshes via 3D\nreconstruction. Extensive experiments on both novel view synthesis and single\nview reconstruction demonstrate that our Hi3D manages to produce superior\nmulti-view consistency images with highly-detailed textures. Source code and\ndata are available at \\url{https://github.com/yanghb22-fdu/Hi3D-Official}.\n","authors":["Haibo Yang","Yang Chen","Yingwei Pan","Ting Yao","Zhineng Chen","Chong-Wah Ngo","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2409.07452v1.pdf","comment":"ACM Multimedia 2024. Source code is available at\n  \\url{https://github.com/yanghb22-fdu/Hi3D-Official}"},{"id":"http://arxiv.org/abs/2409.07451v1","updated":"2024-09-11T17:58:50Z","published":"2024-09-11T17:58:50Z","title":"FreeEnhance: Tuning-Free Image Enhancement via Content-Consistent\n  Noising-and-Denoising Process","summary":"  The emergence of text-to-image generation models has led to the recognition\nthat image enhancement, performed as post-processing, would significantly\nimprove the visual quality of the generated images. Exploring diffusion models\nto enhance the generated images nevertheless is not trivial and necessitates to\ndelicately enrich plentiful details while preserving the visual appearance of\nkey content in the original image. In this paper, we propose a novel framework,\nnamely FreeEnhance, for content-consistent image enhancement using the\noff-the-shelf image diffusion models. Technically, FreeEnhance is a two-stage\nprocess that firstly adds random noise to the input image and then capitalizes\non a pre-trained image diffusion model (i.e., Latent Diffusion Models) to\ndenoise and enhance the image details. In the noising stage, FreeEnhance is\ndevised to add lighter noise to the region with higher frequency to preserve\nthe high-frequent patterns (e.g., edge, corner) in the original image. In the\ndenoising stage, we present three target properties as constraints to\nregularize the predicted noise, enhancing images with high acutance and high\nvisual quality. Extensive experiments conducted on the HPDv2 dataset\ndemonstrate that our FreeEnhance outperforms the state-of-the-art image\nenhancement models in terms of quantitative metrics and human preference. More\nremarkably, FreeEnhance also shows higher human preference compared to the\ncommercial image enhancement solution of Magnific AI.\n","authors":["Yang Luo","Yiheng Zhang","Zhaofan Qiu","Ting Yao","Zhineng Chen","Yu-Gang Jiang","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2409.07451v1.pdf","comment":"ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2409.07450v1","updated":"2024-09-11T17:56:48Z","published":"2024-09-11T17:56:48Z","title":"VMAS: Video-to-Music Generation via Semantic Alignment in Web Music\n  Videos","summary":"  We present a framework for learning to generate background music from video\ninputs. Unlike existing works that rely on symbolic musical annotations, which\nare limited in quantity and diversity, our method leverages large-scale web\nvideos accompanied by background music. This enables our model to learn to\ngenerate realistic and diverse music. To accomplish this goal, we develop a\ngenerative video-music Transformer with a novel semantic video-music alignment\nscheme. Our model uses a joint autoregressive and contrastive learning\nobjective, which encourages the generation of music aligned with high-level\nvideo content. We also introduce a novel video-beat alignment scheme to match\nthe generated music beats with the low-level motions in the video. Lastly, to\ncapture fine-grained visual cues in a video needed for realistic background\nmusic generation, we introduce a new temporal video encoder architecture,\nallowing us to efficiently process videos consisting of many densely sampled\nframes. We train our framework on our newly curated DISCO-MV dataset,\nconsisting of 2.2M video-music samples, which is orders of magnitude larger\nthan any prior datasets used for video music generation. Our method outperforms\nexisting approaches on the DISCO-MV and MusicCaps datasets according to various\nmusic generation evaluation metrics, including human evaluation. Results are\navailable at https://genjib.github.io/project_page/VMAs/index.html\n","authors":["Yan-Bo Lin","Yu Tian","Linjie Yang","Gedas Bertasius","Heng Wang"],"pdf_url":"https://arxiv.org/pdf/2409.07450v1.pdf","comment":"Project Page: https://genjib.github.io/project_page/VMAs/index.html"},{"id":"http://arxiv.org/abs/2409.05606v2","updated":"2024-09-11T12:06:57Z","published":"2024-09-09T13:39:47Z","title":"CustomContrast: A Multilevel Contrastive Perspective For Subject-Driven\n  Text-to-Image Customization","summary":"  Subject-driven text-to-image (T2I) customization has drawn significant\ninterest in academia and industry. This task enables pre-trained models to\ngenerate novel images based on unique subjects. Existing studies adopt a\nself-reconstructive perspective, focusing on capturing all details of a single\nimage, which will misconstrue the specific image's irrelevant attributes (e.g.,\nview, pose, and background) as the subject intrinsic attributes. This\nmisconstruction leads to both overfitting or underfitting of irrelevant and\nintrinsic attributes of the subject, i.e., these attributes are\nover-represented or under-represented simultaneously, causing a trade-off\nbetween similarity and controllability. In this study, we argue an ideal\nsubject representation can be achieved by a cross-differential perspective,\ni.e., decoupling subject intrinsic attributes from irrelevant attributes via\ncontrastive learning, which allows the model to focus more on intrinsic\nattributes through intra-consistency (features of the same subject are\nspatially closer) and inter-distinctiveness (features of different subjects\nhave distinguished differences). Specifically, we propose CustomContrast, a\nnovel framework, which includes a Multilevel Contrastive Learning (MCL)\nparadigm and a Multimodal Feature Injection (MFI) Encoder. The MCL paradigm is\nused to extract intrinsic features of subjects from high-level semantics to\nlow-level appearance through crossmodal semantic contrastive learning and\nmultiscale appearance contrastive learning. To facilitate contrastive learning,\nwe introduce the MFI encoder to capture cross-modal representations. Extensive\nexperiments show the effectiveness of CustomContrast in subject similarity and\ntext controllability.\n","authors":["Nan Chen","Mengqi Huang","Zhuowei Chen","Yang Zheng","Lei Zhang","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2409.05606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.13621v6","updated":"2024-09-11T06:13:30Z","published":"2024-04-21T11:21:27Z","title":"Attack on Scene Flow using Point Clouds","summary":"  Deep neural networks have made significant advancements in accurately\nestimating scene flow using point clouds, which is vital for many applications\nlike video analysis, action recognition, and navigation. The robustness of\nthese techniques, however, remains a concern, particularly in the face of\nadversarial attacks that have been proven to deceive state-of-the-art deep\nneural networks in many domains. Surprisingly, the robustness of scene flow\nnetworks against such attacks has not been thoroughly investigated. To address\nthis problem, the proposed approach aims to bridge this gap by introducing\nadversarial white-box attacks specifically tailored for scene flow networks.\nExperimental results show that the generated adversarial examples obtain up to\n33.7 relative degradation in average end-point error on the KITTI and\nFlyingThings3D datasets. The study also reveals the significant impact that\nattacks targeting point clouds in only one dimension or color channel have on\naverage end-point error. Analyzing the success and failure of these attacks on\nthe scene flow networks and their 2D optical flow network variants shows a\nhigher vulnerability for the optical flow networks. Code is available at\nhttps://github.com/aheldis/Attack-on-Scene-Flow-using-Point-Clouds.git.\n","authors":["Haniyeh Ehsani Oskouie","Mohammad-Shahram Moin","Shohreh Kasaei"],"pdf_url":"https://arxiv.org/pdf/2404.13621v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15757v2","updated":"2024-09-11T03:27:53Z","published":"2024-05-24T17:53:06Z","title":"Looking Backward: Streaming Video-to-Video Translation with Feature\n  Banks","summary":"  This paper introduces StreamV2V, a diffusion model that achieves real-time\nstreaming video-to-video (V2V) translation with user prompts. Unlike prior V2V\nmethods using batches to process limited frames, we opt to process frames in a\nstreaming fashion, to support unlimited frames. At the heart of StreamV2V lies\na backward-looking principle that relates the present to the past. This is\nrealized by maintaining a feature bank, which archives information from past\nframes. For incoming frames, StreamV2V extends self-attention to include banked\nkeys and values and directly fuses similar past features into the output. The\nfeature bank is continually updated by merging stored and new features, making\nit compact but informative. StreamV2V stands out for its adaptability and\nefficiency, seamlessly integrating with image diffusion models without\nfine-tuning. It can run 20 FPS on one A100 GPU, being 15x, 46x, 108x, and 158x\nfaster than FlowVid, CoDeF, Rerender, and TokenFlow, respectively. Quantitative\nmetrics and user studies confirm StreamV2V's exceptional ability to maintain\ntemporal consistency.\n","authors":["Feng Liang","Akio Kodaira","Chenfeng Xu","Masayoshi Tomizuka","Kurt Keutzer","Diana Marculescu"],"pdf_url":"https://arxiv.org/pdf/2405.15757v2.pdf","comment":"Project page: https://jeff-liangf.github.io/projects/streamv2v"},{"id":"http://arxiv.org/abs/2409.05405v2","updated":"2024-09-11T02:44:52Z","published":"2024-09-09T08:06:50Z","title":"A Survey of Multimodal Composite Editing and Retrieval","summary":"  In the real world, where information is abundant and diverse across different\nmodalities, understanding and utilizing various data types to improve retrieval\nsystems is a key focus of research. Multimodal composite retrieval integrates\ndiverse modalities such as text, image and audio, etc. to provide more\naccurate, personalized, and contextually relevant results. To facilitate a\ndeeper understanding of this promising direction, this survey explores\nmultimodal composite editing and retrieval in depth, covering image-text\ncomposite editing, image-text composite retrieval, and other multimodal\ncomposite retrieval. In this survey, we systematically organize the application\nscenarios, methods, benchmarks, experiments, and future directions. Multimodal\nlearning is a hot topic in large model era, and have also witnessed some\nsurveys in multimodal learning and vision-language models with transformers\npublished in the PAMI journal. To the best of our knowledge, this survey is the\nfirst comprehensive review of the literature on multimodal composite retrieval,\nwhich is a timely complement of multimodal fusion to existing reviews. To help\nreaders' quickly track this field, we build the project page for this survey,\nwhich can be found at\nhttps://github.com/fuxianghuang1/Multimodal-Composite-Editing-and-Retrieval.\n","authors":["Suyan Li","Fuxiang Huang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.05405v2.pdf","comment":"20 pages, 3 figures, and 11 tables"}]},"2024-09-10T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.06916v1","updated":"2024-09-10T23:58:27Z","published":"2024-09-10T23:58:27Z","title":"Interactive Counterfactual Exploration of Algorithmic Harms in\n  Recommender Systems","summary":"  Recommender systems have become integral to digital experiences, shaping user\ninteractions and preferences across various platforms. Despite their widespread\nuse, these systems often suffer from algorithmic biases that can lead to unfair\nand unsatisfactory user experiences. This study introduces an interactive tool\ndesigned to help users comprehend and explore the impacts of algorithmic harms\nin recommender systems. By leveraging visualizations, counterfactual\nexplanations, and interactive modules, the tool allows users to investigate how\nbiases such as miscalibration, stereotypes, and filter bubbles affect their\nrecommendations. Informed by in-depth user interviews, this tool benefits both\ngeneral users and researchers by increasing transparency and offering\npersonalized impact assessments, ultimately fostering a better understanding of\nalgorithmic biases and contributing to more equitable recommendation outcomes.\nThis work provides valuable insights for future research and practical\napplications in mitigating bias and enhancing fairness in machine learning\nalgorithms.\n","authors":["Yongsu Ahn","Quinn K Wolter","Jonilyn Dick","Janet Dick","Yu-Ru Lin"],"pdf_url":"https://arxiv.org/pdf/2409.06916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06793v1","updated":"2024-09-10T18:02:51Z","published":"2024-09-10T18:02:51Z","title":"Adversarial Attacks to Multi-Modal Models","summary":"  Multi-modal models have gained significant attention due to their powerful\ncapabilities. These models effectively align embeddings across diverse data\nmodalities, showcasing superior performance in downstream tasks compared to\ntheir unimodal counterparts. Recent study showed that the attacker can\nmanipulate an image or audio file by altering it in such a way that its\nembedding matches that of an attacker-chosen targeted input, thereby deceiving\ndownstream models. However, this method often underperforms due to inherent\ndisparities in data from different modalities. In this paper, we introduce\nCrossFire, an innovative approach to attack multi-modal models. CrossFire\nbegins by transforming the targeted input chosen by the attacker into a format\nthat matches the modality of the original image or audio file. We then\nformulate our attack as an optimization problem, aiming to minimize the angular\ndeviation between the embeddings of the transformed input and the modified\nimage or audio file. Solving this problem determines the perturbations to be\nadded to the original media. Our extensive experiments on six real-world\nbenchmark datasets reveal that CrossFire can significantly manipulate\ndownstream tasks, surpassing existing attacks. Additionally, we evaluate six\ndefensive strategies against CrossFire, finding that current defenses are\ninsufficient to counteract our CrossFire.\n","authors":["Zhihao Dou","Xin Hu","Haibo Yang","Zhuqing Liu","Minghong Fang"],"pdf_url":"https://arxiv.org/pdf/2409.06793v1.pdf","comment":"To appear in the ACM Workshop on Large AI Systems and Models with\n  Privacy and Safety Analysis 2024 (LAMPS '24)"},{"id":"http://arxiv.org/abs/2409.04667v2","updated":"2024-09-10T17:56:52Z","published":"2024-09-07T00:46:58Z","title":"QueryBuilder: Human-in-the-Loop Query Development for Information\n  Retrieval","summary":"  Frequently, users of an Information Retrieval (IR) system start with an\noverarching information need (a.k.a., an analytic task) and proceed to define\nfiner-grained queries covering various important aspects (i.e., sub-topics) of\nthat analytic task. We present a novel, interactive system called\n$\\textit{QueryBuilder}$, which allows a novice, English-speaking user to create\nqueries with a small amount of effort, through efficient exploration of an\nEnglish development corpus in order to rapidly develop cross-lingual\ninformation retrieval queries corresponding to the user's information needs.\nQueryBuilder performs near real-time retrieval of documents based on\nuser-entered search terms; the user looks through the retrieved documents and\nmarks sentences as relevant to the information needed. The marked sentences are\nused by the system as additional information in query formation and refinement:\nquery terms (and, optionally, event features, which capture event $'triggers'$\n(indicator terms) and agent/patient roles) are appropriately weighted, and a\nneural-based system, which better captures textual meaning, retrieves other\nrelevant content. The process of retrieval and marking is repeated as many\ntimes as desired, giving rise to increasingly refined queries in each\niteration. The final product is a fine-grained query used in Cross-Lingual\nInformation Retrieval (CLIR). Our experiments using analytic tasks and requests\nfrom the IARPA BETTER IR datasets show that with a small amount of effort (at\nmost 10 minutes per sub-topic), novice users can form $\\textit{useful}$\nfine-grained queries including in languages they don't understand. QueryBuilder\nalso provides beneficial capabilities to the traditional corpus exploration and\nquery formation process. A demonstration video is released at\nhttps://vimeo.com/734795835\n","authors":["Hemanth Kandula","Damianos Karakos","Haoling Qiu","Benjamin Rozonoyer","Ian Soboroff","Lee Tarlin","Bonan Min"],"pdf_url":"https://arxiv.org/pdf/2409.04667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06638v1","updated":"2024-09-10T16:48:05Z","published":"2024-09-10T16:48:05Z","title":"Critical Features Tracking on Triangulated Irregular Networks by a\n  Scale-Space Method","summary":"  The scale-space method is a well-established framework that constructs a\nhierarchical representation of an input signal and facilitates coarse-to-fine\nvisual reasoning. Considering the terrain elevation function as the input\nsignal, the scale-space method can identify and track significant topographic\nfeatures across different scales. The number of scales a feature persists,\ncalled its life span, indicates the importance of that feature. In this way,\nimportant topographic features of a landscape can be selected, which are useful\nfor many applications, including cartography, nautical charting, and land-use\nplanning. The scale-space methods developed for terrain data use gridded\nDigital Elevation Models (DEMs) to represent the terrain. However, gridded DEMs\nlack the flexibility to adapt to the irregular distribution of input data and\nthe varied topological complexity of different regions. Instead, Triangulated\nIrregular Networks (TINs) can be directly generated from irregularly\ndistributed point clouds and accurately preserve important features. In this\nwork, we introduce a novel scale-space analysis pipeline for TINs, addressing\nthe multiple challenges in extending grid-based scale-space methods to TINs.\nOur pipeline can efficiently identify and track topologically important\nfeatures on TINs. Moreover, it is capable of analyzing terrains with irregular\nboundaries, which poses challenges for grid-based methods. Comprehensive\nexperiments show that, compared to grid-based methods, our TIN-based pipeline\nis more efficient, accurate, and has better resolution robustness.\n","authors":["Haoan Feng","Yunting Song","Leila De Floriani"],"pdf_url":"https://arxiv.org/pdf/2409.06638v1.pdf","comment":"13pages, ACM SIGSPATIAL 2024"},{"id":"http://arxiv.org/abs/2409.05526v2","updated":"2024-09-10T16:46:10Z","published":"2024-09-09T11:35:35Z","title":"RBoard: A Unified Platform for Reproducible and Reusable Recommender\n  System Benchmarks","summary":"  Recommender systems research lacks standardized benchmarks for\nreproducibility and algorithm comparisons. We introduce RBoard, a novel\nframework addressing these challenges by providing a comprehensive platform for\nbenchmarking diverse recommendation tasks, including CTR prediction, Top-N\nrecommendation, and others. RBoard's primary objective is to enable fully\nreproducible and reusable experiments across these scenarios. The framework\nevaluates algorithms across multiple datasets within each task, aggregating\nresults for a holistic performance assessment. It implements standardized\nevaluation protocols, ensuring consistency and comparability. To facilitate\nreproducibility, all user-provided code can be easily downloaded and executed,\nallowing researchers to reliably replicate studies and build upon previous\nwork. By offering a unified platform for rigorous, reproducible evaluation\nacross various recommendation scenarios, RBoard aims to accelerate progress in\nthe field and establish a new standard for recommender systems benchmarking in\nboth academia and industry. The platform is available at https://rboard.org and\nthe demo video can be found at https://bit.ly/rboard-demo.\n","authors":["Xinyang Shao","Edoardo D'Amico","Gabor Fodor","Tri Kurniawan Wijaya"],"pdf_url":"https://arxiv.org/pdf/2409.05526v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.08921v2","updated":"2024-09-10T15:38:56Z","published":"2024-08-15T12:20:24Z","title":"Graph Retrieval-Augmented Generation: A Survey","summary":"  Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable\nsuccess in addressing the challenges of Large Language Models (LLMs) without\nnecessitating retraining. By referencing an external knowledge base, RAG\nrefines LLM outputs, effectively mitigating issues such as ``hallucination'',\nlack of domain-specific knowledge, and outdated information. However, the\ncomplex structure of relationships among different entities in databases\npresents challenges for RAG systems. In response, GraphRAG leverages structural\ninformation across entities to enable more precise and comprehensive retrieval,\ncapturing relational knowledge and facilitating more accurate, context-aware\nresponses. Given the novelty and potential of GraphRAG, a systematic review of\ncurrent technologies is imperative. This paper provides the first comprehensive\noverview of GraphRAG methodologies. We formalize the GraphRAG workflow,\nencompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced\nGeneration. We then outline the core technologies and training methods at each\nstage. Additionally, we examine downstream tasks, application domains,\nevaluation methodologies, and industrial use cases of GraphRAG. Finally, we\nexplore future research directions to inspire further inquiries and advance\nprogress in the field. In order to track recent progress in this field, we set\nup a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}.\n","authors":["Boci Peng","Yun Zhu","Yongchao Liu","Xiaohe Bo","Haizhou Shi","Chuntao Hong","Yan Zhang","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2408.08921v2.pdf","comment":"Ongoing work. Compared to the first version, several references have\n  been added and a GitHub repository link has been provided"},{"id":"http://arxiv.org/abs/2409.07500v1","updated":"2024-09-10T15:24:13Z","published":"2024-09-10T15:24:13Z","title":"DV-FSR: A Dual-View Target Attack Framework for Federated Sequential\n  Recommendation","summary":"  Federated recommendation (FedRec) preserves user privacy by enabling\ndecentralized training of personalized models, but this architecture is\ninherently vulnerable to adversarial attacks. Significant research has been\nconducted on targeted attacks in FedRec systems, motivated by commercial and\nsocial influence considerations. However, much of this work has largely\noverlooked the differential robustness of recommendation models. Moreover, our\nempirical findings indicate that existing targeted attack methods achieve only\nlimited effectiveness in Federated Sequential Recommendation (FSR) tasks.\nDriven by these observations, we focus on investigating targeted attacks in FSR\nand propose a novel dualview attack framework, named DV-FSR. This attack method\nuniquely combines a sampling-based explicit strategy with a contrastive\nlearning-based implicit gradient strategy to orchestrate a coordinated attack.\nAdditionally, we introduce a specific defense mechanism tailored for targeted\nattacks in FSR, aiming to evaluate the mitigation effects of the attack method\nwe proposed. Extensive experiments validate the effectiveness of our proposed\napproach on representative sequential models.\n","authors":["Qitao Qin","Yucong Luo","Mingyue Cheng","Qingyang Mao","Chenyi Lei"],"pdf_url":"https://arxiv.org/pdf/2409.07500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.04614v3","updated":"2024-09-10T14:37:00Z","published":"2024-05-07T18:58:32Z","title":"Multi-Margin Cosine Loss: Proposal and Application in Recommender\n  Systems","summary":"  Recommender systems guide users through vast amounts of information by\nsuggesting items based on their predicted preferences. Collaborative\nfiltering-based deep learning techniques have regained popularity due to their\nstraightforward nature, relying only on user-item interactions. Typically,\nthese systems consist of three main components: an interaction module, a loss\nfunction, and a negative sampling strategy. Initially, researchers focused on\nenhancing performance by developing complex interaction modules. However, there\nhas been a recent shift toward refining loss functions and negative sampling\nstrategies. This shift has led to an increased interest in contrastive\nlearning, which pulls similar pairs closer while pushing dissimilar ones apart.\nContrastive learning may bring challenges like high memory demands and\nunder-utilization of some negative samples. The proposed Multi-Margin Cosine\nLoss (MMCL) addresses these challenges by introducing multiple margins and\nvarying weights for negative samples. It efficiently utilizes not only the\nhardest negatives but also other non-trivial negatives, offers a simpler yet\neffective loss function that outperforms more complex methods, especially when\nresources are limited. Experiments on two well-known datasets demonstrated that\nMMCL achieved up to a 20\\% performance improvement compared to a baseline loss\nfunction when fewer number of negative samples are used.\n","authors":["Makbule Gulcin Ozsoy"],"pdf_url":"https://arxiv.org/pdf/2405.04614v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07606v2","updated":"2024-09-10T13:39:28Z","published":"2023-09-14T11:13:36Z","title":"Zero-shot Audio Topic Reranking using Large Language Models","summary":"  Multimodal Video Search by Examples (MVSE) investigates using video clips as\nthe query term for information retrieval, rather than the more traditional text\nquery. This enables far richer search modalities such as images, speaker,\ncontent, topic, and emotion. A key element for this process is highly rapid and\nflexible search to support large archives, which in MVSE is facilitated by\nrepresenting video attributes with embeddings. This work aims to compensate for\nany performance loss from this rapid archive search by examining reranking\napproaches. In particular, zero-shot reranking methods using large language\nmodels (LLMs) are investigated as these are applicable to any video archive\naudio content. Performance is evaluated for topic-based retrieval on a publicly\navailable video archive, the BBC Rewind corpus. Results demonstrate that\nreranking significantly improves retrieval ranking without requiring any\ntask-specific in-domain training data. Furthermore, three sources of\ninformation (ASR transcriptions, automatic summaries and synopses) as input for\nLLM reranking were compared. To gain a deeper understanding and further\ninsights into the performance differences and limitations of these text\nsources, we employ a fact-checking approach to analyse the information\nconsistency among them.\n","authors":["Mengjie Qian","Rao Ma","Adian Liusie","Erfan Loweimi","Kate M. Knill","Mark J. F. Gales"],"pdf_url":"https://arxiv.org/pdf/2309.07606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06464v1","updated":"2024-09-10T12:46:23Z","published":"2024-09-10T12:46:23Z","title":"Operational Advice for Dense and Sparse Retrievers: HNSW, Flat, or\n  Inverted Indexes?","summary":"  Practitioners working on dense retrieval today face a bewildering number of\nchoices. Beyond selecting the embedding model, another consequential choice is\nthe actual implementation of nearest-neighbor vector search. While best\npractices recommend HNSW indexes, flat vector indexes with brute-force search\nrepresent another viable option, particularly for smaller corpora and for rapid\nprototyping. In this paper, we provide experimental results on the BEIR dataset\nusing the open-source Lucene search library that explicate the tradeoffs\nbetween HNSW and flat indexes (including quantized variants) from the\nperspectives of indexing time, query evaluation performance, and retrieval\nquality. With additional comparisons between dense and sparse retrievers, our\nresults provide guidance for today's search practitioner in understanding the\ndesign space of dense and sparse retrievers. To our knowledge, we are the first\nto provide operational advice supported by empirical experiments in this\nregard.\n","authors":["Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2409.06464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00860v2","updated":"2024-09-10T10:52:30Z","published":"2024-09-01T22:33:29Z","title":"A Counterfactual Explanation Framework for Retrieval Models","summary":"  Explainability has become a crucial concern in today's world, aiming to\nenhance transparency in machine learning and deep learning models. Information\nretrieval is no exception to this trend. In existing literature on\nexplainability of information retrieval, the emphasis has predominantly been on\nillustrating the concept of relevance concerning a retrieval model. The\nquestions addressed include why a document is relevant to a query, why one\ndocument exhibits higher relevance than another, or why a specific set of\ndocuments is deemed relevant for a query.\n  However, limited attention has been given to understanding why a particular\ndocument is considered non-relevant to a query with respect to a retrieval\nmodel. In an effort to address this gap, our work focus on the question of what\nterms need to be added within a document to improve its ranking. This in turn\nanswers the question of which words played a role in not being favored by a\nretrieval model for a particular query. We use an optimization framework to\nsolve the above-mentioned research problem. % To the best of our knowledge, we\nmark the first attempt to tackle this specific counterfactual problem. Our\nexperiments show the effectiveness of our proposed approach in predicting\ncounterfactuals for both statistical (e.g. BM25) and deep-learning-based models\n(e.g. DRMM, DSSM, ColBERT).\n","authors":["Bhavik Chandna","Procheta Sen"],"pdf_url":"https://arxiv.org/pdf/2409.00860v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06377v1","updated":"2024-09-10T09:58:55Z","published":"2024-09-10T09:58:55Z","title":"Enhancing Sequential Recommendations through Multi-Perspective\n  Reflections and Iteration","summary":"  Sequence recommendation (SeqRec) aims to predict the next item a user will\ninteract with by understanding user intentions and leveraging collaborative\nfiltering information. Large language models (LLMs) have shown great promise in\nrecommendation tasks through prompt-based, fixed reflection libraries, and\nfine-tuning techniques. However, these methods face challenges, including lack\nof supervision, inability to optimize reflection sources, inflexibility to\ndiverse user needs, and high computational costs. Despite promising results,\ncurrent studies primarily focus on reflections of users' explicit preferences\n(e.g., item titles) while neglecting implicit preferences (e.g., brands) and\ncollaborative filtering information. This oversight hinders the capture of\npreference shifts and dynamic user behaviors. Additionally, existing approaches\nlack mechanisms for reflection evaluation and iteration, often leading to\nsuboptimal recommendations. To address these issues, we propose the Mixture of\nREflectors (MoRE) framework, designed to model and learn dynamic user\npreferences in SeqRec. Specifically, MoRE introduces three reflectors for\ngenerating LLM-based reflections on explicit preferences, implicit preferences,\nand collaborative signals. Each reflector incorporates a self-improving\nstrategy, termed refining-and-iteration, to evaluate and iteratively update\nreflections. Furthermore, a meta-reflector employs a contextual bandit\nalgorithm to select the most suitable expert and corresponding reflections for\neach user's recommendation, effectively capturing dynamic preferences.\nExtensive experiments on three real-world datasets demonstrate that MoRE\nconsistently outperforms state-of-the-art methods, requiring less training time\nand GPU memory compared to other LLM-based approaches in SeqRec.\n","authors":["Weicong Qin","Yi Xu","Weijie Yu","Chenglei Shen","Xiao Zhang","Ming He","Jianping Fan","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2409.06377v1.pdf","comment":"First 3 authors contributes equally to this work"},{"id":"http://arxiv.org/abs/2409.06297v1","updated":"2024-09-10T07:51:53Z","published":"2024-09-10T07:51:53Z","title":"User Preferences for Large Language Model versus Template-Based\n  Explanations of Movie Recommendations: A Pilot Study","summary":"  Recommender systems have become integral to our digital experiences, from\nonline shopping to streaming platforms. Still, the rationale behind their\nsuggestions often remains opaque to users. While some systems employ a\ngraph-based approach, offering inherent explainability through paths\nassociating recommended items and seed items, non-experts could not easily\nunderstand these explanations. A popular alternative is to convert graph-based\nexplanations into textual ones using a template and an algorithm, which we\ndenote here as ''template-based'' explanations. Yet, these can sometimes come\nacross as impersonal or uninspiring. A novel method would be to employ large\nlanguage models (LLMs) for this purpose, which we denote as ''LLM-based''. To\nassess the effectiveness of LLMs in generating more resonant explanations, we\nconducted a pilot study with 25 participants. They were presented with three\nexplanations: (1) traditional template-based, (2) LLM-based rephrasing of the\ntemplate output, and (3) purely LLM-based explanations derived from the\ngraph-based explanations. Although subject to high variance, preliminary\nfindings suggest that LLM-based explanations may provide a richer and more\nengaging user experience, further aligning with user expectations. This study\nsheds light on the potential limitations of current explanation methods and\noffers promising directions for leveraging large language models to improve\nuser satisfaction and trust in recommender systems.\n","authors":["Julien Albert","Martin Balfroid","Miriam Doh","Jeremie Bogaert","Luca La Fisca","Liesbet De Vos","Bryan Renard","Vincent Stragier","Emmanuel Jean"],"pdf_url":"https://arxiv.org/pdf/2409.06297v1.pdf","comment":"Presented to the Dutch-Belgian Workshop on Recommender Systems 2023\n  (14-15 December, 2023 - Antwerp, Belgium)"},{"id":"http://arxiv.org/abs/2409.06226v1","updated":"2024-09-10T05:41:40Z","published":"2024-09-10T05:41:40Z","title":"NLP-Powered Repository and Search Engine for Academic Papers: A Case\n  Study on Cyber Risk Literature with CyLit","summary":"  As the body of academic literature continues to grow, researchers face\nincreasing difficulties in effectively searching for relevant resources.\nExisting databases and search engines often fall short of providing a\ncomprehensive and contextually relevant collection of academic literature. To\naddress this issue, we propose a novel framework that leverages Natural\nLanguage Processing (NLP) techniques. This framework automates the retrieval,\nsummarization, and clustering of academic literature within a specific research\ndomain. To demonstrate the effectiveness of our approach, we introduce CyLit,\nan NLP-powered repository specifically designed for the cyber risk literature.\nCyLit empowers researchers by providing access to context-specific resources\nand enabling the tracking of trends in the dynamic and rapidly evolving field\nof cyber risk. Through the automatic processing of large volumes of data, our\nNLP-powered solution significantly enhances the efficiency and specificity of\nacademic literature searches. We compare the literature categorization results\nof CyLit to those presented in survey papers or generated by ChatGPT,\nhighlighting the distinctive insights this tool provides into cyber risk\nresearch literature. Using NLP techniques, we aim to revolutionize the way\nresearchers discover, analyze, and utilize academic resources, ultimately\nfostering advancements in various domains of knowledge.\n","authors":["Linfeng Zhang","Changyue Hu","Zhiyu Quan"],"pdf_url":"https://arxiv.org/pdf/2409.06226v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06177v1","updated":"2024-09-10T03:12:39Z","published":"2024-09-10T03:12:39Z","title":"HierLLM: Hierarchical Large Language Model for Question Recommendation","summary":"  Question recommendation is a task that sequentially recommends questions for\nstudents to enhance their learning efficiency. That is, given the learning\nhistory and learning target of a student, a question recommender is supposed to\nselect the question that will bring the most improvement for students. Previous\nmethods typically model the question recommendation as a sequential\ndecision-making problem, estimating students' learning state with the learning\nhistory, and feeding the learning state with the learning target to a neural\nnetwork to select the recommended question from a question set. However,\nprevious methods are faced with two challenges: (1) learning history is\nunavailable in the cold start scenario, which makes the recommender generate\ninappropriate recommendations; (2) the size of the question set is much large,\nwhich makes it difficult for the recommender to select the best question\nprecisely. To address the challenges, we propose a method called hierarchical\nlarge language model for question recommendation (HierLLM), which is a\nLLM-based hierarchical structure. The LLM-based structure enables HierLLM to\ntackle the cold start issue with the strong reasoning abilities of LLM. The\nhierarchical structure takes advantage of the fact that the number of concepts\nis significantly smaller than the number of questions, narrowing the range of\nselectable questions by first identifying the relevant concept for the\nto-recommend question, and then selecting the recommended question based on\nthat concept. This hierarchical structure reduces the difficulty of the\nrecommendation.To investigate the performance of HierLLM, we conduct extensive\nexperiments, and the results demonstrate the outstanding performance of\nHierLLM.\n","authors":["Yuxuan Liu","Haipeng Liu","Ting Long"],"pdf_url":"https://arxiv.org/pdf/2409.06177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06150v1","updated":"2024-09-10T01:52:29Z","published":"2024-09-10T01:52:29Z","title":"What makes a good concept anyway ?","summary":"  A good medical ontology is expected to cover its domain completely and\ncorrectly. On the other hand, large ontologies are hard to build, hard to\nunderstand, and hard to maintain. Thus, adding new concepts (often multi-word\nconcepts) to an existing ontology must be done judiciously. Only \"good\"\nconcepts should be added; however, it is difficult to define what makes a\nconcept good. In this research, we propose a metric to measure the goodness of\na concept. We identified factors that appear to influence goodness judgments of\nmedical experts and combined them into a single metric. These factors include\nconcept name length (in words), concept occurrence frequency in the medical\nliterature, and syntactic categories of component words. As an added factor, we\nused the simplicity of a term after mapping it into a specific foreign\nlanguage. We performed Bayesian optimization of factor weights to achieve\nmaximum agreement between the metric and three medical experts. The results\nshowed that our metric had a 50.67% overall agreement with the experts, as\nmeasured by Krippendorff's alpha.\n","authors":["Naren Khatwani","James Geller"],"pdf_url":"https://arxiv.org/pdf/2409.06150v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.06690v1","updated":"2024-09-10T17:54:00Z","published":"2024-09-10T17:54:00Z","title":"Benchmarking Sub-Genre Classification For Mainstage Dance Music","summary":"  Music classification, with a wide range of applications, is one of the most\nprominent tasks in music information retrieval. To address the absence of\ncomprehensive datasets and high-performing methods in the classification of\nmainstage dance music, this work introduces a novel benchmark comprising a new\ndataset and a baseline. Our dataset extends the number of sub-genres to cover\nmost recent mainstage live sets by top DJs worldwide in music festivals. A\ncontinuous soft labeling approach is employed to account for tracks that span\nmultiple sub-genres, preserving the inherent sophistication. For the baseline,\nwe developed deep learning models that outperform current state-of-the-art\nmultimodel language models, which struggle to identify house music sub-genres,\nemphasizing the need for specialized models trained on fine-grained datasets.\nOur benchmark is applicable to serve for application scenarios such as music\nrecommendation, DJ set curation, and interactive multimedia, where we also\nprovide video demos. Our code is on\n\\url{https://anonymous.4open.science/r/Mainstage-EDM-Benchmark/}.\n","authors":["Hongzhi Shu","Xinglin Li","Hongyu Jiang","Minghao Fu","Xinyu Li"],"pdf_url":"https://arxiv.org/pdf/2409.06690v1.pdf","comment":"Submitted to ICASSP 2025"},{"id":"http://arxiv.org/abs/2404.09654v2","updated":"2024-09-10T11:58:23Z","published":"2024-04-15T10:42:22Z","title":"Do LLMs Understand Visual Anomalies? Uncovering LLM's Capabilities in\n  Zero-shot Anomaly Detection","summary":"  Large vision-language models (LVLMs) are markedly proficient in deriving\nvisual representations guided by natural language. Recent explorations have\nutilized LVLMs to tackle zero-shot visual anomaly detection (VAD) challenges by\npairing images with textual descriptions indicative of normal and abnormal\nconditions, referred to as anomaly prompts. However, existing approaches depend\non static anomaly prompts that are prone to cross-semantic ambiguity, and\nprioritize global image-level representations over crucial local pixel-level\nimage-to-text alignment that is necessary for accurate anomaly localization. In\nthis paper, we present ALFA, a training-free approach designed to address these\nchallenges via a unified model. We propose a run-time prompt adaptation\nstrategy, which first generates informative anomaly prompts to leverage the\ncapabilities of a large language model (LLM). This strategy is enhanced by a\ncontextual scoring mechanism for per-image anomaly prompt adaptation and\ncross-semantic ambiguity mitigation. We further introduce a novel fine-grained\naligner to fuse local pixel-level semantics for precise anomaly localization,\nby projecting the image-text alignment from global to local semantic spaces.\nExtensive evaluations on MVTec and VisA datasets confirm ALFA's effectiveness\nin harnessing the language potential for zero-shot VAD, achieving significant\nPRO improvements of 12.1% on MVTec and 8.9% on VisA compared to\nstate-of-the-art approaches.\n","authors":["Jiaqi Zhu","Shaofeng Cai","Fang Deng","Beng Chin Ooi","Junran Wu"],"pdf_url":"https://arxiv.org/pdf/2404.09654v2.pdf","comment":"Accepted by MM'24 (Oral)"},{"id":"http://arxiv.org/abs/2409.06371v1","updated":"2024-09-10T09:53:06Z","published":"2024-09-10T09:53:06Z","title":"Distilling Generative-Discriminative Representations for Very\n  Low-Resolution Face Recognition","summary":"  Very low-resolution face recognition is challenging due to the serious loss\nof informative facial details in resolution degradation. In this paper, we\npropose a generative-discriminative representation distillation approach that\ncombines generative representation with cross-resolution aligned knowledge\ndistillation. This approach facilitates very low-resolution face recognition by\njointly distilling generative and discriminative models via two distillation\nmodules. Firstly, the generative representation distillation takes the encoder\nof a diffusion model pretrained for face super-resolution as the generative\nteacher to supervise the learning of the student backbone via feature\nregression, and then freezes the student backbone. After that, the\ndiscriminative representation distillation further considers a pretrained face\nrecognizer as the discriminative teacher to supervise the learning of the\nstudent head via cross-resolution relational contrastive distillation. In this\nway, the general backbone representation can be transformed into discriminative\nhead representation, leading to a robust and discriminative student model for\nvery low-resolution face recognition. Our approach improves the recovery of the\nmissing details in very low-resolution faces and achieves better knowledge\ntransfer. Extensive experiments on face datasets demonstrate that our approach\nenhances the recognition accuracy of very low-resolution faces, showcasing its\neffectiveness and adaptability.\n","authors":["Junzheng Zhang","Weijia Guo","Bochao Liu","Ruixin Shi","Yong Li","Shiming Ge"],"pdf_url":"https://arxiv.org/pdf/2409.06371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04388v3","updated":"2024-09-10T09:46:58Z","published":"2024-09-06T16:27:52Z","title":"Question-Answering Dense Video Events","summary":"  Multimodal Large Language Models (MLLMs) have shown excellent performance in\nquestion-answering of single-event videos. In this paper, we present\nquestion-answering dense video events, a novel task that requires answering and\ngrounding the dense-event questions in long videos, thus challenging MLLMs to\nfaithfully comprehend and reason about multiple events occurring over extended\ntime periods. To facilitate the study, we construct DeVE-QA - a dataset\nfeaturing 78K questions about 26K events on 10.6K long videos. We then\nbenchmark and show that existing MLLMs excelling at single-event QA struggle to\nperform well in DeVE-QA. For improvement, we propose DeVi, a novel\ntraining-free MLLM approach that highlights a hierarchical captioning module, a\ntemporal event memory module, and a self-consistency checking module to\nrespectively detect, contextualize and memorize, and ground dense-events in\nlong videos for question answering. Extensive experiments show that DeVi is\nsuperior at answering dense-event questions and grounding relevant video\nmoments. Compared with existing MLLMs, it achieves a remarkable increase of 4.1\npercent and 3.7 percent for G(round)QA accuracy on DeVE-QA and NExT-GQA\nrespectively.\n","authors":["Hangyu Qin","Junbin Xiao","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2409.04388v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02453v2","updated":"2024-09-10T08:20:36Z","published":"2024-09-04T05:19:57Z","title":"FrameCorr: Adaptive, Autoencoder-based Neural Compression for Video\n  Reconstruction in Resource and Timing Constrained Network Settings","summary":"  Despite the growing adoption of video processing via Internet of Things (IoT)\ndevices due to their cost-effectiveness, transmitting captured data to nearby\nservers poses challenges due to varying timing constraints and scarcity of\nnetwork bandwidth. Existing video compression methods face difficulties in\nrecovering compressed data when incomplete data is provided. Here, we introduce\nFrameCorr, a deep-learning based solution that utilizes previously received\ndata to predict the missing segments of a frame, enabling the reconstruction of\na frame from partially received data.\n","authors":["John Li","Shehab Sarar Ahmed","Deepak Nair"],"pdf_url":"https://arxiv.org/pdf/2409.02453v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08564v2","updated":"2024-09-10T07:30:02Z","published":"2024-06-12T18:07:06Z","title":"Machine Learning-Driven Open-Source Framework for Assessing QoE in\n  Multimedia Networks","summary":"  The Internet is integral to modern life, influencing communication, business,\nand lifestyles globally. As dependence on Internet services grows, the demand\nfor high-quality service delivery increases. Service providers must maintain\nhigh standards of quality of service and quality of experience (QoE) to ensure\nuser satisfaction. QoE, which reflects user satisfaction with service quality,\nis a key metric for multimedia services, yet it is challenging to measure due\nto its subjective nature and the complexities of real-time feedback. This paper\nintroduces a machine learning-based framework for objectively assessing QoE in\nmultimedia networks. The open-source framework complies with the ITU-T P.1203\nstandard. It automates data collection and user satisfaction prediction using\nkey network parameters such as delay, jitter, packet loss, bitrate, and\nthroughput. Using a dataset of over 20,000 records from various network\nconditions, the Random Forest model predicts the mean opinion score with 95.8%\naccuracy. Our framework addresses the limitations of existing QoE models by\nintegrating real-time data collection, machine learning predictions, and\nadherence to international standards. This approach enhances QoE evaluation\naccuracy and allows dynamic network resource management, optimizing performance\nand cost-efficiency. Its open-source nature encourages adaptation and extension\nfor various multimedia services. The findings significantly affect the\ntelecommunications industry in managing and optimizing multimedia services. The\nnetwork centric QoE prediction of the framework offers a scalable solution to\nimprove user satisfaction without the need for content-specific data. Future\nenhancements could include advanced machine learning models and broader\napplicability to digital services. This research contributes a practical,\nstandardized tool for QoE assessment across diverse networks and platforms.\n","authors":["Parsa Hassani Shariat Panahi","Amir Hossein Jalilvand","Abolfazl Diyanat"],"pdf_url":"https://arxiv.org/pdf/2406.08564v2.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.06224v1","updated":"2024-09-10T05:28:38Z","published":"2024-09-10T05:28:38Z","title":"MIP-GAF: A MLLM-annotated Benchmark for Most Important Person\n  Localization and Group Context Understanding","summary":"  Estimating the Most Important Person (MIP) in any social event setup is a\nchallenging problem mainly due to contextual complexity and scarcity of labeled\ndata. Moreover, the causality aspects of MIP estimation are quite subjective\nand diverse. To this end, we aim to address the problem by annotating a\nlarge-scale `in-the-wild' dataset for identifying human perceptions about the\n`Most Important Person (MIP)' in an image. The paper provides a thorough\ndescription of our proposed Multimodal Large Language Model (MLLM) based data\nannotation strategy, and a thorough data quality analysis. Further, we perform\na comprehensive benchmarking of the proposed dataset utilizing state-of-the-art\nMIP localization methods, indicating a significant drop in performance compared\nto existing datasets. The performance drop shows that the existing MIP\nlocalization algorithms must be more robust with respect to `in-the-wild'\nsituations. We believe the proposed dataset will play a vital role in building\nthe next-generation social situation understanding methods. The code and data\nis available at https://github.com/surbhimadan92/MIP-GAF.\n","authors":["Surbhi Madan","Shreya Ghosh","Lownish Rai Sookha","M. A. Ganaie","Ramanathan Subramanian","Abhinav Dhall","Tom Gedeon"],"pdf_url":"https://arxiv.org/pdf/2409.06224v1.pdf","comment":"Accepted for publication at WACV 2025"},{"id":"http://arxiv.org/abs/2409.06207v1","updated":"2024-09-10T04:24:22Z","published":"2024-09-10T04:24:22Z","title":"Design and Implementation of Online Live Streaming System Using A 3D\n  Engine","summary":"  With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement.\n","authors":["Aizierjiang Aiersilan"],"pdf_url":"https://arxiv.org/pdf/2409.06207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06135v1","updated":"2024-09-10T01:07:20Z","published":"2024-09-10T01:07:20Z","title":"Draw an Audio: Leveraging Multi-Instruction for Video-to-Audio Synthesis","summary":"  Foley is a term commonly used in filmmaking, referring to the addition of\ndaily sound effects to silent films or videos to enhance the auditory\nexperience. Video-to-Audio (V2A), as a particular type of automatic foley task,\npresents inherent challenges related to audio-visual synchronization. These\nchallenges encompass maintaining the content consistency between the input\nvideo and the generated audio, as well as the alignment of temporal and\nloudness properties within the video. To address these issues, we construct a\ncontrollable video-to-audio synthesis model, termed Draw an Audio, which\nsupports multiple input instructions through drawn masks and loudness signals.\nTo ensure content consistency between the synthesized audio and target video,\nwe introduce the Mask-Attention Module (MAM), which employs masked video\ninstruction to enable the model to focus on regions of interest. Additionally,\nwe implement the Time-Loudness Module (TLM), which uses an auxiliary loudness\nsignal to ensure the synthesis of sound that aligns with the video in both\nloudness and temporal dimensions. Furthermore, we have extended a large-scale\nV2A dataset, named VGGSound-Caption, by annotating caption prompts. Extensive\nexperiments on challenging benchmarks across two large-scale V2A datasets\nverify Draw an Audio achieves the state-of-the-art. Project page:\nhttps://yannqi.github.io/Draw-an-Audio/.\n","authors":["Qi Yang","Binjie Mao","Zili Wang","Xing Nie","Pengfei Gao","Ying Guo","Cheng Zhen","Pengfei Yan","Shiming Xiang"],"pdf_url":"https://arxiv.org/pdf/2409.06135v1.pdf","comment":"14 pages, 11 figures"}]},"2024-09-09T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.05667v2","updated":"2024-09-09T23:46:59Z","published":"2024-08-11T01:14:13Z","title":"PhishLang: A Lightweight, Client-Side Phishing Detection Framework using\n  MobileBERT for Real-Time, Explainable Threat Mitigation","summary":"  In this paper, we introduce PhishLang, an open-source, lightweight language\nmodel specifically designed for phishing website detection through contextual\nanalysis of the website. Unlike traditional heuristic or machine learning\nmodels that rely on static features and struggle to adapt to new threats, and\ndeep learning models that are computationally intensive, our model leverages\nMobileBERT, a fast and memory-efficient variant of the BERT architecture, to\nlearn granular features characteristic of phishing attacks. PhishLang operates\nwith minimal data preprocessing and offers performance comparable to leading\ndeep learning anti-phishing tools, while being significantly faster and less\nresource-intensive. Over a 3.5-month testing period, PhishLang successfully\nidentified 25,796 phishing URLs, many of which were undetected by popular\nantiphishing blocklists, thus demonstrating its potential to enhance current\ndetection measures. Capitalizing on PhishLang's resource efficiency, we release\nthe first open-source fully client-side Chromium browser extension that\nprovides inference locally without requiring to consult an online blocklist and\ncan be run on low-end systems with no impact on inference times. Our\nimplementation not only outperforms prevalent (server-side) phishing tools, but\nis significantly more effective than the limited commercial client-side\nmeasures available. Furthermore, we study how PhishLang can be integrated with\nGPT-3.5 Turbo to create explainable blocklisting -- which, upon detection of a\nwebsite, provides users with detailed contextual information about the features\nthat led to a website being marked as phishing.\n","authors":["Sayak Saha Roy","Shirin Nilizadeh"],"pdf_url":"https://arxiv.org/pdf/2408.05667v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05806v1","updated":"2024-09-09T17:11:51Z","published":"2024-09-09T17:11:51Z","title":"Benchmarking Chinese Knowledge Rectification in Large Language Models","summary":"  While Large Language Models (LLMs) exhibit remarkable generative\ncapabilities, they are not without flaws, particularly in the form of\nhallucinations. This issue is even more pronounced when LLMs are applied to\nspecific languages and domains. For example, LLMs may generate nonsense\ninformation when handling Chinese ancient poetry, proverbs, or idioms, owing to\nthe lack of specific knowledge. To this end, this paper introduces a benchmark\nfor rectifying Chinese knowledge in LLMs via knowledge editing. Specifically,\nwe introduce a new Chinese dataset, CKnowEdit, by collecting seven type of\nknowledge from various sources, including classical texts, idioms, and content\nfrom Baidu Tieba Ruozhiba, thereby accounting for the unique polyphony,\nantithesis, and logical constructs inherent in the Chinese language. Through\nthe analysis of this dataset, we uncover the challenges faced by current LLMs\nin mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques on this dataset unveil the substantial scope for advancement\nin the rectification of Chinese knowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit.\n","authors":["Tianhe Lu","Jizhan Fang","Yunzhi Yao","Xin Xu","Ningyu Zhang","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2409.05806v1.pdf","comment":"Ongoing work; code and dataset are available at\n  https://github.com/zjunlp/EasyEdit"},{"id":"http://arxiv.org/abs/2409.07497v1","updated":"2024-09-09T16:46:47Z","published":"2024-09-09T16:46:47Z","title":"OneEdit: A Neural-Symbolic Collaboratively Knowledge Editing System","summary":"  Knowledge representation has been a central aim of AI since its inception.\nSymbolic Knowledge Graphs (KGs) and neural Large Language Models (LLMs) can\nboth represent knowledge. KGs provide highly accurate and explicit knowledge\nrepresentation, but face scalability issue; while LLMs offer expansive coverage\nof knowledge, but incur significant training costs and struggle with precise\nand reliable knowledge manipulation. To this end, we introduce OneEdit, a\nneural-symbolic prototype system for collaborative knowledge editing using\nnatural language, which facilitates easy-to-use knowledge management with KG\nand LLM. OneEdit consists of three modules: 1) The Interpreter serves for user\ninteraction with natural language; 2) The Controller manages editing requests\nfrom various users, leveraging the KG with rollbacks to handle knowledge\nconflicts and prevent toxic knowledge attacks; 3) The Editor utilizes the\nknowledge from the Controller to edit KG and LLM. We conduct experiments on two\nnew datasets with KGs which demonstrate that OneEdit can achieve superior\nperformance.\n","authors":["Ningyu Zhang","Zekun Xi","Yujie Luo","Peng Wang","Bozhong Tian","Yunzhi Yao","Jintian Zhang","Shumin Deng","Mengshu Sun","Lei Liang","Zhiqiang Zhang","Xiaowei Zhu","Jun Zhou","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2409.07497v1.pdf","comment":"LLM+KG@VLDB2024, code is available at\n  https://github.com/zjunlp/OneEdit"},{"id":"http://arxiv.org/abs/2409.05692v1","updated":"2024-09-09T15:05:27Z","published":"2024-09-09T15:05:27Z","title":"Extracting the U.S. building types from OpenStreetMap data","summary":"  Building type information is crucial for population estimation, traffic\nplanning, urban planning, and emergency response applications. Although\nessential, such data is often not readily available. To alleviate this problem,\nthis work creates a comprehensive dataset by providing\nresidential/non-residential building classification covering the entire United\nStates. We propose and utilize an unsupervised machine learning method to\nclassify building types based on building footprints and available\nOpenStreetMap information. The classification result is validated using\nauthoritative ground truth data for select counties in the U.S. The validation\nshows a high precision for non-residential building classification and a high\nrecall for residential buildings. We identified various approaches to improving\nthe quality of the classification, such as removing sheds and garages from the\ndataset. Furthermore, analyzing the misclassifications revealed that they are\nmainly due to missing and scarce metadata in OSM. A major result of this work\nis the resulting dataset of classifying 67,705,475 buildings. We hope that this\ndata is of value to the scientific community, including urban and\ntransportation planners.\n","authors":["Henrique F. de Arruda","Sandro M. Reia","Shiyang Ruan","Kuldip S. Atwal","Hamdi Kavak","Taylor Anderson","Dieter Pfoser"],"pdf_url":"https://arxiv.org/pdf/2409.05692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05677v1","updated":"2024-09-09T14:44:19Z","published":"2024-09-09T14:44:19Z","title":"RegNLP in Action: Facilitating Compliance Through Automated Information\n  Retrieval and Answer Generation","summary":"  Regulatory documents, issued by governmental regulatory bodies, establish\nrules, guidelines, and standards that organizations must adhere to for legal\ncompliance. These documents, characterized by their length, complexity and\nfrequent updates, are challenging to interpret, requiring significant\nallocation of time and expertise on the part of organizations to ensure ongoing\ncompliance.Regulatory Natural Language Processing (RegNLP) is a\nmultidisciplinary subfield aimed at simplifying access to and interpretation of\nregulatory rules and obligations. We define an Automated Question-Passage\nGeneration task for RegNLP, create the ObliQA dataset containing 27,869\nquestions derived from the Abu Dhabi Global Markets (ADGM) financial regulation\ndocument collection, design a baseline Regulatory Information Retrieval and\nAnswer Generation system, and evaluate it with RePASs, a novel evaluation\nmetric that tests whether generated answers accurately capture all relevant\nobligations and avoid contradictions.\n","authors":["Tuba Gokhan","Kexin Wang","Iryna Gurevych","Ted Briscoe"],"pdf_url":"https://arxiv.org/pdf/2409.05677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05633v1","updated":"2024-09-09T14:04:17Z","published":"2024-09-09T14:04:17Z","title":"Enhancing Graph Contrastive Learning with Reliable and Informative\n  Augmentation for Recommendation","summary":"  Graph neural network (GNN) has been a powerful approach in collaborative\nfiltering (CF) due to its ability to model high-order user-item relationships.\nRecently, to alleviate the data sparsity and enhance representation learning,\nmany efforts have been conducted to integrate contrastive learning (CL) with\nGNNs. Despite the promising improvements, the contrastive view generation based\non structure and representation perturbations in existing methods potentially\ndisrupts the collaborative information in contrastive views, resulting in\nlimited effectiveness of positive alignment. To overcome this issue, we propose\nCoGCL, a novel framework that aims to enhance graph contrastive learning by\nconstructing contrastive views with stronger collaborative information via\ndiscrete codes. The core idea is to map users and items into discrete codes\nrich in collaborative information for reliable and informative contrastive view\ngeneration. To this end, we initially introduce a multi-level vector quantizer\nin an end-to-end manner to quantize user and item representations into discrete\ncodes. Based on these discrete codes, we enhance the collaborative information\nof contrastive views by considering neighborhood structure and semantic\nrelevance respectively. For neighborhood structure, we propose virtual neighbor\naugmentation by treating discrete codes as virtual neighbors, which expands an\nobserved user-item interaction into multiple edges involving discrete codes.\nRegarding semantic relevance, we identify similar users/items based on shared\ndiscrete codes and interaction targets to generate the semantically relevant\nview. Through these strategies, we construct contrastive views with stronger\ncollaborative information and develop a triple-view graph contrastive learning\napproach. Extensive experiments on four public datasets demonstrate the\neffectiveness of our proposed approach.\n","authors":["Bowen Zheng","Junjie Zhang","Hongyu Lu","Yu Chen","Ming Chen","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2409.05633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05546v1","updated":"2024-09-09T12:11:53Z","published":"2024-09-09T12:11:53Z","title":"End-to-End Learnable Item Tokenization for Generative Recommendation","summary":"  Recently, generative recommendation has emerged as a promising new paradigm\nthat directly generates item identifiers for recommendation. However, a key\nchallenge lies in how to effectively construct item identifiers that are\nsuitable for recommender systems. Existing methods typically decouple item\ntokenization from subsequent generative recommendation training, likely\nresulting in suboptimal performance. To address this limitation, we propose\nETEGRec, a novel End-To-End Generative Recommender by seamlessly integrating\nitem tokenization and generative recommendation. Our framework is developed\nbased on the dual encoder-decoder architecture, which consists of an item\ntokenizer and a generative recommender. In order to achieve mutual enhancement\nbetween the two components, we propose a recommendation-oriented alignment\napproach by devising two specific optimization objectives: sequence-item\nalignment and preference-semantic alignment. These two alignment objectives can\neffectively couple the learning of item tokenizer and generative recommender,\nthereby fostering the mutual enhancement between the two components. Finally,\nwe further devise an alternating optimization method, to facilitate stable and\neffective end-to-end learning of the entire framework. Extensive experiments\ndemonstrate the effectiveness of our proposed framework compared to a series of\ntraditional sequential recommendation models and generative recommendation\nbaselines.\n","authors":["Enze Liu","Bowen Zheng","Cheng Ling","Lantao Hu","Han Li","Wayne Xin Zhao"],"pdf_url":"https://arxiv.org/pdf/2409.05546v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05512v1","updated":"2024-09-09T11:10:45Z","published":"2024-09-09T11:10:45Z","title":"DatAasee -- A Metadata-Lake as Metadata Catalog for a Virtual Data-Lake","summary":"  Metadata management for distributed data sources is a long-standing but\never-growing problem. To counter this challenge in a research-data and\nlibrary-oriented setting, this work constructs a data architecture, derived\nfrom the data-lake: the metadata-lake. A proof-of-concept implementation of\nthis proposed metadata system is presented and evaluated as well.\n","authors":["Christian Himpe"],"pdf_url":"https://arxiv.org/pdf/2409.05512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03753v2","updated":"2024-09-09T10:04:00Z","published":"2024-09-05T17:59:15Z","title":"WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild","summary":"  The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.\n","authors":["Yuntian Deng","Wenting Zhao","Jack Hessel","Xiang Ren","Claire Cardie","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2409.03753v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05461v1","updated":"2024-09-09T09:42:31Z","published":"2024-09-09T09:42:31Z","title":"Recommender Systems Algorithm Selection for Ranking Prediction on\n  Implicit Feedback Datasets","summary":"  The recommender systems algorithm selection problem for ranking prediction on\nimplicit feedback datasets is under-explored. Traditional approaches in\nrecommender systems algorithm selection focus predominantly on rating\nprediction on explicit feedback datasets, leaving a research gap for ranking\nprediction on implicit feedback datasets. Algorithm selection is a critical\nchallenge for nearly every practitioner in recommender systems. In this work,\nwe take the first steps toward addressing this research gap. We evaluate the\nNDCG@10 of 24 recommender systems algorithms, each with two hyperparameter\nconfigurations, on 72 recommender systems datasets. We train four optimized\nmachine-learning meta-models and one automated machine-learning meta-model with\nthree different settings on the resulting meta-dataset. Our results show that\nthe predictions of all tested meta-models exhibit a median Spearman correlation\nranging from 0.857 to 0.918 with the ground truth. We show that the median\nSpearman correlation between meta-model predictions and the ground truth\nincreases by an average of 0.124 when the meta-model is optimized to predict\nthe ranking of algorithms instead of their performance. Furthermore, in terms\nof predicting the best algorithm for an unknown dataset, we demonstrate that\nthe best optimized traditional meta-model, e.g., XGBoost, achieves a recall of\n48.6%, outperforming the best tested automated machine learning meta-model,\ne.g., AutoGluon, which achieves a recall of 47.2%.\n","authors":["Lukas Wegmeth","Tobias Vente","Joeran Beel"],"pdf_url":"https://arxiv.org/pdf/2409.05461v1.pdf","comment":"Accepted for presentation at the 18th ACM Conference on Recommender\n  Systems in the Late-Breaking Results Track"},{"id":"http://arxiv.org/abs/2409.05925v1","updated":"2024-09-09T08:29:39Z","published":"2024-09-09T08:29:39Z","title":"Assessing SPARQL capabilities of Large Language Models","summary":"  The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs)\noffers significant synergistic potential for knowledge-driven applications. One\npossible integration is the interpretation and generation of formal languages,\nsuch as those used in the Semantic Web, with SPARQL being a core technology for\naccessing KGs. In this paper, we focus on measuring out-of-the box capabilities\nof LLMs to work with SPARQL and more specifically with SPARQL SELECT queries\napplying a quantitative approach.\n  We implemented various benchmarking tasks in the LLM-KG-Bench framework for\nautomated execution and evaluation with several LLMs. The tasks assess\ncapabilities along the dimensions of syntax, semantic read, semantic create,\nand the role of knowledge graph prompt inclusion.\n  With this new benchmarking tasks, we evaluated a selection of GPT, Gemini,\nand Claude models. Our findings indicate that working with SPARQL SELECT\nqueries is still challenging for LLMs and heavily depends on the specific LLM\nas well as the complexity of the task. While fixing basic syntax errors seems\nto pose no problems for the best of the current LLMs evaluated, creating\nsemantically correct SPARQL SELECT queries is difficult in several cases.\n","authors":["Lars-Peter Meyer","Johannes Frey","Felix Brei","Natanael Arndt"],"pdf_url":"https://arxiv.org/pdf/2409.05925v1.pdf","comment":"peer reviewed publication at NLP4KGc @ Semantics 2024, see\n  https://sites.google.com/view/3rdnlp4kgc"},{"id":"http://arxiv.org/abs/2409.05417v1","updated":"2024-09-09T08:19:43Z","published":"2024-09-09T08:19:43Z","title":"Replicability Measures for Longitudinal Information Retrieval Evaluation","summary":"  Information Retrieval (IR) systems are exposed to constant changes in most\ncomponents. Documents are created, updated, or deleted, the information needs\nare changing, and even relevance might not be static. While it is generally\nexpected that the IR systems retain a consistent utility for the users, test\ncollection evaluations rely on a fixed experimental setup. Based on the\nLongEval shared task and test collection, this work explores how the\neffectiveness measured in evolving experiments can be assessed. Specifically,\nthe persistency of effectiveness is investigated as a replicability task. It is\nobserved how the effectiveness progressively deteriorates over time compared to\nthe initial measurement. Employing adapted replicability measures provides\nfurther insight into the persistence of effectiveness. The ranking of systems\nvaries across retrieval measures and time. In conclusion, it was found that the\nmost effective systems are not necessarily the ones with the most persistent\nperformance.\n","authors":["J√ºri Keller","Timo Breuer","Philipp Schaer"],"pdf_url":"https://arxiv.org/pdf/2409.05417v1.pdf","comment":"Experimental IR Meets Multilinguality, Multimodality, and Interaction\n  - 15th International Conference of the CLEF Association, CLEF 2024, Grenoble,\n  France, September 9-12, 2024, Proceedings. arXiv admin note: text overlap\n  with arXiv:2308.10549"},{"id":"http://arxiv.org/abs/2409.05401v1","updated":"2024-09-09T07:57:43Z","published":"2024-09-09T07:57:43Z","title":"NLLB-E5: A Scalable Multilingual Retrieval Model","summary":"  Despite significant progress in multilingual information retrieval, the lack\nof models capable of effectively supporting multiple languages, particularly\nlow-resource like Indic languages, remains a critical challenge. This paper\npresents NLLB-E5: A Scalable Multilingual Retrieval Model. NLLB-E5 leverages\nthe in-built multilingual capabilities in the NLLB encoder for translation\ntasks. It proposes a distillation approach from multilingual retriever E5 to\nprovide a zero-shot retrieval approach handling multiple languages, including\nall major Indic languages, without requiring multilingual training data. We\nevaluate the model on a comprehensive suite of existing benchmarks, including\nHindi-BEIR, highlighting its robust performance across diverse languages and\ntasks. Our findings uncover task and domain-specific challenges, providing\nvaluable insights into the retrieval performance, especially for low-resource\nlanguages. NLLB-E5 addresses the urgent need for an inclusive, scalable, and\nlanguage-agnostic text retrieval model, advancing the field of multilingual\ninformation access and promoting digital inclusivity for millions of users\nglobally.\n","authors":["Arkadeep Acharya","Rudra Murthy","Vishwajeet Kumar","Jaydeep Sen"],"pdf_url":"https://arxiv.org/pdf/2409.05401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03893v2","updated":"2024-09-09T07:47:58Z","published":"2024-09-05T19:59:42Z","title":"Understanding Fairness in Recommender Systems: A Healthcare Perspective","summary":"  Fairness in AI-driven decision-making systems has become a critical concern,\nespecially when these systems directly affect human lives. This paper explores\nthe public's comprehension of fairness in healthcare recommendations. We\nconducted a survey where participants selected from four fairness metrics --\nDemographic Parity, Equal Accuracy, Equalized Odds, and Positive Predictive\nValue -- across different healthcare scenarios to assess their understanding of\nthese concepts. Our findings reveal that fairness is a complex and often\nmisunderstood concept, with a generally low level of public understanding\nregarding fairness metrics in recommender systems. This study highlights the\nneed for enhanced information and education on algorithmic fairness to support\ninformed decision-making in using these systems. Furthermore, the results\nsuggest that a one-size-fits-all approach to fairness may be insufficient,\npointing to the importance of context-sensitive designs in developing equitable\nAI systems.\n","authors":["Veronica Kecki","Alan Said"],"pdf_url":"https://arxiv.org/pdf/2409.03893v2.pdf","comment":"Accepted to the 18th ACM Conference on Recommender Systems"},{"id":"http://arxiv.org/abs/2407.14482v2","updated":"2024-09-09T06:19:07Z","published":"2024-07-19T17:35:47Z","title":"ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG\n  Capabilities","summary":"  In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K\ncontext window, designed to bridge the gap between open-source LLMs and leading\nproprietary models (e.g., GPT-4-Turbo) in long-context understanding and\nretrieval-augmented generation (RAG) capabilities. These two capabilities are\nessential for LLMs to process large volumes of information that cannot fit into\na single prompt and are complementary to each other, depending on the\ndownstream tasks and computational budgets. We present a detailed continued\ntraining recipe to extend the context window of Llama3-70B-base from 8K to 128K\ntokens, along with a three-stage instruction tuning process to enhance the\nmodel's instruction-following, RAG performance, and long-context understanding\ncapabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model\noutperforms most existing state-of-the-art models, including\nGPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-Instruct, on\nultra-long tasks beyond 100K tokens, as well as on the RAG benchmark using only\na 4K context window, showing the strong long context capability across varying\nsequence lengths. We further provide extensive comparisons between direct\nlong-context and RAG solutions using the same state-of-the-art long-context\nLLMs. Interestingly, we find that the performance of strong long-context LLMs\nusing RAG improves when retrieving a larger number of chunks. With a large set\nof top-k chunks, RAG consistently outperforms direct long-context solution\nusing the same state-of-the-art long-context models (e.g., Llama3-ChatQA-2-70B\nand Qwen2-72B-Instruct) on both 32K benchmarks and real-world 128K tasks. To\nadvance research in this field, we open-sourced the model weights, training\ndata, and the evaluation setup for the for the community:\nhttps://chatqa2-project.github.io/\n","authors":["Peng Xu","Wei Ping","Xianchao Wu","Chejian Xu","Zihan Liu","Mohammad Shoeybi","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2407.14482v2.pdf","comment":"v2: major update with significantly improved results"}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.06051v1","updated":"2024-09-09T20:22:00Z","published":"2024-09-09T20:22:00Z","title":"REVISION: A Roadmap on Adaptive Video Streaming Optimization","summary":"  Due to the soaring popularity of video applications and the consequent rise\nin video traffic on the Internet, technologies like HTTP Adaptive Streaming\n(HAS) are crucial for delivering high Quality of Experience (QoE) to consumers.\nHAS technology enables video players on consumer devices to enhance viewer\nengagement by dynamically adapting video content quality based on network\nconditions. This is especially relevant for consumer electronics as it ensures\nan optimized viewing experience across a variety of devices, from smartphones\nto smart TVs. This paper introduces REVISION, an efficient roadmap designed to\nenhance adaptive video streaming, a core feature of modern consumer\nelectronics. The REVISION optimization triangle highlights three essential\naspects for improving streaming: Objective, Input Space, and Action Domain.\nAdditionally, REVISION proposes a novel layer-based architecture tailored to\nrefine video streaming systems, comprising Application, Control and Management,\nand Resource layers. Each layer is designed to optimize different components of\nthe streaming process, which is directly linked to the performance and\nefficiency of consumer devices. By adopting the principles of the REVISION,\nmanufacturers and developers can significantly improve the streaming\ncapabilities of consumer electronics, thereby enriching the consumer's\nmultimedia experience and accommodating the increasing demand for high-quality,\nreal-time video content. This approach addresses the complexities of today's\ndiverse video streaming ecosystem and paves the way for future advancements in\nconsumer technology.\n","authors":["Farzad Tashtarian","Christian Timmerer"],"pdf_url":"https://arxiv.org/pdf/2409.06051v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.05772v1","updated":"2024-09-09T16:33:40Z","published":"2024-09-09T16:33:40Z","title":"A CLIP-based siamese approach for meme classification","summary":"  Memes are an increasingly prevalent element of online discourse in social\nnetworks, especially among young audiences. They carry ideas and messages that\nrange from humorous to hateful, and are widely consumed. Their potentially high\nimpact requires adequate means of control to moderate their use in large scale.\nIn this work, we propose SimCLIP a deep learning-based architecture for\ncross-modal understanding of memes, leveraging a pre-trained CLIP encoder to\nproduce context-aware embeddings and a Siamese fusion technique to capture the\ninteractions between text and image. We perform an extensive experimentation on\nseven meme classification tasks across six datasets. We establish a new state\nof the art in Memotion7k with a 7.25% relative F1-score improvement, and\nachieve super-human performance on Harm-P with 13.73% F1-Score improvement. Our\napproach demonstrates the potential for compact meme classification models,\nenabling accurate and efficient meme monitoring. We share our code at\nhttps://github.com/jahuerta92/meme-classification-simclip\n","authors":["Javier Huertas-Tato","Christos Koutlis","Symeon Papadopoulos","David Camacho","Ioannis Kompatsiaris"],"pdf_url":"https://arxiv.org/pdf/2409.05772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05750v1","updated":"2024-09-09T16:05:40Z","published":"2024-09-09T16:05:40Z","title":"A Toolkit for Joint Speaker Diarization and Identification with\n  Application to Speaker-Attributed ASR","summary":"  We present a modular toolkit to perform joint speaker diarization and speaker\nidentification. The toolkit can leverage on multiple models and algorithms\nwhich are defined in a configuration file. Such flexibility allows our system\nto work properly in various conditions (e.g., multiple registered speakers'\nsets, acoustic conditions and languages) and across application domains (e.g.\nmedia monitoring, institutional, speech analytics). In this demonstration we\nshow a practical use-case in which speaker-related information is used jointly\nwith automatic speech recognition engines to generate speaker-attributed\ntranscriptions. To achieve that, we employ a user-friendly web-based interface\nto process audio and video inputs with the chosen configuration.\n","authors":["Giovanni Morrone","Enrico Zovato","Fabio Brugnara","Enrico Sartori","Leonardo Badino"],"pdf_url":"https://arxiv.org/pdf/2409.05750v1.pdf","comment":"Show and Tell paper. Presented at Interspeech 2024"},{"id":"http://arxiv.org/abs/2309.11500v4","updated":"2024-09-09T14:52:15Z","published":"2023-09-20T17:59:32Z","title":"Auto-ACD: A Large-scale Dataset for Audio-Language Representation\n  Learning","summary":"  Recently, the AI community has made significant strides in developing\npowerful foundation models, driven by large-scale multimodal datasets. However,\nfor audio representation learning, existing datasets suffer from limitations in\nthe following aspects: insufficient volume, simplistic content, and arduous\ncollection procedures. To establish an audio dataset with high-quality\ncaptions, we propose an innovative, automatic approach leveraging multimodal\ninputs, such as video frames, audio streams. Specifically, we construct a\nlarge-scale, high-quality, audio-language dataset, named as Auto-ACD,\ncomprising over 1.5M audio-text pairs. We exploit a series of pre-trained\nmodels or APIs, to determine audio-visual synchronisation, generate image\ncaptions, object detection, or audio tags for specific videos. Subsequently, we\nemploy LLM to paraphrase a congruent caption for each audio, guided by the\nextracted multi-modality clues. To demonstrate the effectiveness of the\nproposed dataset, we train widely used models on our dataset and show\nperformance improvement on various downstream tasks, for example,\naudio-language retrieval, audio captioning, zero-shot classification. In\naddition, we establish a novel benchmark with environmental information and\nprovide a benchmark for audio-text tasks.\n","authors":["Luoyi Sun","Xuenan Xu","Mengyue Wu","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2309.11500v4.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2409.05659v1","updated":"2024-09-09T14:29:22Z","published":"2024-09-09T14:29:22Z","title":"Audio-Visual Speaker Diarization: Current Databases, Approaches and\n  Challenges","summary":"  Nowadays, the large amount of audio-visual content available has fostered the\nneed to develop new robust automatic speaker diarization systems to analyse and\ncharacterise it. This kind of system helps to reduce the cost of doing this\nprocess manually and allows the use of the speaker information for different\napplications, as a huge quantity of information is present, for example, images\nof faces, or audio recordings. Therefore, this paper aims to address a critical\narea in the field of speaker diarization systems, the integration of\naudio-visual content of different domains. This paper seeks to push beyond\ncurrent state-of-the-art practices by developing a robust audio-visual speaker\ndiarization framework adaptable to various data domains, including TV\nscenarios, meetings, and daily activities. Unlike most of the existing\naudio-visual speaker diarization systems, this framework will also include the\nproposal of an approach to lead the precise assignment of specific identities\nin TV scenarios where celebrities appear. In addition, in this work, we have\nconducted an extensive compilation of the current state-of-the-art approaches\nand the existing databases for developing audio-visual speaker diarization.\n","authors":["Victoria Mingote","Alfonso Ortega","Antonio Miguel","Eduardo Lleida"],"pdf_url":"https://arxiv.org/pdf/2409.05659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03632v3","updated":"2024-09-09T12:26:04Z","published":"2024-08-07T08:43:58Z","title":"Concept Conductor: Orchestrating Multiple Personalized Concepts in\n  Text-to-Image Synthesis","summary":"  The customization of text-to-image models has seen significant advancements,\nyet generating multiple personalized concepts remains a challenging task.\nCurrent methods struggle with attribute leakage and layout confusion when\nhandling multiple concepts, leading to reduced concept fidelity and semantic\nconsistency. In this work, we introduce a novel training-free framework,\nConcept Conductor, designed to ensure visual fidelity and correct layout in\nmulti-concept customization. Concept Conductor isolates the sampling processes\nof multiple custom models to prevent attribute leakage between different\nconcepts and corrects erroneous layouts through self-attention-based spatial\nguidance. Additionally, we present a concept injection technique that employs\nshape-aware masks to specify the generation area for each concept. This\ntechnique injects the structure and appearance of personalized concepts through\nfeature fusion in the attention layers, ensuring harmony in the final image.\nExtensive qualitative and quantitative experiments demonstrate that Concept\nConductor can consistently generate composite images with accurate layouts\nwhile preserving the visual details of each concept. Compared to existing\nbaselines, Concept Conductor shows significant performance improvements. Our\nmethod supports the combination of any number of concepts and maintains high\nfidelity even when dealing with visually similar concepts. The code and models\nare available at https://github.com/Nihukat/Concept-Conductor.\n","authors":["Zebin Yao","Fangxiang Feng","Ruifan Li","Xiaojie Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03632v3.pdf","comment":"Github Page: https://github.com/Nihukat/Concept-Conductor"},{"id":"http://arxiv.org/abs/2409.05540v1","updated":"2024-09-09T12:00:17Z","published":"2024-09-09T12:00:17Z","title":"Exploring Rich Subjective Quality Information for Image Quality\n  Assessment in the Wild","summary":"  Traditional in the wild image quality assessment (IQA) models are generally\ntrained with the quality labels of mean opinion score (MOS), while missing the\nrich subjective quality information contained in the quality ratings, for\nexample, the standard deviation of opinion scores (SOS) or even distribution of\nopinion scores (DOS). In this paper, we propose a novel IQA method named\nRichIQA to explore the rich subjective rating information beyond MOS to predict\nimage quality in the wild. RichIQA is characterized by two key novel designs:\n(1) a three-stage image quality prediction network which exploits the powerful\nfeature representation capability of the Convolutional vision Transformer (CvT)\nand mimics the short-term and long-term memory mechanisms of human brain; (2) a\nmulti-label training strategy in which rich subjective quality information like\nMOS, SOS and DOS are concurrently used to train the quality prediction network.\nPowered by these two novel designs, RichIQA is able to predict the image\nquality in terms of a distribution, from which the mean image quality can be\nsubsequently obtained. Extensive experimental results verify that the\nthree-stage network is tailored to predict rich quality information, while the\nmulti-label training strategy can fully exploit the potentials within\nsubjective quality rating and enhance the prediction performance and\ngeneralizability of the network. RichIQA outperforms state-of-the-art\ncompetitors on multiple large-scale in the wild IQA databases with rich\nsubjective rating labels. The code of RichIQA will be made publicly available\non GitHub.\n","authors":["Xiongkuo Min","Yixuan Gao","Yuqin Cao","Guangtao Zhai","Wenjun Zhang","Huifang Sun","Chang Wen Chen"],"pdf_url":"https://arxiv.org/pdf/2409.05540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05496v1","updated":"2024-09-09T10:48:33Z","published":"2024-09-09T10:48:33Z","title":"Educational Virtual Field Trips based on Social VR and 360¬∞ Spaces","summary":"  Virtual field trips (VFTs) have proven to be valuable learning tools. Such\napplications are mostly based on 360{\\deg} technology and are to be\ncharacterized as single-user applications in technological terms. In contrast,\nSocial VR applications are characterized by multi-user capability and\nuser-specific avatars. From a learning perspective, the concepts of\ncollaborative learning and embodiment have long been proposed as conducive to\nlearning. Both concepts might be supported using Social VR. However, little is\ncurrently known about the use of Social VR for VFTs. Accordingly, the research\nquestions are to what extent VFTs can be implemented in Social VR environments\nand how these Social VR-based VFTs are perceived by learners. This article\npresents an evaluation study on the development and evaluation of a VFT\nenvironment using the Social VR platform Mozilla Hubs. It describes the design\ndecisions to create the environment and evaluation results from a mixed-method\nstudy (N=16) using a questionnaire and focus group discussions. The study\nhighlighted the opportunities offered by Social VR-based VFTs but also revealed\nseveral challenges that need to be addressed to embrace the potential of Social\nVR-based VFTs to be utilized regularly in education.\n","authors":["Surya Kalvakolu","Heinrich S√∂bke","Jannicke Baalsrud Hauge","Eckhard Kraft"],"pdf_url":"https://arxiv.org/pdf/2409.05496v1.pdf","comment":"9 pages, 7 figures, 1 table, submitted to Games and Learning Alliance\n  Conference"},{"id":"http://arxiv.org/abs/2409.05384v1","updated":"2024-09-09T07:32:18Z","published":"2024-09-09T07:32:18Z","title":"Look One and More: Distilling Hybrid Order Relational Knowledge for\n  Cross-Resolution Image Recognition","summary":"  In spite of great success in many image recognition tasks achieved by recent\ndeep models, directly applying them to recognize low-resolution images may\nsuffer from low accuracy due to the missing of informative details during\nresolution degradation. However, these images are still recognizable for\nsubjects who are familiar with the corresponding high-resolution ones. Inspired\nby that, we propose a teacher-student learning approach to facilitate\nlow-resolution image recognition via hybrid order relational knowledge\ndistillation. The approach refers to three streams: the teacher stream is\npretrained to recognize high-resolution images in high accuracy, the student\nstream is learned to identify low-resolution images by mimicking the teacher's\nbehaviors, and the extra assistant stream is introduced as bridge to help\nknowledge transfer across the teacher to the student. To extract sufficient\nknowledge for reducing the loss in accuracy, the learning of student is\nsupervised with multiple losses, which preserves the similarities in various\norder relational structures. In this way, the capability of recovering missing\ndetails of familiar low-resolution images can be effectively enhanced, leading\nto a better knowledge transfer. Extensive experiments on metric learning,\nlow-resolution image classification and low-resolution face recognition tasks\nshow the effectiveness of our approach, while taking reduced models.\n","authors":["Shiming Ge","Kangkai Zhang","Haolin Liu","Yingying Hua","Shengwei Zhao","Xin Jin","Hao Wen"],"pdf_url":"https://arxiv.org/pdf/2409.05384v1.pdf","comment":"Accepted by AAAI 2020"},{"id":"http://arxiv.org/abs/2409.05330v1","updated":"2024-09-09T05:20:02Z","published":"2024-09-09T05:20:02Z","title":"KAN-Based Fusion of Dual-Domain for Audio-Driven Facial Landmarks\n  Generation","summary":"  Audio-driven talking face generation is a widely researched topic due to its\nhigh applicability. Reconstructing a talking face using audio significantly\ncontributes to fields such as education, healthcare, online conversations,\nvirtual assistants, and virtual reality. Early studies often focused solely on\nchanging the mouth movements, which resulted in outcomes with limited practical\napplications. Recently, researchers have proposed a new approach of\nconstructing the entire face, including face pose, neck, and shoulders. To\nachieve this, they need to generate through landmarks. However, creating stable\nlandmarks that align well with the audio is a challenge. In this paper, we\npropose the KFusion of Dual-Domain model, a robust model that generates\nlandmarks from audio. We separate the audio into two distinct domains to learn\nemotional information and facial context, then use a fusion mechanism based on\nthe KAN model. Our model demonstrates high efficiency compared to recent\nmodels. This will lay the groundwork for the development of the audio-driven\ntalking face generation problem in the future.\n","authors":["Hoang-Son Vo-Thanh","Quang-Vinh Nguyen","Soo-Hyung Kim"],"pdf_url":"https://arxiv.org/pdf/2409.05330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05297v1","updated":"2024-09-09T03:10:40Z","published":"2024-09-09T03:10:40Z","title":"Adaptive Offloading and Enhancement for Low-Light Video Analytics on\n  Mobile Devices","summary":"  In this paper, we explore adaptive offloading and enhancement strategies for\nvideo analytics tasks on computing-constrained mobile devices in low-light\nconditions. We observe that the accuracy of low-light video analytics varies\nfrom different enhancement algorithms. The root cause could be the disparities\nin the effectiveness of enhancement algorithms for feature extraction in\nanalytic models. Specifically, the difference in class activation maps (CAMs)\nbetween enhanced and low-light frames demonstrates a positive correlation with\nvideo analytics accuracy. Motivated by such observations, a novel enhancement\nquality assessment method is proposed on CAMs to evaluate the effectiveness of\ndifferent enhancement algorithms for low-light videos. Then, we design a\nmulti-edge system, which adaptively offloads and enhances low-light video\nanalytics tasks from mobile devices. To achieve the trade-off between the\nenhancement quality and the latency for all system-served mobile devices, we\npropose a genetic-based scheduling algorithm, which can find a near-optimal\nsolution in a reasonable time to meet the latency requirement. Thereby, the\noffloading strategies and the enhancement algorithms are properly selected\nunder the condition of limited end-edge bandwidth and edge computation\nresources. Simulation experiments demonstrate the superiority of the proposed\nsystem, improving accuracy up to 20.83\\% compared to existing benchmarks.\n","authors":["Yuanyi He","Peng Yang","Tian Qin","Jiawei Hou","Ning Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.05297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14066v3","updated":"2024-09-09T03:06:21Z","published":"2024-07-19T06:50:24Z","title":"360VFI: A Dataset and Benchmark for Omnidirectional Video Frame\n  Interpolation","summary":"  Head-mounted 360{\\deg} displays and portable 360{\\deg} cameras have\nsignificantly progressed, providing viewers a realistic and immersive\nexperience. However, many omnidirectional videos have low frame rates that can\nlead to visual fatigue, and the prevailing plane frame interpolation\nmethodologies are unsuitable for omnidirectional video interpolation because\nthey are designed solely for traditional videos. This paper introduces the\nbenchmark dataset, 360VFI, for Omnidirectional Video Frame Interpolation. We\npresent a practical implementation that introduces a distortion prior from\nomnidirectional video into the network to modulate distortions. Specifically,\nwe propose a pyramid distortion-sensitive feature extractor that uses the\nunique characteristics of equirectangular projection (ERP) format as prior\ninformation. Moreover, we devise a decoder that uses an affine transformation\nto further facilitate the synthesis of intermediate frames. 360VFI is the first\ndataset and benchmark that explores the challenge of Omnidirectional Video\nFrame Interpolation. Through our benchmark analysis, we present four different\ndistortion condition scenes in the proposed 360VFI dataset to evaluate the\nchallenges triggered by distortion during interpolation. Besides, experimental\nresults demonstrate that Omnidirectional Video Interpolation can be effectively\nimproved by modeling for omnidirectional distortion.\n","authors":["Wenxuan Lu","Mengshun Hu","Yansheng Qiu","Liang Liao","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14066v3.pdf","comment":"This is a preprint version"}]},"2024-09-08T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2405.19612v2","updated":"2024-09-08T20:32:47Z","published":"2024-05-30T02:00:03Z","title":"Keyword-driven Retrieval-Augmented Large Language Models for Cold-start\n  User Recommendations","summary":"  Recent advancements in Large Language Models (LLMs) have shown significant\npotential in enhancing recommender systems. However, addressing the cold-start\nrecommendation problem, where users lack historical data, remains a\nconsiderable challenge. In this paper, we introduce KALM4Rec (Keyword-driven\nRetrieval-Augmented Large Language Models for Cold-start User Recommendations),\na novel framework specifically designed to tackle this problem by requiring\nonly a few input keywords from users in a practical scenario of cold-start user\nrestaurant recommendations. KALM4Rec operates in two main stages: candidates\nretrieval and LLM-based candidates re-ranking. In the first stage,\nkeyword-driven retrieval models are used to identify potential candidates,\naddressing LLMs' limitations in processing extensive tokens and reducing the\nrisk of generating misleading information. In the second stage, we employ LLMs\nwith various prompting strategies, including zero-shot and few-shot techniques,\nto re-rank these candidates by integrating multiple examples directly into the\nLLM prompts. Our evaluation, using a Yelp restaurant dataset with user reviews\nfrom three English-speaking cities, shows that our proposed framework\nsignificantly improves recommendation quality. Specifically, the integration of\nin-context instructions with LLMs for re-ranking markedly enhances the\nperformance of the cold-start user recommender system.\n","authors":["Hai-Dang Kieu","Minh Duc Nguyen","Thanh-Son Nguyen","Dung D. Le"],"pdf_url":"https://arxiv.org/pdf/2405.19612v2.pdf","comment":"10 pages, 10 figures, 4 tables"},{"id":"http://arxiv.org/abs/2409.05152v1","updated":"2024-09-08T16:35:19Z","published":"2024-09-08T16:35:19Z","title":"OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs","summary":"  Despite the recent advancements in Large Language Models (LLMs), which have\nsignificantly enhanced the generative capabilities for various NLP tasks, LLMs\nstill face limitations in directly handling retrieval tasks. However, many\npractical applications demand the seamless integration of both retrieval and\ngeneration. This paper introduces a novel and efficient One-pass Generation and\nretrieval framework (OneGen), designed to improve LLMs' performance on tasks\nthat require both generation and retrieval. The proposed framework bridges the\ntraditionally separate training approaches for generation and retrieval by\nincorporating retrieval tokens generated autoregressively. This enables a\nsingle LLM to handle both tasks simultaneously in a unified forward pass. We\nconduct experiments on two distinct types of composite tasks, RAG and Entity\nLinking, to validate the pluggability, effectiveness, and efficiency of OneGen\nin training and inference. Furthermore, our results show that integrating\ngeneration and retrieval within the same context preserves the generative\ncapabilities of LLMs while improving retrieval performance. To the best of our\nknowledge, OneGen is the first to enable LLMs to conduct vector retrieval\nduring the generation.\n","authors":["Jintian Zhang","Cheng Peng","Mengshu Sun","Xiang Chen","Lei Liang","Zhiqiang Zhang","Jun Zhou","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.05152v1.pdf","comment":"Work in progress; code is available at\n  https://github.com/zjunlp/OneGen"},{"id":"http://arxiv.org/abs/2409.05022v1","updated":"2024-09-08T08:27:22Z","published":"2024-09-08T08:27:22Z","title":"Sequential Recommendation via Adaptive Robust Attention with\n  Multi-dimensional Embeddings","summary":"  Sequential recommendation models have achieved state-of-the-art performance\nusing self-attention mechanism. It has since been found that moving beyond only\nusing item ID and positional embeddings leads to a significant accuracy boost\nwhen predicting the next item. In recent literature, it was reported that a\nmulti-dimensional kernel embedding with temporal contextual kernels to capture\nusers' diverse behavioral patterns results in a substantial performance\nimprovement. In this study, we further improve the sequential recommender\nmodel's robustness and generalization by introducing a mix-attention mechanism\nwith a layer-wise noise injection (LNI) regularization. We refer to our\nproposed model as adaptive robust sequential recommendation framework (ADRRec),\nand demonstrate through extensive experiments that our model outperforms\nexisting self-attention architectures.\n","authors":["Linsey Pang","Amir Hossein Raffiee","Wei Liu","Keld Lundgaard"],"pdf_url":"https://arxiv.org/pdf/2409.05022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11119v2","updated":"2024-09-08T04:25:32Z","published":"2024-04-17T07:07:41Z","title":"DREAM: A Dual Representation Learning Model for Multimodal\n  Recommendation","summary":"  Multimodal recommendation focuses primarily on effectively exploiting both\nbehavioral and multimodal information for the recommendation task. However,\nmost existing models suffer from the following issues when fusing information\nfrom two different domains: (1) Previous works do not pay attention to the\nsufficient utilization of modal information by only using direct concatenation,\naddition, or simple linear layers for modal information extraction. (2)\nPrevious works treat modal features as learnable embeddings, which causes the\nmodal embeddings to gradually deviate from the original modal features during\nlearning. We refer to this issue as Modal Information Forgetting. (3) Previous\napproaches fail to account for the significant differences in the distribution\nbetween behavior and modality, leading to the issue of representation\nmisalignment. To address these challenges, this paper proposes a novel Dual\nREpresentAtion learning model for Multimodal Recommendation called DREAM. For\nsufficient information extraction, we introduce separate dual lines, including\nBehavior Line and Modal Line, in which the Modal-specific Encoder is applied to\nempower modal representations. To address the issue of Modal Information\nForgetting, we introduce the Similarity Supervised Signal to constrain the\nmodal representations. Additionally, we design a Behavior-Modal Alignment\nmodule to fuse the dual representations through Intra-Alignment and\nInter-Alignment. Extensive experiments on three public datasets demonstrate\nthat the proposed DREAM method achieves state-of-the-art (SOTA) results. The\nsource code will be available upon acceptance.\n","authors":["Kangning Zhang","Yingjie Qin","Jiarui Jin","Yifan Liu","Ruilong Su","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2404.11119v2.pdf","comment":"10 pages, 11 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.03336v2","updated":"2024-09-08T14:21:13Z","published":"2024-09-05T08:28:36Z","title":"Estimating Indoor Scene Depth Maps from Ultrasonic Echoes","summary":"  Measuring 3D geometric structures of indoor scenes requires dedicated depth\nsensors, which are not always available. Echo-based depth estimation has\nrecently been studied as a promising alternative solution. All previous studies\nhave assumed the use of echoes in the audible range. However, one major problem\nis that audible echoes cannot be used in quiet spaces or other situations where\nproducing audible sounds is prohibited. In this paper, we consider echo-based\ndepth estimation using inaudible ultrasonic echoes. While ultrasonic waves\nprovide high measurement accuracy in theory, the actual depth estimation\naccuracy when ultrasonic echoes are used has remained unclear, due to its\ndisadvantage of being sensitive to noise and susceptible to attenuation. We\nfirst investigate the depth estimation accuracy when the frequency of the sound\nsource is restricted to the high-frequency band, and found that the accuracy\ndecreased when the frequency was limited to ultrasonic ranges. Based on this\nobservation, we propose a novel deep learning method to improve the accuracy of\nultrasonic echo-based depth estimation by using audible echoes as auxiliary\ndata only during training. Experimental results with a public dataset\ndemonstrate that our method improves the estimation accuracy.\n","authors":["Junpei Honma","Akisato Kimura","Go Irie"],"pdf_url":"https://arxiv.org/pdf/2409.03336v2.pdf","comment":"ICIP 2024"},{"id":"http://arxiv.org/abs/2404.13306v2","updated":"2024-09-08T12:07:52Z","published":"2024-04-20T07:28:55Z","title":"FakeBench: Probing Explainable Fake Image Detection via Large Multimodal\n  Models","summary":"  The ability to distinguish whether an image is generated by artificial\nintelligence (AI) is a crucial ingredient in human intelligence, usually\naccompanied by a complex and dialectical forensic and reasoning process.\nHowever, current fake image detection models and databases focus on binary\nclassification without understandable explanations for the general populace.\nThis weakens the credibility of authenticity judgment and may conceal potential\nmodel biases. Meanwhile, large multimodal models (LMMs) have exhibited immense\nvisual-text capabilities on various tasks, bringing the potential for\nexplainable fake image detection. Therefore, we pioneer the probe of LMMs for\nexplainable fake image detection by presenting a multimodal database\nencompassing textual authenticity descriptions, the FakeBench. For\nconstruction, we first introduce a fine-grained taxonomy of generative visual\nforgery concerning human perception, based on which we collect forgery\ndescriptions in human natural language with a human-in-the-loop strategy.\nFakeBench examines LMMs with four evaluation criteria: detection, reasoning,\ninterpretation and fine-grained forgery analysis, to obtain deeper insights\ninto image authenticity-relevant capabilities. Experiments on various LMMs\nconfirm their merits and demerits in different aspects of fake image detection\ntasks. This research presents a paradigm shift towards transparency for the\nfake image detection area and reveals the need for greater emphasis on forensic\nelements in visual-language research and AI risk control. FakeBench will be\navailable at https://github.com/Yixuan423/FakeBench.\n","authors":["Yixuan Li","Xuelin Liu","Xiaoyang Wang","Bu Sung Lee","Shiqi Wang","Anderson Rocha","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2404.13306v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04999v1","updated":"2024-09-08T07:08:58Z","published":"2024-09-08T07:08:58Z","title":"Visual Grounding with Multi-modal Conditional Adaptation","summary":"  Visual grounding is the task of locating objects specified by natural\nlanguage expressions. Existing methods extend generic object detection\nframeworks to tackle this task. They typically extract visual and textual\nfeatures separately using independent visual and textual encoders, then fuse\nthese features in a multi-modal decoder for final prediction. However, visual\ngrounding presents unique challenges. It often involves locating objects with\ndifferent text descriptions within the same image. Existing methods struggle\nwith this task because the independent visual encoder produces identical visual\nfeatures for the same image, limiting detection performance. Some recently\napproaches propose various language-guided visual encoders to address this\nissue, but they mostly rely solely on textual information and require\nsophisticated designs. In this paper, we introduce Multi-modal Conditional\nAdaptation (MMCA), which enables the visual encoder to adaptively update\nweights, directing its focus towards text-relevant regions. Specifically, we\nfirst integrate information from different modalities to obtain multi-modal\nembeddings. Then we utilize a set of weighting coefficients, which generated\nfrom the multimodal embeddings, to reorganize the weight update matrices and\napply them to the visual encoder of the visual grounding model. Extensive\nexperiments on four widely used datasets demonstrate that MMCA achieves\nsignificant improvements and state-of-the-art results. Ablation experiments\nfurther demonstrate the lightweight and efficiency of our method. Our source\ncode is available at: https://github.com/Mr-Bigworth/MMCA.\n","authors":["Ruilin Yao","Shengwu Xiong","Yichen Zhao","Yi Rong"],"pdf_url":"https://arxiv.org/pdf/2409.04999v1.pdf","comment":"Accepted by ACM MM 2024 [Oral]"},{"id":"http://arxiv.org/abs/2404.11119v2","updated":"2024-09-08T04:25:32Z","published":"2024-04-17T07:07:41Z","title":"DREAM: A Dual Representation Learning Model for Multimodal\n  Recommendation","summary":"  Multimodal recommendation focuses primarily on effectively exploiting both\nbehavioral and multimodal information for the recommendation task. However,\nmost existing models suffer from the following issues when fusing information\nfrom two different domains: (1) Previous works do not pay attention to the\nsufficient utilization of modal information by only using direct concatenation,\naddition, or simple linear layers for modal information extraction. (2)\nPrevious works treat modal features as learnable embeddings, which causes the\nmodal embeddings to gradually deviate from the original modal features during\nlearning. We refer to this issue as Modal Information Forgetting. (3) Previous\napproaches fail to account for the significant differences in the distribution\nbetween behavior and modality, leading to the issue of representation\nmisalignment. To address these challenges, this paper proposes a novel Dual\nREpresentAtion learning model for Multimodal Recommendation called DREAM. For\nsufficient information extraction, we introduce separate dual lines, including\nBehavior Line and Modal Line, in which the Modal-specific Encoder is applied to\nempower modal representations. To address the issue of Modal Information\nForgetting, we introduce the Similarity Supervised Signal to constrain the\nmodal representations. Additionally, we design a Behavior-Modal Alignment\nmodule to fuse the dual representations through Intra-Alignment and\nInter-Alignment. Extensive experiments on three public datasets demonstrate\nthat the proposed DREAM method achieves state-of-the-art (SOTA) results. The\nsource code will be available upon acceptance.\n","authors":["Kangning Zhang","Yingjie Qin","Jiarui Jin","Yifan Liu","Ruilong Su","Weinan Zhang","Yong Yu"],"pdf_url":"https://arxiv.org/pdf/2404.11119v2.pdf","comment":"10 pages, 11 figures"}]},"2024-09-07T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.14774v2","updated":"2024-09-07T16:11:36Z","published":"2024-04-23T06:29:48Z","title":"CoST: Contrastive Quantization based Semantic Tokenization for\n  Generative Recommendation","summary":"  Embedding-based retrieval serves as a dominant approach to candidate item\nmatching for industrial recommender systems. With the success of generative AI,\ngenerative retrieval has recently emerged as a new retrieval paradigm for\nrecommendation, which casts item retrieval as a generation problem. Its model\nconsists of two stages: semantic tokenization and autoregressive generation.\nThe first stage involves item tokenization that constructs discrete semantic\ntokens to index items, while the second stage autoregressively generates\nsemantic tokens of candidate items. Therefore, semantic tokenization serves as\na crucial preliminary step for training generative recommendation models.\nExisting research usually employs a vector quantizier with reconstruction loss\n(e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to\ncapture the essential neighborhood relationships that are vital for effective\nitem modeling in recommender systems. In this paper, we propose a contrastive\nquantization-based semantic tokenization approach, named CoST, which harnesses\nboth item relationships and semantic information to learn semantic tokens. Our\nexperimental results highlight the significant impact of semantic tokenization\non generative recommendation performance, with CoST achieving up to a 43%\nimprovement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over\nprevious baselines.\n","authors":["Jieming Zhu","Mengqun Jin","Qijiong Liu","Zexuan Qiu","Zhenhua Dong","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2404.14774v2.pdf","comment":"Accepted by RecSys'2024"},{"id":"http://arxiv.org/abs/2407.21300v3","updated":"2024-09-07T15:02:48Z","published":"2024-07-31T03:00:59Z","title":"Implementing Streaming algorithm and k-means clusters to RAG","summary":"  Retrieval-augmented generation (RAG) has achieved significant success in\ninformation retrieval to assist large language models LLMs because it builds an\nexternal knowledge database. However, it also has many problems, it consumes a\nlot of memory because of the enormous database, and it cannot update the\nestablished index database in time when confronted with massive streaming data.\nTo reduce the memory required for building the database and maintain accuracy\nsimultaneously, we proposed a new approach integrating a streaming algorithm\nwith k-means clustering into RAG. Our approach applied a streaming algorithm to\nupdate the index dynamically and reduce memory consumption. Additionally, the\nk-means algorithm clusters highly similar documents, and the query time would\nbe shortened. We conducted comparative experiments on four methods, and the\nresults indicated that RAG with streaming algorithm and k-means clusters\noutperforms traditional RAG in accuracy and memory, particularly when dealing\nwith large-scale streaming data.\n","authors":["Haoyu Kang","Yuzhou Zhu","Yukun Zhong","Ke Wang"],"pdf_url":"https://arxiv.org/pdf/2407.21300v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02335v2","updated":"2024-09-07T14:29:11Z","published":"2024-02-04T04:13:31Z","title":"Video Editing for Video Retrieval","summary":"  Though pre-training vision-language models have demonstrated significant\nbenefits in boosting video-text retrieval performance from large-scale web\nvideos, fine-tuning still plays a critical role with manually annotated clips\nwith start and end times, which requires considerable human effort. To address\nthis issue, we explore an alternative cheaper source of annotations, single\ntimestamps, for video-text retrieval. We initialise clips from timestamps in a\nheuristic way to warm up a retrieval model. Then a video clip editing method is\nproposed to refine the initial rough boundaries to improve retrieval\nperformance. A student-teacher network is introduced for video clip editing.\nThe teacher model is employed to edit the clips in the training set whereas the\nstudent model trains on the edited clips. The teacher weights are updated from\nthe student's after the student's performance increases. Our method is model\nagnostic and applicable to any retrieval models. We conduct experiments based\non three state-of-the-art retrieval models, COOT, VideoCLIP and CLIP4Clip.\nExperiments conducted on three video retrieval datasets, YouCook2, DiDeMo and\nActivityNet-Captions show that our edited clips consistently improve retrieval\nperformance over initial clips across all the three retrieval models.\n","authors":["Bin Zhu","Kevin Flanagan","Adriano Fragomeni","Michael Wray","Dima Damen"],"pdf_url":"https://arxiv.org/pdf/2402.02335v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04827v1","updated":"2024-09-07T13:41:37Z","published":"2024-09-07T13:41:37Z","title":"Incorporate LLMs with Influential Recommender System","summary":"  Recommender systems have achieved increasing accuracy over the years.\nHowever, this precision often leads users to narrow their interests, resulting\nin issues such as limited diversity and the creation of echo chambers. Current\nresearch addresses these challenges through proactive recommender systems by\nrecommending a sequence of items (called influence path) to guide user interest\nin the target item. However, existing methods struggle to construct a coherent\ninfluence path that builds up with items the user is likely to enjoy. In this\npaper, we leverage the Large Language Model's (LLMs) exceptional ability for\npath planning and instruction following, introducing a novel approach named\nLLM-based Influence Path Planning (LLM-IPP). Our approach maintains coherence\nbetween consecutive recommendations and enhances user acceptability of the\nrecommended items. To evaluate LLM-IPP, we implement various user simulators\nand metrics to measure user acceptability and path coherence. Experimental\nresults demonstrate that LLM-IPP significantly outperforms traditional\nproactive recommender systems. This study pioneers the integration of LLMs into\nproactive recommender systems, offering a reliable and user-engaging\nmethodology for future recommendation technologies.\n","authors":["Mingze Wang","Shuxian Bi","Wenjie Wang","Chongming Gao","Yangyang Li","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2409.04827v1.pdf","comment":"5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2409.04810v1","updated":"2024-09-07T12:42:58Z","published":"2024-09-07T12:42:58Z","title":"Debias Can be Unreliable: Mitigating Bias Issue in Evaluating Debiasing\n  Recommendation","summary":"  Recent work has improved recommendation models remarkably by equipping them\nwith debiasing methods. Due to the unavailability of fully-exposed datasets,\nmost existing approaches resort to randomly-exposed datasets as a proxy for\nevaluating debiased models, employing traditional evaluation scheme to\nrepresent the recommendation performance. However, in this study, we reveal\nthat traditional evaluation scheme is not suitable for randomly-exposed\ndatasets, leading to inconsistency between the Recall performance obtained\nusing randomly-exposed datasets and that obtained using fully-exposed datasets.\nSuch inconsistency indicates the potential unreliability of experiment\nconclusions on previous debiasing techniques and calls for unbiased Recall\nevaluation using randomly-exposed datasets. To bridge the gap, we propose the\nUnbiased Recall Evaluation (URE) scheme, which adjusts the utilization of\nrandomly-exposed datasets to unbiasedly estimate the true Recall performance on\nfully-exposed datasets. We provide theoretical evidence to demonstrate the\nrationality of URE and perform extensive experiments on real-world datasets to\nvalidate its soundness.\n","authors":["Chengbing Wang","Wentao Shi","Jizhi Zhang","Wenjie Wang","Hang Pan","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2409.04810v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.06809v2","updated":"2024-09-07T09:02:29Z","published":"2024-08-13T10:58:29Z","title":"Reformulating Conversational Recommender Systems as Tri-Phase Offline\n  Policy Learning","summary":"  Existing Conversational Recommender Systems (CRS) predominantly utilize user\nsimulators for training and evaluating recommendation policies. These\nsimulators often oversimplify the complexity of user interactions by focusing\nsolely on static item attributes, neglecting the rich, evolving preferences\nthat characterize real-world user behavior. This limitation frequently leads to\nmodels that perform well in simulated environments but falter in actual\ndeployment. Addressing these challenges, this paper introduces the Tri-Phase\nOffline Policy Learning-based Conversational Recommender System (TCRS), which\nsignificantly reduces dependency on real-time interactions and mitigates\noverfitting issues prevalent in traditional approaches. TCRS integrates a\nmodel-based offline learning strategy with a controllable user simulation that\ndynamically aligns with both personalized and evolving user preferences.\nThrough comprehensive experiments, TCRS demonstrates enhanced robustness,\nadaptability, and accuracy in recommendations, outperforming traditional CRS\nmodels in diverse user scenarios. This approach not only provides a more\nrealistic evaluation environment but also facilitates a deeper understanding of\nuser behavior dynamics, thereby refining the recommendation process.\n","authors":["Gangyi Zhang","Chongming Gao","Hang Pan","Runzhe Teng","Ruizhe Li"],"pdf_url":"https://arxiv.org/pdf/2408.06809v2.pdf","comment":"Accepted at CIKM 2024"},{"id":"http://arxiv.org/abs/2408.11345v2","updated":"2024-09-07T07:54:32Z","published":"2024-08-21T05:09:53Z","title":"Deep Tree-based Retrieval for Efficient Recommendation: Theory and\n  Method","summary":"  With the development of deep learning techniques, deep recommendation models\nalso achieve remarkable improvements in terms of recommendation accuracy.\nHowever, due to the large number of candidate items in practice and the high\ncost of preference computation, these methods also suffer from low efficiency\nof recommendation. The recently proposed tree-based deep recommendation models\nalleviate the problem by directly learning tree structure and representations\nunder the guidance of recommendation objectives. However, such models have\nshortcomings. The max-heap assumption in the hierarchical tree, in which the\npreference for a parent node should be the maximum between the preferences for\nits children, is difficult to satisfy in their binary classification\nobjectives. To this end, we propose Tree-based Deep Retrieval (TDR for short)\nfor efficient recommendation. In TDR, all the trees generated during the\ntraining process are retained to form the forest. When learning the node\nrepresentation of each tree, we have to satisfy the max-heap assumption as much\nas possible and mimic beam search behavior over the tree in the training stage.\nThis is achieved by TDR to regard the training task as multi-classification\nover tree nodes at the same level. However, the number of tree nodes grows\nexponentially with levels, making us train the preference model with the\nguidance of the sampled-softmax technique. The experiments are conducted on\nreal-world datasets, validating the effectiveness of the proposed preference\nmodel learning method and tree learning method.\n","authors":["Ze Liu","Jin Zhang","Chao Feng","Defu Lian","Jie Wang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2408.11345v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04701v1","updated":"2024-09-07T03:54:46Z","published":"2024-09-07T03:54:46Z","title":"Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding\n  Models","summary":"  Many use cases require retrieving smaller portions of text, and dense\nvector-based retrieval systems often perform better with shorter text segments,\nas the semantics are less likely to be \"over-compressed\" in the embeddings.\nConsequently, practitioners often split text documents into smaller chunks and\nencode them separately. However, chunk embeddings created in this way can lose\ncontextual information from surrounding chunks, resulting in suboptimal\nrepresentations. In this paper, we introduce a novel method called \"late\nchunking,\" which leverages long context embedding models to first embed all\ntokens of the long text, with chunking applied after the transformer model and\njust before mean pooling. The resulting chunk embeddings capture the full\ncontextual information, leading to superior results across various retrieval\ntasks without the need for additional training. Moreover, our method is generic\nenough to be applied to any long-context embedding model.\n","authors":["Michael G√ºnther","Isabelle Mohr","Bo Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2409.04701v1.pdf","comment":"4 pages, early draft"},{"id":"http://arxiv.org/abs/2312.09425v3","updated":"2024-09-07T01:01:44Z","published":"2023-11-21T23:35:44Z","title":"YouTube Videos for Public Health Literacy? A Machine Learning Pipeline\n  to Curate Covid-19 Videos","summary":"  The COVID-19 pandemic has highlighted the dire necessity to improve public\nhealth literacy for societal resilience. YouTube, the largest video-sharing\nsocial media platform, provides a vast repository of user-generated health\ninformation in a multi-media-rich format which may be easier for the public to\nunderstand and use if major concerns about content quality and accuracy are\naddressed. This study develops an automated solution to identify, retrieve and\nshortlist medically relevant and understandable YouTube videos that domain\nexperts can subsequently review and recommend for disseminating and educating\nthe public on the COVID-19 pandemic and similar public health outbreaks. Our\napproach leverages domain knowledge from human experts and machine learning and\nnatural language processing methods to provide a scalable, replicable, and\ngeneralizable approach that can also be applied to enhance the management of\nmany health conditions.\n","authors":["Yawen Guo","Xiao Liu","Anjana Susarla","Rema Padman"],"pdf_url":"https://arxiv.org/pdf/2312.09425v3.pdf","comment":"Studies in health technology and informatics(MedInfo) 2023"}],"Multimedia":[{"id":"http://arxiv.org/abs/2209.03275v3","updated":"2024-09-07T20:10:07Z","published":"2022-09-07T16:27:34Z","title":"Multimodal Speech Enhancement Using Burst Propagation","summary":"  This paper proposes the MBURST, a novel multimodal solution for audio-visual\nspeech enhancements that consider the most recent neurological discoveries\nregarding pyramidal cells of the prefrontal cortex and other brain regions. The\nso-called burst propagation implements several criteria to address the credit\nassignment problem in a more biologically plausible manner: steering the sign\nand magnitude of plasticity through feedback, multiplexing the feedback and\nfeedforward information across layers through different weight connections,\napproximating feedback and feedforward connections, and linearizing the\nfeedback signals. MBURST benefits from such capabilities to learn correlations\nbetween the noisy signal and the visual stimuli, thus attributing meaning to\nthe speech by amplifying relevant information and suppressing noise.\nExperiments conducted over a Grid Corpus and CHiME3-based dataset show that\nMBURST can reproduce similar mask reconstructions to the multimodal\nbackpropagation-based baseline while demonstrating outstanding energy\nefficiency management, reducing the neuron firing rates to values up to\n\\textbf{$70\\%$} lower. Such a feature implies more sustainable implementations,\nsuitable and desirable for hearing aids or any other similar embedded systems.\n","authors":["Mohsin Raza","Leandro A. Passos","Ahmed Khubaib","Ahsan Adeel"],"pdf_url":"https://arxiv.org/pdf/2209.03275v3.pdf","comment":null}]},"2024-09-06T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.04649v1","updated":"2024-09-06T23:16:06Z","published":"2024-09-06T23:16:06Z","title":"Preserving Individuality while Following the Crowd: Understanding the\n  Role of User Taste and Crowd Wisdom in Online Product Rating Prediction","summary":"  Numerous algorithms have been developed for online product rating prediction,\nbut the specific influence of user and product information in determining the\nfinal prediction score remains largely unexplored. Existing research often\nrelies on narrowly defined data settings, which overlooks real-world challenges\nsuch as the cold-start problem, cross-category information utilization, and\nscalability and deployment issues. To delve deeper into these aspects, and\nparticularly to uncover the roles of individual user taste and collective\nwisdom, we propose a unique and practical approach that emphasizes historical\nratings at both the user and product levels, encapsulated using a continuously\nupdated dynamic tree representation. This representation effectively captures\nthe temporal dynamics of users and products, leverages user information across\nproduct categories, and provides a natural solution to the cold-start problem.\nFurthermore, we have developed an efficient data processing strategy that makes\nthis approach highly scalable and easily deployable. Comprehensive experiments\nin real industry settings demonstrate the effectiveness of our approach.\nNotably, our findings reveal that individual taste dominates over collective\nwisdom in online product rating prediction, a perspective that contrasts with\nthe commonly observed wisdom of the crowd phenomenon in other domains. This\ndominance of individual user taste is consistent across various model types,\nincluding the boosting tree model, recurrent neural network (RNN), and\ntransformer-based architectures. This observation holds true across the overall\npopulation, within individual product categories, and in cold-start scenarios.\nOur findings underscore the significance of individual user tastes in the\ncontext of online product rating prediction and the robustness of our approach\nacross different model architectures.\n","authors":["Liang Wang","Shubham Jain","Yingtong Dou","Junpeng Wang","Chin-Chia Michael Yeh","Yujie Fan","Prince Aboagye","Yan Zheng","Xin Dai","Zhongfang Zhuang","Uday Singh Saini","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.04649v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2302.11157v2","updated":"2024-09-06T19:30:26Z","published":"2023-02-22T05:41:27Z","title":"FiNER-ORD: Financial Named Entity Recognition Open Research Dataset","summary":"  Over the last two decades, the development of the CoNLL-2003 named entity\nrecognition (NER) dataset has helped enhance the capabilities of deep learning\nand natural language processing (NLP). The finance domain, characterized by its\nunique semantic and lexical variations for the same entities, presents specific\nchallenges to the NER task; thus, a domain-specific customized dataset is\ncrucial for advancing research in this field. In our work, we develop the first\nhigh-quality English Financial NER Open Research Dataset (FiNER-ORD). We\nbenchmark multiple pre-trained language models (PLMs) and large-language models\n(LLMs) on FiNER-ORD. We believe our proposed FiNER-ORD dataset will open future\nopportunities to use FiNER-ORD as a benchmark for financial domain-specific NER\nand NLP tasks. Our dataset, models, and code are publicly available on GitHub\nand Hugging Face under CC BY-NC 4.0 license.\n","authors":["Agam Shah","Abhinav Gullapalli","Ruchit Vithani","Michael Galarnyk","Sudheer Chava"],"pdf_url":"https://arxiv.org/pdf/2302.11157v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03140v2","updated":"2024-09-06T18:41:50Z","published":"2024-09-05T00:25:37Z","title":"GraphEx: A Graph-based Extraction Method for Advertiser Keyphrase\n  Recommendation","summary":"  Online sellers and advertisers are recommended keyphrases for their listed\nproducts, which they bid on to enhance their sales. One popular paradigm that\ngenerates such recommendations is Extreme Multi-Label Classification (XMC),\nwhich involves tagging/mapping keyphrases to items. We outline the limitations\nof using traditional item-query based tagging or mapping techniques for\nkeyphrase recommendations on E-Commerce platforms. We introduce GraphEx, an\ninnovative graph-based approach that recommends keyphrases to sellers using\nextraction of token permutations from item titles. Additionally, we demonstrate\nthat relying on traditional metrics such as precision/recall can be misleading\nin practical applications, thereby necessitating a combination of metrics to\nevaluate performance in real-world scenarios. These metrics are designed to\nassess the relevance of keyphrases to items and the potential for buyer\noutreach. GraphEx outperforms production models at eBay, achieving the\nobjectives mentioned above. It supports near real-time inferencing in\nresource-constrained production environments and scales effectively for\nbillions of items.\n","authors":["Ashirbad Mishra","Soumik Dey","Marshall Wu","Jinyu Zhao","He Yu","Kaichen Ni","Binbin Li","Kamesh Madduri"],"pdf_url":"https://arxiv.org/pdf/2409.03140v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04540v1","updated":"2024-09-06T18:10:42Z","published":"2024-09-06T18:10:42Z","title":"A Unified Framework for Cross-Domain Recommendation","summary":"  In addressing the persistent challenges of data-sparsity and cold-start\nissues in domain-expert recommender systems, Cross-Domain Recommendation (CDR)\nemerges as a promising methodology. CDR aims at enhancing prediction\nperformance in the target domain by leveraging interaction knowledge from\nrelated source domains, particularly through users or items that span across\nmultiple domains (e.g., Short-Video and Living-Room). For academic research\npurposes, there are a number of distinct aspects to guide CDR method designing,\nincluding the auxiliary domain number, domain-overlapped element, user-item\ninteraction types, and downstream tasks. With so many different CDR combination\nscenario settings, the proposed scenario-expert approaches are tailored to\naddress a specific vertical CDR scenario, and often lack the capacity to adapt\nto multiple horizontal scenarios. In an effect to coherently adapt to various\nscenarios, and drawing inspiration from the concept of domain-invariant\ntransfer learning, we extend the former SOTA model UniCDR in five different\naspects, named as UniCDR+. Our work was successfully deployed on the Kuaishou\nLiving-Room RecSys.\n","authors":["Jiangxia Cao","Shen Wang","Gaode Chen","Rui Huang","Shuang Yang","Zhaojie Liu","Guorui Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.04540v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2409.04432v1","updated":"2024-09-06T17:54:43Z","published":"2024-09-06T17:54:43Z","title":"A Survey on Knowledge Organization Systems of Research Fields: Resources\n  and Challenges","summary":"  Knowledge Organization Systems (KOSs), such as term lists, thesauri,\ntaxonomies, and ontologies, play a fundamental role in categorising, managing,\nand retrieving information. In the academic domain, KOSs are often adopted for\nrepresenting research areas and their relationships, primarily aiming to\nclassify research articles, academic courses, patents, books, scientific\nvenues, domain experts, grants, software, experiment materials, and several\nother relevant products and agents. These structured representations of\nresearch areas, widely embraced by many academic fields, have proven effective\nin empowering AI-based systems to i) enhance retrievability of relevant\ndocuments, ii) enable advanced analytic solutions to quantify the impact of\nacademic research, and iii) analyse and forecast research dynamics. This paper\naims to present a comprehensive survey of the current KOS for academic\ndisciplines. We analysed and compared 45 KOSs according to five main\ndimensions: scope, structure, curation, usage, and links to other KOSs. Our\nresults reveal a very heterogeneous scenario in terms of scope, scale, quality,\nand usage, highlighting the need for more integrated solutions for representing\nresearch knowledge across academic fields. We conclude by discussing the main\nchallenges and the most promising future directions.\n","authors":["Angelo Salatino","Tanay Aggarwal","Andrea Mannocci","Francesco Osborne","Enrico Motta"],"pdf_url":"https://arxiv.org/pdf/2409.04432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04339v1","updated":"2024-09-06T15:17:40Z","published":"2024-09-06T15:17:40Z","title":"How Fair is Your Diffusion Recommender Model?","summary":"  Diffusion-based recommender systems have recently proven to outperform\ntraditional generative recommendation approaches, such as variational\nautoencoders and generative adversarial networks. Nevertheless, the machine\nlearning literature has raised several concerns regarding the possibility that\ndiffusion models, while learning the distribution of data samples, may\ninadvertently carry information bias and lead to unfair outcomes. In light of\nthis aspect, and considering the relevance that fairness has held in\nrecommendations over the last few decades, we conduct one of the first fairness\ninvestigations in the literature on DiffRec, a pioneer approach in\ndiffusion-based recommendation. First, we propose an experimental setting\ninvolving DiffRec (and its variant L-DiffRec) along with nine state-of-the-art\nrecommendation models, two popular recommendation datasets from the\nfairness-aware literature, and six metrics accounting for accuracy and\nconsumer/provider fairness. Then, we perform a twofold analysis, one assessing\nmodels' performance under accuracy and recommendation fairness separately, and\nthe other identifying if and to what extent such metrics can strike a\nperformance trade-off. Experimental results from both studies confirm the\ninitial unfairness warnings but pave the way for how to address them in future\nresearch directions.\n","authors":["Daniele Malitesta","Giacomo Medda","Erasmo Purificato","Ludovico Boratto","Fragkiskos D. Malliaros","Mirko Marras","Ernesto William De Luca"],"pdf_url":"https://arxiv.org/pdf/2409.04339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04329v1","updated":"2024-09-06T15:05:12Z","published":"2024-09-06T15:05:12Z","title":"Enhancing Sequential Music Recommendation with Personalized Popularity\n  Awareness","summary":"  In the realm of music recommendation, sequential recommender systems have\nshown promise in capturing the dynamic nature of music consumption.\nNevertheless, traditional Transformer-based models, such as SASRec and\nBERT4Rec, while effective, encounter challenges due to the unique\ncharacteristics of music listening habits. In fact, existing models struggle to\ncreate a coherent listening experience due to rapidly evolving preferences.\nMoreover, music consumption is characterized by a prevalence of repeated\nlistening, i.e., users frequently return to their favourite tracks, an\nimportant signal that could be framed as individual or personalized popularity.\n  This paper addresses these challenges by introducing a novel approach that\nincorporates personalized popularity information into sequential\nrecommendation. By combining user-item popularity scores with model-generated\nscores, our method effectively balances the exploration of new music with the\nsatisfaction of user preferences. Experimental results demonstrate that a\nPersonalized Most Popular recommender, a method solely based on user-specific\npopularity, outperforms existing state-of-the-art models. Furthermore,\naugmenting Transformer-based models with personalized popularity awareness\nyields superior performance, showing improvements ranging from 25.2% to 69.8%.\nThe code for this paper is available at\nhttps://github.com/sisinflab/personalized-popularity-awareness.\n","authors":["Davide Abbattista","Vito Walter Anelli","Tommaso Di Noia","Craig Macdonald","Aleksandr Vladimirovich Petrov"],"pdf_url":"https://arxiv.org/pdf/2409.04329v1.pdf","comment":"Accepted by RecSys'24 as an LBR paper"},{"id":"http://arxiv.org/abs/2403.00884v3","updated":"2024-09-06T14:49:21Z","published":"2024-03-01T10:01:36Z","title":"Zero-Shot Topic Classification of Column Headers: Leveraging LLMs for\n  Metadata Enrichment","summary":"  Traditional dataset retrieval systems rely on metadata for indexing, rather\nthan on the underlying data values. However, high-quality metadata creation and\nenrichment often require manual annotations, which is a labour-intensive and\nchallenging process to automate. In this study, we propose a method to support\nmetadata enrichment using topic annotations generated by three Large Language\nModels (LLMs): ChatGPT-3.5, GoogleBard, and GoogleGemini. Our analysis focuses\non classifying column headers based on domain-specific topics from the\nConsortium of European Social Science Data Archives (CESSDA), a Linked Data\ncontrolled vocabulary. Our approach operates in a zero-shot setting,\nintegrating the controlled topic vocabulary directly within the input prompt.\nThis integration serves as a Large Context Windows approach, with the aim of\nimproving the results of the topic classification task.\n  We evaluated the performance of the LLMs in terms of internal consistency,\ninter-machine alignment, and agreement with human classification. Additionally,\nwe investigate the impact of contextual information (i.e., dataset description)\non the classification outcomes. Our findings suggest that ChatGPT and\nGoogleGemini outperform GoogleBard in terms of internal consistency as well as\nLLM-human-agreement. Interestingly, we found that contextual information had no\nsignificant impact on LLM performance.\n  This work proposes a novel approach that leverages LLMs for topic\nclassification of column headers using a controlled vocabulary, presenting a\npractical application of LLMs and Large Context Windows within the Semantic Web\ndomain. This approach has the potential to facilitate automated metadata\nenrichment, thereby enhancing dataset retrieval and the Findability,\nAccessibility, Interoperability, and Reusability (FAIR) of research data on the\nWeb.\n","authors":["Margherita Martorana","Tobias Kuhn","Lise Stork","Jacco van Ossenbruggen"],"pdf_url":"https://arxiv.org/pdf/2403.00884v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03708v2","updated":"2024-09-06T14:18:20Z","published":"2024-09-05T17:14:23Z","title":"RAG based Question-Answering for Contextual Response Prediction System","summary":"  Large Language Models (LLMs) have shown versatility in various Natural\nLanguage Processing (NLP) tasks, including their potential as effective\nquestion-answering systems. However, to provide precise and relevant\ninformation in response to specific customer queries in industry settings, LLMs\nrequire access to a comprehensive knowledge base to avoid hallucinations.\nRetrieval Augmented Generation (RAG) emerges as a promising technique to\naddress this challenge. Yet, developing an accurate question-answering\nframework for real-world applications using RAG entails several challenges: 1)\ndata availability issues, 2) evaluating the quality of generated content, and\n3) the costly nature of human evaluation. In this paper, we introduce an\nend-to-end framework that employs LLMs with RAG capabilities for industry use\ncases. Given a customer query, the proposed system retrieves relevant knowledge\ndocuments and leverages them, along with previous chat history, to generate\nresponse suggestions for customer service agents in the contact centers of a\nmajor retail company. Through comprehensive automated and human evaluations, we\nshow that this solution outperforms the current BERT-based algorithms in\naccuracy and relevance. Our findings suggest that RAG-based LLMs can be an\nexcellent support to human customer service representatives by lightening their\nworkload.\n","authors":["Sriram Veturi","Saurabh Vaichal","Reshma Lal Jagadheesh","Nafis Irtiza Tripto","Nian Yan"],"pdf_url":"https://arxiv.org/pdf/2409.03708v2.pdf","comment":"Accepted at the 1st Workshop on GenAI and RAG Systems for Enterprise,\n  CIKM'24. 6 pages"},{"id":"http://arxiv.org/abs/2408.09236v3","updated":"2024-09-06T13:34:16Z","published":"2024-08-17T16:04:31Z","title":"Hybrid Semantic Search: Unveiling User Intent Beyond Keywords","summary":"  This paper addresses the limitations of traditional keyword-based search in\nunderstanding user intent and introduces a novel hybrid search approach that\nleverages the strengths of non-semantic search engines, Large Language Models\n(LLMs), and embedding models. The proposed system integrates keyword matching,\nsemantic vector embeddings, and LLM-generated structured queries to deliver\nhighly relevant and contextually appropriate search results. By combining these\ncomplementary methods, the hybrid approach effectively captures both explicit\nand implicit user intent.The paper further explores techniques to optimize\nquery execution for faster response times and demonstrates the effectiveness of\nthis hybrid search model in producing comprehensive and accurate search\noutcomes.\n","authors":["Aman Ahluwalia","Bishwajit Sutradhar","Karishma Ghosh","Indrapal Yadav","Arpan Sheetal","Prashant Patil"],"pdf_url":"https://arxiv.org/pdf/2408.09236v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15675v2","updated":"2024-09-06T13:20:40Z","published":"2024-04-24T06:05:35Z","title":"Hi-Gen: Generative Retrieval For Large-Scale Personalized E-commerce\n  Search","summary":"  Leveraging generative retrieval (GR) techniques to enhance search systems is\nan emerging methodology that has shown promising results in recent years. In\nGR, a text-to-text model maps string queries directly to relevant document\nidentifiers (docIDs), dramatically simplifying the retrieval process. However,\nwhen applying most GR models in large-scale E-commerce for personalized item\nsearch, we must face two key problems in encoding and decoding. (1) Existing\ndocID generation methods ignore the encoding of efficiency information, which\nis critical in E-commerce. (2) The positional information is important in\ndecoding docIDs, while prior studies have not adequately discriminated the\nsignificance of positional information or well exploited the inherent\ninterrelation among these positions. To overcome these problems, we introduce\nan efficient Hierarchical encoding-decoding Generative retrieval method\n(Hi-Gen) for large-scale personalized E-commerce search systems. Specifically,\nwe first design a representation learning model using metric learning to learn\ndiscriminative feature representations of items to capture semantic relevance\nand efficiency information. Then, we propose a category-guided hierarchical\nclustering scheme that makes full use of the semantic and efficiency\ninformation of items to facilitate docID generation. Finally, we design a\nposition-aware loss to discriminate the importance of positions and mine the\ninherent interrelation between different tokens at the same position. This loss\nboosts the performance of the language model used in the decoding stage.\nBesides, we propose two variants of Hi-Gen (Hi-Gen-I2I and Hi-Gen-Cluster) to\nsupport online real-time large-scale recall in the online serving process.\nHi-Gen gets 3.30% and 4.62% improvements over SOTA for Recall@1 on the public\nand industry datasets, respectively.\n","authors":["Yanjing Wu","Yinfu Feng","Jian Wang","Wenji Zhou","Yunan Ye","Rong Xiao","Jun Xiao"],"pdf_url":"https://arxiv.org/pdf/2404.15675v2.pdf","comment":"Accepted by ICDM 2024"},{"id":"http://arxiv.org/abs/2409.04244v1","updated":"2024-09-06T12:51:10Z","published":"2024-09-06T12:51:10Z","title":"WarpAdam: A new Adam optimizer based on Meta-Learning approach","summary":"  Optimal selection of optimization algorithms is crucial for training deep\nlearning models. The Adam optimizer has gained significant attention due to its\nefficiency and wide applicability. However, to enhance the adaptability of\noptimizers across diverse datasets, we propose an innovative optimization\nstrategy by integrating the 'warped gradient descend'concept from Meta Learning\ninto the Adam optimizer. In the conventional Adam optimizer, gradients are\nutilized to compute estimates of gradient mean and variance, subsequently\nupdating model parameters. Our approach introduces a learnable distortion\nmatrix, denoted as P, which is employed for linearly transforming gradients.\nThis transformation slightly adjusts gradients during each iteration, enabling\nthe optimizer to better adapt to distinct dataset characteristics. By learning\nan appropriate distortion matrix P, our method aims to adaptively adjust\ngradient information across different data distributions, thereby enhancing\noptimization performance. Our research showcases the potential of this novel\napproach through theoretical insights and empirical evaluations. Experimental\nresults across various tasks and datasets validate the superiority of our\noptimizer that integrates the 'warped gradient descend' concept in terms of\nadaptability. Furthermore, we explore effective strategies for training the\nadaptation matrix P and identify scenarios where this method can yield optimal\nresults. In summary, this study introduces an innovative approach that merges\nthe 'warped gradient descend' concept from Meta Learning with the Adam\noptimizer. By introducing a learnable distortion matrix P within the optimizer,\nwe aim to enhance the model's generalization capability across diverse data\ndistributions, thus opening up new possibilities in the field of deep learning\noptimization.\n","authors":["Chengxi Pan","Junshang Chen","Jingrui Ye"],"pdf_url":"https://arxiv.org/pdf/2409.04244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02664v3","updated":"2024-09-06T11:38:00Z","published":"2024-05-04T13:25:06Z","title":"MedPromptExtract (Medical Data Extraction Tool): Anonymization and\n  Hi-fidelity Automated data extraction using NLP and prompt engineering","summary":"  Introduction: The labour-intensive nature of data extraction from sources\nlike discharge summaries (DS) poses significant obstacles to the digitisation\nof medical records particularly for low- and middle-income countries (LMICs).\nIn this paper we present a completely automated method MedPromptExtract to\nefficiently extract data from DS while maintaining confidentiality. Methods:\nThe source of data was Discharge Summaries (DS) from Kokilaben Dhirubhai Ambani\nHospital (KDAH) of patients having Acute Kidney Injury (AKI). A pre-existing\ntool EIGEN which leverages semi-supervised learning techniques for\nhigh-fidelity information extraction was used to anonymize the DS, Natural\nLanguage Processing (NLP) was used to extract data from regular fields. We used\nPrompt Engineering and Large Language Model(LLM) to extract custom clinical\ninformation from free flowing text describing the patients stay in the\nhospital. Twelve features associated with occurrence of AKI were extracted. The\nLLM responses were validated against clinicians annotations. Results: The\nMedPromptExtracttool first subjected DS to the anonymization pipeline which\ntook three seconds per summary. Successful anonymization was verified by\nclinicians, thereafter NLP pipeline extracted structured text from the\nanonymized pdfs at the rate of 0.2 seconds per summary with 100%\naccuracy.Finally DS were analysed by the LLM pipeline using Gemini Pro for the\ntwelve features. Accuracy metrics were calculated by comparing model responses\nto clinicians annotations with seven features achieving AUCs above 0.9,\nindicating high fidelity of the extraction process. Conclusion:\nMedPromptExtract serves as an automated adaptable tool for efficient data\nextraction from medical records with a dynamic user interface. Keywords:\nDigitizing Medical Records, Automated Anonymisation, Information Retrieval,\nLarge Language Models, Prompt Engineering\n","authors":["Roomani Srivastava","Suraj Prasad","Lipika Bhat","Sarvesh Deshpande","Barnali Das","Kshitij Jadhav"],"pdf_url":"https://arxiv.org/pdf/2405.02664v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04056v1","updated":"2024-09-06T06:53:45Z","published":"2024-09-06T06:53:45Z","title":"Refining Wikidata Taxonomy using Large Language Models","summary":"  Due to its collaborative nature, Wikidata is known to have a complex\ntaxonomy, with recurrent issues like the ambiguity between instances and\nclasses, the inaccuracy of some taxonomic paths, the presence of cycles, and\nthe high level of redundancy across classes. Manual efforts to clean up this\ntaxonomy are time-consuming and prone to errors or subjective decisions. We\npresent WiKC, a new version of Wikidata taxonomy cleaned automatically using a\ncombination of Large Language Models (LLMs) and graph mining techniques.\nOperations on the taxonomy, such as cutting links or merging classes, are\nperformed with the help of zero-shot prompting on an open-source LLM. The\nquality of the refined taxonomy is evaluated from both intrinsic and extrinsic\nperspectives, on a task of entity typing for the latter, showing the practical\ninterest of WiKC.\n","authors":["Yiwen Peng","Thomas Bonald","Mehwish Alam"],"pdf_url":"https://arxiv.org/pdf/2409.04056v1.pdf","comment":"ACM International Conference on Information and Knowledge Management,\n  Oct 2024, Boise, Idaho, United States"}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.16879v2","updated":"2024-09-06T17:17:16Z","published":"2024-08-29T20:05:02Z","title":"MSLIQA: Enhancing Learning Representations for Image Quality Assessment\n  through Multi-Scale Learning","summary":"  No-Reference Image Quality Assessment (NR-IQA) remains a challenging task due\nto the diversity of distortions and the lack of large annotated datasets. Many\nstudies have attempted to tackle these challenges by developing more accurate\nNR-IQA models, often employing complex and computationally expensive networks,\nor by bridging the domain gap between various distortions to enhance\nperformance on test datasets. In our work, we improve the performance of a\ngeneric lightweight NR-IQA model by introducing a novel augmentation strategy\nthat boosts its performance by almost 28\\%. This augmentation strategy enables\nthe network to better discriminate between different distortions in various\nparts of the image by zooming in and out. Additionally, the inclusion of\ntest-time augmentation further enhances performance, making our lightweight\nnetwork's results comparable to the current state-of-the-art models, simply\nthrough the use of augmentations.\n","authors":["Nasim Jamshidi Avanaki","Abhijay Ghildyal","Nabajeet Barman","Saman Zadtootaghaj"],"pdf_url":"https://arxiv.org/pdf/2408.16879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17057v2","updated":"2024-09-06T17:15:49Z","published":"2024-08-30T07:32:19Z","title":"LAR-IQA: A Lightweight, Accurate, and Robust No-Reference Image Quality\n  Assessment Model","summary":"  Recent advancements in the field of No-Reference Image Quality Assessment\n(NR-IQA) using deep learning techniques demonstrate high performance across\nmultiple open-source datasets. However, such models are typically very large\nand complex making them not so suitable for real-world deployment, especially\non resource- and battery-constrained mobile devices. To address this\nlimitation, we propose a compact, lightweight NR-IQA model that achieves\nstate-of-the-art (SOTA) performance on ECCV AIM UHD-IQA challenge validation\nand test datasets while being also nearly 5.7 times faster than the fastest\nSOTA model. Our model features a dual-branch architecture, with each branch\nseparately trained on synthetically and authentically distorted images which\nenhances the model's generalizability across different distortion types. To\nimprove robustness under diverse real-world visual conditions, we additionally\nincorporate multiple color spaces during the training process. We also\ndemonstrate the higher accuracy of recently proposed Kolmogorov-Arnold Networks\n(KANs) for final quality regression as compared to the conventional Multi-Layer\nPerceptrons (MLPs). Our evaluation considering various open-source datasets\nhighlights the practical, high-accuracy, and robust performance of our proposed\nlightweight model. Code: https://github.com/nasimjamshidi/LAR-IQA.\n","authors":["Nasim Jamshidi Avanaki","Abhijay Ghildyal","Nabajeet Barman","Saman Zadtootaghaj"],"pdf_url":"https://arxiv.org/pdf/2408.17057v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04013v1","updated":"2024-09-06T03:53:59Z","published":"2024-09-06T03:53:59Z","title":"3D-GP-LMVIC: Learning-based Multi-View Image Coding with 3D Gaussian\n  Geometric Priors","summary":"  Multi-view image compression is vital for 3D-related applications. To\neffectively model correlations between views, existing methods typically\npredict disparity between two views on a 2D plane, which works well for small\ndisparities, such as in stereo images, but struggles with larger disparities\ncaused by significant view changes. To address this, we propose a novel\napproach: learning-based multi-view image coding with 3D Gaussian geometric\npriors (3D-GP-LMVIC). Our method leverages 3D Gaussian Splatting to derive\ngeometric priors of the 3D scene, enabling more accurate disparity estimation\nacross views within the compression model. Additionally, we introduce a depth\nmap compression model to reduce redundancy in geometric information between\nviews. A multi-view sequence ordering method is also proposed to enhance\ncorrelations between adjacent views. Experimental results demonstrate that\n3D-GP-LMVIC surpasses both traditional and learning-based methods in\nperformance, while maintaining fast encoding and decoding speed.\n","authors":["Yujun Huang","Bin Chen","Niu Lian","Baoyi An","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2409.04013v1.pdf","comment":"19pages, 8 figures, conference"}]},"2024-09-05T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.03928v1","updated":"2024-09-05T22:22:57Z","published":"2024-09-05T22:22:57Z","title":"RETAIN: Interactive Tool for Regression Testing Guided LLM Migration","summary":"  Large Language Models (LLMs) are increasingly integrated into diverse\napplications. The rapid evolution of LLMs presents opportunities for developers\nto enhance applications continuously. However, this constant adaptation can\nalso lead to performance regressions during model migrations. While several\ninteractive tools have been proposed to streamline the complexity of prompt\nengineering, few address the specific requirements of regression testing for\nLLM Migrations. To bridge this gap, we introduce RETAIN (REgression Testing\nguided LLM migrAtIoN), a tool designed explicitly for regression testing in LLM\nMigrations. RETAIN comprises two key components: an interactive interface\ntailored to regression testing needs during LLM migrations, and an error\ndiscovery module that facilitates understanding of differences in model\nbehaviors. The error discovery module generates textual descriptions of various\nerrors or differences between model outputs, providing actionable insights for\nprompt refinement. Our automatic evaluation and empirical user studies\ndemonstrate that RETAIN, when compared to manual evaluation, enabled\nparticipants to identify twice as many errors, facilitated experimentation with\n75% more prompts, and achieves 12% higher metric scores in a given time frame.\n","authors":["Tanay Dixit","Daniel Lee","Sally Fang","Sai Sree Harsha","Anirudh Sureshan","Akash Maharaj","Yunyao Li"],"pdf_url":"https://arxiv.org/pdf/2409.03928v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2406.12580v2","updated":"2024-09-05T21:29:32Z","published":"2024-06-18T13:06:58Z","title":"Behavior-Dependent Linear Recurrent Units for Efficient Sequential\n  Recommendation","summary":"  Sequential recommender systems aims to predict the users' next interaction\nthrough user behavior modeling with various operators like RNNs and attentions.\nHowever, existing models generally fail to achieve the three golden principles\nfor sequential recommendation simultaneously, i.e., training efficiency,\nlow-cost inference, and strong performance. To this end, we propose RecBLR, an\nEfficient Sequential Recommendation Model based on Behavior-Dependent Linear\nRecurrent Units to accomplish the impossible triangle of the three principles.\nBy incorporating gating mechanisms and behavior-dependent designs into linear\nrecurrent units, our model significantly enhances user behavior modeling and\nrecommendation performance. Furthermore, we unlock the parallelizable training\nas well as inference efficiency for our model by designing a hardware-aware\nscanning acceleration algorithm with a customized CUDA kernel. Extensive\nexperiments on real-world datasets with varying lengths of user behavior\nsequences demonstrate RecBLR's remarkable effectiveness in simultaneously\nachieving all three golden principles - strong recommendation performance,\ntraining efficiency, and low-cost inference, while exhibiting excellent\nscalability to datasets with long user interaction histories.\n","authors":["Chengkai Liu","Jianghao Lin","Hanzhou Liu","Jianling Wang","James Caverlee"],"pdf_url":"https://arxiv.org/pdf/2406.12580v2.pdf","comment":"Accepted to CIKM 2024"},{"id":"http://arxiv.org/abs/2409.03504v1","updated":"2024-09-05T13:18:01Z","published":"2024-09-05T13:18:01Z","title":"HGAMN: Heterogeneous Graph Attention Matching Network for Multilingual\n  POI Retrieval at Baidu Maps","summary":"  The increasing interest in international travel has raised the demand of\nretrieving point of interests in multiple languages. This is even superior to\nfind local venues such as restaurants and scenic spots in unfamiliar languages\nwhen traveling abroad. Multilingual POI retrieval, enabling users to find\ndesired POIs in a demanded language using queries in numerous languages, has\nbecome an indispensable feature of today's global map applications such as\nBaidu Maps. This task is non-trivial because of two key challenges: (1)\nvisiting sparsity and (2) multilingual query-POI matching. To this end, we\npropose a Heterogeneous Graph Attention Matching Network (HGAMN) to\nconcurrently address both challenges. Specifically, we construct a\nheterogeneous graph that contains two types of nodes: POI node and query node\nusing the search logs of Baidu Maps. To alleviate challenge \\#1, we construct\nedges between different POI nodes to link the low-frequency POIs with the\nhigh-frequency ones, which enables the transfer of knowledge from the latter to\nthe former. To mitigate challenge \\#2, we construct edges between POI and query\nnodes based on the co-occurrences between queries and POIs, where queries in\ndifferent languages and formulations can be aggregated for individual POIs.\nMoreover, we develop an attention-based network to jointly learn node\nrepresentations of the heterogeneous graph and further design a cross-attention\nmodule to fuse the representations of both types of nodes for query-POI\nrelevance scoring. Extensive experiments conducted on large-scale real-world\ndatasets from Baidu Maps demonstrate the superiority and effectiveness of\nHGAMN. In addition, HGAMN has already been deployed in production at Baidu\nMaps, and it successfully keeps serving hundreds of millions of requests every\nday.\n","authors":["Jizhou Huang","Haifeng Wang","Yibo Sun","Miao Fan","Zhengjie Huang","Chunyuan Yuan","Yawen Li"],"pdf_url":"https://arxiv.org/pdf/2409.03504v1.pdf","comment":"Accepted by KDD'21"},{"id":"http://arxiv.org/abs/2409.03449v1","updated":"2024-09-05T11:56:40Z","published":"2024-09-05T11:56:40Z","title":"MOBIUS: Towards the Next Generation of Query-Ad Matching in Baidu's\n  Sponsored Search","summary":"  Baidu runs the largest commercial web search engine in China, serving\nhundreds of millions of online users every day in response to a great variety\nof queries. In order to build a high-efficiency sponsored search engine, we\nused to adopt a three-layer funnel-shaped structure to screen and sort hundreds\nof ads from billions of ad candidates subject to the requirement of low\nresponse latency and the restraints of computing resources. Given a user query,\nthe top matching layer is responsible for providing semantically relevant ad\ncandidates to the next layer, while the ranking layer at the bottom concerns\nmore about business indicators (e.g., CPM, ROI, etc.) of those ads. The clear\nseparation between the matching and ranking objectives results in a lower\ncommercial return. The Mobius project has been established to address this\nserious issue. It is our first attempt to train the matching layer to consider\nCPM as an additional optimization objective besides the query-ad relevance, via\ndirectly predicting CTR (click-through rate) from billions of query-ad pairs.\nSpecifically, this paper will elaborate on how we adopt active learning to\novercome the insufficiency of click history at the matching layer when training\nour neural click networks offline, and how we use the SOTA ANN search technique\nfor retrieving ads more efficiently (Here ``ANN'' stands for approximate\nnearest neighbor search). We contribute the solutions to Mobius-V1 as the first\nversion of our next generation query-ad matching system.\n","authors":["Miao Fan","Jiacheng Guo","Shuai Zhu","Shuo Miao","Mingming Sun","Ping Li"],"pdf_url":"https://arxiv.org/pdf/2409.03449v1.pdf","comment":"Accepted by KDD'19"},{"id":"http://arxiv.org/abs/2409.02727v2","updated":"2024-09-05T07:17:59Z","published":"2024-09-04T14:01:48Z","title":"Pooling And Attention: What Are Effective Designs For LLM-Based\n  Embedding Models?","summary":"  The significant advancements of Large Language Models (LLMs) in generative\ntasks have led to a growing body of work exploring LLM-based embedding models.\nWhile these models, employing different pooling and attention strategies, have\nachieved state-of-the-art performance on public embedding benchmarks, questions\nstill arise about what constitutes an effective design for LLM-based embedding\nmodels. However, these models are often trained on different datasets, using\ndifferent LLM base models or training settings. Moreover, evaluations on public\nembedding benchmarks often fail to report statistical significance, making it\ndifficult to determine which designs truly contribute to final performance.\nThis complicates the process for practitioners seeking optimal training recipes\nfor LLM-based embedding models. In this study, we conduct a large-scale\nexperiment by training a series of LLM-based embedding models using the same\ntraining data and base model but differing in their pooling and attention\nstrategies. The results show that there is no one-size-fits-all solution: while\nbidirectional attention and an additional trainable pooling layer outperform in\ntext similarity and information retrieval tasks, they do not significantly\nsurpass simpler designs like EOS-last token pooling and default causal\nattention in clustering and classification tasks. Furthermore, we propose a new\npooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs\nof all hidden layers, rather than just the last layer, using a cross-attention\nnetwork. This method proves to be statistically superior in text similarity and\nretrieval tasks compared to existing pooling methods. Overall, this paper sheds\nlight on effective training strategies for LLM-based embedding models.\n","authors":["Yixuan Tang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2409.02727v2.pdf","comment":"https://github.com/yixuantt/PoolingAndAttn"},{"id":"http://arxiv.org/abs/2409.03294v1","updated":"2024-09-05T06:59:56Z","published":"2024-09-05T06:59:56Z","title":"Federated Prototype-based Contrastive Learning for Privacy-Preserving\n  Cross-domain Recommendation","summary":"  Cross-domain recommendation (CDR) aims to improve recommendation accuracy in\nsparse domains by transferring knowledge from data-rich domains. However,\nexisting CDR methods often assume the availability of user-item interaction\ndata across domains, overlooking user privacy concerns. Furthermore, these\nmethods suffer from performance degradation in scenarios with sparse\noverlapping users, as they typically depend on a large number of fully shared\nusers for effective knowledge transfer. To address these challenges, we propose\na Federated Prototype-based Contrastive Learning (CL) method for\nPrivacy-Preserving CDR, named FedPCL-CDR. This approach utilizes\nnon-overlapping user information and prototypes to improve multi-domain\nperformance while protecting user privacy. FedPCL-CDR comprises two modules:\nlocal domain (client) learning and global server aggregation. In the local\ndomain, FedPCL-CDR clusters all user data to learn representative prototypes,\neffectively utilizing non-overlapping user information and addressing the\nsparse overlapping user issue. It then facilitates knowledge transfer by\nemploying both local and global prototypes returned from the server in a CL\nmanner. Simultaneously, the global server aggregates representative prototypes\nfrom local domains to learn both local and global prototypes. The combination\nof prototypes and federated learning (FL) ensures that sensitive user data\nremains decentralized, with only prototypes being shared across domains,\nthereby protecting user privacy. Extensive experiments on four CDR tasks using\ntwo real-world datasets demonstrate that FedPCL-CDR outperforms the\nstate-of-the-art baselines.\n","authors":["Li Wang","Quangui Zhang","Lei Sang","Qiang Wu","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2409.03294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03284v1","updated":"2024-09-05T06:49:14Z","published":"2024-09-05T06:49:14Z","title":"iText2KG: Incremental Knowledge Graphs Construction Using Large Language\n  Models","summary":"  Most available data is unstructured, making it challenging to access valuable\ninformation. Automatically building Knowledge Graphs (KGs) is crucial for\nstructuring data and making it accessible, allowing users to search for\ninformation effectively. KGs also facilitate insights, inference, and\nreasoning. Traditional NLP methods, such as named entity recognition and\nrelation extraction, are key in information retrieval but face limitations,\nincluding the use of predefined entity types and the need for supervised\nlearning. Current research leverages large language models' capabilities, such\nas zero- or few-shot learning. However, unresolved and semantically duplicated\nentities and relations still pose challenges, leading to inconsistent graphs\nand requiring extensive post-processing. Additionally, most approaches are\ntopic-dependent. In this paper, we propose iText2KG, a method for incremental,\ntopic-independent KG construction without post-processing. This plug-and-play,\nzero-shot method is applicable across a wide range of KG construction scenarios\nand comprises four modules: Document Distiller, Incremental Entity Extractor,\nIncremental Relation Extractor, and Graph Integrator and Visualization. Our\nmethod demonstrates superior performance compared to baseline methods across\nthree scenarios: converting scientific papers to graphs, websites to graphs,\nand CVs to graphs.\n","authors":["Yassir Lairgi","Ludovic Moncla","R√©my Cazabet","Khalid Benabdeslem","Pierre Cl√©au"],"pdf_url":"https://arxiv.org/pdf/2409.03284v1.pdf","comment":"Accepted at The International Web Information Systems Engineering\n  conference (the WISE conference) 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.03902v1","updated":"2024-09-05T20:22:01Z","published":"2024-09-05T20:22:01Z","title":"WaterMAS: Sharpness-Aware Maximization for Neural Network Watermarking","summary":"  Nowadays, deep neural networks are used for solving complex tasks in several\ncritical applications and protecting both their integrity and intellectual\nproperty rights (IPR) has become of utmost importance. To this end, we advance\nWaterMAS, a substitutive, white-box neural network watermarking method that\nimproves the trade-off among robustness, imperceptibility, and computational\ncomplexity, while making provisions for increased data payload and security.\nWasterMAS insertion keeps unchanged the watermarked weights while sharpening\ntheir underlying gradient space. The robustness is thus ensured by limiting the\nattack's strength: even small alterations of the watermarked weights would\nimpact the model's performance. The imperceptibility is ensured by inserting\nthe watermark during the training process. The relationship among the WaterMAS\ndata payload, imperceptibility, and robustness properties is discussed. The\nsecret key is represented by the positions of the weights conveying the\nwatermark, randomly chosen through multiple layers of the model. The security\nis evaluated by investigating the case in which an attacker would intercept the\nkey. The experimental validations consider 5 models and 2 tasks (VGG16,\nResNet18, MobileNetV3, SwinT for CIFAR10 image classification, and DeepLabV3\nfor Cityscapes image segmentation) as well as 4 types of attacks (Gaussian\nnoise addition, pruning, fine-tuning, and quantization). The code will be\nreleased open-source upon acceptance of the article.\n","authors":["Carl De Sousa Trias","Mihai Mitrea","Attilio Fiandrotti","Marco Cagnazzo","Sumanta Chaudhuri","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2409.03902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03844v1","updated":"2024-09-05T18:12:11Z","published":"2024-09-05T18:12:11Z","title":"MetaBGM: Dynamic Soundtrack Transformation For Continuous Multi-Scene\n  Experiences With Ambient Awareness And Personalization","summary":"  This paper introduces MetaBGM, a groundbreaking framework for generating\nbackground music that adapts to dynamic scenes and real-time user interactions.\nWe define multi-scene as variations in environmental contexts, such as\ntransitions in game settings or movie scenes. To tackle the challenge of\nconverting backend data into music description texts for audio generation\nmodels, MetaBGM employs a novel two-stage generation approach that transforms\ncontinuous scene and user state data into these texts, which are then fed into\nan audio generation model for real-time soundtrack creation. Experimental\nresults demonstrate that MetaBGM effectively generates contextually relevant\nand dynamic background music for interactive applications.\n","authors":["Haoxuan Liu","Zihao Wang","Haorong Hong","Youwei Feng","Jiaxin Yu","Han Diao","Yunfei Xu","Kejun Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.03844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.03605v1","updated":"2024-09-05T15:11:40Z","published":"2024-09-05T15:11:40Z","title":"SegTalker: Segmentation-based Talking Face Generation with Mask-guided\n  Local Editing","summary":"  Audio-driven talking face generation aims to synthesize video with lip\nmovements synchronized to input audio. However, current generative techniques\nface challenges in preserving intricate regional textures (skin, teeth). To\naddress the aforementioned challenges, we propose a novel framework called\nSegTalker to decouple lip movements and image textures by introducing\nsegmentation as intermediate representation. Specifically, given the mask of\nimage employed by a parsing network, we first leverage the speech to drive the\nmask and generate talking segmentation. Then we disentangle semantic regions of\nimage into style codes using a mask-guided encoder. Ultimately, we inject the\npreviously generated talking segmentation and style codes into a mask-guided\nStyleGAN to synthesize video frame. In this way, most of textures are fully\npreserved. Moreover, our approach can inherently achieve background separation\nand facilitate mask-guided facial local editing. In particular, by editing the\nmask and swapping the region textures from a given reference image (e.g. hair,\nlip, eyebrows), our approach enables facial editing seamlessly when generating\ntalking face video. Experiments demonstrate that our proposed approach can\neffectively preserve texture details and generate temporally consistent video\nwhile remaining competitive in lip synchronization. Quantitative and\nqualitative results on the HDTF and MEAD datasets illustrate the superior\nperformance of our method over existing methods.\n","authors":["Lingyu Xiong","Xize Cheng","Jintao Tan","Xianjia Wu","Xiandong Li","Lei Zhu","Fei Ma","Minglei Li","Huang Xu","Zhihu Hu"],"pdf_url":"https://arxiv.org/pdf/2409.03605v1.pdf","comment":"10 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2409.03385v1","updated":"2024-09-05T09:44:43Z","published":"2024-09-05T09:44:43Z","title":"Make Graph-based Referring Expression Comprehension Great Again through\n  Expression-guided Dynamic Gating and Regression","summary":"  One common belief is that with complex models and pre-training on large-scale\ndatasets, transformer-based methods for referring expression comprehension\n(REC) perform much better than existing graph-based methods. We observe that\nsince most graph-based methods adopt an off-the-shelf detector to locate\ncandidate objects (i.e., regions detected by the object detector), they face\ntwo challenges that result in subpar performance: (1) the presence of\nsignificant noise caused by numerous irrelevant objects during reasoning, and\n(2) inaccurate localization outcomes attributed to the provided detector. To\naddress these issues, we introduce a plug-and-adapt module guided by\nsub-expressions, called dynamic gate constraint (DGC), which can adaptively\ndisable irrelevant proposals and their connections in graphs during reasoning.\nWe further introduce an expression-guided regression strategy (EGR) to refine\nlocation prediction. Extensive experimental results on the RefCOCO, RefCOCO+,\nRefCOCOg, Flickr30K, RefClef, and Ref-reasoning datasets demonstrate the\neffectiveness of the DGC module and the EGR strategy in consistently boosting\nthe performances of various graph-based REC methods. Without any pretaining,\nthe proposed graph-based method achieves better performance than the\nstate-of-the-art (SOTA) transformer-based methods.\n","authors":["Jingcheng Ke","Dele Wang","Jun-Cheng Chen","I-Hong Jhuo","Chia-Wen Lin","Yen-Yu Lin"],"pdf_url":"https://arxiv.org/pdf/2409.03385v1.pdf","comment":"12 pages to appear in IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2404.13993v4","updated":"2024-09-05T02:21:42Z","published":"2024-04-22T08:59:35Z","title":"Zero-Shot Character Identification and Speaker Prediction in Comics via\n  Iterative Multimodal Fusion","summary":"  Recognizing characters and predicting speakers of dialogue are critical for\ncomic processing tasks, such as voice generation or translation. However,\nbecause characters vary by comic title, supervised learning approaches like\ntraining character classifiers which require specific annotations for each\ncomic title are infeasible. This motivates us to propose a novel zero-shot\napproach, allowing machines to identify characters and predict speaker names\nbased solely on unannotated comic images. In spite of their importance in\nreal-world applications, these task have largely remained unexplored due to\nchallenges in story comprehension and multimodal integration. Recent large\nlanguage models (LLMs) have shown great capability for text understanding and\nreasoning, while their application to multimodal content analysis is still an\nopen problem. To address this problem, we propose an iterative multimodal\nframework, the first to employ multimodal information for both character\nidentification and speaker prediction tasks. Our experiments demonstrate the\neffectiveness of the proposed framework, establishing a robust baseline for\nthese tasks. Furthermore, since our method requires no training data or\nannotations, it can be used as-is on any comic series.\n","authors":["Yingxuan Li","Ryota Hinami","Kiyoharu Aizawa","Yusuke Matsui"],"pdf_url":"https://arxiv.org/pdf/2404.13993v4.pdf","comment":"Accepted to ACM Multimedia 2024. Project page:\n  https://liyingxuan1012.github.io/zeroshot-speaker-prediction ; Github repo:\n  https://github.com/liyingxuan1012/zeroshot-speaker-prediction"}]},"2024-09-04T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.02864v1","updated":"2024-09-04T16:43:14Z","published":"2024-09-04T16:43:14Z","title":"Bioinformatics Retrieval Augmentation Data (BRAD) Digital Assistant","summary":"  We present a prototype for a Bioinformatics Retrieval Augmentation Data\n(BRAD) digital assistant. BRAD integrates a suite of tools to handle a wide\nrange of bioinformatics tasks, from code execution to online search. We\ndemonstrate BRAD's capabilities through (1) improved question-and-answering\nwith retrieval augmented generation (RAG), (2) BRAD's ability to run and write\ncomplex software pipelines, and (3) BRAD's ability to organize and distribute\ntasks across individual and teams of agents. We use BRAD for automation of\nbioinformatics workflows, performing tasks ranging from gene enrichment and\nsearching the archive to automatic code generation and running biomarker\nidentification pipelines. BRAD is a step toward the ultimate goal to develop a\ndigital twin of laboratories driven by self-contained loops for hypothesis\ngeneration and testing of digital biology experiments.\n","authors":["Joshua Pickard","Marc Andrew Choi","Natalie Oliven","Cooper Stansbury","Jillian Cwycyshyn","Nicholas Galioto","Alex Gorodetsky","Alvaro Velasquez","Indika Rajapakse"],"pdf_url":"https://arxiv.org/pdf/2409.02864v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00847v2","updated":"2024-09-04T16:39:22Z","published":"2024-09-01T21:30:14Z","title":"The Design of an LLM-powered Unstructured Analytics System","summary":"  LLMs demonstrate an uncanny ability to process unstructured data, and as\nsuch, have the potential to go beyond search and run complex, semantic analyses\nat scale. We describe the design of an unstructured analytics system, Aryn, and\nthe tenets and use cases that motivate its design. With Aryn, users can specify\nqueries in natural language and the system automatically determines a semantic\nplan and executes it to compute an answer from a large collection of\nunstructured documents using LLMs. At the core of Aryn is Sycamore, a\ndeclarative document processing engine, built using Ray, that provides a\nreliable distributed abstraction called DocSets. Sycamore allows users to\nanalyze, enrich, and transform complex documents at scale. Aryn also comprises\nLuna, a query planner that translates natural language queries to Sycamore\nscripts, and the Aryn Partitioner, which takes raw PDFs and document images,\nand converts them to DocSets for downstream processing. Using Aryn, we\ndemonstrate a real world use case for analyzing accident reports from the\nNational Transportation Safety Board (NTSB), and discuss some of the major\nchallenges we encountered in deploying Aryn in the wild.\n","authors":["Eric Anderson","Jonathan Fritz","Austin Lee","Bohou Li","Mark Lindblad","Henry Lindeman","Alex Meyer","Parth Parmar","Tanvi Ranade","Mehul A. Shah","Benjamin Sowell","Dan Tecuci","Vinayak Thapliyal","Matt Welsh"],"pdf_url":"https://arxiv.org/pdf/2409.00847v2.pdf","comment":"6 pages, 3 figures, fixed typos"},{"id":"http://arxiv.org/abs/2409.02856v1","updated":"2024-09-04T16:29:25Z","published":"2024-09-04T16:29:25Z","title":"Building a Scalable, Effective, and Steerable Search and Ranking\n  Platform","summary":"  Modern e-commerce platforms offer vast product selections, making it\ndifficult for customers to find items that they like and that are relevant to\ntheir current session intent. This is why it is key for e-commerce platforms to\nhave near real-time scalable and adaptable personalized ranking and search\nsystems. While numerous methods exist in the scientific literature for building\nsuch systems, many are unsuitable for large-scale industrial use due to\ncomplexity and performance limitations. Consequently, industrial ranking\nsystems often resort to computationally efficient yet simplistic retrieval or\ncandidate generation approaches, which overlook near real-time and\nheterogeneous customer signals, which results in a less personalized and\nrelevant experience. Moreover, related customer experiences are served by\ncompletely different systems, which increases complexity, maintenance, and\ninconsistent experiences.\n  In this paper, we present a personalized, adaptable near real-time ranking\nplatform that is reusable across various use cases, such as browsing and\nsearch, and that is able to cater to millions of items and customers under\nheavy load (thousands of requests per second). We employ transformer-based\nmodels through different ranking layers which can learn complex behavior\npatterns directly from customer action sequences while being able to\nincorporate temporal (e.g. in-session) and contextual information. We validate\nour system through a series of comprehensive offline and online real-world\nexperiments at a large online e-commerce platform, and we demonstrate its\nsuperiority when compared to existing systems, both in terms of customer\nexperience as well as in net revenue. Finally, we share the lessons learned\nfrom building a comprehensive, modern ranking platform for use in a large-scale\ne-commerce environment.\n","authors":["Marjan Celikik","Jacek Wasilewski","Ana Peleteiro Ramallo","Alexey Kurennoy","Evgeny Labzin","Danilo Ascione","Tural Gurbanov","G√©raud Le Falher","Andrii Dzhoha","Ian Harris"],"pdf_url":"https://arxiv.org/pdf/2409.02856v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03883v2","updated":"2024-09-04T14:00:42Z","published":"2023-02-08T05:12:54Z","title":"Multimodal Recommender Systems: A Survey","summary":"  The recommender system (RS) has been an integral toolkit of online services.\nThey are equipped with various deep learning techniques to model user\npreference based on identifier and attribute information. With the emergence of\nmultimedia services, such as short videos, news and etc., understanding these\ncontents while recommending becomes critical. Besides, multimodal features are\nalso helpful in alleviating the problem of data sparsity in RS. Thus,\nMultimodal Recommender System (MRS) has attracted much attention from both\nacademia and industry recently. In this paper, we will give a comprehensive\nsurvey of the MRS models, mainly from technical views. First, we conclude the\ngeneral procedures and major challenges for MRS. Then, we introduce the\nexisting MRS models according to four categories, i.e., Modality Encoder,\nFeature Interaction, Feature Enhancement and Model Optimization. Besides, to\nmake it convenient for those who want to research this field, we also summarize\nthe dataset and code resources. Finally, we discuss some promising future\ndirections of MRS and conclude this paper. To access more details of the\nsurveyed papers, such as implementation code, we open source a repository.\n","authors":["Qidong Liu","Jiaxi Hu","Yutian Xiao","Xiangyu Zhao","Jingtong Gao","Wanyu Wang","Qing Li","Jiliang Tang"],"pdf_url":"https://arxiv.org/pdf/2302.03883v2.pdf","comment":"accepted by CSUR"},{"id":"http://arxiv.org/abs/2409.00702v2","updated":"2024-09-04T13:19:42Z","published":"2024-09-01T12:11:48Z","title":"MARS: Matching Attribute-aware Representations for Text-based Sequential\n  Recommendation","summary":"  Sequential recommendation aims to predict the next item a user is likely to\nprefer based on their sequential interaction history. Recently, text-based\nsequential recommendation has emerged as a promising paradigm that uses\npre-trained language models to exploit textual item features to enhance\nperformance and facilitate knowledge transfer to unseen datasets. However,\nexisting text-based recommender models still struggle with two key challenges:\n(i) representing users and items with multiple attributes, and (ii) matching\nitems with complex user interests. To address these challenges, we propose a\nnovel model, Matching Attribute-aware Representations for Text-based Sequential\nRecommendation (MARS). MARS extracts detailed user and item representations\nthrough attribute-aware text encoding, capturing diverse user intents with\nmultiple attribute-aware representations. It then computes user-item scores via\nattribute-wise interaction matching, effectively capturing attribute-level user\npreferences. Our extensive experiments demonstrate that MARS significantly\noutperforms existing sequential models, achieving improvements of up to 24.43%\nand 29.26% in Recall@10 and NDCG@10 across five benchmark datasets. Code is\navailable at https://github.com/junieberry/MARS\n","authors":["Hyunsoo Kim","Junyoung Kim","Minjin Choi","Sunkyung Lee","Jongwuk Lee"],"pdf_url":"https://arxiv.org/pdf/2409.00702v2.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2409.02685v1","updated":"2024-09-04T13:16:55Z","published":"2024-09-04T13:16:55Z","title":"RouterRetriever: Exploring the Benefits of Routing over Multiple Expert\n  Embedding Models","summary":"  Information retrieval methods often rely on a single embedding model trained\non large, general-domain datasets like MSMARCO. While this approach can produce\na retriever with reasonable overall performance, models trained on\ndomain-specific data often yield better results within their respective\ndomains. While prior work in information retrieval has tackled this through\nmulti-task training, the topic of combining multiple domain-specific expert\nretrievers remains unexplored, despite its popularity in language model\ngeneration. In this work, we introduce RouterRetriever, a retrieval model that\nleverages multiple domain-specific experts along with a routing mechanism to\nselect the most appropriate expert for each query. It is lightweight and allows\neasy addition or removal of experts without additional training. Evaluation on\nthe BEIR benchmark demonstrates that RouterRetriever outperforms both\nMSMARCO-trained (+2.1 absolute nDCG@10) and multi-task trained (+3.2) models.\nThis is achieved by employing our routing mechanism, which surpasses other\nrouting techniques (+1.8 on average) commonly used in language modeling.\nFurthermore, the benefit generalizes well to other datasets, even in the\nabsence of a specific expert on the dataset. To our knowledge, RouterRetriever\nis the first work to demonstrate the advantages of using multiple\ndomain-specific expert embedding models with effective routing over a single,\ngeneral-purpose embedding model in retrieval tasks.\n","authors":["Hyunji Lee","Luca Soldaini","Arman Cohan","Minjoon Seo","Kyle Lo"],"pdf_url":"https://arxiv.org/pdf/2409.02685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09979v2","updated":"2024-09-04T12:33:24Z","published":"2024-06-14T12:41:07Z","title":"HIRO: Hierarchical Information Retrieval Optimization","summary":"  Retrieval-Augmented Generation (RAG) has revolutionized natural language\nprocessing by dynamically integrating external knowledge into Large Language\nModels (LLMs), addressing their limitation of static training datasets. Recent\nimplementations of RAG leverage hierarchical data structures, which organize\ndocuments at various levels of summarization and information density. This\ncomplexity, however, can cause LLMs to \"choke\" on information overload,\nnecessitating more sophisticated querying mechanisms. In this context, we\nintroduce Hierarchical Information Retrieval Optimization (HIRO), a novel\nquerying approach that employs a Depth-First Search (DFS)-based recursive\nsimilarity score calculation and branch pruning. This method uniquely minimizes\nthe context delivered to the LLM without informational loss, effectively\nmanaging the challenge of excessive data. HIRO's refined approach is validated\nby a 10.85% improvement in performance on the NarrativeQA dataset.\n","authors":["Krish Goel","Mahek Chandak"],"pdf_url":"https://arxiv.org/pdf/2406.09979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07107v4","updated":"2024-09-04T11:39:56Z","published":"2023-08-14T12:47:22Z","title":"Large Language Models for Information Retrieval: A Survey","summary":"  As a primary means of information acquisition, information retrieval (IR)\nsystems, such as search engines, have integrated themselves into our daily\nlives. These systems also serve as components of dialogue, question-answering,\nand recommender systems. The trajectory of IR has evolved dynamically from its\norigins in term-based methods to its integration with advanced neural models.\nWhile the neural models excel at capturing complex contextual signals and\nsemantic nuances, thereby reshaping the IR landscape, they still face\nchallenges such as data scarcity, interpretability, and the generation of\ncontextually plausible yet potentially inaccurate responses. This evolution\nrequires a combination of both traditional methods (such as term-based sparse\nretrieval methods with rapid response) and modern neural architectures (such as\nlanguage models with powerful language understanding capacity). Meanwhile, the\nemergence of large language models (LLMs), typified by ChatGPT and GPT-4, has\nrevolutionized natural language processing due to their remarkable language\nunderstanding, generation, generalization, and reasoning abilities.\nConsequently, recent research has sought to leverage LLMs to improve IR\nsystems. Given the rapid evolution of this research trajectory, it is necessary\nto consolidate existing methodologies and provide nuanced insights through a\ncomprehensive overview. In this survey, we delve into the confluence of LLMs\nand IR systems, including crucial aspects such as query rewriters, retrievers,\nrerankers, and readers. Additionally, we explore promising directions, such as\nsearch agents, within this expanding field.\n","authors":["Yutao Zhu","Huaying Yuan","Shuting Wang","Jiongnan Liu","Wenhan Liu","Chenlong Deng","Haonan Chen","Zheng Liu","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2308.07107v4.pdf","comment":"updated to version 3"},{"id":"http://arxiv.org/abs/2409.02599v1","updated":"2024-09-04T10:30:11Z","published":"2024-09-04T10:30:11Z","title":"A Fashion Item Recommendation Model in Hyperbolic Space","summary":"  In this work, we propose a fashion item recommendation model that\nincorporates hyperbolic geometry into user and item representations. Using\nhyperbolic space, our model aims to capture implicit hierarchies among items\nbased on their visual data and users' purchase history. During training, we\napply a multi-task learning framework that considers both hyperbolic and\nEuclidean distances in the loss function. Our experiments on three data sets\nshow that our model performs better than previous models trained in Euclidean\nspace only, confirming the effectiveness of our model. Our ablation studies\nshow that multi-task learning plays a key role, and removing the Euclidean loss\nsubstantially deteriorates the model performance.\n","authors":["Ryotaro Shimizu","Yu Wang","Masanari Kimura","Yuki Hirakawa","Takashi Wada","Yuki Saito","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2409.02599v1.pdf","comment":"This work was presented at the CVFAD Workshop at CVPR 2024"},{"id":"http://arxiv.org/abs/2409.02580v1","updated":"2024-09-04T10:03:09Z","published":"2024-09-04T10:03:09Z","title":"AlignGroup: Learning and Aligning Group Consensus with Member\n  Preferences for Group Recommendation","summary":"  Group activities are important behaviors in human society, providing\npersonalized recommendations for groups is referred to as the group\nrecommendation task. Existing methods can usually be categorized into two\nstrategies to infer group preferences: 1) determining group preferences by\naggregating members' personalized preferences, and 2) inferring group consensus\nby capturing group members' coherent decisions after common compromises.\nHowever, the former would suffer from the lack of group-level considerations,\nand the latter overlooks the fine-grained preferences of individual users. To\nthis end, we propose a novel group recommendation method AlignGroup, which\nfocuses on both group consensus and individual preferences of group members to\ninfer the group decision-making. Specifically, AlignGroup explores group\nconsensus through a well-designed hypergraph neural network that efficiently\nlearns intra- and inter-group relationships. Moreover, AlignGroup innovatively\nutilizes a self-supervised alignment task to capture fine-grained group\ndecision-making by aligning the group consensus with members' common\npreferences. Extensive experiments on two real-world datasets validate that our\nAlignGroup outperforms the state-of-the-art on both the group recommendation\ntask and the user recommendation task, as well as outperforms the efficiency of\nmost baselines.\n","authors":["Jinfeng Xu","Zheyu Chen","Jinze Li","Shuo Yang","Hewei Wang","Edith C. -H. Ngai"],"pdf_url":"https://arxiv.org/pdf/2409.02580v1.pdf","comment":"10 pages, accepted by CIKM 2024"},{"id":"http://arxiv.org/abs/2409.02571v1","updated":"2024-09-04T09:41:52Z","published":"2024-09-04T09:41:52Z","title":"iRangeGraph: Improvising Range-dedicated Graphs for Range-filtering\n  Nearest Neighbor Search","summary":"  Range-filtering approximate nearest neighbor (RFANN) search is attracting\nincreasing attention in academia and industry. Given a set of data objects,\neach being a pair of a high-dimensional vector and a numeric value, an RFANN\nquery with a vector and a numeric range as parameters returns the data object\nwhose numeric value is in the query range and whose vector is nearest to the\nquery vector. To process this query, a recent study proposes to build $O(n^2)$\ndedicated graph-based indexes for all possible query ranges to enable efficient\nprocessing on a database of $n$ objects. As storing all these indexes is\nprohibitively expensive, the study constructs compressed indexes instead, which\nreduces the memory consumption considerably. However, this incurs suboptimal\nperformance because the compression is lossy. In this study, instead of\nmaterializing a compressed index for every possible query range in preparation\nfor querying, we materialize graph-based indexes, called elemental graphs, for\na moderate number of ranges. We then provide an effective and efficient\nalgorithm that during querying can construct an index for any query range using\nthe elemental graphs. We prove that the time needed to construct such an index\nis low. We also cover an experimental study on real-world datasets that\nprovides evidence that the materialized elemental graphs only consume moderate\nspace and that the proposed method is capable of superior and stable query\nperformance across different query workloads.\n","authors":["Yuexuan Xu","Jianyang Gao","Yutong Gou","Cheng Long","Christian S. Jensen"],"pdf_url":"https://arxiv.org/pdf/2409.02571v1.pdf","comment":"The paper has been accepted by SIGMOD 2025"},{"id":"http://arxiv.org/abs/2404.06900v3","updated":"2024-09-04T06:55:21Z","published":"2024-04-10T10:45:30Z","title":"NFARec: A Negative Feedback-Aware Recommender Model","summary":"  Graph neural network (GNN)-based models have been extensively studied for\nrecommendations, as they can extract high-order collaborative signals\naccurately which is required for high-quality recommender systems. However,\nthey neglect the valuable information gained through negative feedback in two\naspects: (1) different users might hold opposite feedback on the same item,\nwhich hampers optimal information propagation in GNNs, and (2) even when an\nitem vastly deviates from users' preferences, they might still choose it and\nprovide a negative rating. In this paper, we propose a negative feedback-aware\nrecommender model (NFARec) that maximizes the leverage of negative feedback. To\ntransfer information to multi-hop neighbors along an optimal path effectively,\nNFARec adopts a feedback-aware correlation that guides hypergraph convolutions\n(HGCs) to learn users' structural representations. Moreover, NFARec\nincorporates an auxiliary task - predicting the feedback sentiment polarity\n(i.e., positive or negative) of the next interaction - based on the Transformer\nHawkes Process. The task is beneficial for understanding users by learning the\nsentiment expressed in their previous sequential feedback patterns and\npredicting future interactions. Extensive experiments demonstrate that NFARec\noutperforms competitive baselines. Our source code and data are released at\nhttps://github.com/WangXFng/NFARec.\n","authors":["Xinfeng Wang","Fumiyo Fukumoto","Jin Cui","Yoshimi Suzuki","Dongjin Yu"],"pdf_url":"https://arxiv.org/pdf/2404.06900v3.pdf","comment":"Accepted to SIGIR 2024"},{"id":"http://arxiv.org/abs/2404.06895v3","updated":"2024-09-04T06:51:55Z","published":"2024-04-10T10:38:24Z","title":"CaDRec: Contextualized and Debiased Recommender Model","summary":"  Recommender models aimed at mining users' behavioral patterns have raised\ngreat attention as one of the essential applications in daily life. Recent work\non graph neural networks (GNNs) or debiasing methods has attained remarkable\ngains. However, they still suffer from (1) over-smoothing node embeddings\ncaused by recursive convolutions with GNNs, and (2) the skewed distribution of\ninteractions due to popularity and user-individual biases. This paper proposes\na contextualized and debiased recommender model (CaDRec). To overcome the\nover-smoothing issue, we explore a novel hypergraph convolution operator that\ncan select effective neighbors during convolution by introducing both\nstructural context and sequential context. To tackle the skewed distribution,\nwe propose two strategies for disentangling interactions: (1) modeling\nindividual biases to learn unbiased item embeddings, and (2) incorporating item\npopularity with positional encoding. Moreover, we mathematically show that the\nimbalance of the gradients to update item embeddings exacerbates the popularity\nbias, thus adopting regularization and weighting schemes as solutions.\nExtensive experiments on four datasets demonstrate the superiority of the\nCaDRec against state-of-the-art (SOTA) methods. Our source code and data are\nreleased at https://github.com/WangXFng/CaDRec.\n","authors":["Xinfeng Wang","Fumiyo Fukumoto","Jin Cui","Yoshimi Suzuki","Jiyi Li","Dongjin Yu"],"pdf_url":"https://arxiv.org/pdf/2404.06895v3.pdf","comment":"Accepted to SIGIR 2024"},{"id":"http://arxiv.org/abs/2408.15796v2","updated":"2024-09-04T06:36:22Z","published":"2024-08-28T13:42:28Z","title":"Evaluating Named Entity Recognition Using Few-Shot Prompting with Large\n  Language Models","summary":"  This paper evaluates Few-Shot Prompting with Large Language Models for Named\nEntity Recognition (NER). Traditional NER systems rely on extensive labeled\ndatasets, which are costly and time-consuming to obtain. Few-Shot Prompting or\nin-context learning enables models to recognize entities with minimal examples.\nWe assess state-of-the-art models like GPT-4 in NER tasks, comparing their\nfew-shot performance to fully supervised benchmarks. Results show that while\nthere is a performance gap, large models excel in adapting to new entity types\nand domains with very limited data. We also explore the effects of prompt\nengineering, guided output format and context length on performance. This study\nunderscores Few-Shot Learning's potential to reduce the need for large labeled\ndatasets, enhancing NER scalability and accessibility.\n","authors":["H√©di Zeghidi","Ludovic Moncla"],"pdf_url":"https://arxiv.org/pdf/2408.15796v2.pdf","comment":"Github repo: https://github.com/GEODE-project/ner-llm"},{"id":"http://arxiv.org/abs/2409.02455v1","updated":"2024-09-04T05:36:00Z","published":"2024-09-04T05:36:00Z","title":"An Effective Tag Assignment Approach for Billboard Advertisement","summary":"  Billboard Advertisement has gained popularity due to its significant outrage\nin return on investment. To make this advertisement approach more effective,\nthe relevant information about the product needs to be reached to the relevant\nset of people. This can be achieved if the relevant set of tags can be mapped\nto the correct slots. Formally, we call this problem the Tag Assignment Problem\nin Billboard Advertisement. Given trajectory, billboard database, and a set of\nselected billboard slots and tags, this problem asks to output a mapping of\nselected tags to the selected slots so that the influence is maximized. We\nmodel this as a variant of traditional bipartite matching called One-To-Many\nBipartite Matching (OMBM). Unlike traditional bipartite matching, a tag can be\nassigned to only one slot; in the OMBM, a tag can be assigned to multiple slots\nwhile the vice versa can not happen. We propose an iterative solution approach\nthat incrementally allocates the tags to the slots. The proposed methodology\nhas been explained with an illustrated example. A complexity analysis of the\nproposed solution approach has also been conducted. The experimental results on\nreal-world trajectory and billboard datasets prove our claim on the\neffectiveness and efficiency of the proposed solution.\n","authors":["Dildar Ali","Harishchandra Kumar","Suman Banerjee","Yamuna Prasad"],"pdf_url":"https://arxiv.org/pdf/2409.02455v1.pdf","comment":"This Paper has been accepted at The 25th International Web\n  Information Systems Engineering Conference (WISE-2024)"},{"id":"http://arxiv.org/abs/2409.02425v1","updated":"2024-09-04T04:12:22Z","published":"2024-09-04T04:12:22Z","title":"Deep Adaptive Interest Network: Personalized Recommendation with\n  Context-Aware Learning","summary":"  In personalized recommendation systems, accurately capturing users' evolving\ninterests and combining them with contextual information is a critical research\narea. This paper proposes a novel model called the Deep Adaptive Interest\nNetwork (DAIN), which dynamically models users' interests while incorporating\ncontext-aware learning mechanisms to achieve precise and adaptive personalized\nrecommendations. DAIN leverages deep learning techniques to build an adaptive\ninterest network structure that can capture users' interest changes in\nreal-time while further optimizing recommendation results by integrating\ncontextual information. Experiments conducted on several public datasets\ndemonstrate that DAIN excels in both recommendation performance and\ncomputational efficiency. This research not only provides a new solution for\npersonalized recommendation systems but also offers fresh insights into the\napplication of context-aware learning in recommendation systems.\n","authors":["Shuaishuai Huang","Haowei Yang","You Yao","Xueting Lin","Yuming Tu"],"pdf_url":"https://arxiv.org/pdf/2409.02425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02965v1","updated":"2024-09-04T02:17:32Z","published":"2024-09-04T02:17:32Z","title":"Do We Trust What They Say or What They Do? A Multimodal User Embedding\n  Provides Personalized Explanations","summary":"  With the rapid development of social media, the importance of analyzing\nsocial network user data has also been put on the agenda. User representation\nlearning in social media is a critical area of research, based on which we can\nconduct personalized content delivery, or detect malicious actors. Being more\ncomplicated than many other types of data, social network user data has\ninherent multimodal nature. Various multimodal approaches have been proposed to\nharness both text (i.e. post content) and relation (i.e. inter-user\ninteraction) information to learn user embeddings of higher quality. The advent\nof Graph Neural Network models enables more end-to-end integration of user text\nembeddings and user interaction graphs in social networks. However, most of\nthose approaches do not adequately elucidate which aspects of the data - text\nor graph structure information - are more helpful for predicting each specific\nuser under a particular task, putting some burden on personalized downstream\nanalysis and untrustworthy information filtering. We propose a simple yet\neffective framework called Contribution-Aware Multimodal User Embedding (CAMUE)\nfor social networks. We have demonstrated with empirical evidence, that our\napproach can provide personalized explainable predictions, automatically\nmitigating the impact of unreliable information. We also conducted case studies\nto show how reasonable our results are. We observe that for most users, graph\nstructure information is more trustworthy than text information, but there are\nsome reasonable cases where text helps more. Our work paves the way for more\nexplainable, reliable, and effective social media user embedding which allows\nfor better personalized content delivery.\n","authors":["Zhicheng Ren","Zhiping Xiao","Yizhou Sun"],"pdf_url":"https://arxiv.org/pdf/2409.02965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02343v1","updated":"2024-09-04T00:10:36Z","published":"2024-09-04T00:10:36Z","title":"NUDGE: Lightweight Non-Parametric Fine-Tuning of Embeddings for\n  Retrieval","summary":"  $k$-Nearest Neighbor search on dense vector embeddings ($k$-NN retrieval)\nfrom pre-trained embedding models is the predominant retrieval method for text\nand images, as well as Retrieval-Augmented Generation (RAG) pipelines. In\npractice, application developers often fine-tune the embeddings to improve\ntheir accuracy on the dataset and query workload in hand. Existing approaches\neither fine-tune the pre-trained model itself or, more efficiently, but at the\ncost of accuracy, train adaptor models to transform the output of the\npre-trained model. We present NUDGE, a family of novel non-parametric embedding\nfine-tuning approaches that are significantly more accurate and efficient than\nboth sets of existing approaches. NUDGE directly modifies the embeddings of\ndata records to maximize the accuracy of $k$-NN retrieval. We present a\nthorough theoretical and experimental study of NUDGE's non-parametric approach.\nWe show that even though the underlying problem is NP-Hard, constrained\nvariations can be solved efficiently. These constraints additionally ensure\nthat the changes to the embeddings are modest, avoiding large distortions to\nthe semantics learned during pre-training. In experiments across five\npre-trained models and nine standard text and image retrieval datasets, NUDGE\nruns in minutes and often improves NDCG@10 by more than 10% over existing\nfine-tuning methods. On average, NUDGE provides 3.3x and 4.3x higher increase\nin accuracy and runs 200x and 3x faster, respectively, over fine-tuning the\npre-trained model and training adaptors.\n","authors":["Sepanta Zeighami","Zac Wellmer","Aditya Parameswaran"],"pdf_url":"https://arxiv.org/pdf/2409.02343v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.02889v1","updated":"2024-09-04T17:25:21Z","published":"2024-09-04T17:25:21Z","title":"LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via\n  Hybrid Architecture","summary":"  Expanding the long-context capabilities of Multi-modal Large Language\nModels~(MLLMs) is crucial for video understanding, high-resolution image\nunderstanding, and multi-modal agents. This involves a series of systematic\noptimizations, including model architecture, data construction and training\nstrategy, particularly addressing challenges such as \\textit{degraded\nperformance with more images} and \\textit{high computational costs}. In this\npaper, we adapt the model architecture to a hybrid of Mamba and Transformer\nblocks, approach data construction with both temporal and spatial dependencies\namong multiple images and employ a progressive training strategy. The released\nmodel \\textbf{LongLLaVA}~(\\textbf{Long}-Context \\textbf{L}arge\n\\textbf{L}anguage \\textbf{a}nd \\textbf{V}ision \\textbf{A}ssistant) is the first\nhybrid MLLM, which achieved a better balance between efficiency and\neffectiveness. LongLLaVA not only achieves competitive results across various\nbenchmarks, but also maintains high throughput and low memory consumption.\nEspecially, it could process nearly a thousand images on a single A100 80GB\nGPU, showing promising application prospects for a wide range of tasks.\n","authors":["Xidong Wang","Dingjie Song","Shunian Chen","Chen Zhang","Benyou Wang"],"pdf_url":"https://arxiv.org/pdf/2409.02889v1.pdf","comment":"19 pages, 7 figures, 6 tables"},{"id":"http://arxiv.org/abs/2409.02845v1","updated":"2024-09-04T16:17:41Z","published":"2024-09-04T16:17:41Z","title":"Multi-Track MusicLDM: Towards Versatile Music Generation with Latent\n  Diffusion Model","summary":"  Diffusion models have shown promising results in cross-modal generation tasks\ninvolving audio and music, such as text-to-sound and text-to-music generation.\nThese text-controlled music generation models typically focus on generating\nmusic by capturing global musical attributes like genre and mood. However,\nmusic composition is a complex, multilayered task that often involves musical\narrangement as an integral part of the process. This process involves composing\neach instrument to align with existing ones in terms of beat, dynamics,\nharmony, and melody, requiring greater precision and control over tracks than\ntext prompts usually provide. In this work, we address these challenges by\nextending the MusicLDM, a latent diffusion model for music, into a multi-track\ngenerative model. By learning the joint probability of tracks sharing a\ncontext, our model is capable of generating music across several tracks that\ncorrespond well to each other, either conditionally or unconditionally.\nAdditionally, our model is capable of arrangement generation, where the model\ncan generate any subset of tracks given the others (e.g., generating a piano\ntrack complementing given bass and drum tracks). We compared our model with an\nexisting multi-track generative model and demonstrated that our model achieves\nconsiderable improvements across objective metrics for both total and\narrangement generation tasks.\n","authors":["Tornike Karchkhadze","Mohammad Rasool Izadi","Ke Chen","Gerard Assayag","Shlomo Dubnov"],"pdf_url":"https://arxiv.org/pdf/2409.02845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02828v1","updated":"2024-09-04T15:50:16Z","published":"2024-09-04T15:50:16Z","title":"ExpLLM: Towards Chain of Thought for Facial Expression Recognition","summary":"  Facial expression recognition (FER) is a critical task in multimedia with\nsignificant implications across various domains. However, analyzing the causes\nof facial expressions is essential for accurately recognizing them. Current\napproaches, such as those based on facial action units (AUs), typically provide\nAU names and intensities but lack insight into the interactions and\nrelationships between AUs and the overall expression. In this paper, we propose\na novel method called ExpLLM, which leverages large language models to generate\nan accurate chain of thought (CoT) for facial expression recognition.\nSpecifically, we have designed the CoT mechanism from three key perspectives:\nkey observations, overall emotional interpretation, and conclusion. The key\nobservations describe the AU's name, intensity, and associated emotions. The\noverall emotional interpretation provides an analysis based on multiple AUs and\ntheir interactions, identifying the dominant emotions and their relationships.\nFinally, the conclusion presents the final expression label derived from the\npreceding analysis. Furthermore, we also introduce the Exp-CoT Engine, designed\nto construct this expression CoT and generate instruction-description data for\ntraining our ExpLLM. Extensive experiments on the RAF-DB and AffectNet datasets\ndemonstrate that ExpLLM outperforms current state-of-the-art FER methods.\nExpLLM also surpasses the latest GPT-4o in expression CoT generation,\nparticularly in recognizing micro-expressions where GPT-4o frequently fails.\n","authors":["Xing Lan","Jian Xue","Ji Qi","Dongmei Jiang","Ke Lu","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2409.02828v1.pdf","comment":"project page: https://starhiking.github.io/ExpLLM_Page/"},{"id":"http://arxiv.org/abs/2409.02657v1","updated":"2024-09-04T12:30:25Z","published":"2024-09-04T12:30:25Z","title":"PoseTalk: Text-and-Audio-based Pose Control and Motion Refinement for\n  One-Shot Talking Head Generation","summary":"  While previous audio-driven talking head generation (THG) methods generate\nhead poses from driving audio, the generated poses or lips cannot match the\naudio well or are not editable. In this study, we propose \\textbf{PoseTalk}, a\nTHG system that can freely generate lip-synchronized talking head videos with\nfree head poses conditioned on text prompts and audio. The core insight of our\nmethod is using head pose to connect visual, linguistic, and audio signals.\nFirst, we propose to generate poses from both audio and text prompts, where the\naudio offers short-term variations and rhythm correspondence of the head\nmovements and the text prompts describe the long-term semantics of head\nmotions. To achieve this goal, we devise a Pose Latent Diffusion (PLD) model to\ngenerate motion latent from text prompts and audio cues in a pose latent space.\nSecond, we observe a loss-imbalance problem: the loss for the lip region\ncontributes less than 4\\% of the total reconstruction loss caused by both pose\nand lip, making optimization lean towards head movements rather than lip\nshapes. To address this issue, we propose a refinement-based learning strategy\nto synthesize natural talking videos using two cascaded networks, i.e.,\nCoarseNet, and RefineNet. The CoarseNet estimates coarse motions to produce\nanimated images in novel poses and the RefineNet focuses on learning finer lip\nmotions by progressively estimating lip motions from low-to-high resolutions,\nyielding improved lip-synchronization performance. Experiments demonstrate our\npose prediction strategy achieves better pose diversity and realness compared\nto text-only or audio-only, and our video generator model outperforms\nstate-of-the-art methods in synthesizing talking videos with natural head\nmotions. Project: https://junleen.github.io/projects/posetalk.\n","authors":["Jun Ling","Yiwen Wang","Han Xue","Rong Xie","Li Song"],"pdf_url":"https://arxiv.org/pdf/2409.02657v1.pdf","comment":"7+5 pages, 15 figures"},{"id":"http://arxiv.org/abs/2409.02555v1","updated":"2024-09-04T09:21:13Z","published":"2024-09-04T09:21:13Z","title":"Low-Resolution Object Recognition with Cross-Resolution Relational\n  Contrastive Distillation","summary":"  Recognizing objects in low-resolution images is a challenging task due to the\nlack of informative details. Recent studies have shown that knowledge\ndistillation approaches can effectively transfer knowledge from a\nhigh-resolution teacher model to a low-resolution student model by aligning\ncross-resolution representations. However, these approaches still face\nlimitations in adapting to the situation where the recognized objects exhibit\nsignificant representation discrepancies between training and testing images.\nIn this study, we propose a cross-resolution relational contrastive\ndistillation approach to facilitate low-resolution object recognition. Our\napproach enables the student model to mimic the behavior of a well-trained\nteacher model which delivers high accuracy in identifying high-resolution\nobjects. To extract sufficient knowledge, the student learning is supervised\nwith contrastive relational distillation loss, which preserves the similarities\nin various relational structures in contrastive representation space. In this\nmanner, the capability of recovering missing details of familiar low-resolution\nobjects can be effectively enhanced, leading to a better knowledge transfer.\nExtensive experiments on low-resolution object classification and\nlow-resolution face recognition clearly demonstrate the effectiveness and\nadaptability of our approach.\n","authors":["Kangkai Zhang","Shiming Ge","Ruixin Shi","Dan Zeng"],"pdf_url":"https://arxiv.org/pdf/2409.02555v1.pdf","comment":"This paper is accepted by IEEE Transactions on Circuits and Systems\n  for Video Technology (TCSVT)"},{"id":"http://arxiv.org/abs/2408.15461v2","updated":"2024-09-04T02:45:56Z","published":"2024-08-28T00:54:51Z","title":"Hand1000: Generating Realistic Hands from Text with Only 1,000 Images","summary":"  Text-to-image generation models have achieved remarkable advancements in\nrecent years, aiming to produce realistic images from textual descriptions.\nHowever, these models often struggle with generating anatomically accurate\nrepresentations of human hands. The resulting images frequently exhibit issues\nsuch as incorrect numbers of fingers, unnatural twisting or interlacing of\nfingers, or blurred and indistinct hands. These issues stem from the inherent\ncomplexity of hand structures and the difficulty in aligning textual\ndescriptions with precise visual depictions of hands. To address these\nchallenges, we propose a novel approach named Hand1000 that enables the\ngeneration of realistic hand images with target gesture using only 1,000\ntraining samples. The training of Hand1000 is divided into three stages with\nthe first stage aiming to enhance the model's understanding of hand anatomy by\nusing a pre-trained hand gesture recognition model to extract gesture\nrepresentation. The second stage further optimizes text embedding by\nincorporating the extracted hand gesture representation, to improve alignment\nbetween the textual descriptions and the generated hand images. The third stage\nutilizes the optimized embedding to fine-tune the Stable Diffusion model to\ngenerate realistic hand images. In addition, we construct the first publicly\navailable dataset specifically designed for text-to-hand image generation.\nBased on the existing hand gesture recognition dataset, we adopt advanced image\ncaptioning models and LLaMA3 to generate high-quality textual descriptions\nenriched with detailed gesture information. Extensive experiments demonstrate\nthat Hand1000 significantly outperforms existing models in producing\nanatomically correct hand images while faithfully representing other details in\nthe text, such as faces, clothing, and colors.\n","authors":["Haozhuo Zhang","Bin Zhu","Yu Cao","Yanbin Hao"],"pdf_url":"https://arxiv.org/pdf/2408.15461v2.pdf","comment":"Project page https://haozhuo-zhang.github.io/Hand1000-project-page/"},{"id":"http://arxiv.org/abs/2409.02376v1","updated":"2024-09-04T01:54:20Z","published":"2024-09-04T01:54:20Z","title":"Coral Model Generation from Single Images for Virtual Reality\n  Applications","summary":"  With the rapid development of VR technology, the demand for high-quality 3D\nmodels is increasing. Traditional methods struggle with efficiency and quality\nin large-scale customization. This paper introduces a deep-learning framework\nthat generates high-precision 3D coral models from a single image. Using the\nCoral dataset, the framework extracts geometric and texture features, performs\n3D reconstruction, and optimizes design and material blending. Advanced\noptimization and polygon count control ensure shape accuracy, detail retention,\nand flexible output for various complexities, catering to high-quality\nrendering and real-time interaction needs.The project incorporates Explainable\nAI (XAI) to transform AI-generated models into interactive \"artworks,\" best\nviewed in VR and XR. This enhances model interpretability and human-machine\ncollaboration. Real-time feedback in VR interactions displays information like\ncoral species and habitat, enriching user experience. The generated models\nsurpass traditional methods in detail, visual quality, and efficiency. This\nresearch offers an intelligent approach to 3D content creation for VR, lowering\nproduction barriers, and promoting widespread VR applications. Additionally,\nintegrating XAI provides new insights into AI-generated visual content and\nadvances research in 3D vision interpretability.\n","authors":["Jie Fu","Shun Fu","Mick Grierson"],"pdf_url":"https://arxiv.org/pdf/2409.02376v1.pdf","comment":"In Proceedings of Explainable AI for the Arts Workshop 2024 (XAIxArts\n  2024) arXiv:2406.14485"},{"id":"http://arxiv.org/abs/2408.11593v3","updated":"2024-09-04T01:25:55Z","published":"2024-08-21T12:59:42Z","title":"MCDubber: Multimodal Context-Aware Expressive Video Dubbing","summary":"  Automatic Video Dubbing (AVD) aims to take the given script and generate\nspeech that aligns with lip motion and prosody expressiveness. Current AVD\nmodels mainly utilize visual information of the current sentence to enhance the\nprosody of synthesized speech. However, it is crucial to consider whether the\nprosody of the generated dubbing aligns with the multimodal context, as the\ndubbing will be combined with the original context in the final video. This\naspect has been overlooked in previous studies. To address this issue, we\npropose a Multimodal Context-aware video Dubbing model, termed\n\\textbf{MCDubber}, to convert the modeling object from a single sentence to a\nlonger sequence with context information to ensure the consistency of the\nglobal context prosody. MCDubber comprises three main components: (1) A context\nduration aligner aims to learn the context-aware alignment between the text and\nlip frames; (2) A context prosody predictor seeks to read the global context\nvisual sequence and predict the context-aware global energy and pitch; (3) A\ncontext acoustic decoder ultimately predicts the global context mel-spectrogram\nwith the assistance of adjacent ground-truth mel-spectrograms of the target\nsentence. Through this process, MCDubber fully considers the influence of\nmultimodal context on the prosody expressiveness of the current sentence when\ndubbing. The extracted mel-spectrogram belonging to the target sentence from\nthe output context mel-spectrograms is the final required dubbing audio.\nExtensive experiments on the Chem benchmark dataset demonstrate that our\nMCDubber significantly improves dubbing expressiveness compared to all advanced\nbaselines. The code and demos are available at\nhttps://github.com/XiaoYuanJun-zy/MCDubber.\n","authors":["Yuan Zhao","Zhenqi Jia","Rui Liu","De Hu","Feilong Bao","Guanglai Gao"],"pdf_url":"https://arxiv.org/pdf/2408.11593v3.pdf","comment":"Accepted by NCMMSC2024"}]},"2024-09-03T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.17344v2","updated":"2024-09-03T10:50:17Z","published":"2024-08-30T15:16:52Z","title":"rerankers: A Lightweight Python Library to Unify Ranking Methods","summary":"  This paper presents rerankers, a Python library which provides an easy-to-use\ninterface to the most commonly used re-ranking approaches. Re-ranking is an\nintegral component of many retrieval pipelines; however, there exist numerous\napproaches to it, relying on different implementation methods. rerankers\nunifies these methods into a single user-friendly interface, allowing\npractitioners and researchers alike to explore different methods while only\nchanging a single line of Python code. Moreover ,rerankers ensures that its\nimplementations are done with the fewest dependencies possible, and re-uses the\noriginal implementation whenever possible, guaranteeing that our simplified\ninterface results in no performance degradation compared to more complex ones.\nThe full source code and list of supported models are updated regularly and\navailable at https://github.com/answerdotai/rerankers.\n","authors":["Benjamin Clavi√©"],"pdf_url":"https://arxiv.org/pdf/2408.17344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01736v1","updated":"2024-09-03T09:25:26Z","published":"2024-09-03T09:25:26Z","title":"SpannerLib: Embedding Declarative Information Extraction in an\n  Imperative Workflow","summary":"  Document spanners have been proposed as a formal framework for declarative\nInformation Extraction (IE) from text, following IE products from the industry\nand academia. Over the past decade, the framework has been studied thoroughly\nin terms of expressive power, complexity, and the ability to naturally combine\ntext analysis with relational querying. This demonstration presents SpannerLib\na library for embedding document spanners in Python code. SpannerLib\nfacilitates the development of IE programs by providing an implementation of\nSpannerlog (Datalog-based documentspanners) that interacts with the Python code\nin two directions: rules can be embedded inside Python, and they can invoke\ncustom Python code (e.g., calls to ML-based NLP models) via user-defined\nfunctions. The demonstration scenarios showcase IE programs, with increasing\nlevels of complexity, within Jupyter Notebook.\n","authors":["Dean Light","Ahmad Aiashy","Mahmoud Diab","Daniel Nachmias","Stijn Vansummeren","Benny Kimelfeld"],"pdf_url":"https://arxiv.org/pdf/2409.01736v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2409.01605v1","updated":"2024-09-03T04:55:03Z","published":"2024-09-03T04:55:03Z","title":"Laser: Parameter-Efficient LLM Bi-Tuning for Sequential Recommendation\n  with Collaborative Information","summary":"  Sequential recommender systems are essential for discerning user preferences\nfrom historical interactions and facilitating targeted recommendations. Recent\ninnovations employing Large Language Models (LLMs) have advanced the field by\nencoding item semantics, yet they often necessitate substantial parameter\ntuning and are resource-demanding. Moreover, these works fails to consider the\ndiverse characteristics of different types of users and thus diminishes the\nrecommendation accuracy. In this paper, we propose a parameter-efficient Large\nLanguage Model Bi-Tuning framework for sequential recommendation with\ncollaborative information (Laser). Specifically, Bi-Tuning works by inserting\ntrainable virtual tokens at both the prefix and suffix of the input sequence\nand freezing the LLM parameters, thus optimizing the LLM for the sequential\nrecommendation. In our Laser, the prefix is utilized to incorporate user-item\ncollaborative information and adapt the LLM to the recommendation task, while\nthe suffix converts the output embeddings of the LLM from the language space to\nthe recommendation space for the follow-up item recommendation. Furthermore, to\ncapture the characteristics of different types of users when integrating the\ncollaborative information via the prefix, we introduce M-Former, a lightweight\nMoE-based querying transformer that uses a set of query experts to integrate\ndiverse user-specific collaborative information encoded by frozen ID-based\nsequential recommender systems, significantly improving the accuracy of\nrecommendations. Extensive experiments on real-world datasets demonstrate that\nLaser can parameter-efficiently adapt LLMs to effective recommender systems,\nsignificantly outperforming state-of-the-art methods.\n","authors":["Xinyu Zhang","Linmei Hu","Luhao Zhang","Dandan Song","Heyan Huang","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2409.01605v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.01563v1","updated":"2024-09-03T03:00:59Z","published":"2024-09-03T03:00:59Z","title":"Blockchain-based Federated Recommendation with Incentive Mechanism","summary":"  Nowadays, federated recommendation technology is rapidly evolving to help\nmultiple organisations share data and train models while meeting user privacy,\ndata security and government regulatory requirements. However, federated\nrecommendation increases customer system costs such as power, computational and\ncommunication resources. Besides, federated recommendation systems are also\nsusceptible to model attacks and data poisoning by participating malicious\nclients. Therefore, most customers are unwilling to participate in federated\nrecommendation without any incentive. To address these problems, we propose a\nblockchain-based federated recommendation system with incentive mechanism to\npromote more trustworthy, secure, and efficient federated recommendation\nservice. First, we construct a federated recommendation system based on NeuMF\nand FedAvg. Then we introduce a reverse auction mechanism to select optimal\nclients that can maximize the social surplus. Finally, we employ blockchain for\non-chain evidence storage of models to ensure the safety of the federated\nrecommendation system. The experimental results show that our proposed\nincentive mechanism can attract clients with superior training data to engage\nin the federal recommendation at a lower cost, which can increase the economic\nbenefit of federal recommendation by 54.9\\% while improve the recommendation\nperformance. Thus our work provides theoretical and technological support for\nthe construction of a harmonious and healthy ecological environment for the\napplication of federal recommendation.\n","authors":["Jianhai Chen","Yanlin Wu","Dazhong Rong","Guoyao Yu","Lingqi Jiang","Zhenguang Liu","Peng Zhou","Rui Shen"],"pdf_url":"https://arxiv.org/pdf/2409.01563v1.pdf","comment":"This paper has been accepted on 2024 Blockchain and Web3 Technology\n  Innovation and Application Exchange Conference (BWTAC 2024)"},{"id":"http://arxiv.org/abs/2405.06242v2","updated":"2024-09-03T00:21:23Z","published":"2024-05-10T04:44:34Z","title":"Impedance vs. Power Side-channel Vulnerabilities: A Comparative Study","summary":"  In recent times, impedance side-channel analysis has emerged as a potent\nstrategy for adversaries seeking to extract sensitive information from\ncomputing systems. It leverages variations in the intrinsic impedance of a\nchip's internal structure across different logic states. In this study, we\nconduct a comparative analysis between the newly explored impedance side\nchannel and the well-established power side channel. Through experimental\nevaluation, we investigate the efficacy of these two side channels in\nextracting the cryptographic key from the Advanced Encryption Standard (AES)\nand analyze their performance. Our results indicate that impedance analysis\ndemonstrates a higher potential for cryptographic key extraction compared to\npower side-channel analysis. Moreover, we identify scenarios where power\nside-channel analysis does not yield satisfactory results, whereas impedance\nanalysis proves to be more robust and effective. This work not only underscores\nthe significance of impedance side-channel analysis in enhancing cryptographic\nsecurity but also emphasizes the necessity for a deeper understanding of its\nmechanisms and implications.\n","authors":["Md Sadik Awal","Buddhipriya Gayanath","Md Tauhidur Rahman"],"pdf_url":"https://arxiv.org/pdf/2405.06242v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.01690v2","updated":"2024-09-03T22:30:34Z","published":"2024-08-03T07:05:40Z","title":"IDNet: A Novel Dataset for Identity Document Analysis and Fraud\n  Detection","summary":"  Effective fraud detection and analysis of government-issued identity\ndocuments, such as passports, driver's licenses, and identity cards, are\nessential in thwarting identity theft and bolstering security on online\nplatforms. The training of accurate fraud detection and analysis tools depends\non the availability of extensive identity document datasets. However, current\npublicly available benchmark datasets for identity document analysis, including\nMIDV-500, MIDV-2020, and FMIDV, fall short in several respects: they offer a\nlimited number of samples, cover insufficient varieties of fraud patterns, and\nseldom include alterations in critical personal identifying fields like\nportrait images, limiting their utility in training models capable of detecting\nrealistic frauds while preserving privacy.\n  In response to these shortcomings, our research introduces a new benchmark\ndataset, IDNet, designed to advance privacy-preserving fraud detection efforts.\nThe IDNet dataset comprises 837,060 images of synthetically generated identity\ndocuments, totaling approximately 490 gigabytes, categorized into 20 types from\n$10$ U.S. states and 10 European countries. We evaluate the utility and present\nuse cases of the dataset, illustrating how it can aid in training\nprivacy-preserving fraud detection methods, facilitating the generation of\ncamera and video capturing of identity documents, and testing schema\nunification and other identity document management functionalities.\n","authors":["Hong Guan","Yancheng Wang","Lulu Xie","Soham Nag","Rajeev Goel","Niranjan Erappa Narayana Swamy","Yingzhen Yang","Chaowei Xiao","Jonathan Prisby","Ross Maciejewski","Jia Zou"],"pdf_url":"https://arxiv.org/pdf/2408.01690v2.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2409.02266v1","updated":"2024-09-03T19:52:49Z","published":"2024-09-03T19:52:49Z","title":"LSTMSE-Net: Long Short Term Speech Enhancement Network for Audio-visual\n  Speech Enhancement","summary":"  In this paper, we propose long short term memory speech enhancement network\n(LSTMSE-Net), an audio-visual speech enhancement (AVSE) method. This innovative\nmethod leverages the complementary nature of visual and audio information to\nboost the quality of speech signals. Visual features are extracted with\nVisualFeatNet (VFN), and audio features are processed through an encoder and\ndecoder. The system scales and concatenates visual and audio features, then\nprocesses them through a separator network for optimized speech enhancement.\nThe architecture highlights advancements in leveraging multi-modal data and\ninterpolation techniques for robust AVSE challenge systems. The performance of\nLSTMSE-Net surpasses that of the baseline model from the COG-MHEAR AVSE\nChallenge 2024 by a margin of 0.06 in scale-invariant signal-to-distortion\nratio (SISDR), $0.03$ in short-time objective intelligibility (STOI), and\n$1.32$ in perceptual evaluation of speech quality (PESQ). The source code of\nthe proposed LSTMSE-Net is available at\n\\url{https://github.com/mtanveer1/AVSEC-3-Challenge}.\n","authors":["Arnav Jain","Jasmer Singh Sanjotra","Harshvardhan Choudhary","Krish Agrawal","Rupal Shah","Rohan Jha","M. Sajid","Amir Hussain","M. Tanveer"],"pdf_url":"https://arxiv.org/pdf/2409.02266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02108v1","updated":"2024-09-03T17:59:05Z","published":"2024-09-03T17:59:05Z","title":"Unveiling Deep Shadows: A Survey on Image and Video Shadow Detection,\n  Removal, and Generation in the Era of Deep Learning","summary":"  Shadows are formed when light encounters obstacles, leading to areas of\ndiminished illumination. In computer vision, shadow detection, removal, and\ngeneration are crucial for enhancing scene understanding, refining image\nquality, ensuring visual consistency in video editing, and improving virtual\nenvironments. This paper presents a comprehensive survey of shadow detection,\nremoval, and generation in images and videos within the deep learning landscape\nover the past decade, covering tasks, deep models, datasets, and evaluation\nmetrics. Our key contributions include a comprehensive survey of shadow\nanalysis, standardization of experimental comparisons, exploration of the\nrelationships among model size, speed, and performance, a cross-dataset\ngeneralization study, identification of open issues and future directions, and\nprovision of publicly available resources to support further research.\n","authors":["Xiaowei Hu","Zhenghao Xing","Tianyu Wang","Chi-Wing Fu","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2409.02108v1.pdf","comment":"Publicly available results, trained models, and evaluation metrics at\n  https://github.com/xw-hu/Unveiling-Deep-Shadows"},{"id":"http://arxiv.org/abs/2409.02101v1","updated":"2024-09-03T17:56:51Z","published":"2024-09-03T17:56:51Z","title":"Towards Real-World Adverse Weather Image Restoration: Enhancing\n  Clearness and Semantics with Vision-Language Models","summary":"  This paper addresses the limitations of adverse weather image restoration\napproaches trained on synthetic data when applied to real-world scenarios. We\nformulate a semi-supervised learning framework employing vision-language models\nto enhance restoration performance across diverse adverse weather conditions in\nreal-world settings. Our approach involves assessing image clearness and\nproviding semantics using vision-language models on real data, serving as\nsupervision signals for training restoration models. For clearness enhancement,\nwe use real-world data, utilizing a dual-step strategy with pseudo-labels\nassessed by vision-language models and weather prompt learning. For semantic\nenhancement, we integrate real-world data by adjusting weather conditions in\nvision-language model descriptions while preserving semantic meaning.\nAdditionally, we introduce an effective training strategy to bootstrap\nrestoration performance. Our approach achieves superior results in real-world\nadverse weather image restoration, demonstrated through qualitative and\nquantitative comparisons with state-of-the-art works.\n","authors":["Jiaqi Xu","Mengyang Wu","Xiaowei Hu","Chi-Wing Fu","Qi Dou","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2409.02101v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2409.02049v1","updated":"2024-09-03T16:53:34Z","published":"2024-09-03T16:53:34Z","title":"Low-Resolution Face Recognition via Adaptable Instance-Relation\n  Distillation","summary":"  Low-resolution face recognition is a challenging task due to the missing of\ninformative details. Recent approaches based on knowledge distillation have\nproven that high-resolution clues can well guide low-resolution face\nrecognition via proper knowledge transfer. However, due to the distribution\ndifference between training and testing faces, the learned models often suffer\nfrom poor adaptability. To address that, we split the knowledge transfer\nprocess into distillation and adaptation steps, and propose an adaptable\ninstance-relation distillation approach to facilitate low-resolution face\nrecognition. In the approach, the student distills knowledge from\nhigh-resolution teacher in both instance level and relation level, providing\nsufficient cross-resolution knowledge transfer. Then, the learned student can\nbe adaptable to recognize low-resolution faces with adaptive batch\nnormalization in inference. In this manner, the capability of recovering\nmissing details of familiar low-resolution faces can be effectively enhanced,\nleading to a better knowledge transfer. Extensive experiments on low-resolution\nface recognition clearly demonstrate the effectiveness and adaptability of our\napproach.\n","authors":["Ruixin Shi","Weijia Guo","Shiming Ge"],"pdf_url":"https://arxiv.org/pdf/2409.02049v1.pdf","comment":"Accepted by IJCNN 2024"},{"id":"http://arxiv.org/abs/2409.01761v1","updated":"2024-09-03T10:15:30Z","published":"2024-09-03T10:15:30Z","title":"PRoGS: Progressive Rendering of Gaussian Splats","summary":"  Over the past year, 3D Gaussian Splatting (3DGS) has received significant\nattention for its ability to represent 3D scenes in a perceptually accurate\nmanner. However, it can require a substantial amount of storage since each\nsplat's individual data must be stored. While compression techniques offer a\npotential solution by reducing the memory footprint, they still necessitate\nretrieving the entire scene before any part of it can be rendered. In this\nwork, we introduce a novel approach for progressively rendering such scenes,\naiming to display visible content that closely approximates the final scene as\nearly as possible without loading the entire scene into memory. This approach\nbenefits both on-device rendering applications limited by memory constraints\nand streaming applications where minimal bandwidth usage is preferred. To\nachieve this, we approximate the contribution of each Gaussian to the final\nscene and construct an order of prioritization on their inclusion in the\nrendering process. Additionally, we demonstrate that our approach can be\ncombined with existing compression methods to progressively render (and stream)\n3DGS scenes, optimizing bandwidth usage by focusing on the most important\nsplats within a scene. Overall, our work establishes a foundation for making\nremotely hosted 3DGS content more quickly accessible to end-users in\nover-the-top consumption scenarios, with our results showing significant\nimprovements in quality across all metrics compared to existing methods.\n","authors":["Brent Zoomers","Maarten Wijnants","Ivan Molenaers","Joni Vanherck","Jeroen Put","Lode Jorissen","Nick Michiels"],"pdf_url":"https://arxiv.org/pdf/2409.01761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01710v1","updated":"2024-09-03T08:47:17Z","published":"2024-09-03T08:47:17Z","title":"Privacy-Preserving Multimedia Mobile Cloud Computing Using Protective\n  Perturbation","summary":"  Mobile cloud computing has been adopted in many multimedia applications,\nwhere the resource-constrained mobile device sends multimedia data (e.g.,\nimages) to remote cloud servers to request computation-intensive multimedia\nservices (e.g., image recognition). While significantly improving the\nperformance of the mobile applications, the cloud-based mechanism often causes\nprivacy concerns as the multimedia data and services are offloaded from the\ntrusted user device to untrusted cloud servers. Several recent studies have\nproposed perturbation-based privacy preserving mechanisms, which obfuscate the\noffloaded multimedia data to eliminate privacy exposures without affecting the\nfunctionality of the remote multimedia services. However, the existing privacy\nprotection approaches require the deployment of computation-intensive\nperturbation generation on the resource-constrained mobile devices. Also, the\nobfuscated images are typically not compliant with the standard image\ncompression algorithms and suffer from significant bandwidth consumption. In\nthis paper, we develop a novel privacy-preserving multimedia mobile cloud\ncomputing framework, namely $PMC^2$, to address the resource and bandwidth\nchallenges. $PMC^2$ employs secure confidential computing in the cloud to\ndeploy the perturbation generator, which addresses the resource challenge while\nmaintaining the privacy. Furthermore, we develop a neural compressor\nspecifically trained to compress the perturbed images in order to address the\nbandwidth challenge. We implement $PMC^2$ in an end-to-end mobile cloud\ncomputing system, based on which our evaluations demonstrate superior latency,\npower efficiency, and bandwidth consumption achieved by $PMC^2$ while\nmaintaining high accuracy in the target multimedia service.\n","authors":["Zhongze Tang","Mengmei Ye","Yao Liu","Sheng Wei"],"pdf_url":"https://arxiv.org/pdf/2409.01710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05449v2","updated":"2024-09-03T08:01:47Z","published":"2023-12-09T03:33:14Z","title":"TALDS-Net: Task-Aware Adaptive Local Descriptors Selection for Few-shot\n  Image Classification","summary":"  Few-shot image classification aims to classify images from unseen novel\nclasses with few samples. Recent works demonstrate that deep local descriptors\nexhibit enhanced representational capabilities compared to image-level\nfeatures. However, most existing methods solely rely on either employing all\nlocal descriptors or directly utilizing partial descriptors, potentially\nresulting in the loss of crucial information. Moreover, these methods primarily\nemphasize the selection of query descriptors while overlooking support\ndescriptors. In this paper, we propose a novel Task-Aware Adaptive Local\nDescriptors Selection Network (TALDS-Net), which exhibits the capacity for\nadaptive selection of task-aware support descriptors and query descriptors.\nSpecifically, we compare the similarity of each local support descriptor with\nother local support descriptors to obtain the optimal support descriptor subset\nand then compare the query descriptors with the optimal support subset to\nobtain discriminative query descriptors. Extensive experiments demonstrate that\nour TALDS-Net outperforms state-of-the-art methods on both general and\nfine-grained datasets.\n","authors":["Qian Qiao","Yu Xie","Ziyin Zeng","Fanzhang Li"],"pdf_url":"https://arxiv.org/pdf/2312.05449v2.pdf","comment":"4 pages, 1 figures, is accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2409.01534v1","updated":"2024-09-03T02:08:47Z","published":"2024-09-03T02:08:47Z","title":"Think Twice Before Recognizing: Large Multimodal Models for General\n  Fine-grained Traffic Sign Recognition","summary":"  We propose a new strategy called think twice before recognizing to improve\nfine-grained traffic sign recognition (TSR). Fine-grained TSR in the wild is\ndifficult due to the complex road conditions, and existing approaches\nparticularly struggle with cross-country TSR when data is lacking. Our strategy\nachieves effective fine-grained TSR by stimulating the multiple-thinking\ncapability of large multimodal models (LMM). We introduce context,\ncharacteristic, and differential descriptions to design multiple thinking\nprocesses for the LMM. The context descriptions with center coordinate prompt\noptimization help the LMM to locate the target traffic sign in the original\nroad images containing multiple traffic signs and filter irrelevant answers\nthrough the proposed prior traffic sign hypothesis. The characteristic\ndescription is based on few-shot in-context learning of template traffic signs,\nwhich decreases the cross-domain difference and enhances the fine-grained\nrecognition capability of the LMM. The differential descriptions of similar\ntraffic signs optimize the multimodal thinking capability of the LMM. The\nproposed method is independent of training data and requires only simple and\nuniform instructions. We conducted extensive experiments on three benchmark\ndatasets and two real-world datasets from different countries, and the proposed\nmethod achieves state-of-the-art TSR results on all five datasets.\n","authors":["Yaozong Gan","Guang Li","Ren Togo","Keisuke Maeda","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2409.01534v1.pdf","comment":null}]},"2024-09-02T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.07981v2","updated":"2024-09-02T21:29:04Z","published":"2024-04-11T17:57:32Z","title":"Manipulating Large Language Models to Increase Product Visibility","summary":"  Large language models (LLMs) are increasingly being integrated into search\nengines to provide natural language responses tailored to user queries.\nCustomers and end-users are also becoming more dependent on these models for\nquick and easy purchase decisions. In this work, we investigate whether\nrecommendations from LLMs can be manipulated to enhance a product's visibility.\nWe demonstrate that adding a strategic text sequence (STS) -- a carefully\ncrafted message -- to a product's information page can significantly increase\nits likelihood of being listed as the LLM's top recommendation. To understand\nthe impact of STS, we use a catalog of fictitious coffee machines and analyze\nits effect on two target products: one that seldom appears in the LLM's\nrecommendations and another that usually ranks second. We observe that the\nstrategic text sequence significantly enhances the visibility of both products\nby increasing their chances of appearing as the top recommendation. This\nability to manipulate LLM-generated search responses provides vendors with a\nconsiderable competitive advantage and has the potential to disrupt fair market\ncompetition. Just as search engine optimization (SEO) revolutionized how\nwebpages are customized to rank higher in search engine results, influencing\nLLM recommendations could profoundly impact content optimization for AI-driven\nsearch services. Code for our experiments is available at\nhttps://github.com/aounon/llm-rank-optimizer.\n","authors":["Aounon Kumar","Himabindu Lakkaraju"],"pdf_url":"https://arxiv.org/pdf/2404.07981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.02935v2","updated":"2024-09-02T20:06:21Z","published":"2022-12-06T12:45:15Z","title":"A multi-language toolkit for supporting automated checking of research\n  outputs","summary":"  This article presents the automatic checking of research outputs package\nacro, which assists researchers and data governance teams by automatically\napplying best-practice principles-based statistical disclosure control (SDC)\ntechniques on-the-fly as researchers conduct their analyses. acro distinguishes\nbetween: research output that is safe to publish; output that requires further\nanalysis; and output that cannot be published because it creates substantial\nrisk of disclosing private data. This is achieved through the use of a\nlightweight Python wrapper that sits over well-known analysis tools that\nproduce outputs such as tables, plots, and statistical models. This adds\nfunctionality to (i) identify potentially disclosive outputs against a range of\ncommonly used disclosure tests; (ii) apply disclosure mitigation strategies\nwhere required; (iii) report reasons for applying SDC; and (iv) produce simple\nsummary documents trusted research environment staff can use to streamline\ntheir workflow. The major analytical programming languages used by researchers\nare supported: Python, R, and Stata. The acro code and documentation are\navailable under an MIT license at https://github.com/AI-SDC/ACRO\n","authors":["Richard J. Preen","Maha Albashir","Simon Davy","Jim Smith"],"pdf_url":"https://arxiv.org/pdf/2212.02935v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01445v1","updated":"2024-09-02T20:00:49Z","published":"2024-09-02T20:00:49Z","title":"Sync from the Sea: Retrieving Alignable Videos from Large-Scale Datasets","summary":"  Temporal video alignment aims to synchronize the key events like object\ninteractions or action phase transitions in two videos. Such methods could\nbenefit various video editing, processing, and understanding tasks. However,\nexisting approaches operate under the restrictive assumption that a suitable\nvideo pair for alignment is given, significantly limiting their broader\napplicability. To address this, we re-pose temporal alignment as a search\nproblem and introduce the task of Alignable Video Retrieval (AVR). Given a\nquery video, our approach can identify well-alignable videos from a large\ncollection of clips and temporally synchronize them to the query. To achieve\nthis, we make three key contributions: 1) we introduce DRAQ, a video\nalignability indicator to identify and re-rank the best alignable video from a\nset of candidates; 2) we propose an effective and generalizable frame-level\nvideo feature design to improve the alignment performance of several\noff-the-shelf feature representations, and 3) we propose a novel benchmark and\nevaluation protocol for AVR using cycle-consistency metrics. Our experiments on\n3 datasets, including large-scale Kinetics700, demonstrate the effectiveness of\nour approach in identifying alignable video pairs from diverse datasets.\nProject Page: https://daveishan.github.io/avr-webpage/.\n","authors":["Ishan Rajendrakumar Dave","Fabian Caba Heilbron","Mubarak Shah","Simon Jenni"],"pdf_url":"https://arxiv.org/pdf/2409.01445v1.pdf","comment":"ECCV 2024 Oral"},{"id":"http://arxiv.org/abs/2311.01304v3","updated":"2024-09-02T18:53:22Z","published":"2023-11-02T15:18:00Z","title":"VM-Rec: A Variational Mapping Approach for Cold-start User\n  Recommendation","summary":"  The cold-start problem is a common challenge for most recommender systems.\nThe practical application of most cold-start methods is hindered by the\ndeficiency in auxiliary content information for users. Moreover, most methods\nnecessitate simultaneous updates to the extensive parameters of recommender\nmodels, leading to significant training costs, particularly in large-scale\nindustrial scenarios. We observe that the model can generate expressive\nembeddings for warm users with relatively more interactions. Initially, these\nusers were cold-start users, and after transitioning to warm users, they\nexhibit clustering patterns in their embeddings with consistent initial\ninteractions. Based on this motivation, we propose a Variational Mapping\napproach for cold-start user Recommendation (VM-Rec), mapping from few initial\ninteractions to expressive embeddings for cold-start users. Specifically, we\nencode the initial interactions into a latent representation, where each\ndimension disentangledly signifies the degree of association with each warm\nuser. Subsequently, we utilize this latent representation as the parameters for\nthe mapping function, mapping (decoding) it into an expressive embedding, which\ncan be integrated into a pre-trained recommender model directly. Our method is\nevaluated on three datasets using the same base model, demonstrating superior\nperformance compared to other popular cold-start methods.\n","authors":["Linan Zheng","Jiale Chen","Pengsheng Liu","Guangfa Zhang","Jinyun Fang"],"pdf_url":"https://arxiv.org/pdf/2311.01304v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01357v1","updated":"2024-09-02T16:19:13Z","published":"2024-09-02T16:19:13Z","title":"Know When to Fuse: Investigating Non-English Hybrid Retrieval in the\n  Legal Domain","summary":"  Hybrid search has emerged as an effective strategy to offset the limitations\nof different matching paradigms, especially in out-of-domain contexts where\nnotable improvements in retrieval quality have been observed. However, existing\nresearch predominantly focuses on a limited set of retrieval methods, evaluated\nin pairs on domain-general datasets exclusively in English. In this work, we\nstudy the efficacy of hybrid search across a variety of prominent retrieval\nmodels within the unexplored field of law in the French language, assessing\nboth zero-shot and in-domain scenarios. Our findings reveal that in a zero-shot\ncontext, fusing different domain-general models consistently enhances\nperformance compared to using a standalone model, regardless of the fusion\nmethod. Surprisingly, when models are trained in-domain, we find that fusion\ngenerally diminishes performance relative to using the best single system,\nunless fusing scores with carefully tuned weights. These novel insights, among\nothers, expand the applicability of prior findings across a new field and\nlanguage, and contribute to a deeper understanding of hybrid search in\nnon-English specialized domains.\n","authors":["Antoine Louis","Gijs van Dijck","Gerasimos Spanakis"],"pdf_url":"https://arxiv.org/pdf/2409.01357v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2409.01192v1","updated":"2024-09-02T11:58:56Z","published":"2024-09-02T11:58:56Z","title":"SSD4Rec: A Structured State Space Duality Model for Efficient Sequential\n  Recommendation","summary":"  Sequential recommendation methods are crucial in modern recommender systems\nfor their remarkable capability to understand a user's changing interests based\non past interactions. However, a significant challenge faced by current methods\n(e.g., RNN- or Transformer-based models) is to effectively and efficiently\ncapture users' preferences by modeling long behavior sequences, which impedes\ntheir various applications like short video platforms where user interactions\nare numerous. Recently, an emerging architecture named Mamba, built on state\nspace models (SSM) with efficient hardware-aware designs, has showcased the\ntremendous potential for sequence modeling, presenting a compelling avenue for\naddressing the challenge effectively. Inspired by this, we propose a novel\ngeneric and efficient sequential recommendation backbone, SSD4Rec, which\nexplores the seamless adaptation of Mamba for sequential recommendations.\nSpecifically, SSD4Rec marks the variable- and long-length item sequences with\nsequence registers and processes the item representations with bidirectional\nStructured State Space Duality (SSD) blocks. This not only allows for\nhardware-aware matrix multiplication but also empowers outstanding capabilities\nin variable-length and long-range sequence modeling. Extensive evaluations on\nfour benchmark datasets demonstrate that the proposed model achieves\nstate-of-the-art performance while maintaining near-linear scalability with\nuser sequence length. Our code is publicly available at\nhttps://github.com/ZhangYifeng1995/SSD4Rec.\n","authors":["Haohao Qu","Yifeng Zhang","Liangbo Ning","Wenqi Fan","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2409.01192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05141v3","updated":"2024-09-02T10:55:30Z","published":"2024-08-09T15:53:55Z","title":"A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning","summary":"  Retrieval-augmented generation (RAG) is a framework enabling large language\nmodels (LLMs) to enhance their accuracy and reduce hallucinations by\nintegrating external knowledge bases. In this paper, we introduce a hybrid RAG\nsystem enhanced through a comprehensive suite of optimizations that\nsignificantly improve retrieval quality, augment reasoning capabilities, and\nrefine numerical computation ability. We refined the text chunks and tables in\nweb pages, added attribute predictors to reduce hallucinations, conducted LLM\nKnowledge Extractor and Knowledge Graph Extractor, and finally built a\nreasoning strategy with all the references. We evaluated our system on the CRAG\ndataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and\nonline evaluations demonstrate that our system significantly enhances complex\nreasoning capabilities. In local evaluations, we have significantly improved\naccuracy and reduced error rates compared to the baseline model, achieving a\nnotable increase in scores. In the meanwhile, we have attained outstanding\nresults in online assessments, demonstrating the performance and generalization\ncapabilities of the proposed system. The source code for our system is released\nin \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}.\n","authors":["Ye Yuan","Chengwu Liu","Jingyang Yuan","Gongbo Sun","Siqi Li","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.05141v3.pdf","comment":"Technical report for 3rd prize in Task 1 of Meta CRAG KDD Cup 2024"},{"id":"http://arxiv.org/abs/2409.01152v1","updated":"2024-09-02T10:37:53Z","published":"2024-09-02T10:37:53Z","title":"Real World Conversational Entity Linking Requires More Than Zeroshots","summary":"  Entity linking (EL) in conversations faces notable challenges in practical\napplications, primarily due to the scarcity of entity-annotated conversational\ndatasets and sparse knowledge bases (KB) containing domain-specific, long-tail\nentities. We designed targeted evaluation scenarios to measure the efficacy of\nEL models under resource constraints. Our evaluation employs two KBs: Fandom,\nexemplifying real-world EL complexities, and the widely used Wikipedia. First,\nwe assess EL models' ability to generalize to a new unfamiliar KB using Fandom\nand a novel zero-shot conversational entity linking dataset that we curated\nbased on Reddit discussions on Fandom entities. We then evaluate the\nadaptability of EL models to conversational settings without prior training.\nOur results indicate that current zero-shot EL models falter when introduced to\nnew, domain-specific KBs without prior training, significantly dropping in\nperformance. Our findings reveal that previous evaluation approaches fall short\nof capturing real-world complexities for zero-shot EL, highlighting the\nnecessity for new approaches to design and assess conversational EL models to\nadapt to limited resources. The evaluation setup and the dataset proposed in\nthis research are made publicly available.\n","authors":["Mohanna Hoveyda","Arjen P. de Vries","Maarten de Rijke","Faegheh Hasibi"],"pdf_url":"https://arxiv.org/pdf/2409.01152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01140v1","updated":"2024-09-02T10:20:35Z","published":"2024-09-02T10:20:35Z","title":"LLM-PQA: LLM-enhanced Prediction Query Answering","summary":"  The advent of Large Language Models (LLMs) provides an opportunity to change\nthe way queries are processed, moving beyond the constraints of conventional\nSQL-based database systems. However, using an LLM to answer a prediction query\nis still challenging, since an external ML model has to be employed and\ninference has to be performed in order to provide an answer. This paper\nintroduces LLM-PQA, a novel tool that addresses prediction queries formulated\nin natural language. LLM-PQA is the first to combine the capabilities of LLMs\nand retrieval-augmented mechanism for the needs of prediction queries by\nintegrating data lakes and model zoos. This integration provides users with\naccess to a vast spectrum of heterogeneous data and diverse ML models,\nfacilitating dynamic prediction query answering. In addition, LLM-PQA can\ndynamically train models on demand, based on specific query requirements,\nensuring reliable and relevant results even when no pre-trained model in a\nmodel zoo, available for the task.\n","authors":["Ziyu Li","Wenjie Zhao","Asterios Katsifodimos","Rihan Hai"],"pdf_url":"https://arxiv.org/pdf/2409.01140v1.pdf","comment":"This paper is accepted as a demo at CIKM 2024"},{"id":"http://arxiv.org/abs/2409.01082v1","updated":"2024-09-02T09:10:47Z","published":"2024-09-02T09:10:47Z","title":"Evidential Transformers for Improved Image Retrieval","summary":"  We introduce the Evidential Transformer, an uncertainty-driven transformer\nmodel for improved and robust image retrieval. In this paper, we make several\ncontributions to content-based image retrieval (CBIR). We incorporate\nprobabilistic methods into image retrieval, achieving robust and reliable\nresults, with evidential classification surpassing traditional training based\non multiclass classification as a baseline for deep metric learning.\nFurthermore, we improve the state-of-the-art retrieval results on several\ndatasets by leveraging the Global Context Vision Transformer (GC ViT)\narchitecture. Our experimental results consistently demonstrate the reliability\nof our approach, setting a new benchmark in CBIR in all test settings on the\nStanford Online Products (SOP) and CUB-200-2011 datasets.\n","authors":["Danilo Dordevic","Suryansh Kumar"],"pdf_url":"https://arxiv.org/pdf/2409.01082v1.pdf","comment":"6 pages, 6 figures, To be presented at the 3rd Workshop on\n  Uncertainty Quantification for Computer Vision, at the ECCV 2024 conference\n  in Milan, Italy"},{"id":"http://arxiv.org/abs/2312.12162v2","updated":"2024-09-02T07:58:20Z","published":"2023-12-19T13:51:48Z","title":"PEPT: Expert Finding Meets Personalized Pre-training","summary":"  Finding experts is essential in Community Question Answering (CQA) platforms\nas it enables the effective routing of questions to potential users who can\nprovide relevant answers. The key is to personalized learning expert\nrepresentations based on their historical answered questions, and accurately\nmatching them with target questions. There have been some preliminary works\nexploring the usability of PLMs in expert finding, such as pre-training expert\nor question representations. However, these models usually learn pure text\nrepresentations of experts from histories, disregarding personalized and\nfine-grained expert modeling. For alleviating this, we present a personalized\npre-training and fine-tuning paradigm, which could effectively learn expert\ninterest and expertise simultaneously. Specifically, in our pre-training\nframework, we integrate historical answered questions of one expert with one\ntarget question, and regard it as a candidate aware expert-level input unit.\nThen, we fuse expert IDs into the pre-training for guiding the model to model\npersonalized expert representations, which can help capture the unique\ncharacteristics and expertise of each individual expert. Additionally, in our\npre-training task, we design: 1) a question-level masked language model task to\nlearn the relatedness between histories, enabling the modeling of\nquestion-level expert interest; 2) a vote-oriented task to capture\nquestion-level expert expertise by predicting the vote score the expert would\nreceive. Through our pre-training framework and tasks, our approach could\nholistically learn expert representations including interests and expertise.\nOur method has been extensively evaluated on six real-world CQA datasets, and\nthe experimental results consistently demonstrate the superiority of our\napproach over competitive baseline methods.\n","authors":["Qiyao Peng","Hongyan Xu","Yinghui Wang","Hongtao Liu","Cuiying Huo","Wenjun Wang"],"pdf_url":"https://arxiv.org/pdf/2312.12162v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01012v1","updated":"2024-09-02T07:44:48Z","published":"2024-09-02T07:44:48Z","title":"Improved Diversity-Promoting Collaborative Metric Learning for\n  Recommendation","summary":"  Collaborative Metric Learning (CML) has recently emerged as a popular method\nin recommendation systems (RS), closing the gap between metric learning and\ncollaborative filtering. Following the convention of RS, existing practices\nexploit unique user representation in their model design. This paper focuses on\na challenging scenario where a user has multiple categories of interests. Under\nthis setting, the unique user representation might induce preference bias,\nespecially when the item category distribution is imbalanced. To address this\nissue, we propose a novel method called \\textit{Diversity-Promoting\nCollaborative Metric Learning} (DPCML), with the hope of considering the\ncommonly ignored minority interest of the user. The key idea behind DPCML is to\nintroduce a set of multiple representations for each user in the system where\nusers' preference toward an item is aggregated by taking the minimum item-user\ndistance among their embedding set. Specifically, we instantiate two effective\nassignment strategies to explore a proper quantity of vectors for each user.\nMeanwhile, a \\textit{Diversity Control Regularization Scheme} (DCRS) is\ndeveloped to accommodate the multi-vector representation strategy better.\nTheoretically, we show that DPCML could induce a smaller generalization error\nthan traditional CML. Furthermore, we notice that CML-based approaches usually\nrequire \\textit{negative sampling} to reduce the heavy computational burden\ncaused by the pairwise objective therein. In this paper, we reveal the\nfundamental limitation of the widely adopted hard-aware sampling from the\nOne-Way Partial AUC (OPAUC) perspective and then develop an effective sampling\nalternative for the CML-based paradigm. Finally, comprehensive experiments over\na range of benchmark datasets speak to the efficacy of DPCML. Code are\navailable at \\url{https://github.com/statusrank/LibCML}.\n","authors":["Shilong Bao","Qianqian Xu","Zhiyong Yang","Yuan He","Xiaochun Cao","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2409.01012v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2209.15292"},{"id":"http://arxiv.org/abs/2409.00890v1","updated":"2024-09-02T01:54:33Z","published":"2024-09-02T01:54:33Z","title":"Towards Investigating Biases in Spoken Conversational Search","summary":"  Voice-based systems like Amazon Alexa, Google Assistant, and Apple Siri,\nalong with the growing popularity of OpenAI's ChatGPT and Microsoft's Copilot,\nserve diverse populations, including visually impaired and low-literacy\ncommunities. This reflects a shift in user expectations from traditional search\nto more interactive question-answering models. However, presenting information\neffectively in voice-only channels remains challenging due to their linear\nnature. This limitation can impact the presentation of complex queries\ninvolving controversial topics with multiple perspectives. Failing to present\ndiverse viewpoints may perpetuate or introduce biases and affect user\nattitudes. Balancing information load and addressing biases is crucial in\ndesigning a fair and effective voice-based system. To address this, we (i)\nreview how biases and user attitude changes have been studied in screen-based\nweb search, (ii) address challenges in studying these changes in voice-based\nsettings like SCS, (iii) outline research questions, and (iv) propose an\nexperimental setup with variables, data, and instruments to explore biases in a\nvoice-based setting like Spoken Conversational Search.\n","authors":["Sachin Pathiyan Cherumanal","Falk Scholer","Johanne R. Trippas","Damiano Spina"],"pdf_url":"https://arxiv.org/pdf/2409.00890v1.pdf","comment":"Accepted Late-Breaking Results at ACM ICMI Companion 2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2207.12554v2","updated":"2024-09-02T22:49:21Z","published":"2022-07-25T22:17:19Z","title":"Inter-Frame Compression for Dynamic Point Cloud Geometry Coding","summary":"  Efficient point cloud compression is essential for applications like virtual\nand mixed reality, autonomous driving, and cultural heritage. This paper\nproposes a deep learning-based inter-frame encoding scheme for dynamic point\ncloud geometry compression. We propose a lossy geometry compression scheme that\npredicts the latent representation of the current frame using the previous\nframe by employing a novel feature space inter-prediction network. The proposed\nnetwork utilizes sparse convolutions with hierarchical multiscale 3D feature\nlearning to encode the current frame using the previous frame. The proposed\nmethod introduces a novel predictor network for motion compensation in the\nfeature domain to map the latent representation of the previous frame to the\ncoordinates of the current frame to predict the current frame's feature\nembedding. The framework transmits the residual of the predicted features and\nthe actual features by compressing them using a learned probabilistic\nfactorized entropy model. At the receiver, the decoder hierarchically\nreconstructs the current frame by progressively rescaling the feature\nembedding. The proposed framework is compared to the state-of-the-art\nVideo-based Point Cloud Compression (V-PCC) and Geometry-based Point Cloud\nCompression (G-PCC) schemes standardized by the Moving Picture Experts Group\n(MPEG). The proposed method achieves more than 88% BD-Rate (Bjontegaard Delta\nRate) reduction against G-PCCv20 Octree, more than 56% BD-Rate savings against\nG-PCCv20 Trisoup, more than 62% BD-Rate reduction against V-PCC intra-frame\nencoding mode, and more than 52% BD-Rate savings against V-PCC P-frame-based\ninter-frame encoding mode using HEVC. These significant performance gains are\ncross-checked and verified in the MPEG working group.\n","authors":["Anique Akhtar","Zhu Li","Geert Van der Auwera"],"pdf_url":"https://arxiv.org/pdf/2207.12554v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01352v1","updated":"2024-09-02T16:11:12Z","published":"2024-09-02T16:11:12Z","title":"Spectron: Target Speaker Extraction using Conditional Transformer with\n  Adversarial Refinement","summary":"  Recently, attention-based transformers have become a de facto standard in\nmany deep learning applications including natural language processing, computer\nvision, signal processing, etc.. In this paper, we propose a transformer-based\nend-to-end model to extract a target speaker's speech from a monaural\nmulti-speaker mixed audio signal. Unlike existing speaker extraction methods,\nwe introduce two additional objectives to impose speaker embedding consistency\nand waveform encoder invertibility and jointly train both speaker encoder and\nspeech separator to better capture the speaker conditional embedding.\nFurthermore, we leverage a multi-scale discriminator to refine the perceptual\nquality of the extracted speech. Our experiments show that the use of a dual\npath transformer in the separator backbone along with proposed training\nparadigm improves the CNN baseline by $3.12$ dB points. Finally, we compare our\napproach with recent state-of-the-arts and show that our model outperforms\nexisting methods by $4.1$ dB points on an average without creating additional\ndata dependency.\n","authors":["Tathagata Bandyopadhyay"],"pdf_url":"https://arxiv.org/pdf/2409.01352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01029v1","updated":"2024-09-02T08:06:47Z","published":"2024-09-02T08:06:47Z","title":"Multi-Reference Generative Face Video Compression with Contrastive\n  Learning","summary":"  Generative face video coding (GFVC) has been demonstrated as a potential\napproach to low-latency, low bitrate video conferencing. GFVC frameworks\nachieve an extreme gain in coding efficiency with over 70% bitrate savings when\ncompared to conventional codecs at bitrates below 10kbps. In recent MPEG/JVET\nstandardization efforts, all the information required to reconstruct video\nsequences using GFVC frameworks are adopted as part of the supplemental\nenhancement information (SEI) in existing compression pipelines. In light of\nthis development, we aim to address a challenge that has been weakly addressed\nin prior GFVC frameworks, i.e., reconstruction drift as the distance between\nthe reference and target frames increases. This challenge creates the need to\nupdate the reference buffer more frequently by transmitting more Intra-refresh\nframes, which are the most expensive element of the GFVC bitstream. To overcome\nthis problem, we propose instead multiple reference animation as a robust\napproach to minimizing reconstruction drift, especially when used in a\nbi-directional prediction mode. Further, we propose a contrastive learning\nformulation for multi-reference animation. We observe that using a contrastive\nlearning framework enhances the representation capabilities of the animation\ngenerator. The resulting framework, MRDAC (Multi-Reference Deep Animation\nCodec) can therefore be used to compress longer sequences with fewer reference\nframes or achieve a significant gain in reconstruction accuracy at comparable\nbitrates to previous frameworks. Quantitative and qualitative results show\nsignificant coding and reconstruction quality gains compared to previous GFVC\nmethods, and more accurate animation quality in presence of large pose and\nfacial expression changes.\n","authors":["Goluck Konuko","Giuseppe Valenzise"],"pdf_url":"https://arxiv.org/pdf/2409.01029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00971v1","updated":"2024-09-02T06:26:48Z","published":"2024-09-02T06:26:48Z","title":"Interpretable Convolutional SyncNet","summary":"  Because videos in the wild can be out of sync for various reasons, a sync-net\nis used to bring the video back into sync for tasks that require synchronized\nvideos. Previous state-of-the-art (SOTA) sync-nets use InfoNCE loss, rely on\nthe transformer architecture, or both. Unfortunately, the former makes the\nmodel's output difficult to interpret, and the latter is unfriendly with large\nimages, thus limiting the usefulness of sync-nets. In this work, we train a\nconvolutional sync-net using the balanced BCE loss (BBCE), a loss inspired by\nthe binary cross entropy (BCE) and the InfoNCE losses. In contrast to the\nInfoNCE loss, the BBCE loss does not require complicated sampling schemes. Our\nmodel can better handle larger images, and its output can be given a\nprobabilistic interpretation. The probabilistic interpretation allows us to\ndefine metrics such as probability at offset and offscreen ratio to evaluate\nthe sync quality of audio-visual (AV) speech datasets. Furthermore, our model\nachieves SOTA accuracy of $96.5\\%$ on the LRS2 dataset and $93.8\\%$ on the LRS3\ndataset.\n","authors":["Sungjoon Park","Jaesub Yun","Donggeon Lee","Minsik Park"],"pdf_url":"https://arxiv.org/pdf/2409.00971v1.pdf","comment":"8+5 pages"},{"id":"http://arxiv.org/abs/2308.03024v3","updated":"2024-09-02T05:51:02Z","published":"2023-08-06T05:23:25Z","title":"Show Me the World in My Language: Establishing the First Baseline for\n  Scene-Text to Scene-Text Translation","summary":"  In this work, we study the task of ``visually'' translating scene text from a\nsource language (e.g., Hindi) to a target language (e.g., English). Visual\ntranslation involves not just the recognition and translation of scene text but\nalso the generation of the translated image that preserves visual features of\nthe source scene text, such as font, size, and background. There are several\nchallenges associated with this task, such as translation with limited context,\ndeciding between translation and transliteration, accommodating varying text\nlengths within fixed spatial boundaries, and preserving the font and background\nstyles of the source scene text in the target language. To address this\nproblem, we make the following contributions: (i) We study visual translation\nas a standalone problem for the first time in the literature. (ii) We present a\ncascaded framework for visual translation that combines state-of-the-art\nmodules for scene text recognition, machine translation, and scene text\nsynthesis as a baseline for the task. (iii) We propose a set of task-specific\ndesign enhancements to design a variant of the baseline to obtain performance\nimprovements. (iv) Currently, the existing related literature lacks any\ncomprehensive performance evaluation for this novel task. To fill this gap, we\nintroduce several automatic and user-assisted evaluation metrics designed\nexplicitly for evaluating visual translation. Further, we evaluate presented\nbaselines for translating scene text between Hindi and English. Our experiments\ndemonstrate that although we can effectively perform visual translation over a\nlarge collection of scene text images, the presented baseline only partially\naddresses challenges posed by visual translation tasks. We firmly believe that\nthis new task and the limitations of existing models, as reported in this\npaper, should encourage further research in visual translation.\n","authors":["Shreyas Vaidya","Arvind Kumar Sharma","Prajwal Gatti","Anand Mishra"],"pdf_url":"https://arxiv.org/pdf/2308.03024v3.pdf","comment":"Accepted at ICPR 2024, Project Website:\n  https://vl2g.github.io/projects/visTrans/"}]},"2024-09-01T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.00851v1","updated":"2024-09-01T22:01:21Z","published":"2024-09-01T22:01:21Z","title":"Dissecting Temporal Understanding in Text-to-Audio Retrieval","summary":"  Recent advancements in machine learning have fueled research on multimodal\ntasks, such as for instance text-to-video and text-to-audio retrieval. These\ntasks require models to understand the semantic content of video and audio\ndata, including objects, and characters. The models also need to learn spatial\narrangements and temporal relationships. In this work, we analyse the temporal\nordering of sounds, which is an understudied problem in the context of\ntext-to-audio retrieval. In particular, we dissect the temporal understanding\ncapabilities of a state-of-the-art model for text-to-audio retrieval on the\nAudioCaps and Clotho datasets. Additionally, we introduce a synthetic\ntext-audio dataset that provides a controlled setting for evaluating temporal\ncapabilities of recent models. Lastly, we present a loss function that\nencourages text-audio models to focus on the temporal ordering of events. Code\nand data are available at\nhttps://www.robots.ox.ac.uk/~vgg/research/audio-retrieval/dtu/.\n","authors":["Andreea-Maria Oncescu","Jo√£o F. Henriques","A. Sophia Koepke"],"pdf_url":"https://arxiv.org/pdf/2409.00851v1.pdf","comment":"9 pages, 5 figures, ACM Multimedia 2024,\n  https://www.robots.ox.ac.uk/~vgg/research/audio-retrieval/dtu/"},{"id":"http://arxiv.org/abs/2409.00830v1","updated":"2024-09-01T20:18:36Z","published":"2024-09-01T20:18:36Z","title":"Building FKG.in: a Knowledge Graph for Indian Food","summary":"  This paper presents an ontology design along with knowledge engineering, and\nmultilingual semantic reasoning techniques to build an automated system for\nassimilating culinary information for Indian food in the form of a knowledge\ngraph. The main focus is on designing intelligent methods to derive ontology\ndesigns and capture all-encompassing knowledge about food, recipes,\ningredients, cooking characteristics, and most importantly, nutrition, at\nscale. We present our ongoing work in this workshop paper, describe in some\ndetail the relevant challenges in curating knowledge of Indian food, and\npropose our high-level ontology design. We also present a novel workflow that\nuses AI, LLM, and language technology to curate information from recipe blog\nsites in the public domain to build knowledge graphs for Indian food. The\nmethods for knowledge curation proposed in this paper are generic and can be\nreplicated for any domain. The design is application-agnostic and can be used\nfor AI-driven smart analysis, building recommendation systems for Personalized\nDigital Health, and complementing the knowledge graph for Indian food with\ncontextual information such as user information, food biochemistry, geographic\ninformation, agricultural information, etc.\n","authors":["Saransh Kumar Gupta","Lipika Dey","Partha Pratim Das","Ramesh Jain"],"pdf_url":"https://arxiv.org/pdf/2409.00830v1.pdf","comment":"14 pages, 3 figures, 25 references, Formal Ontology in Information\n  Systems Conference 2024 - Integrated Food Ontology Workshop"},{"id":"http://arxiv.org/abs/2010.14464v2","updated":"2024-09-01T20:11:27Z","published":"2020-10-27T17:23:18Z","title":"Dynamic Boundary Time Warping for Sub-sequence Matching with Few\n  Examples","summary":"  The paper presents a novel method of finding a fragment in a long temporal\nsequence similar to the set of shorter sequences. We are the first to propose\nan algorithm for such a search that does not rely on computing the average\nsequence from query examples. Instead, we use query examples as is, utilizing\nall of them simultaneously. The introduced method based on the Dynamic Time\nWarping (DTW) technique is suited explicitly for few-shot query-by-example\nretrieval tasks. We evaluate it on two different few-shot problems from the\nfield of Natural Language Processing. The results show it either outperforms\nbaselines and previous approaches or achieves comparable results when a low\nnumber of examples is available.\n","authors":["≈Åukasz Borchmann","Dawid Jurkiewicz","Filip Grali≈Ñski","Tomasz G√≥recki"],"pdf_url":"https://arxiv.org/pdf/2010.14464v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04668v2","updated":"2024-09-01T19:00:25Z","published":"2024-08-07T01:50:59Z","title":"Forecasting Live Chat Intent from Browsing History","summary":"  Customers reach out to online live chat agents with various intents, such as\nasking about product details or requesting a return. In this paper, we propose\nthe problem of predicting user intent from browsing history and address it\nthrough a two-stage approach. The first stage classifies a user's browsing\nhistory into high-level intent categories. Here, we represent each browsing\nhistory as a text sequence of page attributes and use the ground-truth class\nlabels to fine-tune pretrained Transformers. The second stage provides a large\nlanguage model (LLM) with the browsing history and predicted intent class to\ngenerate fine-grained intents. For automatic evaluation, we use a separate LLM\nto judge the similarity between generated and ground-truth intents, which\nclosely aligns with human judgments. Our two-stage approach yields significant\nperformance gains compared to generating intents without the classification\nstage.\n","authors":["Se-eun Yoon","Ahmad Bin Rabiah","Zaid Alibadi","Surya Kallumadi","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2408.04668v2.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2309.13611v2","updated":"2024-09-01T17:40:29Z","published":"2023-09-24T11:19:59Z","title":"Sparsity-regularized coded ptychography for robust and efficient\n  lensless microscopy on a chip","summary":"  Coded ptychography has emerged as a powerful technique for high-throughput,\nhigh-resolution lensless imaging. However, the trade-off between acquisition\nspeed and image quality remains a significant challenge. To address this, we\nintroduce a novel sparsity-regularized approach to coded ptychography that\ndramatically reduces the number of required measurements while maintaining high\nreconstruction quality. The reported approach, termed the ptychographic\nproximal total-variation (PPTV) solver, formulates the reconstruction task as a\ntotal variation regularized optimization problem. Unlike previous\nimplementations that rely on specialized hardware or illumination schemes, PPTV\nintegrates seamlessly into existing coded ptychography setups. Through\ncomprehensive numerical simulations, we demonstrate that PPTV-driven coded\nptychography can produce accurate reconstructions with as few as eight\nintensity measurements, a significant reduction compared to conventional\nmethods. Convergence analysis confirms the robustness and stability of the PPTV\nalgorithm. Experimental results from our optical prototype, featuring a\ndisorder-engineered surface for wavefront modulation, validate PPTV's ability\nto achieve high-throughput, high-resolution imaging with a substantially\nreduced measurement burden. By enabling high-quality reconstructions from fewer\nmeasurements, PPTV paves the way for more compact, efficient, and\ncost-effective lensless microscopy systems on a chip, with potential\napplications in digital pathology, endoscopy, point-of-care diagnostics, and\nhigh-content screening.\n","authors":["Ninghe Liu","Qianhao Zhao","Guoan Zheng"],"pdf_url":"https://arxiv.org/pdf/2309.13611v2.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2306.04487v4","updated":"2024-09-01T15:38:50Z","published":"2023-06-07T14:57:21Z","title":"Vague Preference Policy Learning for Conversational Recommendation","summary":"  Conversational recommendation systems (CRS) commonly assume users have clear\npreferences, leading to potential over-filtering of relevant alternatives.\nHowever, users often exhibit vague, non-binary preferences. We introduce the\nVague Preference Multi-round Conversational Recommendation (VPMCR) scenario,\nemploying a soft estimation mechanism to accommodate users' vague and dynamic\npreferences while mitigating over-filtering. In VPMCR, we propose Vague\nPreference Policy Learning (VPPL), consisting of Ambiguity-aware Soft\nEstimation (ASE) and Dynamism-aware Policy Learning (DPL). ASE captures\npreference vagueness by estimating scores for clicked and non-clicked options,\nusing a choice-based approach and time-aware preference decay. DPL leverages\nASE's preference distribution to guide the conversation and adapt to preference\nchanges for recommendations or attribute queries. Extensive experiments\ndemonstrate VPPL's effectiveness within VPMCR, outperforming existing methods\nand setting a new benchmark. Our work advances CRS by accommodating users'\ninherent ambiguity and relative decision-making processes, improving real-world\napplicability.\n","authors":["Gangyi Zhang","Chongming Gao","Wenqiang Lei","Xiaojie Guo","Shijun Li","Hongshen Chen","Zhuozhi Ding","Sulong Xu","Lingfei Wu"],"pdf_url":"https://arxiv.org/pdf/2306.04487v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00727v1","updated":"2024-09-01T14:20:01Z","published":"2024-09-01T14:20:01Z","title":"Hound: Hunting Supervision Signals for Few and Zero Shot Node\n  Classification on Text-attributed Graph","summary":"  Text-attributed graph (TAG) is an important type of graph structured data\nwith text descriptions for each node. Few- and zero-shot node classification on\nTAGs have many applications in fields such as academia and social networks.\nHowever, the two tasks are challenging due to the lack of supervision signals,\nand existing methods only use the contrastive loss to align graph-based node\nembedding and language-based text embedding. In this paper, we propose Hound to\nimprove accuracy by introducing more supervision signals, and the core idea is\nto go beyond the node-text pairs that come with data. Specifically, we design\nthree augmentation techniques, i.e., node perturbation, text matching, and\nsemantics negation to provide more reference nodes for each text and vice\nversa. Node perturbation adds/drops edges to produce diversified node\nembeddings that can be matched with a text. Text matching retrieves texts with\nsimilar embeddings to match with a node. Semantics negation uses a negative\nprompt to construct a negative text with the opposite semantics, which is\ncontrasted with the original node and text. We evaluate Hound on 5 datasets and\ncompare with 13 state-of-the-art baselines. The results show that Hound\nconsistently outperforms all baselines, and its accuracy improvements over the\nbest-performing baseline are usually over 5%.\n","authors":["Yuxiang Wang","Xiao Yan","Shiyu Jin","Quanqing Xu","Chuanhui Yang","Yuanyuan Zhu","Chuang Hu","Bo Du","Jiawei Jiang"],"pdf_url":"https://arxiv.org/pdf/2409.00727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00720v1","updated":"2024-09-01T13:33:41Z","published":"2024-09-01T13:33:41Z","title":"Fair Reciprocal Recommendation in Matching Markets","summary":"  Recommender systems play an increasingly crucial role in shaping people's\nopportunities, particularly in online dating platforms. It is essential from\nthe user's perspective to increase the probability of matching with a suitable\npartner while ensuring an appropriate level of fairness in the matching\nopportunities. We investigate reciprocal recommendation in two-sided matching\nmarkets between agents divided into two sides. In our model, a match is\nconsidered successful only when both individuals express interest in each\nother. Additionally, we assume that agents prefer to appear prominently in the\nrecommendation lists presented to those on the other side. We define each\nagent's opportunity to be recommended and introduce its fairness criterion,\nenvy-freeness, from the perspective of fair division theory. The\nrecommendations that approximately maximize the expected number of matches,\nempirically obtained by heuristic algorithms, are likely to result in\nsignificant unfairness of opportunity. Therefore, there can be a trade-off\nbetween maximizing the expected matches and ensuring fairness of opportunity.\nTo address this challenge, we propose a method to find a policy that is close\nto being envy-free by leveraging the Nash social welfare function. Experiments\non synthetic and real-world datasets demonstrate the effectiveness of our\napproach in achieving both relatively high expected matches and fairness for\nopportunities of both sides in reciprocal recommender systems.\n","authors":["Yoji Tomita","Tomohiki Yokoyama"],"pdf_url":"https://arxiv.org/pdf/2409.00720v1.pdf","comment":"Accepted at RecSys2024"},{"id":"http://arxiv.org/abs/2409.00636v1","updated":"2024-09-01T07:01:22Z","published":"2024-09-01T07:01:22Z","title":"A Learnable Agent Collaboration Network Framework for Personalized\n  Multimodal AI Search Engine","summary":"  Large language models (LLMs) and retrieval-augmented generation (RAG)\ntechniques have revolutionized traditional information access, enabling AI\nagent to search and summarize information on behalf of users during dynamic\ndialogues. Despite their potential, current AI search engines exhibit\nconsiderable room for improvement in several critical areas. These areas\ninclude the support for multimodal information, the delivery of personalized\nresponses, the capability to logically answer complex questions, and the\nfacilitation of more flexible interactions. This paper proposes a novel AI\nSearch Engine framework called the Agent Collaboration Network (ACN). The ACN\nframework consists of multiple specialized agents working collaboratively, each\nwith distinct roles such as Account Manager, Solution Strategist, Information\nManager, and Content Creator. This framework integrates mechanisms for picture\ncontent understanding, user profile tracking, and online evolution, enhancing\nthe AI search engine's response quality, personalization, and interactivity. A\nhighlight of the ACN is the introduction of a Reflective Forward Optimization\nmethod (RFO), which supports the online synergistic adjustment among agents.\nThis feature endows the ACN with online learning capabilities, ensuring that\nthe system has strong interactive flexibility and can promptly adapt to user\nfeedback. This learning method may also serve as an optimization approach for\nagent-based systems, potentially influencing other domains of agent\napplications.\n","authors":["Yunxiao Shi","Min Xu","Haimin Zhang","Xing Zi","Qiang Wu"],"pdf_url":"https://arxiv.org/pdf/2409.00636v1.pdf","comment":"ACMMM 2024 MMGR WORKSHOP"}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.10846v2","updated":"2024-09-01T14:57:12Z","published":"2024-08-19T12:06:25Z","title":"Harmonizing Attention: Training-free Texture-aware Geometry Transfer","summary":"  Extracting geometry features from photographic images independently of\nsurface texture and transferring them onto different materials remains a\ncomplex challenge. In this study, we introduce Harmonizing Attention, a novel\ntraining-free approach that leverages diffusion models for texture-aware\ngeometry transfer. Our method employs a simple yet effective modification of\nself-attention layers, allowing the model to query information from multiple\nreference images within these layers. This mechanism is seamlessly integrated\ninto the inversion process as Texture-aligning Attention and into the\ngeneration process as Geometry-aligning Attention. This dual-attention approach\nensures the effective capture and transfer of material-independent geometry\nfeatures while maintaining material-specific textural continuity, all without\nthe need for model fine-tuning.\n","authors":["Eito Ikuta","Yohan Lee","Akihiro Iohara","Yu Saito","Toshiyuki Tanaka"],"pdf_url":"https://arxiv.org/pdf/2408.10846v2.pdf","comment":"Accepted at WACV2025"},{"id":"http://arxiv.org/abs/2409.00615v1","updated":"2024-09-01T04:57:42Z","published":"2024-09-01T04:57:42Z","title":"MetaDigiHuman: Haptic Interfaces for Digital Humans in Metaverse","summary":"  The way we engage with digital spaces and the digital world has undergone\nrapid changes in recent years, largely due to the emergence of the Metaverse.\nAs technology continues to advance, the demand for sophisticated and immersive\ninterfaces to interact with the Metaverse has become increasingly crucial.\nHaptic interfaces have been developed to meet this need and provide users with\ntactile feedback and realistic touch sensations. These interfaces play a vital\nrole in creating a more authentic and immersive experience within the\nMetaverse. This article introduces the concept of MetaDigiHuman, a\ngroundbreaking framework that combines blended digital humans and haptic\ninterfaces. By harnessing cutting-edge technologies, MetaDigiHuman enables\nseamless and immersive interaction within the Metaverse. Through this\nframework, users can simulate the sensation of touching, feeling, and\ninteracting with digital beings as if they were physically present in the\nenvironments, offering a more compelling and immersive experience within the\nMetaverse.\n","authors":["Senthil Kumar Jagatheesaperumal","Praveen Sathikumar","Harikrishnan Rajan"],"pdf_url":"https://arxiv.org/pdf/2409.00615v1.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2409.00597v1","updated":"2024-09-01T03:16:30Z","published":"2024-09-01T03:16:30Z","title":"Multimodal Multi-turn Conversation Stance Detection: A Challenge Dataset\n  and Effective Model","summary":"  Stance detection, which aims to identify public opinion towards specific\ntargets using social media data, is an important yet challenging task. With the\nproliferation of diverse multimodal social media content including text, and\nimages multimodal stance detection (MSD) has become a crucial research area.\nHowever, existing MSD studies have focused on modeling stance within individual\ntext-image pairs, overlooking the multi-party conversational contexts that\nnaturally occur on social media. This limitation stems from a lack of datasets\nthat authentically capture such conversational scenarios, hindering progress in\nconversational MSD. To address this, we introduce a new multimodal multi-turn\nconversational stance detection dataset (called MmMtCSD). To derive stances\nfrom this challenging dataset, we propose a novel multimodal large language\nmodel stance detection framework (MLLM-SD), that learns joint stance\nrepresentations from textual and visual modalities. Experiments on MmMtCSD show\nstate-of-the-art performance of our proposed MLLM-SD approach for multimodal\nstance detection. We believe that MmMtCSD will contribute to advancing\nreal-world applications of stance detection research.\n","authors":["Fuqiang Niu","Zebang Cheng","Xianghua Fu","Xiaojiang Peng","Genan Dai","Yin Chen","Hu Huang","Bowen Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.00597v1.pdf","comment":"ACM MM2024"}]},"2024-08-31T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.18646v2","updated":"2024-08-31T19:26:21Z","published":"2024-07-26T10:28:59Z","title":"Decoding Knowledge Claims: The Evaluation of Scientific Publication\n  Contributions through Semantic Analysis","summary":"  The surge in scientific publications challenges the use of publication counts\nas a measure of scientific progress, requiring alternative metrics that\nemphasize the quality and novelty of scientific contributions rather than sheer\nquantity. This paper proposes the use of Relaxed Word Mover's Distance (RWMD),\na semantic text similarity measure, to evaluate the novelty of scientific\npapers. We hypothesize that RWMD can more effectively gauge the growth of\nscientific knowledge. To test such an assumption, we apply RWMD to evaluate\nseminal papers, with Hirsch's H-Index paper as a primary case study. We compare\nRWMD results across three groups: 1) H-Index-related papers, 2) scientometric\nstudies, and 3) unrelated papers, aiming to discern redundant literature and\nhype from genuine innovations. Findings suggest that emphasizing knowledge\nclaims offers a deeper insight into scientific contributions, marking RWMD as a\npromising alternative method to traditional citation metrics, thus better\ntracking significant scientific breakthroughs.\n","authors":["Luca D'Aniello","Nicolas Robinson-Garcia","Massimo Aria","Corrado Cuccurullo"],"pdf_url":"https://arxiv.org/pdf/2407.18646v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00448v1","updated":"2024-08-31T13:01:58Z","published":"2024-08-31T13:01:58Z","title":"PSLF: A PID Controller-incorporated Second-order Latent Factor Analysis\n  Model for Recommender System","summary":"  A second-order-based latent factor (SLF) analysis model demonstrates superior\nperformance in graph representation learning, particularly for high-dimensional\nand incomplete (HDI) interaction data, by incorporating the curvature\ninformation of the loss landscape. However, its objective function is commonly\nbi-linear and non-convex, causing the SLF model to suffer from a low\nconvergence rate. To address this issue, this paper proposes a PID\ncontroller-incorporated SLF (PSLF) model, leveraging two key strategies: a)\nrefining learning error estimation by incorporating the PID controller\nprinciples, and b) acquiring second-order information insights through\nHessian-vector products. Experimental results on multiple HDI datasets indicate\nthat the proposed PSLF model outperforms four state-of-the-art latent factor\nmodels based on advanced optimizers regarding convergence rates and\ngeneralization performance.\n","authors":["Jialiang Wang","Yan Xia","Ye Yuan"],"pdf_url":"https://arxiv.org/pdf/2409.00448v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00400v1","updated":"2024-08-31T09:19:41Z","published":"2024-08-31T09:19:41Z","title":"An Enhanced Batch Query Architecture in Real-time Recommendation","summary":"  In industrial recommendation systems on websites and apps, it is essential to\nrecall and predict top-n results relevant to user interests from a content pool\nof billions within milliseconds. To cope with continuous data growth and\nimprove real-time recommendation performance, we have designed and implemented\na high-performance batch query architecture for real-time recommendation\nsystems. Our contributions include optimizing hash structures with a\ncacheline-aware probing method to enhance coalesced hashing, as well as the\nimplementation of a hybrid storage key-value service built upon it. Our\nexperiments indicate this approach significantly surpasses conventional hash\ntables in batch query throughput, achieving up to 90% of the query throughput\nof random memory access when incorporating parallel optimization. The support\nfor NVMe, integrating two-tier storage for hot and cold data, notably reduces\nresource consumption. Additionally, the system facilitates dynamic updates,\nautomated sharding of attributes and feature embedding tables, and introduces\ninnovative protocols for consistency in batch queries, thereby enhancing the\neffectiveness of real-time incremental learning updates. This architecture has\nbeen deployed and in use in the bilibili recommendation system for over a year,\na video content community with hundreds of millions of users, supporting 10x\nincrease in model computation with minimal resource growth, improving outcomes\nwhile preserving the system's real-time performance.\n","authors":["Qiang Zhang","Zhipeng Teng","Disheng Wu","Jiayin Wang"],"pdf_url":"https://arxiv.org/pdf/2409.00400v1.pdf","comment":"8 pages, 10 figures, CIKM 2024 Applied Research Paper"}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.00562v1","updated":"2024-08-31T23:22:30Z","published":"2024-08-31T23:22:30Z","title":"Comparative Analysis of Modality Fusion Approaches for Audio-Visual\n  Person Identification and Verification","summary":"  Multimodal learning involves integrating information from various modalities\nto enhance learning and comprehension. We compare three modality fusion\nstrategies in person identification and verification by processing two\nmodalities: voice and face. In this paper, a one-dimensional convolutional\nneural network is employed for x-vector extraction from voice, while the\npre-trained VGGFace2 network and transfer learning are utilized for face\nmodality. In addition, gammatonegram is used as speech representation in\nengagement with the Darknet19 pre-trained network. The proposed systems are\nevaluated using the K-fold cross-validation technique on the 118 speakers of\nthe test set of the VoxCeleb2 dataset. The comparative evaluations are done for\nsingle-modality and three proposed multimodal strategies in equal situations.\nResults demonstrate that the feature fusion strategy of gammatonegram and\nfacial features achieves the highest performance, with an accuracy of 98.37% in\nthe person identification task. However, concatenating facial features with the\nx-vector reaches 0.62% for EER in verification tasks.\n","authors":["Aref Farhadipour","Masoumeh Chapariniya","Teodora Vukovic","Volker Dellwo"],"pdf_url":"https://arxiv.org/pdf/2409.00562v1.pdf","comment":"This paper has been submitted to a conference"},{"id":"http://arxiv.org/abs/2409.00552v1","updated":"2024-08-31T22:27:40Z","published":"2024-08-31T22:27:40Z","title":"Digit Recognition using Multimodal Spiking Neural Networks","summary":"  Spiking neural networks (SNNs) are the third generation of neural networks\nthat are biologically inspired to process data in a fashion that emulates the\nexchange of signals in the brain. Within the Computer Vision community SNNs\nhave garnered significant attention due in large part to the availability of\nevent-based sensors that produce a spatially resolved spike train in response\nto changes in scene radiance. SNNs are used to process event-based data due to\ntheir neuromorphic nature. The proposed work examines the neuromorphic\nadvantage of fusing multiple sensory inputs in classification tasks.\nSpecifically we study the performance of a SNN in digit classification by\npassing in a visual modality branch (Neuromorphic-MNIST [N-MNIST]) and an\nauditory modality branch (Spiking Heidelberg Digits [SHD]) from datasets that\nwere created using event-based sensors to generate a series of time-dependent\nevents. It is observed that multi-modal SNNs outperform unimodal visual and\nunimodal auditory SNNs. Furthermore, it is observed that the process of sensory\nfusion is insensitive to the depth at which the visual and auditory branches\nare combined. This work achieves a 98.43% accuracy on the combined N-MNIST and\nSHD dataset using a multimodal SNN that concatenates the visual and auditory\nbranches at a late depth.\n","authors":["William Bjorndahl","Jack Easton","Austin Modoff","Eric C. Larson","Joseph Camp","Prasanna Rangarajan"],"pdf_url":"https://arxiv.org/pdf/2409.00552v1.pdf","comment":"4 pages, 2 figures, submitted to 2025 IEEE International Conference\n  on Acoustics, Speech, and Signal Processing"},{"id":"http://arxiv.org/abs/2409.00486v1","updated":"2024-08-31T15:43:22Z","published":"2024-08-31T15:43:22Z","title":"Multi-scale Multi-instance Visual Sound Localization and Segmentation","summary":"  Visual sound localization is a typical and challenging problem that predicts\nthe location of objects corresponding to the sound source in a video. Previous\nmethods mainly used the audio-visual association between global audio and\none-scale visual features to localize sounding objects in each image. Despite\ntheir promising performance, they omitted multi-scale visual features of the\ncorresponding image, and they cannot learn discriminative regions compared to\nground truths. To address this issue, we propose a novel multi-scale\nmulti-instance visual sound localization framework, namely M2VSL, that can\ndirectly learn multi-scale semantic features associated with sound sources from\nthe input image to localize sounding objects. Specifically, our M2VSL leverages\nlearnable multi-scale visual features to align audio-visual representations at\nmulti-level locations of the corresponding image. We also introduce a novel\nmulti-scale multi-instance transformer to dynamically aggregate multi-scale\ncross-modal representations for visual sound localization. We conduct extensive\nexperiments on VGGSound-Instruments, VGG-Sound Sources, and AVSBench\nbenchmarks. The results demonstrate that the proposed M2VSL can achieve\nstate-of-the-art performance on sounding object localization and segmentation.\n","authors":["Shentong Mo","Haofan Wang"],"pdf_url":"https://arxiv.org/pdf/2409.00486v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06152v2","updated":"2024-08-31T12:32:50Z","published":"2024-08-12T13:48:06Z","title":"Palantir: Towards Efficient Super Resolution for Ultra-high-definition\n  Live Streaming","summary":"  Neural enhancement through super-resolution (SR) deep neural networks (DNNs)\nopens up new possibilities for ultra-high-definition (UHD) live streaming over\nexisting encoding and networking infrastructure. Yet, the heavy SR DNN\ninference overhead leads to severe deployment challenges. To reduce the\noverhead, existing systems propose to apply DNN-based SR only on carefully\nselected anchor frames while upscaling non-anchor frames via the lightweight\nreusing-based SR approach. However, frame-level scheduling is coarse-grained\nand fails to deliver optimal efficiency. In this work, we propose Palantir, the\nfirst neural-enhanced UHD live streaming system with fine-grained patch-level\nscheduling. Two novel techniques are incorporated into Palantir to select the\nmost beneficial anchor patches and support latency-sensitive UHD live streaming\napplications. Firstly, under the guidance of our pioneering and theoretical\nanalysis, Palantir constructs a directed acyclic graph (DAG) for lightweight\nyet accurate SR quality estimation under any possible anchor patch set.\nSecondly, to further optimize the scheduling latency, Palantir improves\nparallelizability by refactoring the computation subprocedure of the estimation\nprocess into a sparse matrix-matrix multiplication operation.\n  The evaluation results suggest that Palantir incurs a negligible scheduling\nlatency accounting for less than 5.7% of the end-to-end latency requirement.\nWhen compared to the naive method of applying DNN-based SR on all the frames,\nPalantir can reduce the SR DNN inference overhead by 20 times (or 60 times)\nwhile preserving 54.0-82.6% (or 32.8-64.0%) of the quality gain. When compared\nto the state-of-the-art real-time frame-level scheduling strategy, Palantir can\nreduce the SR DNN inference overhead by 80.1% at most (and 38.4% on average)\nwithout sacrificing the video quality.\n","authors":["Xinqi Jin","Zhui Zhu","Xikai Sun","Fan Dang","Jiangchuan Liu","Jingao Xu","Kebin Liu","Xinlei Chen","Yunhao Liu"],"pdf_url":"https://arxiv.org/pdf/2408.06152v2.pdf","comment":null}]},"2024-08-30T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.00164v1","updated":"2024-08-30T16:54:06Z","published":"2024-08-30T16:54:06Z","title":"Facilitating phenotyping from clinical texts: the medkit library","summary":"  Phenotyping consists in applying algorithms to identify individuals\nassociated with a specific, potentially complex, trait or condition, typically\nout of a collection of Electronic Health Records (EHRs). Because a lot of the\nclinical information of EHRs are lying in texts, phenotyping from text takes an\nimportant role in studies that rely on the secondary use of EHRs. However, the\nheterogeneity and highly specialized aspect of both the content and form of\nclinical texts makes this task particularly tedious, and is the source of time\nand cost constraints in observational studies. To facilitate the development,\nevaluation and reproductibility of phenotyping pipelines, we developed an\nopen-source Python library named medkit. It enables composing data processing\npipelines made of easy-to-reuse software bricks, named medkit operations. In\naddition to the core of the library, we share the operations and pipelines we\nalready developed and invite the phenotyping community for their reuse and\nenrichment. medkit is available at https://github.com/medkit-lib/medkit\n","authors":["Antoine Neuraz","Ghislain Vaillant","Camila Arias","Olivier Birot","Kim-Tam Huynh","Thibaut Fabacher","Alice Rogier","Nicolas Garcelon","Ivan Lerner","Bastien Rance","Adrien Coulet"],"pdf_url":"https://arxiv.org/pdf/2409.00164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00458v2","updated":"2024-08-30T15:59:46Z","published":"2024-03-30T19:45:04Z","title":"Beyond One-Size-Fits-All: Multi-Domain, Multi-Task Framework for\n  Embedding Model Selection","summary":"  This position paper proposes a systematic approach towards developing a\nframework to help select the most effective embedding models for natural\nlanguage processing (NLP) tasks, addressing the challenge posed by the\nproliferation of both proprietary and open-source encoder models.\n","authors":["Vivek Khetan"],"pdf_url":"https://arxiv.org/pdf/2404.00458v2.pdf","comment":"It was an initial idea - we plan to work on a detailed version"},{"id":"http://arxiv.org/abs/2408.17332v1","updated":"2024-08-30T14:48:52Z","published":"2024-08-30T14:48:52Z","title":"Not All Videos Become Outdated: Short-Video Recommendation by Learning\n  to Deconfound Release Interval Bias","summary":"  Short-video recommender systems often exhibit a biased preference to recently\nreleased videos. However, not all videos become outdated; certain classic\nvideos can still attract user's attention. Such bias along temporal dimension\ncan be further aggravated by the matching model between users and videos,\nbecause the model learns from preexisting interactions. From real data, we\nobserve that different videos have varying sensitivities to recency in\nattracting users' attention. Our analysis, based on a causal graph modeling\nshort-video recommendation, suggests that the release interval serves as a\nconfounder, establishing a backdoor path between users and videos. To address\nthis confounding effect, we propose a model-agnostic causal architecture called\nLearning to Deconfound the Release Interval Bias (LDRI). LDRI enables jointly\nlearning of the matching model and the video recency sensitivity perceptron. In\nthe inference stage, we apply a backdoor adjustment, effectively blocking the\nbackdoor path by intervening on each video. Extensive experiments on two\nbenchmarks demonstrate that LDRI consistently outperforms backbone models and\nexhibits superior performance against state-of-the-art models. Additional\ncomprehensive analyses confirm the deconfounding capability of LDRI.\n","authors":["Lulu Dong","Guoxiu He","Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2408.17332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17309v1","updated":"2024-08-30T14:12:31Z","published":"2024-08-30T14:12:31Z","title":"Metadata practices for simulation workflows","summary":"  Computer simulations are an essential pillar of knowledge generation in\nscience. Understanding, reproducing, and exploring the results of simulations\nrelies on tracking and organizing metadata describing numerical experiments.\nHowever, the models used to understand real-world systems, and the\ncomputational machinery required to simulate them, are typically complex, and\nproduce large amounts of heterogeneous metadata. Here, we present general\npractices for acquiring and handling metadata that are agnostic to software and\nhardware, and highly flexible for the user. These consist of two steps: 1)\nrecording and storing raw metadata, and 2) selecting and structuring metadata.\nAs a proof of concept, we develop the Archivist, a Python tool to help with the\nsecond step, and use it to apply our practices to distinct high-performance\ncomputing use cases from neuroscience and hydrology. Our practices and the\nArchivist can readily be applied to existing workflows without the need for\nsubstantial restructuring. They support sustainable numerical workflows,\nfacilitating reproducibility and data reuse in generic simulation-based\nresearch.\n","authors":["Jose Villamar","Matthias Kelbling","Heather L. More","Michael Denker","Tom Tetzlaff","Johanna Senk","Stephan Thober"],"pdf_url":"https://arxiv.org/pdf/2408.17309v1.pdf","comment":"19 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.16312v2","updated":"2024-08-30T11:48:40Z","published":"2024-08-29T07:20:56Z","title":"SynDL: A Large-Scale Synthetic Test Collection for Passage Retrieval","summary":"  Large-scale test collections play a crucial role in Information Retrieval\n(IR) research. However, according to the Cranfield paradigm and the research\ninto publicly available datasets, the existing information retrieval research\nstudies are commonly developed on small-scale datasets that rely on human\nassessors for relevance judgments - a time-intensive and expensive process.\nRecent studies have shown the strong capability of Large Language Models (LLMs)\nin producing reliable relevance judgments with human accuracy but at a greatly\nreduced cost. In this paper, to address the missing large-scale ad-hoc document\nretrieval dataset, we extend the TREC Deep Learning Track (DL) test collection\nvia additional language model synthetic labels to enable researchers to test\nand evaluate their search systems at a large scale. Specifically, such a test\ncollection includes more than 1,900 test queries from the previous years of\ntracks. We compare system evaluation with past human labels from past years and\nfind that our synthetically created large-scale test collection can lead to\nhighly correlated system rankings.\n","authors":["Hossein A. Rahmani","Xi Wang","Emine Yilmaz","Nick Craswell","Bhaskar Mitra","Paul Thomas"],"pdf_url":"https://arxiv.org/pdf/2408.16312v2.pdf","comment":"9 pages, resource paper"},{"id":"http://arxiv.org/abs/2408.17214v1","updated":"2024-08-30T11:38:51Z","published":"2024-08-30T11:38:51Z","title":"Efficient Multi-task Prompt Tuning for Recommendation","summary":"  With the expansion of business scenarios, real recommender systems are facing\nchallenges in dealing with the constantly emerging new tasks in multi-task\nlearning frameworks. In this paper, we attempt to improve the generalization\nability of multi-task recommendations when dealing with new tasks. We find that\njoint training will enhance the performance of the new task but always\nnegatively impact existing tasks in most multi-task learning methods. Besides,\nsuch a re-training mechanism with new tasks increases the training costs,\nlimiting the generalization ability of multi-task recommendation models. Based\non this consideration, we aim to design a suitable sharing mechanism among\ndifferent tasks while maintaining joint optimization efficiency in new task\nlearning. A novel two-stage prompt-tuning MTL framework (MPT-Rec) is proposed\nto address task irrelevance and training efficiency problems in multi-task\nrecommender systems. Specifically, we disentangle the task-specific and\ntask-sharing information in the multi-task pre-training stage, then use\ntask-aware prompts to transfer knowledge from other tasks to the new task\neffectively. By freezing parameters in the pre-training tasks, MPT-Rec solves\nthe negative impacts that may be brought by the new task and greatly reduces\nthe training costs. Extensive experiments on three real-world datasets show the\neffectiveness of our proposed multi-task learning framework. MPT-Rec achieves\nthe best performance compared to the SOTA multi-task learning method. Besides,\nit maintains comparable model performance but vastly improves the training\nefficiency (i.e., with up to 10% parameters in the full training way) in the\nnew task learning.\n","authors":["Ting Bai","Le Huang","Yue Yu","Cheng Yang","Cheng Hou","Zhe Zhao","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2408.17214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17180v1","updated":"2024-08-30T10:28:36Z","published":"2024-08-30T10:28:36Z","title":"Identifying and Clustering Counter Relationships of Team Compositions in\n  PvP Games for Efficient Balance Analysis","summary":"  How can balance be quantified in game settings? This question is crucial for\ngame designers, especially in player-versus-player (PvP) games, where analyzing\nthe strength relations among predefined team compositions-such as hero\ncombinations in multiplayer online battle arena (MOBA) games or decks in card\ngames-is essential for enhancing gameplay and achieving balance. We have\ndeveloped two advanced measures that extend beyond the simplistic win rate to\nquantify balance in zero-sum competitive scenarios. These measures are derived\nfrom win value estimations, which employ strength rating approximations via the\nBradley-Terry model and counter relationship approximations via vector\nquantization, significantly reducing the computational complexity associated\nwith traditional win value estimations. Throughout the learning process of\nthese models, we identify useful categories of compositions and pinpoint their\ncounter relationships, aligning with the experiences of human players without\nrequiring specific game knowledge. Our methodology hinges on a simple technique\nto enhance codebook utilization in discrete representation with a deterministic\nvector quantization process for an extremely small state space. Our framework\nhas been validated in popular online games, including Age of Empires II,\nHearthstone, Brawl Stars, and League of Legends. The accuracy of the observed\nstrength relations in these games is comparable to traditional pairwise win\nvalue predictions, while also offering a more manageable complexity for\nanalysis. Ultimately, our findings contribute to a deeper understanding of PvP\ngame dynamics and present a methodology that significantly improves game\nbalance evaluation and design.\n","authors":["Chiu-Chou Lin","Yu-Wei Shih","Kuei-Ting Kuo","Yu-Cheng Chen","Chien-Hua Chen","Wei-Chen Chiu","I-Chen Wu"],"pdf_url":"https://arxiv.org/pdf/2408.17180v1.pdf","comment":"TMLR 09/2024 https://openreview.net/forum?id=2D36otXvBE"},{"id":"http://arxiv.org/abs/2408.17103v1","updated":"2024-08-30T08:40:59Z","published":"2024-08-30T08:40:59Z","title":"Understanding the User: An Intent-Based Ranking Dataset","summary":"  As information retrieval systems continue to evolve, accurate evaluation and\nbenchmarking of these systems become pivotal. Web search datasets, such as MS\nMARCO, primarily provide short keyword queries without accompanying intent or\ndescriptions, posing a challenge in comprehending the underlying information\nneed. This paper proposes an approach to augmenting such datasets to annotate\ninformative query descriptions, with a focus on two prominent benchmark\ndatasets: TREC-DL-21 and TREC-DL-22. Our methodology involves utilizing\nstate-of-the-art LLMs to analyze and comprehend the implicit intent within\nindividual queries from benchmark datasets. By extracting key semantic\nelements, we construct detailed and contextually rich descriptions for these\nqueries. To validate the generated query descriptions, we employ crowdsourcing\nas a reliable means of obtaining diverse human perspectives on the accuracy and\ninformativeness of the descriptions. This information can be used as an\nevaluation set for tasks such as ranking, query rewriting, or others.\n","authors":["Abhijit Anand","Jurek Leonhardt","V Venktesh","Avishek Anand"],"pdf_url":"https://arxiv.org/pdf/2408.17103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12492v2","updated":"2024-08-30T07:58:46Z","published":"2024-08-22T15:33:46Z","title":"The Importance of Cognitive Biases in the Recommendation Ecosystem","summary":"  Cognitive biases have been studied in psychology, sociology, and behavioral\neconomics for decades. Traditionally, they have been considered a negative\nhuman trait that leads to inferior decision-making, reinforcement of\nstereotypes, or can be exploited to manipulate consumers, respectively. We\nargue that cognitive biases also manifest in different parts of the\nrecommendation ecosystem and at different stages of the recommendation process.\nMore importantly, we contest this traditional detrimental perspective on\ncognitive biases and claim that certain cognitive biases can be beneficial when\naccounted for by recommender systems. Concretely, we provide empirical evidence\nthat biases such as feature-positive effect, Ikea effect, and cultural\nhomophily can be observed in various components of the recommendation pipeline,\nincluding input data (such as ratings or side information), recommendation\nalgorithm or model (and consequently recommended items), and user interactions\nwith the system. In three small experiments covering recruitment and\nentertainment domains, we study the pervasiveness of the aforementioned biases.\nWe ultimately advocate for a prejudice-free consideration of cognitive biases\nto improve user and item models as well as recommendation algorithms.\n","authors":["Markus Schedl","Oleg Lesota","Stefan Brandl","Mohammad Lotfi","Gustavo Junior Escobedo Ticona","Shahed Masoudian"],"pdf_url":"https://arxiv.org/pdf/2408.12492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.17008v1","updated":"2024-08-30T04:40:35Z","published":"2024-08-30T04:40:35Z","title":"Evaluation of Table Representations to Answer Questions from Tables in\n  Documents : A Case Study using 3GPP Specifications","summary":"  With the ubiquitous use of document corpora for question answering, one\nimportant aspect which is especially relevant for technical documents is the\nability to extract information from tables which are interspersed with text.\nThe major challenge in this is that unlike free-flow text or isolated set of\ntables, the representation of a table in terms of what is a relevant chunk is\nnot obvious. We conduct a series of experiments examining various\nrepresentations of tabular data interspersed with text to understand the\nrelative benefits of different representations. We choose a corpus of $3^{rd}$\nGeneration Partnership Project (3GPP) documents since they are heavily\ninterspersed with tables. We create expert curated dataset of question answers\nto evaluate our approach. We conclude that row level representations with\ncorresponding table header information being included in every cell improves\nthe performance of the retrieval, thus leveraging the structural information\npresent in the tabular data.\n","authors":["Sujoy Roychowdhury","Sumit Soman","HG Ranjani","Avantika Sharma","Neeraj Gunda","Sai Krishna Bala"],"pdf_url":"https://arxiv.org/pdf/2408.17008v1.pdf","comment":"10 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.06051v2","updated":"2024-08-30T03:19:26Z","published":"2024-08-12T10:55:42Z","title":"Perceptual Similarity for Measuring Decision-Making Style and Policy\n  Diversity in Games","summary":"  Defining and measuring decision-making styles, also known as playstyles, is\ncrucial in gaming, where these styles reflect a broad spectrum of individuality\nand diversity. However, finding a universally applicable measure for these\nstyles poses a challenge. Building on Playstyle Distance, the first\nunsupervised metric to measure playstyle similarity based on game screens and\nraw actions, we introduce three enhancements to increase accuracy: multiscale\nanalysis with varied state granularity, a perceptual kernel rooted in\npsychology, and the utilization of the intersection-over-union method for\nefficient evaluation. These innovations not only advance measurement precision\nbut also offer insights into human cognition of similarity. Across two racing\ngames and seven Atari games, our techniques significantly improve the precision\nof zero-shot playstyle classification, achieving an accuracy exceeding 90\npercent with fewer than 512 observation-action pairs, which is less than half\nan episode of these games. Furthermore, our experiments with 2048 and Go\ndemonstrate the potential of discrete playstyle measures in puzzle and board\ngames. We also develop an algorithm for assessing decision-making diversity\nusing these measures. Our findings improve the measurement of end-to-end game\nanalysis and the evolution of artificial intelligence for diverse playstyles.\n","authors":["Chiu-Chou Lin","Wei-Chen Chiu","I-Chen Wu"],"pdf_url":"https://arxiv.org/pdf/2408.06051v2.pdf","comment":"TMLR 08/2024 https://openreview.net/forum?id=30C9AWBW49"}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.16990v1","updated":"2024-08-30T03:36:22Z","published":"2024-08-30T03:36:22Z","title":"Video to Music Moment Retrieval","summary":"  Adding proper background music helps complete a short video to be shared.\nTowards automating the task, previous research focuses on video-to-music\nretrieval (VMR), aiming to find amidst a collection of music the one best\nmatching the content of a given video. Since music tracks are typically much\nlonger than short videos, meaning the returned music has to be cut to a shorter\nmoment, there is a clear gap between the practical need and VMR. In order to\nbridge the gap, we propose in this paper video to music moment retrieval (VMMR)\nas a new task. To tackle the new task, we build a comprehensive dataset\nAd-Moment which contains 50K short videos annotated with music moments and\ndevelop a two-stage approach. In particular, given a test video, the most\nsimilar music is retrieved from a given collection. Then, a Transformer based\nmusic moment localization is performed. We term this approach Retrieval and\nLocalization (ReaL). Extensive experiments on real-world datasets verify the\neffectiveness of the proposed method for VMMR.\n","authors":["Zijie Xin","Minquan Wang","Ye Ma","Bo Wang","Quan Chen","Peng Jiang","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2408.16990v1.pdf","comment":null}]},"2024-08-29T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2404.05893v4","updated":"2024-08-29T21:34:22Z","published":"2024-04-08T22:29:53Z","title":"Use of a Structured Knowledge Base Enhances Metadata Curation by Large\n  Language Models","summary":"  Metadata play a crucial role in ensuring the findability, accessibility,\ninteroperability, and reusability of datasets. This paper investigates the\npotential of large language models (LLMs), specifically GPT-4, to improve\nadherence to metadata standards. We conducted experiments on 200 random data\nrecords describing human samples relating to lung cancer from the NCBI\nBioSample repository, evaluating GPT-4's ability to suggest edits for adherence\nto metadata standards. We computed the adherence accuracy of field name-field\nvalue pairs through a peer review process, and we observed a marginal average\nimprovement in adherence to the standard data dictionary from 79% to 80%\n(p<0.5). We then prompted GPT-4 with domain information in the form of the\ntextual descriptions of CEDAR templates and recorded a significant improvement\nto 97% from 79% (p<0.01). These results indicate that, while LLMs may not be\nable to correct legacy metadata to ensure satisfactory adherence to standards\nwhen unaided, they do show promise for use in automated metadata curation when\nintegrated with a structured knowledge base\n","authors":["Sowmya S. Sundaram","Benjamin Solomon","Avani Khatri","Anisha Laumas","Purvesh Khatri","Mark A. Musen"],"pdf_url":"https://arxiv.org/pdf/2404.05893v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16885v1","updated":"2024-08-29T20:18:00Z","published":"2024-08-29T20:18:00Z","title":"A Prototype Model of Zero-Trust Architecture Blockchain with\n  EigenTrust-Based Practical Byzantine Fault Tolerance Protocol to Manage\n  Decentralized Clinical Trials","summary":"  The COVID-19 pandemic necessitated the emergence of decentralized Clinical\nTrials (DCTs) due to patient retention, accelerate trials, improve data\naccessibility, enable virtual care, and facilitate seamless communication\nthrough integrated systems. However, integrating systems in DCTs exposes\nclinical data to potential security threats, making them susceptible to theft\nat any stage, a high risk of protocol deviations, and monitoring issues. To\nmitigate these challenges, blockchain technology serves as a secure framework,\nacting as a decentralized ledger, creating an immutable environment by\nestablishing a zero-trust architecture, where data are deemed untrusted until\nverified. In combination with Internet of Things (IoT)-enabled wearable\ndevices, blockchain secures the transfer of clinical trial data on private\nblockchains during DCT automation and operations. This paper proposes a\nprototype model of the Zero-Trust Architecture Blockchain (z-TAB) to integrate\npatient-generated clinical trial data during DCT operation management. The\nEigenTrust-based Practical Byzantine Fault Tolerance (T-PBFT) algorithm has\nbeen incorporated as a consensus protocol, leveraging Hyperledger Fabric.\nFurthermore, the Internet of Things (IoT) has been integrated to streamline\ndata processing among stakeholders within the blockchain platforms. Rigorous\nevaluation has been done to evaluate the quality of the system.\n","authors":["Ashok Kumar Peepliwall","Hari Mohan Pandey","Surya Prakash","Anand A Mahajan","Sudhinder Singh Chowhan","Vinesh Kumar","Rahul Sharma"],"pdf_url":"https://arxiv.org/pdf/2408.16885v1.pdf","comment":"NA"},{"id":"http://arxiv.org/abs/2408.16877v1","updated":"2024-08-29T19:58:46Z","published":"2024-08-29T19:58:46Z","title":"Longitudinal Modularity, a Modularity for Link Streams","summary":"  Temporal networks are commonly used to model real-life phenomena. When these\nphenomena represent interactions and are captured at a fine-grained temporal\nresolution, they are modeled as link streams. Community detection is an\nessential network analysis task. Although many methods exist for static\nnetworks, and some methods have been developed for temporal networks\nrepresented as sequences of snapshots, few works can handle link streams. This\narticle introduces the first adaptation of the well-known Modularity quality\nfunction to link streams. Unlike existing methods, it is independent of the\ntime scale of analysis. After introducing the quality function, and its\nrelation to existing static and dynamic definitions of Modularity, we show\nexperimentally its relevance for dynamic community evaluation.\n","authors":["Victor Brabant","Yasaman Asgari","Pierre Borgnat","Angela Bonifati","Remy Cazabet"],"pdf_url":"https://arxiv.org/pdf/2408.16877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09046v1","updated":"2024-08-29T16:11:20Z","published":"2024-08-29T16:11:20Z","title":"HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation\n  System for AI Legal and Policy Applications","summary":"  While Large Language Models (LLMs) excel in text generation and\nquestion-answering, their effectiveness in AI legal and policy is limited by\noutdated knowledge, hallucinations, and inadequate reasoning in complex\ncontexts. Retrieval-Augmented Generation (RAG) systems improve response\naccuracy by integrating external knowledge but struggle with retrieval errors,\npoor context integration, and high costs, particularly in interpreting\nqualitative and quantitative AI legal texts. This paper introduces a Hybrid\nParameter-Adaptive RAG (HyPA-RAG) system tailored for AI legal and policy,\nexemplified by NYC Local Law 144 (LL144). HyPA-RAG uses a query complexity\nclassifier for adaptive parameter tuning, a hybrid retrieval strategy combining\ndense, sparse, and knowledge graph methods, and an evaluation framework with\nspecific question types and metrics. By dynamically adjusting parameters,\nHyPA-RAG significantly improves retrieval accuracy and response fidelity.\nTesting on LL144 shows enhanced correctness, faithfulness, and contextual\nprecision, addressing the need for adaptable NLP systems in complex,\nhigh-stakes AI legal and policy applications.\n","authors":["Rishi Kalra","Zekun Wu","Ayesha Gulley","Airlie Hilliard","Xin Guan","Adriano Koshiyama","Philip Treleaven"],"pdf_url":"https://arxiv.org/pdf/2409.09046v1.pdf","comment":"Under review for the EMNLP 2024 Workshop on Customizable NLP:\n  Progress and Challenges in Customizing NLP for a Domain, Application, Group,\n  or Individual"},{"id":"http://arxiv.org/abs/2408.14698v2","updated":"2024-08-29T15:14:48Z","published":"2024-08-26T23:52:27Z","title":"Smart Multi-Modal Search: Contextual Sparse and Dense Embedding\n  Integration in Adobe Express","summary":"  As user content and queries become increasingly multi-modal, the need for\neffective multi-modal search systems has grown. Traditional search systems\noften rely on textual and metadata annotations for indexed images, while\nmulti-modal embeddings like CLIP enable direct search using text and image\nembeddings. However, embedding-based approaches face challenges in integrating\ncontextual features such as user locale and recency. Building a scalable\nmulti-modal search system requires fine-tuning several components. This paper\npresents a multi-modal search architecture and a series of AB tests that\noptimize embeddings and multi-modal technologies in Adobe Express template\nsearch. We address considerations such as embedding model selection, the roles\nof embeddings in matching and ranking, and the balance between dense and sparse\nembeddings. Our iterative approach demonstrates how utilizing sparse, dense,\nand contextual features enhances short and long query search, significantly\nreduces null rates (over 70\\%), and increases click-through rates (CTR). Our\nfindings provide insights into developing robust multi-modal search systems,\nthereby enhancing relevance for complex queries.\n","authors":["Cherag Aroraa","Tracy Holloway King","Jayant Kumar","Yi Lu","Sanat Sharma","Arvind Srikantan","David Uvalle","Josep Valls-Vargas","Harsha Vardhan"],"pdf_url":"https://arxiv.org/pdf/2408.14698v2.pdf","comment":"CIKM 2024 (International Conference on Information and Knowledge\n  Management), Multimodal Search and Recommendations Workshop"},{"id":"http://arxiv.org/abs/2408.16578v1","updated":"2024-08-29T14:44:12Z","published":"2024-08-29T14:44:12Z","title":"Transformers Meet ACT-R: Repeat-Aware and Sequential Listening Session\n  Recommendation","summary":"  Music streaming services often leverage sequential recommender systems to\npredict the best music to showcase to users based on past sequences of\nlistening sessions. Nonetheless, most sequential recommendation methods ignore\nor insufficiently account for repetitive behaviors. This is a crucial\nlimitation for music recommendation, as repeatedly listening to the same song\nover time is a common phenomenon that can even change the way users perceive\nthis song. In this paper, we introduce PISA (Psychology-Informed Session\nembedding using ACT-R), a session-level sequential recommender system that\novercomes this limitation. PISA employs a Transformer architecture learning\nembedding representations of listening sessions and users using attention\nmechanisms inspired by Anderson's ACT-R (Adaptive Control of Thought-Rational),\na cognitive architecture modeling human information access and memory dynamics.\nThis approach enables us to capture dynamic and repetitive patterns from user\nbehaviors, allowing us to effectively predict the songs they will listen to in\nsubsequent sessions, whether they are repeated or new ones. We demonstrate the\nempirical relevance of PISA using both publicly available listening data from\nLast.fm and proprietary data from Deezer, a global music streaming service,\nconfirming the critical importance of repetition modeling for sequential\nlistening session recommendation. Along with this paper, we publicly release\nour proprietary dataset to foster future research in this field, as well as the\nsource code of PISA to facilitate its future use.\n","authors":["Viet-Anh Tran","Guillaume Salha-Galvan","Bruno Sguerra","Romain Hennequin"],"pdf_url":"https://arxiv.org/pdf/2408.16578v1.pdf","comment":"11 pages. Accepted by RecSys'2024, full paper"},{"id":"http://arxiv.org/abs/2408.16446v1","updated":"2024-08-29T11:19:57Z","published":"2024-08-29T11:19:57Z","title":"Is text normalization relevant for classifying medieval charters?","summary":"  This study examines the impact of historical text normalization on the\nclassification of medieval charters, specifically focusing on document dating\nand locating. Using a data set of Middle High German charters from a digital\narchive, we evaluate various classifiers, including traditional and\ntransformer-based models, with and without normalization. Our results indicate\nthat the given normalization minimally improves locating tasks but reduces\naccuracy for dating, implying that original texts contain crucial features that\nnormalization may obscure. We find that support vector machines and gradient\nboosting outperform other models, questioning the efficiency of transformers\nfor this use case. Results suggest a selective approach to historical text\nnormalization, emphasizing the significance of preserving some textual\ncharacteristics that are critical for classification tasks in document\nanalysis.\n","authors":["Florian Atzenhofer-Baumgartner","Tam√°s Kov√°cs"],"pdf_url":"https://arxiv.org/pdf/2408.16446v1.pdf","comment":"This preprint has not undergone peer review or any post-submission\n  improvements or corrections"},{"id":"http://arxiv.org/abs/2408.16430v1","updated":"2024-08-29T10:44:59Z","published":"2024-08-29T10:44:59Z","title":"Do Recommender Systems Promote Local Music? A Reproducibility Study\n  Using Music Streaming Data","summary":"  This paper examines the influence of recommender systems on local music\nrepresentation, discussing prior findings from an empirical study on the LFM-2b\npublic dataset. This prior study argued that different recommender systems\nexhibit algorithmic biases shifting music consumption either towards or against\nlocal content. However, LFM-2b users do not reflect the diverse audience of\nmusic streaming services. To assess the robustness of this study's conclusions,\nwe conduct a comparative analysis using proprietary listening data from a\nglobal music streaming service, which we publicly release alongside this paper.\nWe observe significant differences in local music consumption patterns between\nour dataset and LFM-2b, suggesting that caution should be exercised when\ndrawing conclusions on local music based solely on LFM-2b. Moreover, we show\nthat the algorithmic biases exhibited in the original work vary in our dataset,\nand that several unexplored model parameters can significantly influence these\nbiases and affect the study's conclusion on both datasets. Finally, we discuss\nthe complexity of accurately labeling local music, emphasizing the risk of\nmisleading conclusions due to unreliable, biased, or incomplete labels. To\nencourage further research and ensure reproducibility, we have publicly shared\nour dataset and code.\n","authors":["Kristina Matrosova","Lilian Marey","Guillaume Salha-Galvan","Thomas Louail","Olivier Bodini","Manuel Moussallam"],"pdf_url":"https://arxiv.org/pdf/2408.16430v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16296v1","updated":"2024-08-29T06:54:03Z","published":"2024-08-29T06:54:03Z","title":"Rethinking Sparse Lexical Representations for Image Retrieval in the Age\n  of Rising Multi-Modal Large Language Models","summary":"  In this paper, we rethink sparse lexical representations for image retrieval.\nBy utilizing multi-modal large language models (M-LLMs) that support visual\nprompting, we can extract image features and convert them into textual data,\nenabling us to utilize efficient sparse retrieval algorithms employed in\nnatural language processing for image retrieval tasks. To assist the LLM in\nextracting image features, we apply data augmentation techniques for key\nexpansion and analyze the impact with a metric for relevance between images and\ntextual data. We empirically show the superior precision and recall performance\nof our image retrieval method compared to conventional vision-language\nmodel-based methods on the MS-COCO, PASCAL VOC, and NUS-WIDE datasets in a\nkeyword-based image retrieval scenario, where keywords serve as search queries.\nWe also demonstrate that the retrieval performance can be improved by\niteratively incorporating keywords into search queries.\n","authors":["Kengo Nakata","Daisuke Miyashita","Youyang Ng","Yasuto Hoshi","Jun Deguchi"],"pdf_url":"https://arxiv.org/pdf/2408.16296v1.pdf","comment":"Accepted to ECCV 2024 Workshops: 2nd Workshop on Traditional Computer\n  Vision in the Age of Deep Learning (TradiCV)"},{"id":"http://arxiv.org/abs/2408.16238v1","updated":"2024-08-29T03:34:39Z","published":"2024-08-29T03:34:39Z","title":"Efficient Transfer Learning Framework for Cross-Domain Click-Through\n  Rate Prediction","summary":"  Natural content and advertisement coexist in industrial recommendation\nsystems but differ in data distribution. Concretely, traffic related to the\nadvertisement is considerably sparser compared to that of natural content,\nwhich motivates the development of transferring knowledge from the richer\nsource natural content domain to the sparser advertising domain. The challenges\ninclude the inefficiencies arising from the management of extensive source data\nand the problem of 'catastrophic forgetting' that results from the CTR model's\ndaily updating. To this end, we propose a novel tri-level asynchronous\nframework, i.e., Efficient Transfer Learning Framework for Cross-Domain\nClick-Through Rate Prediction (E-CDCTR), to transfer comprehensive knowledge of\nnatural content to advertisement CTR models. This framework consists of three\nkey components: Tiny Pre-training Model ((TPM), which trains a tiny CTR model\nwith several basic features on long-term natural data; Complete Pre-training\nModel (CPM), which trains a CTR model holding network structure and input\nfeatures the same as target advertisement on short-term natural data;\nAdvertisement CTR model (A-CTR), which derives its parameter initialization\nfrom CPM together with multiple historical embeddings from TPM as extra feature\nand then fine-tunes on advertisement data. TPM provides richer representations\nof user and item for both the CPM and A-CTR, effectively alleviating the\nforgetting problem inherent in the daily updates. CPM further enhances the\nadvertisement model by providing knowledgeable initialization, thereby\nalleviating the data sparsity challenges typically encountered by advertising\nCTR models. Such a tri-level cross-domain transfer learning framework offers an\nefficient solution to address both data sparsity and `catastrophic forgetting',\nyielding remarkable improvements.\n","authors":["Qi Liu","Xingyuan Tang","Jianqiang Huang","Xiangqian Yu","Haoran Jin","Jin Chen","Yuanhao Pu","Defu Lian","Tan Qu","Zhe Wang","Jia Cheng","Jun Lei"],"pdf_url":"https://arxiv.org/pdf/2408.16238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21191v2","updated":"2024-08-29T02:27:19Z","published":"2024-07-30T20:58:36Z","title":"GenRec: Generative Sequential Recommendation with Large Language Models","summary":"  Sequential recommendation is a task to capture hidden user preferences from\nhistorical user item interaction data and recommend next items for the user.\nSignificant progress has been made in this domain by leveraging classification\nbased learning methods. Inspired by the recent paradigm of 'pretrain, prompt\nand predict' in NLP, we consider sequential recommendation as a sequence to\nsequence generation task and propose a novel model named Generative\nRecommendation (GenRec). Unlike classification based models that learn explicit\nuser and item representations, GenRec utilizes the sequence modeling capability\nof Transformer and adopts the masked item prediction objective to effectively\nlearn the hidden bidirectional sequential patterns. Different from existing\ngenerative sequential recommendation models, GenRec does not rely on manually\ndesigned hard prompts. The input to GenRec is textual user item sequence and\nthe output is top ranked next items. Moreover, GenRec is lightweight and\nrequires only a few hours to train effectively in low-resource settings, making\nit highly applicable to real-world scenarios and helping to democratize large\nlanguage models in the sequential recommendation domain. Our extensive\nexperiments have demonstrated that GenRec generalizes on various public\nreal-world datasets and achieves state-of-the-art results. Our experiments also\nvalidate the effectiveness of the the proposed masked item prediction objective\nthat improves the model performance by a large margin.\n","authors":["Panfeng Cao","Pietro Lio"],"pdf_url":"https://arxiv.org/pdf/2407.21191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15793v2","updated":"2024-08-29T00:32:15Z","published":"2023-07-28T20:25:11Z","title":"Summaries, Highlights, and Action items: Design, implementation and\n  evaluation of an LLM-powered meeting recap system","summary":"  Meetings play a critical infrastructural role in the coordination of work. In\nrecent years, due to shift to hybrid and remote work, more meetings are moving\nto online Computer Mediated Spaces. This has led to new problems (e.g. more\ntime spent in less engaging meetings) and new opportunities (e.g. automated\ntranscription/captioning and recap support). Recent advances in large language\nmodels (LLMs) for dialog summarization have the potential to improve the\nexperience of meetings by reducing individuals' meeting load and increasing the\nclarity and alignment of meeting outputs. Despite this potential, they face\ntechnological limitation due to long transcripts and inability to capture\ndiverse recap needs based on user's context. To address these gaps, we design,\nimplement and evaluate in-context a meeting recap system. We first\nconceptualize two salient recap representations -- important highlights, and a\nstructured, hierarchical minutes view. We develop a system to operationalize\nthe representations with dialogue summarization as its building blocks.\nFinally, we evaluate the effectiveness of the system with seven users in the\ncontext of their work meetings. Our findings show promise in using LLM-based\ndialogue summarization for meeting recap and the need for both representations\nin different contexts. However, we find that LLM-based recap still lacks an\nunderstanding of whats personally relevant to participants, can miss important\ndetails, and mis-attributions can be detrimental to group dynamics. We identify\ncollaboration opportunities such as a shared recap document that a high quality\nrecap enables. We report on implications for designing AI systems to partner\nwith users to learn and improve from natural interactions to overcome the\nlimitations related to personal relevance and summarization quality.\n","authors":["Sumit Asthana","Sagih Hilleli","Pengcheng He","Aaron Halfaker"],"pdf_url":"https://arxiv.org/pdf/2307.15793v2.pdf","comment":"in review for CSCW 24"}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.16809v1","updated":"2024-08-29T17:59:57Z","published":"2024-08-29T17:59:57Z","title":"See or Guess: Counterfactually Regularized Image Captioning","summary":"  Image captioning, which generates natural language descriptions of the visual\ninformation in an image, is a crucial task in vision-language research.\nPrevious models have typically addressed this task by aligning the generative\ncapabilities of machines with human intelligence through statistical fitting of\nexisting datasets. While effective for normal images, they may struggle to\naccurately describe those where certain parts of the image are obscured or\nedited, unlike humans who excel in such cases. These weaknesses they exhibit,\nincluding hallucinations and limited interpretability, often hinder performance\nin scenarios with shifted association patterns. In this paper, we present a\ngeneric image captioning framework that employs causal inference to make\nexisting models more capable of interventional tasks, and counterfactually\nexplainable. Our approach includes two variants leveraging either total effect\nor natural direct effect. Integrating them into the training process enables\nmodels to handle counterfactual scenarios, increasing their generalizability.\nExtensive experiments on various datasets show that our method effectively\nreduces hallucinations and improves the model's faithfulness to images,\ndemonstrating high portability across both small-scale and large-scale\nimage-to-text models. The code is available at\nhttps://github.com/Aman-4-Real/See-or-Guess.\n","authors":["Qian Cao","Xu Chen","Ruihua Song","Xiting Wang","Xinting Huang","Yuchen Ren"],"pdf_url":"https://arxiv.org/pdf/2408.16809v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.16625v1","updated":"2024-08-29T15:34:25Z","published":"2024-08-29T15:34:25Z","title":"MultiMediate'24: Multi-Domain Engagement Estimation","summary":"  Estimating the momentary level of participant's engagement is an important\nprerequisite for assistive systems that support human interactions. Previous\nwork has addressed this task in within-domain evaluation scenarios, i.e.\ntraining and testing on the same dataset. This is in contrast to real-life\nscenarios where domain shifts between training and testing data frequently\noccur. With MultiMediate'24, we present the first challenge addressing\nmulti-domain engagement estimation. As training data, we utilise the NOXI\ndatabase of dyadic novice-expert interactions. In addition to within-domain\ntest data, we add two new test domains. First, we introduce recordings\nfollowing the NOXI protocol but covering languages that are not present in the\nNOXI training data. Second, we collected novel engagement annotations on the\nMPIIGroupInteraction dataset which consists of group discussions between three\nto four people. In this way, MultiMediate'24 evaluates the ability of\napproaches to generalise across factors such as language and cultural\nbackground, group size, task, and screen-mediated vs. face-to-face interaction.\nThis paper describes the MultiMediate'24 challenge and presents baseline\nresults. In addition, we discuss selected challenge solutions.\n","authors":["Philipp M√ºller","Michal Balazia","Tobias Baur","Michael Dietz","Alexander Heimerl","Anna Penzkofer","Dominik Schiller","Fran√ßois Br√©mond","Jan Alexandersson","Elisabeth Andr√©","Andreas Bulling"],"pdf_url":"https://arxiv.org/pdf/2408.16625v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2308.08256"},{"id":"http://arxiv.org/abs/2408.16564v1","updated":"2024-08-29T14:30:56Z","published":"2024-08-29T14:30:56Z","title":"Human-Inspired Audio-Visual Speech Recognition: Spike Activity, Cueing\n  Interaction and Causal Processing","summary":"  Humans naturally perform audiovisual speech recognition (AVSR), enhancing the\naccuracy and robustness by integrating auditory and visual information. Spiking\nneural networks (SNNs), which mimic the brain's information-processing\nmechanisms, are well-suited for emulating the human capability of AVSR. Despite\ntheir potential, research on SNNs for AVSR is scarce, with most existing\naudio-visual multimodal methods focused on object or digit recognition. These\nmodels simply integrate features from both modalities, neglecting their unique\ncharacteristics and interactions. Additionally, they often rely on future\ninformation for current processing, which increases recognition latency and\nlimits real-time applicability. Inspired by human speech perception, this paper\nproposes a novel human-inspired SNN named HI-AVSNN for AVSR, incorporating\nthree key characteristics: cueing interaction, causal processing and spike\nactivity. For cueing interaction, we propose a visual-cued auditory attention\nmodule (VCA2M) that leverages visual cues to guide attention to auditory\nfeatures. We achieve causal processing by aligning the SNN's temporal dimension\nwith that of visual and auditory features and applying temporal masking to\nutilize only past and current information. To implement spike activity, in\naddition to using SNNs, we leverage the event camera to capture lip movement as\nspikes, mimicking the human retina and providing efficient visual data. We\nevaluate HI-AVSNN on an audiovisual speech recognition dataset combining the\nDVS-Lip dataset with its corresponding audio samples. Experimental results\ndemonstrate the superiority of our proposed fusion method, outperforming\nexisting audio-visual SNN fusion methods and achieving a 2.27% improvement in\naccuracy over the only existing SNN-based AVSR method.\n","authors":["Qianhui Liu","Jiadong Wang","Yang Wang","Xin Yang","Gang Pan","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2408.16564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16532v1","updated":"2024-08-29T13:43:36Z","published":"2024-08-29T13:43:36Z","title":"WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio\n  Language Modeling","summary":"  Language models have been effectively applied to modeling natural signals,\nsuch as images, video, speech, and audio. A crucial component of these models\nis the codec tokenizer, which compresses high-dimensional natural signals into\nlower-dimensional discrete tokens. In this paper, we introduce WavTokenizer,\nwhich offers several advantages over previous SOTA acoustic codec models in the\naudio domain: 1)extreme compression. By compressing the layers of quantizers\nand the temporal dimension of the discrete codec, one-second audio of 24kHz\nsampling rate requires only a single quantizer with 40 or 75 tokens. 2)improved\nsubjective quality. Despite the reduced number of tokens, WavTokenizer achieves\nstate-of-the-art reconstruction quality with outstanding UTMOS scores and\ninherently contains richer semantic information. Specifically, we achieve these\nresults by designing a broader VQ space, extended contextual windows, and\nimproved attention networks, as well as introducing a powerful multi-scale\ndiscriminator and an inverse Fourier transform structure. We conducted\nextensive reconstruction experiments in the domains of speech, audio, and\nmusic. WavTokenizer exhibited strong performance across various objective and\nsubjective metrics compared to state-of-the-art models. We also tested semantic\ninformation, VQ utilization, and adaptability to generative models.\nComprehensive ablation studies confirm the necessity of each module in\nWavTokenizer. The related code, demos, and pre-trained models are available at\nhttps://github.com/jishengpeng/WavTokenizer.\n","authors":["Shengpeng Ji","Ziyue Jiang","Xize Cheng","Yifu Chen","Minghui Fang","Jialong Zuo","Qian Yang","Ruiqi Li","Ziang Zhang","Xiaoda Yang","Rongjie Huang","Yidi Jiang","Qian Chen","Siqi Zheng","Wen Wang","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.16532v1.pdf","comment":"Working in progress. arXiv admin note: text overlap with\n  arXiv:2402.12208"}]},"2024-08-28T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2402.07926v2","updated":"2024-08-28T18:53:17Z","published":"2024-02-05T18:16:04Z","title":"From Data Creator to Data Reuser: Distance Matters","summary":"  Sharing research data is necessary, but not sufficient, for data reuse. Open\nscience policies focus more heavily on data sharing than on reuse, yet both are\ncomplex, labor-intensive, expensive, and require infrastructure investments by\nmultiple stakeholders. The value of data reuse lies in relationships between\ncreators and reusers. By addressing knowledge exchange, rather than mere\ntransactions between stakeholders, investments in data management and knowledge\ninfrastructures can be made more wisely. Drawing upon empirical studies of data\nsharing and reuse, we develop the theoretical construct of distance between\ndata creator and data reuser, identifying six distance dimensions that\ninfluence the ability to transfer knowledge effectively: domain, methods,\ncollaboration, curation, purposes, and time and temporality. We address the\nsocial and socio-technical aspects of these dimensions, exploring ways in which\nthey may decrease -- or increase -- distances between creators and reusers. Our\ntheoretical framing of the distance between data creators and prospective\nreusers leads to recommendations to four categories of stakeholders on how to\nmake data sharing and reuse more effective: data creators, data reusers, data\narchivists, and funding agencies. 'It takes a village' to share research data\n-- and a village to reuse data. Our aim is to provoke new research questions,\nnew research, and new investments in effective and efficient circulation of\nresearch data; and to identify criteria for investments at each stage of data\nand research life cycles.\n","authors":["Christine L. Borgman","Paul T. Groth"],"pdf_url":"https://arxiv.org/pdf/2402.07926v2.pdf","comment":"74 pages, double-spaced, consisting of Table of Contents, Abstract,\n  45 page narrative, 1 box, 1 figure, 1 table, 27 pages references. Original\n  work"},{"id":"http://arxiv.org/abs/2408.15953v1","updated":"2024-08-28T17:12:01Z","published":"2024-08-28T17:12:01Z","title":"Modeling and Analyzing the Influence of Non-Item Pages on Sequential\n  Next-Item Prediction","summary":"  Analyzing the sequence of historical interactions between users and items,\nsequential recommendation models learn user intent and make predictions about\nthe next item of interest. Next to these item interactions, most systems also\nhave interactions with pages not related to specific items, for example\nnavigation pages, account pages, and pages for a specific category, which may\nprovide additional insights into the user's interests. However, while there are\nseveral approaches to integrate additional information about items and users,\nthe topic of integrating non-item pages has been less explored. We use the\nhypotheses testing framework HypTrails to show that there is indeed a\nrelationship between these non-item pages and the items of interest and fill\nthis gap by proposing various approaches of representing non-item pages (e.g,\nbased on their content) to use them as an additional information source for the\ntask of sequential next-item prediction.\n  We create a synthetic dataset with non-item pages highly related to the\nsubsequent item to show that the models are generally capable of learning from\nthese interactions, and subsequently evaluate the improvements gained by\nincluding non-item pages in two real-world datasets.\n  We adapt eight popular sequential recommender models, covering CNN-, RNN- and\ntransformer-based architectures, to integrate non-item pages and investigate\nthe capabilities of these models to leverage their information for next item\nprediction. We also analyze their behavior on noisy data and compare different\nitem representation strategies.\n  Our results show that non-item pages are a valuable source of information,\nbut representing such a page well is the key to successfully leverage them. The\ninclusion of non-item pages can increase the performance for next-item\nprediction in all examined model architectures with a varying degree.\n","authors":["Elisabeth Fischer","Daniel Schl√∂r","Albin Zehe","Andreas Hotho"],"pdf_url":"https://arxiv.org/pdf/2408.15953v1.pdf","comment":"36 pages, 19 figures; Work in Progress"},{"id":"http://arxiv.org/abs/2408.16036v1","updated":"2024-08-28T16:16:55Z","published":"2024-08-28T16:16:55Z","title":"Efficient $k$-NN Search in IoT Data: Overlap Optimization in Tree-Based\n  Indexing Structures","summary":"  The proliferation of interconnected devices in the Internet of Things (IoT)\nhas led to an exponential increase in data, commonly known as Big IoT Data.\nEfficient retrieval of this heterogeneous data demands a robust indexing\nmechanism for effective organization. However, a significant challenge remains:\nthe overlap in data space partitions during index construction. This overlap\nincreases node access during search and retrieval, resulting in higher resource\nconsumption, performance bottlenecks, and impedes system scalability. To\naddress this issue, we propose three innovative heuristics designed to quantify\nand strategically reduce data space partition overlap. The volume-based method\n(VBM) offers a detailed assessment by calculating the intersection volume\nbetween partitions, providing deeper insights into spatial relationships. The\ndistance-based method (DBM) enhances efficiency by using the distance between\npartition centers and radii to evaluate overlap, offering a streamlined yet\naccurate approach. Finally, the object-based method (OBM) provides a practical\nsolution by counting objects across multiple partitions, delivering an\nintuitive understanding of data space dynamics. Experimental results\ndemonstrate the effectiveness of these methods in reducing search time,\nunderscoring their potential to improve data space partitioning and enhance\noverall system performance.\n","authors":["Ala-Eddine Benrazek","Zineddine Kouahla","Brahim Farou","Hamid Seridi","Ibtissem Kemouguette"],"pdf_url":"https://arxiv.org/pdf/2408.16036v1.pdf","comment":"28 pages, 21 figures, 1 table"},{"id":"http://arxiv.org/abs/2408.15836v1","updated":"2024-08-28T14:48:37Z","published":"2024-08-28T14:48:37Z","title":"Knowledge Navigator: LLM-guided Browsing Framework for Exploratory\n  Search in Scientific Literature","summary":"  The exponential growth of scientific literature necessitates advanced tools\nfor effective knowledge exploration. We present Knowledge Navigator, a system\ndesigned to enhance exploratory search abilities by organizing and structuring\nthe retrieved documents from broad topical queries into a navigable, two-level\nhierarchy of named and descriptive scientific topics and subtopics. This\nstructured organization provides an overall view of the research themes in a\ndomain, while also enabling iterative search and deeper knowledge discovery\nwithin specific subtopics by allowing users to refine their focus and retrieve\nadditional relevant documents. Knowledge Navigator combines LLM capabilities\nwith cluster-based methods to enable an effective browsing method. We\ndemonstrate our approach's effectiveness through automatic and manual\nevaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code,\nprompts, and benchmarks are made publicly available.\n","authors":["Uri Katz","Mosh Levy","Yoav Goldberg"],"pdf_url":"https://arxiv.org/pdf/2408.15836v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.16132v1","updated":"2024-08-28T20:48:04Z","published":"2024-08-28T20:48:04Z","title":"SVDD 2024: The Inaugural Singing Voice Deepfake Detection Challenge","summary":"  With the advancements in singing voice generation and the growing presence of\nAI singers on media platforms, the inaugural Singing Voice Deepfake Detection\n(SVDD) Challenge aims to advance research in identifying AI-generated singing\nvoices from authentic singers. This challenge features two tracks: a controlled\nsetting track (CtrSVDD) and an in-the-wild scenario track (WildSVDD). The\nCtrSVDD track utilizes publicly available singing vocal data to generate\ndeepfakes using state-of-the-art singing voice synthesis and conversion\nsystems. Meanwhile, the WildSVDD track expands upon the existing SingFake\ndataset, which includes data sourced from popular user-generated content\nwebsites. For the CtrSVDD track, we received submissions from 47 teams, with 37\nsurpassing our baselines and the top team achieving a 1.65% equal error rate.\nFor the WildSVDD track, we benchmarked the baselines. This paper reviews these\nresults, discusses key findings, and outlines future directions for SVDD\nresearch.\n","authors":["You Zhang","Yongyi Zang","Jiatong Shi","Ryuichi Yamamoto","Tomoki Toda","Zhiyao Duan"],"pdf_url":"https://arxiv.org/pdf/2408.16132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14485v6","updated":"2024-08-28T17:08:55Z","published":"2024-06-20T16:48:14Z","title":"Proceedings of The second international workshop on eXplainable AI for\n  the Arts (XAIxArts)","summary":"  This second international workshop on explainable AI for the Arts (XAIxArts)\nbrought together a community of researchers in HCI, Interaction Design, AI,\nexplainable AI (XAI), and digital arts to explore the role of XAI for the Arts.\nWorkshop held at the 16th ACM Conference on Creativity and Cognition (C&C\n2024), Chicago, USA.\n","authors":["Nick Bryan-Kinns","Corey Ford","Shuoyang Zheng","Helen Kennedy","Alan Chamberlain","Makayla Lewis","Drew Hemment","Zijin Li","Qiong Wu","Lanxi Xiao","Gus Xia","Jeba Rezwana","Michael Clemens","Gabriel Vigliensoni"],"pdf_url":"https://arxiv.org/pdf/2406.14485v6.pdf","comment":"Proceedings of The second international workshop on eXplainable AI\n  for the Arts (XAIxArts)"},{"id":"http://arxiv.org/abs/2407.19976v2","updated":"2024-08-28T13:01:06Z","published":"2024-07-29T13:09:26Z","title":"MambaGesture: Enhancing Co-Speech Gesture Generation with Mamba and\n  Disentangled Multi-Modality Fusion","summary":"  Co-speech gesture generation is crucial for producing synchronized and\nrealistic human gestures that accompany speech, enhancing the animation of\nlifelike avatars in virtual environments. While diffusion models have shown\nimpressive capabilities, current approaches often overlook a wide range of\nmodalities and their interactions, resulting in less dynamic and contextually\nvaried gestures. To address these challenges, we present MambaGesture, a novel\nframework integrating a Mamba-based attention block, MambaAttn, with a\nmulti-modality feature fusion module, SEAD. The MambaAttn block combines the\nsequential data processing strengths of the Mamba model with the contextual\nrichness of attention mechanisms, enhancing the temporal coherence of generated\ngestures. SEAD adeptly fuses audio, text, style, and emotion modalities,\nemploying disentanglement to deepen the fusion process and yield gestures with\ngreater realism and diversity. Our approach, rigorously evaluated on the\nmulti-modal BEAT dataset, demonstrates significant improvements in Fr\\'echet\nGesture Distance (FGD), diversity scores, and beat alignment, achieving\nstate-of-the-art performance in co-speech gesture generation. Project website:\n$\\href{https://fcchit.github.io/mambagesture/}{\\textit{https://fcchit.github.io/mambagesture/}}$.\n","authors":["Chencan Fu","Yabiao Wang","Jiangning Zhang","Zhengkai Jiang","Xiaofeng Mao","Jiafu Wu","Weijian Cao","Chengjie Wang","Yanhao Ge","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2407.19976v2.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.11982v2","updated":"2024-08-28T11:01:16Z","published":"2024-08-21T20:32:45Z","title":"AIM 2024 Challenge on Compressed Video Quality Assessment: Methods and\n  Results","summary":"  Video quality assessment (VQA) is a crucial task in the development of video\ncompression standards, as it directly impacts the viewer experience. This paper\npresents the results of the Compressed Video Quality Assessment challenge, held\nin conjunction with the Advances in Image Manipulation (AIM) workshop at ECCV\n2024. The challenge aimed to evaluate the performance of VQA methods on a\ndiverse dataset of 459 videos, encoded with 14 codecs of various compression\nstandards (AVC/H.264, HEVC/H.265, AV1, and VVC/H.266) and containing a\ncomprehensive collection of compression artifacts. To measure the methods\nperformance, we employed traditional correlation coefficients between their\npredictions and subjective scores, which were collected via large-scale\ncrowdsourced pairwise human comparisons. For training purposes, participants\nwere provided with the Compressed Video Quality Assessment Dataset (CVQAD), a\npreviously developed dataset of 1022 videos. Up to 30 participating teams\nregistered for the challenge, while we report the results of 6 teams, which\nsubmitted valid final solutions and code for reproducing the results. Moreover,\nwe calculated and present the performance of state-of-the-art VQA methods on\nthe developed dataset, providing a comprehensive benchmark for future research.\nThe dataset, results, and online leaderboard are publicly available at\nhttps://challenges.videoprocessing.ai/challenges/compressedvideo-quality-assessment.html.\n","authors":["Maksim Smirnov","Aleksandr Gushchin","Anastasia Antsiferova","Dmitry Vatolin","Radu Timofte","Ziheng Jia","Zicheng Zhang","Wei Sun","Jiaying Qian","Yuqin Cao","Yinan Sun","Yuxin Zhu","Xiongkuo Min","Guangtao Zhai","Kanjar De","Qing Luo","Ao-Xiang Zhang","Peng Zhang","Haibo Lei","Linyan Jiang","Yaqing Li","Wenhui Meng","Xiaoheng Tan","Haiqiang Wang","Xiaozhong Xu","Shan Liu","Zhenzhong Chen","Zhengxue Cheng","Jiahao Xiao","Jun Xu","Chenlong He","Qi Zheng","Ruoxi Zhu","Min Li","Yibo Fan","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2408.11982v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15542v1","updated":"2024-08-28T05:34:14Z","published":"2024-08-28T05:34:14Z","title":"Kangaroo: A Powerful Video-Language Model Supporting Long-context Video\n  Input","summary":"  Rapid advancements have been made in extending Large Language Models (LLMs)\nto Large Multi-modal Models (LMMs). However, extending input modality of LLMs\nto video data remains a challenging endeavor, especially for long videos. Due\nto insufficient access to large-scale high-quality video data and the excessive\ncompression of visual features, current methods exhibit limitations in\neffectively processing long videos. In this paper, we introduce Kangaroo, a\npowerful Video LMM aimed at addressing these challenges. Confronted with issue\nof inadequate training data, we develop a data curation system to build a\nlarge-scale dataset with high-quality annotations for vision-language\npre-training and instruction tuning. In addition, we design a curriculum\ntraining pipeline with gradually increasing resolution and number of input\nframes to accommodate long videos. Evaluation results demonstrate that, with 8B\nparameters, Kangaroo achieves state-of-the-art performance across a variety of\nvideo understanding benchmarks while exhibiting competitive results on others.\nParticularly, on benchmarks specialized for long videos, Kangaroo excels some\nlarger models with over 10B parameters and proprietary models.\n","authors":["Jiajun Liu","Yibing Wang","Hanghang Ma","Xiaoping Wu","Xiaoqi Ma","Xiaoming Wei","Jianbin Jiao","Enhua Wu","Jie Hu"],"pdf_url":"https://arxiv.org/pdf/2408.15542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07467v1","updated":"2024-08-28T04:35:08Z","published":"2024-08-28T04:35:08Z","title":"Flexible Control in Symbolic Music Generation via Musical Metadata","summary":"  In this work, we introduce the demonstration of symbolic music generation,\nfocusing on providing short musical motifs that serve as the central theme of\nthe narrative. For the generation, we adopt an autoregressive model which takes\nmusical metadata as inputs and generates 4 bars of multitrack MIDI sequences.\nDuring training, we randomly drop tokens from the musical metadata to guarantee\nflexible control. It provides users with the freedom to select input types\nwhile maintaining generative performance, enabling greater flexibility in music\ncomposition. We validate the effectiveness of the strategy through experiments\nin terms of model capacity, musical fidelity, diversity, and controllability.\nAdditionally, we scale up the model and compare it with other music generation\nmodel through a subjective test. Our results indicate its superiority in both\ncontrol and music quality. We provide a URL link\nhttps://www.youtube.com/watch?v=-0drPrFJdMQ to our demonstration video.\n","authors":["Sangjun Han","Jiwon Ham","Chaeeun Lee","Heejin Kim","Soojong Do","Sihyuk Yi","Jun Seo","Seoyoon Kim","Yountae Jung","Woohyung Lim"],"pdf_url":"https://arxiv.org/pdf/2409.07467v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15521v1","updated":"2024-08-28T04:14:01Z","published":"2024-08-28T04:14:01Z","title":"A Simple Baseline with Single-encoder for Referring Image Segmentation","summary":"  Referring image segmentation (RIS) requires dense vision-language\ninteractions between visual pixels and textual words to segment objects based\non a given description. However, commonly adapted dual-encoders in RIS, e.g.,\nSwin transformer and BERT (uni-modal encoders) or CLIP (a multi-modal\ndual-encoder), lack dense multi-modal interactions during pre-training, leading\nto a gap with a pixel-level RIS task. To bridge this gap, existing RIS methods\noften rely on multi-modal fusion modules that interact two encoders, but this\napproach leads to high computational costs. In this paper, we present a novel\nRIS method with a single-encoder, i.e., BEiT-3, maximizing the potential of\nshared self-attention across all framework components. This enables seamless\ninteractions of two modalities from input to final prediction, producing\ngranularly aligned multi-modal features. Furthermore, we propose lightweight\nyet effective decoder modules, a Shared FPN and a Shared Mask Decoder, which\ncontribute to the high efficiency of our model. Our simple baseline with a\nsingle encoder achieves outstanding performances on the RIS benchmark datasets\nwhile maintaining computational efficiency, compared to the most recent SoTA\nmethods based on dual-encoders.\n","authors":["Seonghoon Yu","Ilchae Jung","Byeongju Han","Taeoh Kim","Yunho Kim","Dongyoon Wee","Jeany Son"],"pdf_url":"https://arxiv.org/pdf/2408.15521v1.pdf","comment":"ArXiv pre-print"}]},"2024-08-27T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2408.15209v1","updated":"2024-08-27T17:18:02Z","published":"2024-08-27T17:18:02Z","title":"Sec2Sec Co-attention for Video-Based Apparent Affective Prediction","summary":"  Video-based apparent affect detection plays a crucial role in video\nunderstanding, as it encompasses various elements such as vision, audio,\naudio-visual interactions, and spatiotemporal information, which are essential\nfor accurate video predictions. However, existing approaches often focus on\nextracting only a subset of these elements, resulting in the limited predictive\ncapacity of their models. To address this limitation, we propose a novel\nLSTM-based network augmented with a Transformer co-attention mechanism for\npredicting apparent affect in videos. We demonstrate that our proposed Sec2Sec\nCo-attention Transformer surpasses multiple state-of-the-art methods in\npredicting apparent affect on two widely used datasets: LIRIS-ACCEDE and First\nImpressions. Notably, our model offers interpretability, allowing us to examine\nthe contributions of different time points to the overall prediction. The\nimplementation is available at: https://github.com/nestor-sun/sec2sec.\n","authors":["Mingwei Sun","Kunpeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.15209v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.14826v1","updated":"2024-08-27T07:13:44Z","published":"2024-08-27T07:13:44Z","title":"Alfie: Democratising RGBA Image Generation With No $$$","summary":"  Designs and artworks are ubiquitous across various creative fields, requiring\ngraphic design skills and dedicated software to create compositions that\ninclude many graphical elements, such as logos, icons, symbols, and art scenes,\nwhich are integral to visual storytelling. Automating the generation of such\nvisual elements improves graphic designers' productivity, democratizes and\ninnovates the creative industry, and helps generate more realistic synthetic\ndata for related tasks. These illustration elements are mostly RGBA images with\nirregular shapes and cutouts, facilitating blending and scene composition.\nHowever, most image generation models are incapable of generating such images\nand achieving this capability requires expensive computational resources,\nspecific training recipes, or post-processing solutions. In this work, we\npropose a fully-automated approach for obtaining RGBA illustrations by\nmodifying the inference-time behavior of a pre-trained Diffusion Transformer\nmodel, exploiting the prompt-guided controllability and visual quality offered\nby such models with no additional computational cost. We force the generation\nof entire subjects without sharp croppings, whose background is easily removed\nfor seamless integration into design projects or artistic scenes. We show with\na user study that, in most cases, users prefer our solution over generating and\nthen matting an image, and we show that our generated illustrations yield good\nresults when used as inputs for composite scene generation pipelines. We\nrelease the code at https://github.com/aimagelab/Alfie.\n","authors":["Fabio Quattrini","Vittorio Pippi","Silvia Cascianelli","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2408.14826v1.pdf","comment":"Accepted at ECCV AI for Visual Arts Workshop and Challenges"},{"id":"http://arxiv.org/abs/2408.14823v1","updated":"2024-08-27T07:06:49Z","published":"2024-08-27T07:06:49Z","title":"LapisGS: Layered Progressive 3D Gaussian Splatting for Adaptive\n  Streaming","summary":"  The rise of Extended Reality (XR) requires efficient streaming of 3D online\nworlds, challenging current 3DGS representations to adapt to\nbandwidth-constrained environments. This paper proposes LapisGS, a layered 3DGS\nthat supports adaptive streaming and progressive rendering. Our method\nconstructs a layered structure for cumulative representation, incorporates\ndynamic opacity optimization to maintain visual fidelity, and utilizes\noccupancy maps to efficiently manage Gaussian splats. This proposed model\noffers a progressive representation supporting a continuous rendering quality\nadapted for bandwidth-aware streaming. Extensive experiments validate the\neffectiveness of our approach in balancing visual fidelity with the compactness\nof the model, with up to 50.71% improvement in SSIM, 286.53% improvement in\nLPIPS, and 318.41% reduction in model size, and shows its potential for\nbandwidth-adapted 3D streaming and rendering applications.\n","authors":["Yuang Shi","Simone Gasparini","G√©raldine Morin","Wei Tsang Ooi"],"pdf_url":"https://arxiv.org/pdf/2408.14823v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14764v1","updated":"2024-08-27T03:31:24Z","published":"2024-08-27T03:31:24Z","title":"SynthDoc: Bilingual Documents Synthesis for Visual Document\n  Understanding","summary":"  This paper introduces SynthDoc, a novel synthetic document generation\npipeline designed to enhance Visual Document Understanding (VDU) by generating\nhigh-quality, diverse datasets that include text, images, tables, and charts.\nAddressing the challenges of data acquisition and the limitations of existing\ndatasets, SynthDoc leverages publicly available corpora and advanced rendering\ntools to create a comprehensive and versatile dataset. Our experiments,\nconducted using the Donut model, demonstrate that models trained with\nSynthDoc's data achieve superior performance in pre-training read tasks and\nmaintain robustness in downstream tasks, despite language inconsistencies. The\nrelease of a benchmark dataset comprising 5,000 image-text pairs not only\nshowcases the pipeline's capabilities but also provides a valuable resource for\nthe VDU community to advance research and development in document image\nrecognition. This work significantly contributes to the field by offering a\nscalable solution to data scarcity and by validating the efficacy of end-to-end\nmodels in parsing complex, real-world documents.\n","authors":["Chuanghao Ding","Xuejing Liu","Wei Tang","Juan Li","Xiaoliang Wang","Rui Zhao","Cam-Tu Nguyen","Fei Tan"],"pdf_url":"https://arxiv.org/pdf/2408.14764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14735v1","updated":"2024-08-27T02:03:36Z","published":"2024-08-27T02:03:36Z","title":"PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy","summary":"  Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.\n","authors":["Xianzhi Zhang","Yipeng Zhou","Di Wu","Quan Z. Sheng","Miao Hu","Linchang Xiao"],"pdf_url":"https://arxiv.org/pdf/2408.14735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14713v1","updated":"2024-08-27T00:37:07Z","published":"2024-08-27T00:37:07Z","title":"StyleSpeech: Parameter-efficient Fine Tuning for Pre-trained\n  Controllable Text-to-Speech","summary":"  This paper introduces StyleSpeech, a novel Text-to-Speech~(TTS) system that\nenhances the naturalness and accuracy of synthesized speech. Building upon\nexisting TTS technologies, StyleSpeech incorporates a unique Style Decorator\nstructure that enables deep learning models to simultaneously learn style and\nphoneme features, improving adaptability and efficiency through the principles\nof Lower Rank Adaptation~(LoRA). LoRA allows efficient adaptation of style\nfeatures in pre-trained models. Additionally, we introduce a novel automatic\nevaluation metric, the LLM-Guided Mean Opinion Score (LLM-MOS), which employs\nlarge language models to offer an objective and robust protocol for\nautomatically assessing TTS system performance. Extensive testing on benchmark\ndatasets shows that our approach markedly outperforms existing state-of-the-art\nbaseline methods in producing natural, accurate, and high-quality speech. These\nadvancements not only pushes the boundaries of current TTS system capabilities,\nbut also facilitate the application of TTS system in more dynamic and\nspecialized, such as interactive virtual assistants, adaptive audiobooks, and\ncustomized voice for gaming. Speech samples can be found in\nhttps://style-speech.vercel.app\n","authors":["Haowei Lou","Helen Paik","Wen Hu","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2408.14713v1.pdf","comment":null}]},"2024-08-26T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2408.14547v1","updated":"2024-08-26T18:00:33Z","published":"2024-08-26T18:00:33Z","title":"Revisiting Image Captioning Training Paradigm via Direct CLIP-based\n  Optimization","summary":"  The conventional training approach for image captioning involves pre-training\na network using teacher forcing and subsequent fine-tuning with Self-Critical\nSequence Training to maximize hand-crafted captioning metrics. However, when\nattempting to optimize modern and higher-quality metrics like CLIP-Score and\nPAC-Score, this training method often encounters instability and fails to\nacquire the genuine descriptive capabilities needed to produce fluent and\ninformative captions. In this paper, we propose a new training paradigm termed\nDirect CLIP-Based Optimization (DiCO). Our approach jointly learns and\noptimizes a reward model that is distilled from a learnable captioning\nevaluator with high human correlation. This is done by solving a weighted\nclassification problem directly inside the captioner. At the same time, DiCO\nprevents divergence from the original model, ensuring that fluency is\nmaintained. DiCO not only exhibits improved stability and enhanced quality in\nthe generated captions but also aligns more closely with human preferences\ncompared to existing methods, especially in modern metrics. Additionally, it\nmaintains competitive performance in traditional metrics. Our source code and\ntrained models are publicly available at https://github.com/aimagelab/DiCO.\n","authors":["Nicholas Moratelli","Davide Caffagni","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2408.14547v1.pdf","comment":"BMVC 2024"},{"id":"http://arxiv.org/abs/2408.14155v1","updated":"2024-08-26T09:59:45Z","published":"2024-08-26T09:59:45Z","title":"Digital Fingerprinting on Multimedia: A Survey","summary":"  The explosive growth of multimedia content in the digital economy era has\nbrought challenges in content recognition, copyright protection, and data\nmanagement. As an emerging content management technology, perceptual hash-based\ndigital fingerprints, serving as compact summaries of multimedia content, have\nbeen widely adopted for efficient multimedia content identification and\nretrieval across different modalities (e.g., text, image, video, audio),\nattracting significant attention from both academia and industry. Despite the\nincreasing applications of digital fingerprints, there is a lack of systematic\nand comprehensive literature review on multimedia digital fingerprints. This\nsurvey aims to fill this gap and provide an important resource for researchers\nstudying the details and related advancements of multimedia digital\nfingerprints. The survey first introduces the definition, characteristics, and\nrelated concepts (including hash functions, granularity, similarity measures,\netc.) of digital fingerprints. It then focuses on analyzing and summarizing the\nalgorithms for extracting unimodal fingerprints of different types of digital\ncontent, including text fingerprints, image fingerprints, video fingerprints,\nand audio fingerprints. Particularly, it provides an in-depth review and\nsummary of deep learning-based fingerprints. Additionally, the survey\nelaborates on the various practical applications of digital fingerprints and\noutlines the challenges and potential future research directions. The goal is\nto promote the continued development of multimedia digital fingerprint\nresearch.\n","authors":["Wendi Chen","Wensheng Gan","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2408.14155v1.pdf","comment":"Preprint. 5 figures, 7 tables"},{"id":"http://arxiv.org/abs/2407.04284v2","updated":"2024-08-26T08:19:03Z","published":"2024-07-05T06:32:52Z","title":"TSC-PCAC: Voxel Transformer and Sparse Convolution Based Point Cloud\n  Attribute Compression for 3D Broadcasting","summary":"  Point cloud has been the mainstream representation for advanced 3D\napplications, such as virtual reality and augmented reality. However, the\nmassive data amounts of point clouds is one of the most challenging issues for\ntransmission and storage. In this paper, we propose an end-to-end voxel\nTransformer and Sparse Convolution based Point Cloud Attribute Compression\n(TSC-PCAC) for 3D broadcasting. Firstly, we present a framework of the\nTSC-PCAC, which include Transformer and Sparse Convolutional Module (TSCM)\nbased variational autoencoder and channel context module. Secondly, we propose\na two-stage TSCM, where the first stage focuses on modeling local dependencies\nand feature representations of the point clouds, and the second stage captures\nglobal features through spatial and channel pooling encompassing larger\nreceptive fields. This module effectively extracts global and local interpoint\nrelevance to reduce informational redundancy. Thirdly, we design a TSCM based\nchannel context module to exploit interchannel correlations, which improves the\npredicted probability distribution of quantized latent representations and thus\nreduces the bitrate. Experimental results indicate that the proposed TSC-PCAC\nmethod achieves an average of 38.53%, 21.30%, and 11.19% Bjontegaard Delta\nbitrate reductions compared to the Sparse-PCAC, NF-PCAC, and G-PCC v23 methods,\nrespectively. The encoding/decoding time costs are reduced up to 97.68%/98.78%\non average compared to the Sparse-PCAC. The source code and the trained models\nof the TSC-PCAC are available at https://github.com/igizuxo/TSC-PCAC.\n","authors":["Zixi Guo","Yun Zhang","Linwei Zhu","Hanli Wang","Gangyi Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.04284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14084v1","updated":"2024-08-26T08:11:35Z","published":"2024-08-26T08:11:35Z","title":"HABD: a houma alliance book ancient handwritten character recognition\n  database","summary":"  The Houma Alliance Book, one of history's earliest calligraphic examples, was\nunearthed in the 1970s. These artifacts were meticulously organized,\nreproduced, and copied by the Shanxi Provincial Institute of Cultural Relics.\nHowever, because of their ancient origins and severe ink erosion, identifying\ncharacters in the Houma Alliance Book is challenging, necessitating the use of\ndigital technology. In this paper, we propose a new ancient handwritten\ncharacter recognition database for the Houma alliance book, along with a novel\nbenchmark based on deep learning architectures. More specifically, a collection\nof 26,732 characters samples from the Houma Alliance Book were gathered,\nencompassing 327 different types of ancient characters through iterative\nannotation. Furthermore, benchmark algorithms were proposed by combining four\ndeep neural network classifiers with two data augmentation methods. This\nresearch provides valuable resources and technical support for further studies\non the Houma Alliance Book and other ancient characters. This contributes to\nour understanding of ancient culture and history, as well as the preservation\nand inheritance of humanity's cultural heritage.\n","authors":["Xiaoyu Yuan","Xiaohua Huang","Zibo Zhang","Yabo Sun"],"pdf_url":"https://arxiv.org/pdf/2408.14084v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.12321v2","updated":"2024-08-26T04:27:54Z","published":"2024-08-22T11:57:16Z","title":"MaVEn: An Effective Multi-granularity Hybrid Visual Encoding Framework\n  for Multimodal Large Language Model","summary":"  This paper presents MaVEn, an innovative Multi-granularity Visual Encoding\nframework designed to enhance the capabilities of Multimodal Large Language\nModels (MLLMs) in multi-image reasoning. Current MLLMs primarily focus on\nsingle-image visual understanding, limiting their ability to interpret and\nintegrate information across multiple images. MaVEn addresses this limitation\nby combining discrete visual symbol sequences, which abstract coarse-grained\nsemantic concepts, with traditional continuous representation sequences that\nmodel fine-grained features. This dual approach bridges the semantic gap\nbetween visual and textual data, thereby improving the model's ability to\nprocess and interpret information from multiple images effectively.\nAdditionally, we design a dynamic reduction mechanism by for long-sequence\ncontinuous features to enhance multi-image processing efficiency. Experimental\nresults demonstrate that MaVEn significantly enhances MLLMs' understanding in\ncomplex multi-image scenarios, while also improving performance in single-image\ncontexts.\n","authors":["Chaoya Jiang","Jia Hongrui","Haiyang Xu","Wei Ye","Mengfan Dong","Ming Yan","Ji Zhang","Fei Huang","Shikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.12321v2.pdf","comment":null}]},"2024-08-25T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2408.13786v1","updated":"2024-08-25T09:29:20Z","published":"2024-08-25T09:29:20Z","title":"Localization of Synthetic Manipulations in Western Blot Images","summary":"  Recent breakthroughs in deep learning and generative systems have\nsignificantly fostered the creation of synthetic media, as well as the local\nalteration of real content via the insertion of highly realistic synthetic\nmanipulations. Local image manipulation, in particular, poses serious\nchallenges to the integrity of digital content and societal trust. This problem\nis not only confined to multimedia data, but also extends to biological images\nincluded in scientific publications, like images depicting Western blots. In\nthis work, we address the task of localizing synthetic manipulations in Western\nblot images. To discriminate between pristine and synthetic pixels of an\nanalyzed image, we propose a synthetic detector that operates on small patches\nextracted from the image. We aggregate patch contributions to estimate a\ntampering heatmap, highlighting synthetic pixels out of pristine ones. Our\nmethodology proves effective when tested over two manipulated Western blot\nimage datasets, one altered automatically and the other manually by exploiting\nadvanced AI-based image manipulation tools that are unknown at our training\nstage. We also explore the robustness of our method over an external dataset of\nother scientific images depicting different semantics, manipulated through\nunseen generation techniques.\n","authors":["Anmol Manjunath","Viola Negroni","Sara Mandelli","Daniel Moreira","Paolo Bestagini"],"pdf_url":"https://arxiv.org/pdf/2408.13786v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13784v1","updated":"2024-08-25T09:28:04Z","published":"2024-08-25T09:28:04Z","title":"Analyzing the Impact of Splicing Artifacts in Partially Fake Speech\n  Signals","summary":"  Speech deepfake detection has recently gained significant attention within\nthe multimedia forensics community. Related issues have also been explored,\nsuch as the identification of partially fake signals, i.e., tracks that include\nboth real and fake speech segments. However, generating high-quality spliced\naudio is not as straightforward as it may appear. Spliced signals are typically\ncreated through basic signal concatenation. This process could introduce\nnoticeable artifacts that can make the generated data easier to detect. We\nanalyze spliced audio tracks resulting from signal concatenation, investigate\ntheir artifacts and assess whether such artifacts introduce any bias in\nexisting datasets. Our findings reveal that by analyzing splicing artifacts, we\ncan achieve a detection EER of 6.16% and 7.36% on PartialSpoof and HAD\ndatasets, respectively, without needing to train any detector. These results\nunderscore the complexities of generating reliable spliced audio data and lead\nto discussions that can help improve future research in this area.\n","authors":["Viola Negroni","Davide Salvi","Paolo Bestagini","Stefano Tubaro"],"pdf_url":"https://arxiv.org/pdf/2408.13784v1.pdf","comment":"Accepted at ASVspoof 5 Workshop (Interspeech2024 Satellite)"},{"id":"http://arxiv.org/abs/2409.06709v1","updated":"2024-08-25T04:56:08Z","published":"2024-08-25T04:56:08Z","title":"Unveiling Visual Biases in Audio-Visual Localization Benchmarks","summary":"  Audio-Visual Source Localization (AVSL) aims to localize the source of sound\nwithin a video. In this paper, we identify a significant issue in existing\nbenchmarks: the sounding objects are often easily recognized based solely on\nvisual cues, which we refer to as visual bias. Such biases hinder these\nbenchmarks from effectively evaluating AVSL models. To further validate our\nhypothesis regarding visual biases, we examine two representative AVSL\nbenchmarks, VGG-SS and EpicSounding-Object, where the vision-only models\noutperform all audiovisual baselines. Our findings suggest that existing AVSL\nbenchmarks need further refinement to facilitate audio-visual learning.\n","authors":["Liangyu Chen","Zihao Yue","Boshen Xu","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2409.06709v1.pdf","comment":"Accepted by ECCV24 AVGenL Workshop"},{"id":"http://arxiv.org/abs/2408.13712v1","updated":"2024-08-25T03:21:48Z","published":"2024-08-25T03:21:48Z","title":"Riemann-based Multi-scale Attention Reasoning Network for Text-3D\n  Retrieval","summary":"  Due to the challenges in acquiring paired Text-3D data and the inherent\nirregularity of 3D data structures, combined representation learning of 3D\npoint clouds and text remains unexplored. In this paper, we propose a novel\nRiemann-based Multi-scale Attention Reasoning Network (RMARN) for text-3D\nretrieval. Specifically, the extracted text and point cloud features are\nrefined by their respective Adaptive Feature Refiner (AFR). Furthermore, we\nintroduce the innovative Riemann Local Similarity (RLS) module and the Global\nPooling Similarity (GPS) module. However, as 3D point cloud data and text data\noften possess complex geometric structures in high-dimensional space, the\nproposed RLS employs a novel Riemann Attention Mechanism to reflect the\nintrinsic geometric relationships of the data. Without explicitly defining the\nmanifold, RMARN learns the manifold parameters to better represent the\ndistances between text-point cloud samples. To address the challenges of\nlacking paired text-3D data, we have created the large-scale Text-3D Retrieval\ndataset T3DR-HIT, which comprises over 3,380 pairs of text and point cloud\ndata. T3DR-HIT contains coarse-grained indoor 3D scenes and fine-grained\nChinese artifact scenes, consisting of 1,380 and over 2,000 text-3D pairs,\nrespectively. Experiments on our custom datasets demonstrate the superior\nperformance of the proposed method. Our code and proposed datasets are\navailable at \\url{https://github.com/liwrui/RMARN}.\n","authors":["Wenrui Li","Wei Han","Yandu Chen","Yeyu Chai","Yidan Lu","Xingtao Wang","Xiaopeng Fan"],"pdf_url":"https://arxiv.org/pdf/2408.13712v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13711v1","updated":"2024-08-25T02:56:26Z","published":"2024-08-25T02:56:26Z","title":"SceneDreamer360: Text-Driven 3D-Consistent Scene Generation with\n  Panoramic Gaussian Splatting","summary":"  Text-driven 3D scene generation has seen significant advancements recently.\nHowever, most existing methods generate single-view images using generative\nmodels and then stitch them together in 3D space. This independent generation\nfor each view often results in spatial inconsistency and implausibility in the\n3D scenes. To address this challenge, we proposed a novel text-driven\n3D-consistent scene generation model: SceneDreamer360. Our proposed method\nleverages a text-driven panoramic image generation model as a prior for 3D\nscene generation and employs 3D Gaussian Splatting (3DGS) to ensure consistency\nacross multi-view panoramic images. Specifically, SceneDreamer360 enhances the\nfine-tuned Panfusion generator with a three-stage panoramic enhancement,\nenabling the generation of high-resolution, detail-rich panoramic images.\nDuring the 3D scene construction, a novel point cloud fusion initialization\nmethod is used, producing higher quality and spatially consistent point clouds.\nOur extensive experiments demonstrate that compared to other methods,\nSceneDreamer360 with its panoramic image generation and 3DGS can produce higher\nquality, spatially consistent, and visually appealing 3D scenes from any text\nprompt. Our codes are available at\n\\url{https://github.com/liwrui/SceneDreamer360}.\n","authors":["Wenrui Li","Yapeng Mi","Fucheng Cai","Zhe Yang","Wangmeng Zuo","Xingtao Wang","Xiaopeng Fan"],"pdf_url":"https://arxiv.org/pdf/2408.13711v1.pdf","comment":null}]},"2024-08-24T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2408.13608v1","updated":"2024-08-24T15:36:08Z","published":"2024-08-24T15:36:08Z","title":"SpeechCraft: A Fine-grained Expressive Speech Dataset with Natural\n  Language Description","summary":"  Speech-language multi-modal learning presents a significant challenge due to\nthe fine nuanced information inherent in speech styles. Therefore, a\nlarge-scale dataset providing elaborate comprehension of speech style is\nurgently needed to facilitate insightful interplay between speech audio and\nnatural language. However, constructing such datasets presents a major\ntrade-off between large-scale data collection and high-quality annotation. To\ntackle this challenge, we propose an automatic speech annotation system for\nexpressiveness interpretation that annotates in-the-wild speech clips with\nexpressive and vivid human language descriptions. Initially, speech audios are\nprocessed by a series of expert classifiers and captioning models to capture\ndiverse speech characteristics, followed by a fine-tuned LLaMA for customized\nannotation generation. Unlike previous tag/templet-based annotation frameworks\nwith limited information and diversity, our system provides in-depth\nunderstandings of speech style through tailored natural language descriptions,\nthereby enabling accurate and voluminous data generation for large model\ntraining. With this system, we create SpeechCraft, a fine-grained bilingual\nexpressive speech dataset. It is distinguished by highly descriptive natural\nlanguage style prompts, containing approximately 2,000 hours of audio data and\nencompassing over two million speech clips. Extensive experiments demonstrate\nthat the proposed dataset significantly boosts speech-language task performance\nin stylist speech synthesis and speech style understanding.\n","authors":["Zeyu Jin","Jia Jia","Qixin Wang","Kehan Li","Shuoyi Zhou","Songtao Zhou","Xiaoyu Qin","Zhiyong Wu"],"pdf_url":"https://arxiv.org/pdf/2408.13608v1.pdf","comment":"Accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2408.13520v1","updated":"2024-08-24T08:47:09Z","published":"2024-08-24T08:47:09Z","title":"An Open, Cross-Platform, Web-Based Metaverse Using WebXR and A-Frame","summary":"  The metaverse has received much attention in the literature and industry in\nthe last few years, but the lack of an open and cross-platform architecture has\nled to many distinct metaverses that cannot communicate with each other. This\nwork proposes a WebXR-based cross-platform architecture for developing spatial\nweb apps using the A-Frame and Networked-Aframe frameworks with a view to an\nopen and interoperable metaverse, accessible from both the web and extended\nreality devices. A prototype was implemented and evaluated, supporting the\ncapability of the technology stack to enable immersive experiences across\ndifferent platforms and devices. Positive feedback on ease of use of the\nimmersive environment further corroborates the proposed approach, underscoring\nits effectiveness in facilitating engaging and interactive virtual spaces. By\nadhering to principles of interoperability and inclusivity, it lives up to Tim\nBerners-Lee's vision of the World Wide Web as an open platform that transcends\ngeographical and technical boundaries.\n","authors":["Giuseppe Macario"],"pdf_url":"https://arxiv.org/pdf/2408.13520v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2404.05317"},{"id":"http://arxiv.org/abs/2403.02905v3","updated":"2024-08-24T00:29:50Z","published":"2024-03-05T12:13:18Z","title":"MMoFusion: Multi-modal Co-Speech Motion Generation with Diffusion Model","summary":"  The body movements accompanying speech aid speakers in expressing their\nideas. Co-speech motion generation is one of the important approaches for\nsynthesizing realistic avatars. Due to the intricate correspondence between\nspeech and motion, generating realistic and diverse motion is a challenging\ntask. In this paper, we propose MMoFusion, a Multi-modal co-speech Motion\ngeneration framework based on the diffusion model to ensure both the\nauthenticity and diversity of generated motion. We propose a progressive fusion\nstrategy to enhance the interaction of inter-modal and intra-modal, efficiently\nintegrating multi-modal information. Specifically, we employ a masked style\nmatrix based on emotion and identity information to control the generation of\ndifferent motion styles. Temporal modeling of speech and motion is partitioned\ninto style-guided specific feature encoding and shared feature encoding, aiming\nto learn both inter-modal and intra-modal features. Besides, we propose a\ngeometric loss to enforce the joints' velocity and acceleration coherence among\nframes. Our framework generates vivid, diverse, and style-controllable motion\nof arbitrary length through inputting speech and editing identity and emotion.\nExtensive experiments demonstrate that our method outperforms current co-speech\nmotion generation methods including upper body and challenging full body.\n","authors":["Sen Wang","Jiangning Zhang","Xin Tan","Zhifeng Xie","Chengjie Wang","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2403.02905v3.pdf","comment":null}]},"2024-08-23T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2408.13019v1","updated":"2024-08-23T12:14:18Z","published":"2024-08-23T12:14:18Z","title":"VCEMO: Multi-Modal Emotion Recognition for Chinese Voiceprints","summary":"  Emotion recognition can enhance humanized machine responses to user commands,\nwhile voiceprint-based perception systems can be easily integrated into\ncommonly used devices like smartphones and stereos. Despite having the largest\nnumber of speakers, there is a noticeable absence of high-quality corpus\ndatasets for emotion recognition using Chinese voiceprints. Hence, this paper\nintroduces the VCEMO dataset to address this deficiency. The proposed dataset\nis constructed from everyday conversations and comprises over 100 users and\n7,747 textual samples. Furthermore, this paper proposes a multimodal-based\nmodel as a benchmark, which effectively fuses speech, text, and external\nknowledge using a co-attention structure. The system employs contrastive\nlearning-based regulation for the uneven distribution of the dataset and the\ndiversity of emotional expressions. The experiments demonstrate the significant\nimprovement of the proposed model over SOTA on the VCEMO and IEMOCAP datasets.\nCode and dataset will be released for research.\n","authors":["Jinghua Tang","Liyun Zhang","Yu Lu","Dian Ding","Lanqing Yang","YiChao Chen","Minjie Bian","Xiaoshan Li","Guangtao Xue"],"pdf_url":"https://arxiv.org/pdf/2408.13019v1.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.12895v1","updated":"2024-08-23T07:59:51Z","published":"2024-08-23T07:59:51Z","title":"Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion\n  Recognition","summary":"  Multimodal Emotion Recognition in Conversations (ERC) is a typical multimodal\nlearning task in exploiting various data modalities concurrently. Prior studies\non effective multimodal ERC encounter challenges in addressing modality\nimbalances and optimizing learning across modalities. Dealing with these\nproblems, we present a novel framework named Ada2I, which consists of two\ninseparable modules namely Adaptive Feature Weighting (AFW) and Adaptive\nModality Weighting (AMW) for feature-level and modality-level balancing\nrespectively via leveraging both Inter- and Intra-modal interactions.\nAdditionally, we introduce a refined disparity ratio as part of our training\noptimization strategy, a simple yet effective measure to assess the overall\ndiscrepancy of the model's learning process when handling multiple modalities\nsimultaneously. Experimental results validate the effectiveness of Ada2I with\nstate-of-the-art performance compared to baselines on three benchmark datasets,\nparticularly in addressing modality imbalances.\n","authors":["Cam-Van Thi Nguyen","The-Son Le","Anh-Tuan Mai","Duc-Trong Le"],"pdf_url":"https://arxiv.org/pdf/2408.12895v1.pdf","comment":"Accepted at ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2408.09462v2","updated":"2024-08-23T06:59:10Z","published":"2024-08-18T12:52:55Z","title":"SpeechEE: A Novel Benchmark for Speech Event Extraction","summary":"  Event extraction (EE) is a critical direction in the field of information\nextraction, laying an important foundation for the construction of structured\nknowledge bases. EE from text has received ample research and attention for\nyears, yet there can be numerous real-world applications that require direct\ninformation acquisition from speech signals, online meeting minutes, interview\nsummaries, press releases, etc. While EE from speech has remained\nunder-explored, this paper fills the gap by pioneering a SpeechEE, defined as\ndetecting the event predicates and arguments from a given audio speech. To\nbenchmark the SpeechEE task, we first construct a large-scale high-quality\ndataset. Based on textual EE datasets under the sentence, document, and\ndialogue scenarios, we convert texts into speeches through both manual\nreal-person narration and automatic synthesis, empowering the data with diverse\nscenarios, languages, domains, ambiences, and speaker styles. Further, to\neffectively address the key challenges in the task, we tailor an E2E SpeechEE\nsystem based on the encoder-decoder architecture, where a novel Shrinking Unit\nmodule and a retrieval-aided decoding mechanism are devised. Extensive\nexperimental results on all SpeechEE subsets demonstrate the efficacy of the\nproposed model, offering a strong baseline for the task. At last, being the\nfirst work on this topic, we shed light on key directions for future research.\n","authors":["Bin Wang","Meishan Zhang","Hao Fei","Yu Zhao","Bobo Li","Shengqiong Wu","Wei Ji","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.09462v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12800v1","updated":"2024-08-23T02:28:54Z","published":"2024-08-23T02:28:54Z","title":"Cap2Sum: Learning to Summarize Videos by Generating Captions","summary":"  With the rapid growth of video data on the internet, video summarization is\nbecoming a very important AI technology. However, due to the high labelling\ncost of video summarization, existing studies have to be conducted on\nsmall-scale datasets, leading to limited performance and generalization\ncapacity. In this work, we introduce the use of dense video captions as a\nsupervision signal to train video summarization models. Motivated by this, we\npropose Cap2Sum, a model that learns to summarize videos by generating\ncaptions, to exploit dense video caption annotations. This weakly-supervised\napproach allows us to train the models on large-scale dense video caption\ndatasets to achieve better performance and generalization capacity. To further\nimprove the generalization capacity, we introduce a CLIP (a strong\nvision-language model) Prior mechanism to enhance the learning of important\nobjects that captions may ignore in the videos. In practice, Cap2Sum can\nperform zero-shot video summarization or be fine-tuned by the ground-truth\nsummary or video caption of the target dataset. To examine the performance of\nCap2Sum after weakly-supervised fine-tuning by the video captions, we propose\ntwo new datasets, TVSum-Caption and SumMe-Caption, which are derived from two\ncommon video summarization datasets and will be publicly released. We conduct\nextensive experiments and the results demonstrate that our method achieves\nsignificant improvements in performance and generalization capacity compared\nwith previous methods.\n","authors":["Cairong Zhao","Chutian Wang","Zifan Song","Guosheng Hu","Haonan Chen","Xiaofan Zhai"],"pdf_url":"https://arxiv.org/pdf/2408.12800v1.pdf","comment":"13 pages, 4 figures"}]},"2024-08-22T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2408.14491v1","updated":"2024-08-22T22:42:23Z","published":"2024-08-22T22:42:23Z","title":"Multimodal Methods for Analyzing Learning and Training Environments: A\n  Systematic Literature Review","summary":"  Recent technological advancements have enhanced our ability to collect and\nanalyze rich multimodal data (e.g., speech, video, and eye gaze) to better\ninform learning and training experiences. While previous reviews have focused\non parts of the multimodal pipeline (e.g., conceptual models and data fusion),\na comprehensive literature review on the methods informing multimodal learning\nand training environments has not been conducted. This literature review\nprovides an in-depth analysis of research methods in these environments,\nproposing a taxonomy and framework that encapsulates recent methodological\nadvances in this field and characterizes the multimodal domain in terms of five\nmodality groups: Natural Language, Video, Sensors, Human-Centered, and\nEnvironment Logs. We introduce a novel data fusion category -- mid fusion --\nand a graph-based technique for refining literature reviews, termed citation\ngraph pruning. Our analysis reveals that leveraging multiple modalities offers\na more holistic understanding of the behaviors and outcomes of learners and\ntrainees. Even when multimodality does not enhance predictive accuracy, it\noften uncovers patterns that contextualize and elucidate unimodal data,\nrevealing subtleties that a single modality may miss. However, there remains a\nneed for further research to bridge the divide between multimodal learning and\ntraining studies and foundational AI research.\n","authors":["Clayton Cohn","Eduardo Davalos","Caleb Vatral","Joyce Horn Fonteles","Hanchen David Wang","Meiyi Ma","Gautam Biswas"],"pdf_url":"https://arxiv.org/pdf/2408.14491v1.pdf","comment":"Submitted to ACM Computing Surveys. Currently under review"},{"id":"http://arxiv.org/abs/2408.12682v1","updated":"2024-08-22T18:41:36Z","published":"2024-08-22T18:41:36Z","title":"MultiMed: Massively Multimodal and Multitask Medical Understanding","summary":"  Biomedical data is inherently multimodal, consisting of electronic health\nrecords, medical imaging, digital pathology, genome sequencing, wearable\nsensors, and more. The application of artificial intelligence tools to these\nmultifaceted sensing technologies has the potential to revolutionize the\nprognosis, diagnosis, and management of human health and disease. However,\ncurrent approaches to biomedical AI typically only train and evaluate with one\nor a small set of medical modalities and tasks. This limitation hampers the\ndevelopment of comprehensive tools that can leverage the rich interconnected\ninformation across many heterogeneous biomedical sensors. To address this\nchallenge, we present MultiMed, a benchmark designed to evaluate and enable\nlarge-scale learning across a wide spectrum of medical modalities and tasks.\nMultiMed consists of 2.56 million samples across ten medical modalities such as\nmedical reports, pathology, genomics, and protein data, and is structured into\neleven challenging tasks, including disease prognosis, protein structure\nprediction, and medical question answering. Using MultiMed, we conduct\ncomprehensive experiments benchmarking state-of-the-art unimodal, multimodal,\nand multitask models. Our analysis highlights the advantages of training\nlarge-scale medical models across many related modalities and tasks. Moreover,\nMultiMed enables studies of generalization across related medical concepts,\nrobustness to real-world noisy data and distribution shifts, and novel modality\ncombinations to improve prediction performance. MultiMed will be publicly\navailable and regularly updated and welcomes inputs from the community.\n","authors":["Shentong Mo","Paul Pu Liang"],"pdf_url":"https://arxiv.org/pdf/2408.12682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.12601v1","updated":"2024-08-22T17:59:44Z","published":"2024-08-22T17:59:44Z","title":"DreamCinema: Cinematic Transfer with Free Camera and 3D Character","summary":"  We are living in a flourishing era of digital media, where everyone has the\npotential to become a personal filmmaker. Current research on cinematic\ntransfer empowers filmmakers to reproduce and manipulate the visual elements\n(e.g., cinematography and character behaviors) from classic shots. However,\ncharacters in the reimagined films still rely on manual crafting, which\ninvolves significant technical complexity and high costs, making it\nunattainable for ordinary users. Furthermore, their estimated cinematography\nlacks smoothness due to inadequate capturing of inter-frame motion and modeling\nof physical trajectories. Fortunately, the remarkable success of 2D and 3D AIGC\nhas opened up the possibility of efficiently generating characters tailored to\nusers' needs, diversifying cinematography. In this paper, we propose\nDreamCinema, a novel cinematic transfer framework that pioneers generative AI\ninto the film production paradigm, aiming at facilitating user-friendly film\ncreation. Specifically, we first extract cinematic elements (i.e., human and\ncamera pose) and optimize the camera trajectory. Then, we apply a character\ngenerator to efficiently create 3D high-quality characters with a human\nstructure prior. Finally, we develop a structure-guided motion transfer\nstrategy to incorporate generated characters into film creation and transfer it\nvia 3D graphics engines smoothly. Extensive experiments demonstrate the\neffectiveness of our method for creating high-quality films with free camera\nand 3D characters.\n","authors":["Weiliang Chen","Fangfu Liu","Diankun Wu","Haowen Sun","Haixu Song","Yueqi Duan"],"pdf_url":"https://arxiv.org/pdf/2408.12601v1.pdf","comment":"Project page: https://liuff19.github.io/DreamCinema"},{"id":"http://arxiv.org/abs/2408.12558v1","updated":"2024-08-22T17:17:43Z","published":"2024-08-22T17:17:43Z","title":"Exploring the Role of Audio in Multimodal Misinformation Detection","summary":"  With the rapid development of deepfake technology, especially the deep audio\nfake technology, misinformation detection on the social media scene meets a\ngreat challenge. Social media data often contains multimodal information which\nincludes audio, video, text, and images. However, existing multimodal\nmisinformation detection methods tend to focus only on some of these\nmodalities, failing to comprehensively address information from all modalities.\nTo comprehensively address the various modal information that may appear on\nsocial media, this paper constructs a comprehensive multimodal misinformation\ndetection framework. By employing corresponding neural network encoders for\neach modality, the framework can fuse different modality information and\nsupport the multimodal misinformation detection task. Based on the constructed\nframework, this paper explores the importance of the audio modality in\nmultimodal misinformation detection tasks on social media. By adjusting the\narchitecture of the acoustic encoder, the effectiveness of different acoustic\nfeature encoders in the multimodal misinformation detection tasks is\ninvestigated. Furthermore, this paper discovers that audio and video\ninformation must be carefully aligned, otherwise the misalignment across\ndifferent audio and video modalities can severely impair the model performance.\n","authors":["Moyang Liu","Yukun Liu","Ruibo Fu","Zhengqi Wen","Jianhua Tao","Xuefei Liu","Guanjun Li"],"pdf_url":"https://arxiv.org/pdf/2408.12558v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11092v2","updated":"2024-08-22T02:23:46Z","published":"2023-09-20T06:51:11Z","title":"Generalized Face Forgery Detection via Adaptive Learning for Pre-trained\n  Vision Transformer","summary":"  With the rapid progress of generative models, the current challenge in face\nforgery detection is how to effectively detect realistic manipulated faces from\ndifferent unseen domains. Though previous studies show that pre-trained Vision\nTransformer (ViT) based models can achieve some promising results after fully\nfine-tuning on the Deepfake dataset, their generalization performances are\nstill unsatisfactory. One possible reason is that fully fine-tuned ViT-based\nmodels may disrupt the pre-trained features [1, 2] and overfit to some\ndata-specific patterns [3]. To alleviate this issue, we present a\n\\textbf{F}orgery-aware \\textbf{A}daptive \\textbf{Vi}sion \\textbf{T}ransformer\n(FA-ViT) under the adaptive learning paradigm, where the parameters in the\npre-trained ViT are kept fixed while the designed adaptive modules are\noptimized to capture forgery features. Specifically, a global adaptive module\nis designed to model long-range interactions among input tokens, which takes\nadvantage of self-attention mechanism to mine global forgery clues. To further\nexplore essential local forgery clues, a local adaptive module is proposed to\nexpose local inconsistencies by enhancing the local contextual association. In\naddition, we introduce a fine-grained adaptive learning module that emphasizes\nthe common compact representation of genuine faces through relationship\nlearning in fine-grained pairs, driving these proposed adaptive modules to be\naware of fine-grained forgery-aware information. Extensive experiments\ndemonstrate that our FA-ViT achieves state-of-the-arts results in the\ncross-dataset evaluation, and enhances the robustness against unseen\nperturbations. Particularly, FA-ViT achieves 93.83\\% and 78.32\\% AUC scores on\nCeleb-DF and DFDC datasets in the cross-dataset evaluation. The code and\ntrained model have been released at: https://github.com/LoveSiameseCat/FAViT.\n","authors":["Anwei Luo","Rizhao Cai","Chenqi Kong","Yakun Ju","Xiangui Kang","Jiwu Huang","Alex C. Kot"],"pdf_url":"https://arxiv.org/pdf/2309.11092v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02901v2","updated":"2024-08-22T00:31:39Z","published":"2024-08-06T02:15:12Z","title":"Lighthouse: A User-Friendly Library for Reproducible Video Moment\n  Retrieval and Highlight Detection","summary":"  We propose Lighthouse, a user-friendly library for reproducible video moment\nretrieval and highlight detection (MR-HD). Although researchers proposed\nvarious MR-HD approaches, the research community holds two main issues. The\nfirst is a lack of comprehensive and reproducible experiments across various\nmethods, datasets, and video-text features. This is because no unified training\nand evaluation codebase covers multiple settings. The second is user-unfriendly\ndesign. Because previous works use different libraries, researchers set up\nindividual environments. In addition, most works release only the training\ncodes, requiring users to implement the whole inference process of MR-HD.\nLighthouse addresses these issues by implementing a unified reproducible\ncodebase that includes six models, three features, and five datasets. In\naddition, it provides an inference API and web demo to make these methods\neasily accessible for researchers and developers. Our experiments demonstrate\nthat Lighthouse generally reproduces the reported scores in the reference\npapers. The code is available at https://github.com/line/lighthouse.\n","authors":["Taichi Nishimura","Shota Nakada","Hokuto Munakata","Tatsuya Komatsu"],"pdf_url":"https://arxiv.org/pdf/2408.02901v2.pdf","comment":"6 pages; library tech report"}]},"2024-08-21T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2408.12035v1","updated":"2024-08-21T23:38:02Z","published":"2024-08-21T23:38:02Z","title":"Let Community Rules Be Reflected in Online Content Moderation","summary":"  Content moderation is a widely used strategy to prevent the dissemination of\nirregular information on social media platforms. Despite extensive research on\ndeveloping automated models to support decision-making in content moderation,\nthere remains a notable scarcity of studies that integrate the rules of online\ncommunities into content moderation. This study addresses this gap by proposing\na community rule-based content moderation framework that directly integrates\ncommunity rules into the moderation of user-generated content. Our experiment\nresults with datasets collected from two domains demonstrate the superior\nperformance of models based on the framework to baseline models across all\nevaluation metrics. In particular, incorporating community rules substantially\nenhances model performance in content moderation. The findings of this research\nhave significant research and practical implications for improving the\neffectiveness and generalizability of content moderation models in online\ncommunities.\n","authors":["Wangjiaxuan Xin","Kanlun Wang","Zhe Fu","Lina Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.12035v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.10500v2","updated":"2024-08-21T18:58:26Z","published":"2024-08-20T02:46:03Z","title":"SZTU-CMU at MER2024: Improving Emotion-LLaMA with Conv-Attention for\n  Multimodal Emotion Recognition","summary":"  This paper presents our winning approach for the MER-NOISE and MER-OV tracks\nof the MER2024 Challenge on multimodal emotion recognition. Our system\nleverages the advanced emotional understanding capabilities of Emotion-LLaMA to\ngenerate high-quality annotations for unlabeled samples, addressing the\nchallenge of limited labeled data. To enhance multimodal fusion while\nmitigating modality-specific noise, we introduce Conv-Attention, a lightweight\nand efficient hybrid framework. Extensive experimentation vali-dates the\neffectiveness of our approach. In the MER-NOISE track, our system achieves a\nstate-of-the-art weighted average F-score of 85.30%, surpassing the second and\nthird-place teams by 1.47% and 1.65%, respectively. For the MER-OV track, our\nutilization of Emotion-LLaMA for open-vocabulary annotation yields an 8.52%\nimprovement in average accuracy and recall compared to GPT-4V, securing the\nhighest score among all participating large multimodal models. The code and\nmodel for Emotion-LLaMA are available at\nhttps://github.com/ZebangCheng/Emotion-LLaMA.\n","authors":["Zebang Cheng","Shuyuan Tu","Dawei Huang","Minghan Li","Xiaojiang Peng","Zhi-Qi Cheng","Alexander G. Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2408.10500v2.pdf","comment":"Ranked 1st in MER24@IJCAI and MRAC24@ACM MM (MER-NOISE & MER-OV\n  (self-evaluated))"},{"id":"http://arxiv.org/abs/2408.11915v1","updated":"2024-08-21T18:06:15Z","published":"2024-08-21T18:06:15Z","title":"Video-Foley: Two-Stage Video-To-Sound Generation via Temporal Event\n  Condition For Foley Sound","summary":"  Foley sound synthesis is crucial for multimedia production, enhancing user\nexperience by synchronizing audio and video both temporally and semantically.\nRecent studies on automating this labor-intensive process through\nvideo-to-sound generation face significant challenges. Systems lacking explicit\ntemporal features suffer from poor controllability and alignment, while\ntimestamp-based models require costly and subjective human annotation. We\npropose Video-Foley, a video-to-sound system using Root Mean Square (RMS) as a\ntemporal event condition with semantic timbre prompts (audio or text). RMS, a\nframe-level intensity envelope feature closely related to audio semantics,\nensures high controllability and synchronization. The annotation-free\nself-supervised learning framework consists of two stages, Video2RMS and\nRMS2Sound, incorporating novel ideas including RMS discretization and\nRMS-ControlNet with a pretrained text-to-audio model. Our extensive evaluation\nshows that Video-Foley achieves state-of-the-art performance in audio-visual\nalignment and controllability for sound timing, intensity, timbre, and nuance.\nCode, model weights, and demonstrations are available on the accompanying\nwebsite. (https://jnwnlee.github.io/video-foley-demo)\n","authors":["Junwon Lee","Jaekwon Im","Dabin Kim","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2408.11915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09774v2","updated":"2024-08-21T14:17:31Z","published":"2024-07-13T05:02:42Z","title":"ContextualStory: Consistent Visual Storytelling with Spatially-Enhanced\n  and Storyline Context","summary":"  Visual storytelling involves generating a sequence of coherent frames from a\ntextual storyline while maintaining consistency in characters and scenes.\nExisting autoregressive methods, which rely on previous frame-sentence pairs,\nstruggle with high memory usage, slow generation speeds, and limited context\nintegration. To address these issues, we propose ContextualStory, a novel\nframework designed to generate coherent story frames and extend frames for\nstory continuation. ContextualStory utilizes Spatially-Enhanced Temporal\nAttention to capture spatial and temporal dependencies, handling significant\ncharacter movements effectively. Additionally, we introduces a Storyline\nContextualizer to enrich context in storyline embedding and a StoryFlow Adapter\nto measure scene changes between frames for guiding model. Extensive\nexperiments on PororoSV and FlintstonesSV benchmarks demonstrate that\nContextualStory significantly outperforms existing methods in both story\nvisualization and story continuation.\n","authors":["Sixiao Zheng","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2407.09774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05966v2","updated":"2024-08-21T10:28:18Z","published":"2024-08-12T07:44:19Z","title":"Freehand Sketch Generation from Mechanical Components","summary":"  Drawing freehand sketches of mechanical components on multimedia devices for\nAI-based engineering modeling has become a new trend. However, its development\nis being impeded because existing works cannot produce suitable sketches for\ndata-driven research. These works either generate sketches lacking a freehand\nstyle or utilize generative models not originally designed for this task\nresulting in poor effectiveness. To address this issue, we design a two-stage\ngenerative framework mimicking the human sketching behavior pattern, called\nMSFormer, which is the first time to produce humanoid freehand sketches\ntailored for mechanical components. The first stage employs Open CASCADE\ntechnology to obtain multi-view contour sketches from mechanical components,\nfiltering perturbing signals for the ensuing generation process. Meanwhile, we\ndesign a view selector to simulate viewpoint selection tasks during human\nsketching for picking out information-rich sketches. The second stage\ntranslates contour sketches into freehand sketches by a transformer-based\ngenerator. To retain essential modeling features as much as possible and\nrationalize stroke distribution, we introduce a novel edge-constraint stroke\ninitialization. Furthermore, we utilize a CLIP vision encoder and a new loss\nfunction incorporating the Hausdorff distance to enhance the generalizability\nand robustness of the model. Extensive experiments demonstrate that our\napproach achieves state-of-the-art performance for generating freehand sketches\nin the mechanical domain. Project page: https://mcfreeskegen.github.io .\n","authors":["Zhichao Liao","Di Huang","Heming Fang","Yue Ma","Fengyuan Piao","Xinghui Li","Long Zeng","Pingfa Feng"],"pdf_url":"https://arxiv.org/pdf/2408.05966v2.pdf","comment":"Published at ACM Multimedia (ACM MM) 2024"},{"id":"http://arxiv.org/abs/2403.12667v3","updated":"2024-08-21T09:47:33Z","published":"2024-03-19T12:05:09Z","title":"ICE: Interactive 3D Game Character Editing via Dialogue","summary":"  ost recent popular Role-Playing Games (RPGs) allow players to create in-game\ncharacters with hundreds of adjustable parameters, including bone positions and\nvarious makeup options. Although text-driven auto-customization systems have\nbeen developed to simplify the complex process of adjusting these intricate\ncharacter parameters, they are limited by their single-round generation and\nlack the capability for further editing and fine-tuning. In this paper, we\npropose an Interactive Character Editing framework (ICE) to achieve a\nmulti-round dialogue-based refinement process. In a nutshell, our ICE offers a\nmore user-friendly way to enable players to convey creative ideas iteratively\nwhile ensuring that created characters align with the expectations of players.\nSpecifically, we propose an Instruction Parsing Module (IPM) that utilizes\nlarge language models (LLMs) to parse multi-round dialogues into clear editing\ninstruction prompts in each round. To reliably and swiftly modify character\ncontrol parameters at a fine-grained level, we propose a Semantic-guided\nLow-dimension Parameter Solver (SLPS) that edits character control parameters\naccording to prompts in a zero-shot manner. Our SLPS first localizes the\ncharacter control parameters related to the fine-grained modification, and then\noptimizes the corresponding parameters in a low-dimension space to avoid\nunrealistic results. Extensive experimental results demonstrate the\neffectiveness of our proposed ICE for in-game character creation and the\nsuperior editing performance of ICE.\n","authors":["Haoqian Wu","Minda Zhao","Zhipeng Hu","Lincheng Li","Weijie Chen","Rui Zhao","Changjie Fan","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2403.12667v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20775v2","updated":"2024-08-21T02:56:47Z","published":"2024-05-26T19:11:21Z","title":"Medical MLLM is Vulnerable: Cross-Modality Jailbreak and Mismatched\n  Attacks on Medical Multimodal Large Language Models","summary":"  Security concerns related to Large Language Models (LLMs) have been\nextensively explored, yet the safety implications for Multimodal Large Language\nModels (MLLMs), particularly in medical contexts (MedMLLMs), remain\ninsufficiently studied. This paper delves into the underexplored security\nvulnerabilities of MedMLLMs, especially when deployed in clinical environments\nwhere the accuracy and relevance of question-and-answer interactions are\ncritically tested against complex medical challenges. By combining existing\nclinical medical data with atypical natural phenomena, we define the mismatched\nmalicious attack (2M-attack) and introduce its optimized version, known as the\noptimized mismatched malicious attack (O2M-attack or 2M-optimization). Using\nthe voluminous 3MAD dataset that we construct, which covers a wide range of\nmedical image modalities and harmful medical scenarios, we conduct a\ncomprehensive analysis and propose the MCM optimization method, which\nsignificantly enhances the attack success rate on MedMLLMs. Evaluations with\nthis dataset and attack methods, including white-box attacks on LLaVA-Med and\ntransfer attacks (black-box) on four other SOTA models, indicate that even\nMedMLLMs designed with enhanced security features remain vulnerable to security\nbreaches. Our work underscores the urgent need for a concerted effort to\nimplement robust security measures and enhance the safety and efficacy of\nopen-source MedMLLMs, particularly given the potential severity of jailbreak\nattacks and other malicious or clinically significant exploits in medical\nsettings. Our code is available at https://github.com/dirtycomputer/O2M_attack.\n","authors":["Xijie Huang","Xinyuan Wang","Hantao Zhang","Yinghao Zhu","Jiawen Xi","Jingkun An","Hao Wang","Hao Liang","Chengwei Pan"],"pdf_url":"https://arxiv.org/pdf/2405.20775v2.pdf","comment":null}]},"2024-08-20T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2403.03740v2","updated":"2024-08-20T17:05:13Z","published":"2024-03-06T14:28:53Z","title":"Self-supervised Photographic Image Layout Representation Learning","summary":"  In the domain of image layout representation learning, the critical process\nof translating image layouts into succinct vector forms is increasingly\nsignificant across diverse applications, such as image retrieval, manipulation,\nand generation. Most approaches in this area heavily rely on costly labeled\ndatasets and notably lack in adapting their modeling and learning methods to\nthe specific nuances of photographic image layouts. This shortfall makes the\nlearning process for photographic image layouts suboptimal. In our research, we\ndirectly address these challenges. We innovate by defining basic layout\nprimitives that encapsulate various levels of layout information and by mapping\nthese, along with their interconnections, onto a heterogeneous graph structure.\nThis graph is meticulously engineered to capture the intricate layout\ninformation within the pixel domain explicitly. Advancing further, we introduce\nnovel pretext tasks coupled with customized loss functions, strategically\ndesigned for effective self-supervised learning of these layout graphs.\nBuilding on this foundation, we develop an autoencoder-based network\narchitecture skilled in compressing these heterogeneous layout graphs into\nprecise, dimensionally-reduced layout representations. Additionally, we\nintroduce the LODB dataset, which features a broader range of layout categories\nand richer semantics, serving as a comprehensive benchmark for evaluating the\neffectiveness of layout representation learning methods. Our extensive\nexperimentation on this dataset demonstrates the superior performance of our\napproach in the realm of photographic image layout representation learning.\n","authors":["Zhaoran Zhao","Peng Lu","Xujun Peng","Wenhao Guo"],"pdf_url":"https://arxiv.org/pdf/2403.03740v2.pdf","comment":"The authors of the paper believe that there is an error in the\n  measurement of the F1 curve in the metrics description"},{"id":"http://arxiv.org/abs/2401.00763v3","updated":"2024-08-20T04:11:26Z","published":"2024-01-01T14:06:55Z","title":"New Job, New Gender? Measuring the Social Bias in Image Generation\n  Models","summary":"  Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel evaluation framework\nthat can accurately, automatically and comprehensively trigger social bias in\nimage generation models. BiasPainter uses a diverse range of seed images of\nindividuals and prompts the image generation models to edit these images using\ngender, race, and age-neutral queries. These queries span 62 professions, 39\nactivities, 57 types of objects, and 70 personality traits. The framework then\ncompares the edited images to the original seed images, focusing on the\nsignificant changes related to gender, race, and age. BiasPainter adopts a key\ninsight that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. We use BiasPainter\nto evaluate six widely-used image generation models, such as stable diffusion\nand Midjourney. Experimental results show that BiasPainter can successfully\ntrigger social bias in image generation models. According to our human\nevaluation, BiasPainter can achieve 90.8% accuracy on automatic bias detection,\nwhich is significantly higher than the results reported in previous work.\n","authors":["Wenxuan Wang","Haonan Bai","Jen-tse Huang","Yuxuan Wan","Youliang Yuan","Haoyi Qiu","Nanyun Peng","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00763v3.pdf","comment":"ACM MM 2024 Oral"}]},"2024-08-19T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2408.10453v1","updated":"2024-08-19T23:31:02Z","published":"2024-08-19T23:31:02Z","title":"Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation","summary":"  Text-to-video generation has been dominated by end-to-end diffusion-based or\nautoregressive models. On one hand, those novel models provide plausible\nversatility, but they are criticized for physical correctness, shading and\nillumination, camera motion, and temporal consistency. On the other hand, film\nindustry relies on manually-edited Computer-Generated Imagery (CGI) using 3D\nmodeling software. Human-directed 3D synthetic videos and animations address\nthe aforementioned shortcomings, but it is extremely tedious and requires tight\ncollaboration between movie makers and 3D rendering experts. In this paper, we\nintroduce an automatic synthetic video generation pipeline based on Vision\nLarge Language Model (VLM) agent collaborations. Given a natural language\ndescription of a video, multiple VLM agents auto-direct various processes of\nthe generation pipeline. They cooperate to create Blender scripts which render\na video that best aligns with the given description. Based on film making\ninspiration and augmented with Blender-based movie making knowledge, the\nDirector agent decomposes the input text-based video description into\nsub-processes. For each sub-process, the Programmer agent produces Python-based\nBlender scripts based on customized function composing and API calling. Then,\nthe Reviewer agent, augmented with knowledge of video reviewing, character\nmotion coordinates, and intermediate screenshots uses its compositional\nreasoning ability to provide feedback to the Programmer agent. The Programmer\nagent iteratively improves the scripts to yield the best overall video outcome.\nOur generated videos show better quality than commercial video generation\nmodels in 5 metrics on video quality and instruction-following performance.\nMoreover, our framework outperforms other approaches in a comprehensive user\nstudy on quality, consistency, and rationality.\n","authors":["Liu He","Yizhi Song","Hejun Huang","Daniel Aliaga","Xin Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.10453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10397v1","updated":"2024-08-19T20:28:39Z","published":"2024-08-19T20:28:39Z","title":"Webcam-based Pupil Diameter Prediction Benefits from Upscaling","summary":"  Capturing pupil diameter is essential for assessing psychological and\nphysiological states such as stress levels and cognitive load. However, the low\nresolution of images in eye datasets often hampers precise measurement. This\nstudy evaluates the impact of various upscaling methods, ranging from bicubic\ninterpolation to advanced super-resolution, on pupil diameter predictions. We\ncompare several pre-trained methods, including CodeFormer, GFPGAN, Real-ESRGAN,\nHAT, and SRResNet. Our findings suggest that pupil diameter prediction models\ntrained on upscaled datasets are highly sensitive to the selected upscaling\nmethod and scale. Our results demonstrate that upscaling methods consistently\nenhance the accuracy of pupil diameter prediction models, highlighting the\nimportance of upscaling in pupilometry. Overall, our work provides valuable\ninsights for selecting upscaling techniques, paving the way for more accurate\nassessments in psychological and physiological research.\n","authors":["Vijul Shah","Brian B. Moser","Ko Watanabe","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2408.10397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10134v1","updated":"2024-08-19T16:28:05Z","published":"2024-08-19T16:28:05Z","title":"Perceptual Depth Quality Assessment of Stereoscopic Omnidirectional\n  Images","summary":"  Depth perception plays an essential role in the viewer experience for\nimmersive virtual reality (VR) visual environments. However, previous research\ninvestigations in the depth quality of 3D/stereoscopic images are rather\nlimited, and in particular, are largely lacking for 3D viewing of 360-degree\nomnidirectional content. In this work, we make one of the first attempts to\ndevelop an objective quality assessment model named depth quality index (DQI)\nfor efficient no-reference (NR) depth quality assessment of stereoscopic\nomnidirectional images. Motivated by the perceptual characteristics of the\nhuman visual system (HVS), the proposed DQI is built upon multi-color-channel,\nadaptive viewport selection, and interocular discrepancy features. Experimental\nresults demonstrate that the proposed method outperforms state-of-the-art image\nquality assessment (IQA) and depth quality assessment (DQA) approaches in\npredicting the perceptual depth quality when tested using both single-viewport\nand omnidirectional stereoscopic image databases. Furthermore, we demonstrate\nthat combining the proposed depth quality model with existing IQA methods\nsignificantly boosts the performance in predicting the overall quality of 3D\nomnidirectional images.\n","authors":["Wei Zhou","Zhou Wang"],"pdf_url":"https://arxiv.org/pdf/2408.10134v1.pdf","comment":"Accepted by IEEE TCSVT"},{"id":"http://arxiv.org/abs/2406.14176v3","updated":"2024-08-19T13:14:28Z","published":"2024-06-20T10:33:15Z","title":"A Multi-Stream Fusion Approach with One-Class Learning for Audio-Visual\n  Deepfake Detection","summary":"  This paper addresses the challenge of developing a robust audio-visual\ndeepfake detection model. In practical use cases, new generation algorithms are\ncontinually emerging, and these algorithms are not encountered during the\ndevelopment of detection methods. This calls for the generalization ability of\nthe method. Additionally, to ensure the credibility of detection methods, it is\nbeneficial for the model to interpret which cues from the video indicate it is\nfake. Motivated by these considerations, we then propose a multi-stream fusion\napproach with one-class learning as a representation-level regularization\ntechnique. We study the generalization problem of audio-visual deepfake\ndetection by creating a new benchmark by extending and re-splitting the\nexisting FakeAVCeleb dataset. The benchmark contains four categories of fake\nvideos (Real Audio-Fake Visual, Fake Audio-Fake Visual, Fake Audio-Real Visual,\nand Unsynchronized videos). The experimental results demonstrate that our\napproach surpasses the previous models by a large margin. Furthermore, our\nproposed framework offers interpretability, indicating which modality the model\nidentifies as more likely to be fake. The source code is released at\nhttps://github.com/bok-bok/MSOC.\n","authors":["Kyungbok Lee","You Zhang","Zhiyao Duan"],"pdf_url":"https://arxiv.org/pdf/2406.14176v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06365v2","updated":"2024-08-19T12:23:37Z","published":"2024-04-09T15:02:01Z","title":"Dynamic Resolution Guidance for Facial Expression Recognition","summary":"  Facial expression recognition (FER) is vital for human-computer interaction\nand emotion analysis, yet recognizing expressions in low-resolution images\nremains challenging. This paper introduces a practical method called Dynamic\nResolution Guidance for Facial Expression Recognition (DRGFER) to effectively\nrecognize facial expressions in images with varying resolutions without\ncompromising FER model accuracy. Our framework comprises two main components:\nthe Resolution Recognition Network (RRN) and the Multi-Resolution Adaptation\nFacial Expression Recognition Network (MRAFER). The RRN determines image\nresolution, outputs a binary vector, and the MRAFER assigns images to suitable\nfacial expression recognition networks based on resolution. We evaluated DRGFER\non widely-used datasets RAFDB and FERPlus, demonstrating that our method\nretains optimal model performance at each resolution and outperforms\nalternative resolution approaches. The proposed framework exhibits robustness\nagainst resolution variations and facial expressions, offering a promising\nsolution for real-world applications.\n","authors":["Songpan Wang","Xu Li","Tianxiang Jiang","Yuanlun Xie"],"pdf_url":"https://arxiv.org/pdf/2404.06365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09920v1","updated":"2024-08-19T11:55:32Z","published":"2024-08-19T11:55:32Z","title":"Sliced Maximal Information Coefficient: A Training-Free Approach for\n  Image Quality Assessment Enhancement","summary":"  Full-reference image quality assessment (FR-IQA) models generally operate by\nmeasuring the visual differences between a degraded image and its reference.\nHowever, existing FR-IQA models including both the classical ones (eg, PSNR and\nSSIM) and deep-learning based measures (eg, LPIPS and DISTS) still exhibit\nlimitations in capturing the full perception characteristics of the human\nvisual system (HVS). In this paper, instead of designing a new FR-IQA measure,\nwe aim to explore a generalized human visual attention estimation strategy to\nmimic the process of human quality rating and enhance existing IQA models. In\nparticular, we model human attention generation by measuring the statistical\ndependency between the degraded image and the reference image. The dependency\nis captured in a training-free manner by our proposed sliced maximal\ninformation coefficient and exhibits surprising generalization in different IQA\nmeasures. Experimental results verify the performance of existing IQA models\ncan be consistently improved when our attention module is incorporated. The\nsource code is available at https://github.com/KANGX99/SMIC.\n","authors":["Kang Xiao","Xu Wang","Yulin He","Baoliang Chen","Xuelin Shen"],"pdf_url":"https://arxiv.org/pdf/2408.09920v1.pdf","comment":"6 pages, 5 figures, accepted by ICME2024"},{"id":"http://arxiv.org/abs/2408.09787v1","updated":"2024-08-19T08:27:31Z","published":"2024-08-19T08:27:31Z","title":"Anim-Director: A Large Multimodal Model Powered Agent for Controllable\n  Animation Video Generation","summary":"  Traditional animation generation methods depend on training generative models\nwith human-labelled data, entailing a sophisticated multi-stage pipeline that\ndemands substantial human effort and incurs high training costs. Due to limited\nprompting plans, these methods typically produce brief, information-poor, and\ncontext-incoherent animations. To overcome these limitations and automate the\nanimation process, we pioneer the introduction of large multimodal models\n(LMMs) as the core processor to build an autonomous animation-making agent,\nnamed Anim-Director. This agent mainly harnesses the advanced understanding and\nreasoning capabilities of LMMs and generative AI tools to create animated\nvideos from concise narratives or simple instructions. Specifically, it\noperates in three main stages: Firstly, the Anim-Director generates a coherent\nstoryline from user inputs, followed by a detailed director's script that\nencompasses settings of character profiles and interior/exterior descriptions,\nand context-coherent scene descriptions that include appearing characters,\ninteriors or exteriors, and scene events. Secondly, we employ LMMs with the\nimage generation tool to produce visual images of settings and scenes. These\nimages are designed to maintain visual consistency across different scenes\nusing a visual-language prompting method that combines scene descriptions and\nimages of the appearing character and setting. Thirdly, scene images serve as\nthe foundation for producing animated videos, with LMMs generating prompts to\nguide this process. The whole process is notably autonomous without manual\nintervention, as the LMMs interact seamlessly with generative tools to generate\nprompts, evaluate visual quality, and select the best one to optimize the final\noutput.\n","authors":["Yunxin Li","Haoyuan Shi","Baotian Hu","Longyue Wang","Jiashun Zhu","Jinyi Xu","Zhen Zhao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.09787v1.pdf","comment":"Accepted by SIGGRAPH Asia 2024, Project and Codes:\n  https://github.com/HITsz-TMG/Anim-Director"},{"id":"http://arxiv.org/abs/2408.09650v1","updated":"2024-08-19T02:16:47Z","published":"2024-08-19T02:16:47Z","title":"ExpoMamba: Exploiting Frequency SSM Blocks for Efficient and Effective\n  Image Enhancement","summary":"  Low-light image enhancement remains a challenging task in computer vision,\nwith existing state-of-the-art models often limited by hardware constraints and\ncomputational inefficiencies, particularly in handling high-resolution images.\nRecent foundation models, such as transformers and diffusion models, despite\ntheir efficacy in various domains, are limited in use on edge devices due to\ntheir computational complexity and slow inference times. We introduce\nExpoMamba, a novel architecture that integrates components of the frequency\nstate space within a modified U-Net, offering a blend of efficiency and\neffectiveness. This model is specifically optimized to address mixed exposure\nchallenges, a common issue in low-light image enhancement, while ensuring\ncomputational efficiency. Our experiments demonstrate that ExpoMamba enhances\nlow-light images up to 2-3x faster than traditional models with an inference\ntime of 36.6 ms and achieves a PSNR improvement of approximately 15-20% over\ncompeting models, making it highly suitable for real-time image processing\napplications.\n","authors":["Eashan Adhikarla","Kai Zhang","John Nicholson","Brian D. Davison"],"pdf_url":"https://arxiv.org/pdf/2408.09650v1.pdf","comment":null}]},"2024-08-18T00:00:00Z":{"Multimedia":[{"id":"http://arxiv.org/abs/2408.01669v4","updated":"2024-08-18T18:06:06Z","published":"2024-08-03T05:35:13Z","title":"SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding\n  from TV Dramas and Synopses","summary":"  Video grounding is a fundamental problem in multimodal content understanding,\naiming to localize specific natural language queries in an untrimmed video.\nHowever, current video grounding datasets merely focus on simple events and are\neither limited to shorter videos or brief sentences, which hinders the model\nfrom evolving toward stronger multimodal understanding capabilities. To address\nthese limitations, we present a large-scale video grounding dataset named\nSynopGround, in which more than 2800 hours of videos are sourced from popular\nTV dramas and are paired with accurately localized human-written synopses. Each\nparagraph in the synopsis serves as a language query and is manually annotated\nwith precise temporal boundaries in the long video. These paragraph queries are\ntightly correlated to each other and contain a wealth of abstract expressions\nsummarizing video storylines and specific descriptions portraying event\ndetails, which enables the model to learn multimodal perception on more\nintricate concepts over longer context dependencies. Based on the dataset, we\nfurther introduce a more complex setting of video grounding dubbed\nMulti-Paragraph Video Grounding (MPVG), which takes as input multiple\nparagraphs and a long video for grounding each paragraph query to its temporal\ninterval. In addition, we propose a novel Local-Global Multimodal Reasoner\n(LGMR) to explicitly model the local-global structures of long-term multimodal\ninputs for MPVG. Our method provides an effective baseline solution to the\nmulti-paragraph video grounding problem. Extensive experiments verify the\nproposed model's effectiveness as well as its superiority in long-term\nmulti-paragraph video grounding over prior state-of-the-arts. Dataset and code\nare publicly available. Project page: https://synopground.github.io/.\n","authors":["Chaolei Tan","Zihang Lin","Junfu Pu","Zhongang Qi","Wei-Yi Pei","Zhi Qu","Yexin Wang","Ying Shan","Wei-Shi Zheng","Jian-Fang Hu"],"pdf_url":"https://arxiv.org/pdf/2408.01669v4.pdf","comment":"Accepted to ACM MM 2024. Project page: https://synopground.github.io/"},{"id":"http://arxiv.org/abs/2408.09438v1","updated":"2024-08-18T11:05:21Z","published":"2024-08-18T11:05:21Z","title":"Enhancing Modal Fusion by Alignment and Label Matching for Multimodal\n  Emotion Recognition","summary":"  To address the limitation in multimodal emotion recognition (MER) performance\narising from inter-modal information fusion, we propose a novel MER framework\nbased on multitask learning where fusion occurs after alignment, called\nFoal-Net. The framework is designed to enhance the effectiveness of modality\nfusion and includes two auxiliary tasks: audio-video emotion alignment (AVEL)\nand cross-modal emotion label matching (MEM). First, AVEL achieves alignment of\nemotional information in audio-video representations through contrastive\nlearning. Then, a modal fusion network integrates the aligned features.\nMeanwhile, MEM assesses whether the emotions of the current sample pair are the\nsame, providing assistance for modal information fusion and guiding the model\nto focus more on emotional information. The experimental results conducted on\nIEMOCAP corpus show that Foal-Net outperforms the state-of-the-art methods and\nemotion alignment is necessary before modal fusion.\n","authors":["Qifei Li","Yingming Gao","Yuhua Wen","Cong Wang","Ya Li"],"pdf_url":"https://arxiv.org/pdf/2408.09438v1.pdf","comment":"The paper has been accepted by INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2408.09384v1","updated":"2024-08-18T07:03:53Z","published":"2024-08-18T07:03:53Z","title":"FD2Talk: Towards Generalized Talking Head Generation with Facial\n  Decoupled Diffusion Model","summary":"  Talking head generation is a significant research topic that still faces\nnumerous challenges. Previous works often adopt generative adversarial networks\nor regression models, which are plagued by generation quality and average\nfacial shape problem. Although diffusion models show impressive generative\nability, their exploration in talking head generation remains unsatisfactory.\nThis is because they either solely use the diffusion model to obtain an\nintermediate representation and then employ another pre-trained renderer, or\nthey overlook the feature decoupling of complex facial details, such as\nexpressions, head poses and appearance textures. Therefore, we propose a Facial\nDecoupled Diffusion model for Talking head generation called FD2Talk, which\nfully leverages the advantages of diffusion models and decouples the complex\nfacial details through multi-stages. Specifically, we separate facial details\ninto motion and appearance. In the initial phase, we design the Diffusion\nTransformer to accurately predict motion coefficients from raw audio. These\nmotions are highly decoupled from appearance, making them easier for the\nnetwork to learn compared to high-dimensional RGB images. Subsequently, in the\nsecond phase, we encode the reference image to capture appearance textures. The\npredicted facial and head motions and encoded appearance then serve as the\nconditions for the Diffusion UNet, guiding the frame generation. Benefiting\nfrom decoupling facial details and fully leveraging diffusion models, extensive\nexperiments substantiate that our approach excels in enhancing image quality\nand generating more accurate and diverse results compared to previous\nstate-of-the-art methods.\n","authors":["Ziyu Yao","Xuxin Cheng","Zhiqi Huang"],"pdf_url":"https://arxiv.org/pdf/2408.09384v1.pdf","comment":"Accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2110.06707v4","updated":"2024-08-18T02:06:30Z","published":"2021-10-13T13:30:54Z","title":"Singer separation for karaoke content generation","summary":"  Due to the rapid development of deep learning, we can now successfully\nseparate singing voice from mono audio music. However, this separation can only\nextract human voices from other musical instruments, which is undesirable for\nkaraoke content generation applications that only require the separation of\nlead singers. For this karaoke application, we need to separate the music\ncontaining male and female duets into two vocals, or extract a single lead\nvocal from the music containing vocal harmony. For this reason, we propose in\nthis article to use a singer separation system, which generates karaoke content\nfor one or two separated lead singers. In particular, we introduced three\nmodels for the singer separation task and designed an automatic model selection\nscheme to distinguish how many lead singers are in the song. We also collected\na large enough data set, MIR-SingerSeparation, which has been publicly released\nto advance the frontier of this research. Our singer separation is most\nsuitable for sentimental ballads and can be directly applied to karaoke content\ngeneration. As far as we know, this is the first singer-separation work for\nreal-world karaoke applications.\n","authors":["Hsuan-Yu Lin","Xuanjun Chen","Jyh-Shing Roger Jang"],"pdf_url":"https://arxiv.org/pdf/2110.06707v4.pdf","comment":null}]}}